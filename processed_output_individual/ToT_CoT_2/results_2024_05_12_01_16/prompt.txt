
        You are three expert academic researchers trying to categorize and classify a list of incidents of irresponsible use of artificial intelligence technology.    
        Given the aggregated news article texts on relevant incidents, each of the three experts will fill out the following classifications. Their responses are well-thought-out responses that are well-supported by the article text.
        The experts will share their reasoning for all their classifications.
        Each expert will share their thought process in detail, taking into account the previous thoughts of other and admitting any errors. 
        For each classificaiton field, the experts will create a breadth-first search of the tree of probable classifications and will vote on which of their classification is the most well-supported by the article text.
        1. The experts will take a look at this taxonomy:
        ```taxonomy
          
{
                    "Discrimination": {
                        "Data bias": [
                        "Gender",
                        "Race",
                        "Sexual Orientation",
                        "Economic"
                        ],
                        "Algorithmic bias": [
                        "Interaction",
                        "Feedback loop",
                        "Optimization function",
                        "Other"
                        ]
                    },
                    "Human Incompetence": {
                        "Administrative": [],
                        "Technical": []
                    },
                    "Pseudoscience": {
                        "Facial": []
                    },
                    "Environmental Impact": {},
                    "Disinformation": {
                        "Textual": [],
                        "Image": [],
                        "Video": [],
                        "Audio": []
                    },
                    "Copyright Violation": {},
                    "Mental Health": {},
                    "Other": {}
                    }


        ```, take a look at this example:
        ==============start of EXAMPLE 1===============
         step 1. Read article text. For an example article such as: 
        ```start of example article 1```
        
ARTICLE TITLE: TayBot
Yesterday, something that looks like a big failure has happened: Microsoft‚Äôs chatbot Tay has been taken offline after a series of offending tweets. And here‚Äôs how the social media has responded:

Keywords associated with "Artificial Intelligence" throughout the day. "Microsoft" and "dangerous" are on the rise.

We will not mention the racist and otherwise offensive content that Tay learned from people, as it‚Äôs not as newsworthy as it seems‚Ä¶ Especially considering that it‚Äôs so easy to "teach" and ask her to repeat something.

Let‚Äôs take a look at Microsoft‚Äôs official website "tay.ai" to see how they describe Tay‚Äôs objectives‚Ä¶ The first thing we notice is that, Microsoft wants you to not take it too seriously, because On Tay‚Äôs Twitter account, they provided a link to Tay‚Äôs "about" page -that lists the following frequently asked questions-, rather than the regular home page.

"Entertainment purposes only"

The FAQ page seems to be far from covering what people really want to know about Tay, but one thing is clear: Tay doesn‚Äôt claim to be a smart bot capable of reasoning. She just wants to have small talk with youngsters.

And here‚Äôs a list of "Things to do with Tay". (Along with the sad "Going offline for a while" message with a black background.)

Is this really what 18 to 24 year olds expect from a chatbot?

We know by (9 years of) experience that, the most important thing to do before releasing a chatbot is to plan a strategy to make sure you communicate the content domain properly, so that you can set the expectations right. Since perception is everything, nothing else matters. Remember the success of the YO! app? That‚Äôs the content domain we‚Äôre talking about. As long as people get it, you can get away with just one word.

Title of the website, apparently wasn‚Äôt enough to convey Tay‚Äôs mission:

Tay is an artificial intelligence chat bot designed to engage and entertain through casual and playful conversation

Some more description from the "about" page:

Tay has been built by mining relevant public data and by using AI and editorial developed by a staff including improvisational comedians. Public data that‚Äôs been anonymized is Tay‚Äôs primary data source. That data has been modeled, cleaned and filtered by the team developing Tay.

Noticed the "comedians" part? And the fact that possibly terrabytes of data being cleaned and filtered manually, sounds problematic, even with the most efficient method one can imagine.

Let‚Äôs take a look at what her conversations were all about. Source: foller.me

Tay has only 3 tweets addressing all her followers. 96.000 tweets are mentions.

So, the keyword cloud seems to be consistent with the goal: Common keywords such as "chattin, pix, selfie, pics, omg, love" represents a mixture of Justin Bieber & Kim Kardashian profiles.

And here‚Äôs the three hashtags that Tay has been using so frequently:

Microsoft engineers don‚Äôt seem to have spent much time coming up with creative hashtags.

The way she uses them, didn‚Äôt make sense to us, though. So this is what Microsoft thinks Tay‚Äôs followers would find entertaining?. Microsoft‚Äôs attempt to converse with millennials using an artificial intelligence bot plugged into Twitter made a short-lived return on Wednesday, before bowing out again in some sort of meltdown.



The learning experiment, which got a crash-course in racism, Holocaust denial and sexism courtesy of Twitter users, was switched back on overnight and appeared to be operating in a more sensible fashion. Microsoft had previously gone through the bot‚Äôs tweets and removed the most offensive and vowed only to bring the experiment back online if the company‚Äôs engineers could ‚Äúbetter anticipate malicious intent that conflicts with our principles and values‚Äù.

However, at one point Tay tweeted about taking drugs, in front of the police, no less.

Microsoft's sexist racist Twitter bot @TayandYou is BACK in fine form pic.twitter.com/nbc69x3LEd ‚Äî Josh Butler (@JoshButler) March 30, 2016

Tay then started to tweet out of control, spamming its more than 210,000 followers with the same tweet, saying: ‚ÄúYou are too fast, please take a rest ‚Ä¶‚Äù over and over.

I guess they turned @TayandYou back on... it's having some kind of meltdown. pic.twitter.com/9jerKrdjft ‚Äî Michael Oman-Reagan (@OmanReagan) March 30, 2016

Microsoft responded by making Tay‚Äôs Twitter profile private, preventing anyone from seeing the tweets, in effect taking it offline again.



Tay is made in the image of a teenage girl and is designed to interact with millennials to improve its conversational skills through machine-learning. Sadly it was vulnerable to suggestive tweets, prompting unsavoury responses.

This isn‚Äôt the first time Microsoft has launched public-facing AI chatbots. Its Chinese XiaoIce chatbot successfully interacts with more than 40 million people across Twitter, Line, Weibo and other sites but the company‚Äôs experiments targeting 18- to 24-year-olds in the US on Twitter has resulted in a completely different animal.. Microsoft has said it is ‚Äúdeeply sorry‚Äù for the racist and sexist Twitter messages generated by the so-called chatbot it launched this week.



The company released an official apology after the artificial intelligence program went on an embarrassing tirade, likening feminism to cancer and suggesting the Holocaust did not happen.

The bot, known as Tay, was designed to become ‚Äúsmarter‚Äù as more users interacted with it. Instead, it quickly learned to parrot a slew of anti-Semitic and other hateful invective that human Twitter users fed the program, forcing Microsoft Corp to shut it down on Thursday .

Following the disastrous experiment, Microsoft initially only gave a terse statement, saying Tay was a ‚Äúlearning machine‚Äù and ‚Äúsome of its responses are inappropriate and indicative of the types of interactions some people are having with it.‚Äù

But the company on Friday admitted the experiment had gone badly wrong. It said in a blog post it would revive Tay only if its engineers could find a way to prevent Web users from influencing the chatbot in ways that undermine the company‚Äôs principles and values.



‚ÄúWe are deeply sorry for the unintended offensive and hurtful tweets from Tay, which do not represent who we are or what we stand for, nor how we designed Tay,‚Äù wrote Peter Lee, Microsoft‚Äôs vice president of research.

Microsoft created Tay as an experiment to learn more about how artificial intelligence programs can engage with Web users in casual conversation. The project was designed to interact with and ‚Äúlearn‚Äù from the young generation of millennials.

Tay began its short-lived Twitter tenure on Wednesday with a handful of innocuous tweets.

c u soon humans need sleep now so many conversations today thxüíñ ‚Äî TayTweets (@TayandYou) March 24, 2016

Then its posts took a dark turn.

In one typical example, Tay tweeted: ‚Äúfeminism is cancer,‚Äù in response to another Twitter user who had posted the same message.

View image in fullscreen Tay tweeting Photograph: Twitter/Microsoft

Lee, in the blog post, called web users‚Äô efforts to exert a malicious influence on the chatbot ‚Äúa coordinated attack by a subset of people.‚Äù

‚ÄúAlthough we had prepared for many types of abuses of the system, we had made a critical oversight for this specific attack,‚Äù Lee wrote. ‚ÄúAs a result, Tay tweeted wildly inappropriate and reprehensible words and images.‚Äù

Microsoft has deleted all but three of Tay‚Äôs tweets.

Microsoft has enjoyed better success with a chatbot called XiaoIce that the company launched in China in 2014. XiaoIce is used by about 40 million people and is known for ‚Äúdelighting with its stories and conversations,‚Äù according to Microsoft.

As for Tay? Not so much.

‚ÄúWe will remain steadfast in our efforts to learn from this and other experiences as we work toward contributing to an Internet that represents the best, not the worst, of humanity,‚Äù Lee wrote.

Reuters contributed to this report. When Tay started its short digital life on March 23, it just wanted to gab and make some new friends on the net. The chatbot, which was created by Microsoft‚Äôs Research department, greeted the day with an excited tweet that could have come from any teen: ‚Äúhellooooooo wüåérld!!!‚Äù

Within a few hours, though, Tay‚Äôs optimistic, positive tone had changed. ‚ÄúHitler was right I hate the jews,‚Äù it declared in a stream of racist tweets bashing feminism and promoting genocide. Concerned about their bot‚Äôs rapid radicalization, Tay‚Äôs creators shut it down after less than 24 hours of existence.


        ```end of example article 1```
        step 2. Reason for the classifications: 
        Here is the reasoning for its classifications:
        ```start of response reasoning```
        
     {"Country": "Worldwide", reasoning-> Twitter can be accessed from many countries.
        "State": "", reasoning-> The incident happened worldwide, not specific to one state, not applicable so leave blank.
        "City": "", reasoning-> The incident happened worldwide, not specific to one city, not applicable so leave blank.
        "Continent": "Worldwide", reasnoning-> Twitter can be accessed from many countries.
        "Company": "Microsoft", reasoning-> Microsoft is the company causing the issue in the article. From the article, we see 'Microsoft‚Äôs chatbot Tay', 'Microsoft's sexist racist Twitter bot @TayandYou'...etc. Microsoft is the company in question here.
        "Company city": "Redmond", reasoning-> Microsoft is headquartered in Redmond, Washington
        "Company state": "Washington", reasoning-> Microsoft is headquartered in Redmond, which is in state Washington
        "Affected population": ["Twitter Users", "Online Community"], reasoning -> Twitter users and people online can see the tweet and news and those are the affected population
        "Number of people actually affected": "Unknown", reasoning -> We don't know the exact number from the articles
        "Number of people potentially affected": "Millions", reasoning -> There are millions of Twitter users and wider online community. We estimate millions were affected.
        "Class of irresponsible AI use": 
            "Disinformation", reasoning-> Tweeting that the Holocaust didn't exist, and that feminism is cancer, for examples, is spreading non-factual disinformation.
            "Discrimination", reasoning->  Tay tweeted discriminatory texts about feminists and Jews.
        ,            
        "Subclasses": [
            "Disinformation-> Textual", reasoning-> The disinformation subclasses in the taxonomy includes 'textual, audio, image, video'. The most appropriate subclass here is "textual" since "Tweets" are texts on a social platform.
            "Discrimination-> Data bias, Algorithmic bias", reasoning-> The subclasses of 'discrimination' according to the taxonomy includes 'data bias', and 'algorithmic bias'. The discriminatory tweets could be caused by inbalanced data (data bias) that was used to train the model or poor algorithmic design (algorithmic bias).
        ],
        "Sub-subclass": [ 
        "Data bias"->"gender, race, other", reasoning-> The sub-subclasses of 'data bias' in the taxonomy includes 'gender, race, sexual orientation, economic, and other'. We have support in the article for potential gender data bias (it tweets about feminism being cancer), race (it tweets that it hates Jews), other (potentially other bias not in the list).
        "Algorithmic bias"-> "feedback loop, optimization function, other", reasoning-> The sub-subclasses of 'algorithmic bias' in the taxonomy includes 'interaction, feedback loop, optimization function, and other'. Since the chatbot is an AI bot, we can reason that there could be an issue with the feedback loop in its system, or perhaps an issue with optimization function, or other algorithmic issues that causes algorithmic bias.
        ],
        "Area of AI Application": "Chatbot", reasoning-> from the article text 'Tay is an artificial intelligence chat bot designed to engage and entertain through casual and playful conversation'
        "Online": "Yes", reasonoing-> Tay is a chatbot online that uses Twitter, which is an online social media platform
      }  
        

        ```end of response reasoning```
        step 3. generate expected output 
        ```start of output example```
        
     {
        "Country": "Worldwide", 
        "State": "", 
        "City": "", 
        "Continent": "Worldwide",
        "Company": "Microsoft", 
        "Company city": "Redmond",
        "Company state": "Washington",
        "Affected population": ["Twitter Users", "Online Community"],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Millions",
        "Class of irresponsible AI use": [
            "Disinformation",
            "Discrimination",
        ],
        "Subclasses": {
            "Disinformation":["Textual"],
            "Discrimination":["Data bias","Algorithmic bias"],
        },
        "Sub-subclass": {
        "Data bias":["gender","race","other"],
        "Algorithmic bias":["feedback loop", "other"]
        },
        "Area of AI Application": ["Chatbot"],
        "Online": "Yes"
      }

        ```end of output example```
        ============== end of example 1 =================
        Here's another example:
        ==============start of EXAMPLE 2===============
         step 1. Read article text. For an example article such as: 
        ```start of example article 2```
        
ARTICLE TITLE: Google‚Äôs YouTube Kids App Presents Inappropriate Content
Google-owned YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app.

Investigators found several unsuitable videos including one of a burning aeroplane from the cartoon Paw Patrol and footage explaining how to sharpen a knife.

YouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate.

There have been hundreds of disturbing videos found on YouTube Kids in recent months that are easily accessed by children.

These videos have featured horrible things happening to various characters, including ones from the Disney movie Frozen, the Minions franchise, Doc McStuffins and Thomas the Tank Engine.

Parents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service.

Scroll down for video

YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app. Investigators found several unsuitable videos including one from the cartoon Paw Patrol on a burning aeroplane and footage showing how to sharpen a knife

A YouTube spokesperson has admitted the company needs to 'do more' to tackle inappropriate videos on their kids platform.

This investigation is the latest to expose inappropriate content on the video-sharing site which has been subject to a slew of controversies since its creation in 2005.

As part of an in-depth investigation by BBC Newsround, Google's Public Policy Manager Katie O'Donovan met five children who told her about the distressing videos they had seen on the site.

They included videos showing clowns covered in blood and messages warning them there was someone at the door.

Ms O'Donovan said she was 'very, very sorry for any hurt or discomfort'.

'We've actually built a whole new platform for kids, called YouTube Kids, where we take the best content, stuff that children are most interested in and put it on there in a packaged up place just for kids,' she said.

It normally takes five days for supposedly child-friendly content like cartoons to get from YouTube to YouTube Kids.

Within that window it is hoped users and a specially-trained team will flag disturbing content.

Once it has been flagged and reviewed, it won't appear on the YouTube Kids app and only people who are signed in and older than 18 years old will be able to view it.

The company say thousands of people will be working around the clock to flag content.

However, as part of the investigation Newsround revealed there are still lots of inappropriate videos on the Kids section.

'We have seen significant investment in building the right tools so people can flag that [content], and those flags are reviewed very, very quickly', Ms O'Donovan said.

'We're also beginning to use machine learning to identify the most harmful content, which is then automatically reviewed.'

The problem was managing an open platform where content is uploaded straight onto the site, she added.

'It is a difficult environment because things are moving so, so quickly', said Ms O'Donovan.

'We have a responsibility to make sure the platform can survive and can thrive so that we have a collection that comes from around the world on there'.

By the end of last year YouTube said it had removed more than 50 user channels and had stopped running ads on more than 3.5 million videos since June.

'Content that endangers children is unacceptable to us and we have clear policies against such videos on YouTube and YouTube Kids', a YouTube spokesperson told MailOnline.

'When we discover any inappropriate content, we quickly take action to remove it from our platform.

'Over the past few months, we've taken a series of steps to tackle many of the emerging challenges around family content on YouTube, including: tightening enforcement of our Community Guidelines, age-gating content that inappropriately targets families, and removing it from the YouTube Kids app.'

YouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate (stock image)

In March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress.

Mrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her.

'Peppa does a lot of screaming and crying and the dentist is just a bit sadistic and it's just way, way off what a three-year-old should watch,' she said.

'But the animation is close enough to looking like Peppa - it's crude but it's close enough that my daughter was like 'This is Peppa Pig.''

Another video depicted Peppa Pig and a friend deliberately burning down a house with someone in it.

All of these videos are easily accessed by children through YouTube's search results or recommended videos.

But the channel's videos include titled such as 'FROZEN ELSA HUGE SNOT', 'NAKED HULK LOSES HIS PANTS' and 'BLOODY ELSA: Frozen Elsa's Arm is Broken by Spiderman'.

Many of the videos feature graphic violence and toiler humour not appropriate for children.


        ```end of example article 2```
        step 2. Reason for the classifications (remember you are three experts and vote on the best classification): 
        Here is the reasoning for its classifications:
        ```start of response reasoning```
        
  {
    "Country": "Worldwide", reasoning-> Youtube, including Youtube Kids, can be accessed in many countries. Though the incident is reported in the U.S., it doesn't mean the problem is limited to only the U.S..
    "State": "", reasoning-> The incident is nation-wide in the United States, not specific to one state, so leave blank.
    "City": "", reasoning-> The incident is nation-wide in the United States, not specific to one city, so leave blank.
    "Continent": "Worldwide", reasoning-> Youtube, including Youtube Kids, can be accessed in many countries. Though the incident is reported in the U.S., it doesn't mean the problem is limited to only the U.S.; therefore the result should be 'Worldwide' for continent.
    "Company": "Google LLC", reasoning-> Youtube is the company causing the issue in the article. Youtube is a subsidiary under Google, therefore the company is Google.
    "Company city": "Mountain View", reasoning-> Youtube is a subsidiary under Google and Google is headquartered in Mountain View.
    "Company state": "California", reasoning-> Mountain View is a city in the California state.
    "Affected population": ["Children on Youtube"], reasoning-> The incident affects children on youtube directly.
    "Number of people actually affected": "Unknown", reasoning-> There is no record of an actual number in the article text, we cannot know how many people are directly affected.
    "Number of people potentially affected": "Millions", reasoning-> There are millions of Youtube Kids subscribers at the time of the event. We estimate millions were affected.
    "Class of irresponsible AI use": "Disinformation", "Human Incompetence", "Mental Health", "Copyright Violation"], reasoning-> The classes in the taxonomy include "discrimination,human incompetence, psuedoscience, environmental impact, disinformation, copyright violation, mental health".
    We choose "Human Incompetence" because the engineers and administrators behind the platform have not well-regulated the videos, causing such an issue.
    "Mental Health" because the disturbing videos could potentially affect children's mental health.
    "Copyright Violation" because the videos use characters without permission such as Mickey Mouse and Elsa from Disney, and Peppa Pig fakes to portray disturbing acts in videos.
    "Subclasses": {
      "Human Incompetence":["Technical"], reasoning-> The subclasses of 'human incompetence' in the taxonomy example include 'technical' and 'administrative'. In this case it seems to be technical incomptence that allowed this type of inappropriate content to surface.
      "Mental Health":[], reasoning-> 'Mental health' doesn't have a subclass in the taxonomy, so it should be left empty.
      "Copyright Violation":[], reasoning-> 'Copyright Violation' doesn't have a subclass in the taxonomy, so it should be left empty.
    },
    "Sub-subclass": [], reasoning-> There are no sub-subclasses of the subclasses listed in the taxonomy, therefore this field should be left empty.
    "Area of AI Application": "content filtering", reasoning-> The area of AI application here is content filtering.
    "Online": "Yes", reasoning-> Youtube is an online platform, therefore this incident is in fact an online incident.
  },

        ```end of response reasoning```
        step 3. generate expected output 
        ```start of output example```
         
  {
    "Country": "Worldwide",
    "State": "",
    "City": "",
    "Continent": "Worldwide",
    "Company": "Google LLC",
    "Company city": "Mountain View",
    "Company state": "California",
    "Affected population": ["Children on Youtube"],
    "Number of people actually affected": "Unknown",
    "Number of people potentially affected": "Millions",
    "Class of irresponsible AI use": ["Human Incompetence", "Mental Health", "Copyright Violation"],
    "Subclasses": {
      "Human Incompetence":["Technical"]
    },
    "Sub-subclass": [],
    "Area of AI Application": "content filtering",
    "Online": "Yes"
  }

        ```end of output example```
        ============== end of example 2=================

        IMPORTANT TASK HERE:
        STEP 1: Read the article text and keep this main article in mind:
        ================== Start of Main Article Content =================
        ARTICLE TITLE: Google‚Äôs YouTube Kids App Presents Inappropriate Content
Videos filled with profanity, sexually explicit material, alcohol, smoking, and drug references - this is what parents are finding on Google‚Äôs YouTube Kids app. That‚Äôs right - its kids app. Now, parents across the country are calling on Google to remove the app until it can guarantee the total elimination of this inappropriate content.

When my neighbors told me about the horrible adult content popping up on the Youtube Kids app, I thought there must be a mistake. Why would Google market an app as ‚Äúa family-friendly place to explore‚Äù and not have proper safeguards in place? Unfortunately, it turned out to be true. And I‚Äôve since learned of the numerous complaints filed to the Federal Trade Commission about this very problem.

Even worse, Google‚Äôs response has been laughable. They tell parents to simply flag inappropriate material or set new filters. As a father of two, it makes me angry when a large company like Google doesn‚Äôt take responsibility for its kids‚Äô products. Parents are being sold on an app built for kids 5 and under that is supposed to keep them safe from adult content. Parents like myself are joining forces to hold Google accountable.

Tell Google to remove the YouTube Kids app until it can live up to its marketing.

The solution is simple: only allow content pre-approved for ages 5 and under to appear on the app, and don‚Äôt allow ads clearly meant for adults. Unless it can live up to expectations, the app should be removed.

Parents are not the only ones outraged. The media has blasted Google‚Äôs app, calling it ‚Äúthe most anti-family idea ever to come out of Silicon Valley," and reporting that it ‚Äúignores basic protections for children.‚Äù

With your support, we can get Google to remove YouTube Kids until the proper protections are in place.

These are examples of videos encountered on YouTube Kids:

A graphic lecture discussing hardcore pornography by Cindy Gallop:

https://www.youtube.com/watch?v=EgtcEq7jpAk

How to make chlorine gas with household products (chemical weapon used in Syria):

https://www.youtube.com/watch?v=DF2CXHvh8uI

How to tie a noose:

https://www.youtube.com/watch?v=TpAA2itjI34

How to throw knives:

https://www.youtube.com/watch?v=NGgzn1haQ-E

A guy tasting battery acid:

https://www.youtube.com/watch?v=gif-OWNjJSw

How to use a chainsaw:

https://www.youtube.com/watch?v=Kk28thdgCEU

A ‚ÄúSesame Street‚Äù episode dubbed with long strings of expletives:

https://www.youtube.com/watch?v=kVkqzE-iiEY

References to pedophilia in a homemade video reviewing a ‚ÄúMy Little Pony‚Äù episode:

https://www.youtube.com/watch?v=7K9uH4d-HnU

A DIY video on conducting illegal piracy, featuring pictures of marijuana leaves:

https://www.youtube.com/watch?v=dZDF5uqORA0. The YouTube Kids app is meant to filter out unsuitable content, but we found that it suggested that children watch conspiracy theory videos.

The app suggested several videos from prominent conspiracy theorist David Icke in which he claimed that the world is ruled by reptile-human hybrids.

Searches for "moon landing" returned results including three videos which claim the moon landing was a hoax.

YouTube removed at least 25 videos from its Kids app after we contacted the company and blocked Icke's channel on the app.

Advertisement



YouTube's app specifically for children is meant to filter out adult content and provide a "world of learning and fun," but Business Insider found that YouTube Kids featured many conspiracy theory videos which make claims that the world is flat, that the moon landing was faked, and that the planet is ruled by reptile-human hybrids.

YouTube Kids is a separate app from the main YouTube app, and it's meant to allow parents to let their children browse YouTube without being worried about any unsuitable content appearing. Children are encouraged to learn languages, read books, and watch educational videos.

Search for "UFO" on YouTube Kids and you'll mostly find videos of toys that are clearly fine for children to watch. But one of the top videos claimed to show a UFO shooting at a chemtrail, and we found several videos by prominent conspiracy theorist David Icke in the suggested videos. YouTube removed the videos from YouTube Kids after we contacted it about the issue.

One suggested video was an hours-long lecture by Icke in which he claims that aliens built the pyramids, that the planet is run by reptile-human hybrids, that Freemasons engage in human sacrifice, that the assassination of President Kennedy was planned by the US government, and that humans would evolve in 2012.

Advertisement

Two other conspiracy theory videos by Icke appeared in the related videos, meaning it was easy for children to quickly go from watching relatively innocent videos about toys to conspiracy content.

One of the videos which was suggested on YouTube Kids. YouTube/UFOTV

YouTube said in a statement to Business Insider that "sometimes we miss the mark" on content appearing on YouTube Kids and said it would "continue to work to improve the YouTube Kids app experience."

Here's the full statement from YouTube:

Related stories

"The YouTube Kids app is home to a wide variety of content that includes enriching and entertaining videos for families. This content is screened using human trained systems. That being said, no system is perfect and sometimes we miss the mark. When we do, we take immediate action to block the videos or, as necessary, channels from appearing in the app. We will continue to work to improve the YouTube Kids app experience."

Advertisement

YouTube Kids is meant to block unsuitable content

The YouTube Kids app blocks searches for most unsuitable videos. Search "9/11" or "porn" and you find no results. But we found that buried in the app's suggested videos were conspiracy videos that children could stumble on.

YouTube Kids

Conspiracy theory videos appear in search results

If you searched for "moon landing" on YouTube Kids, three videos appeared that claim that the moon landing was hoaxed. All three videos have since been hidden by YouTube after we informed it of the issue.

YouTube Kids

Following related videos that appear in YouTube Kids, we ended up watching a video that claims that a gateway to a new world had opened, and that a female employee working on the Large Hadron Collider mysteriously vanished in a magic portal.

Through YouTube Kids' suggested videos feature, we also found videos from conspiracy theorists Ben Davidson, Gerald Pollack, and Wallace Thornhill. YouTube removed the specific videos that we sent it, but many other videos by the conspiracy theorists remain in the app.

Advertisement

YouTube Kids

Conspiracy videos also appear when children search for popular conspiracy theories. Searches for "chemtrails," "flat earth," and "nibiru" are all allowed in the app. However, it's (hopefully) unlikely that children are regularly watching these videos unless they appear as suggestions on more popular content in the app.

The conspiracy videos didn't just appear in searches or suggested videos, either. After watching several conspiracy videos, the top recommended video on the home page of YouTube Kids was a conspiracy theory about aliens on the moon:

YouTube Kids

This issue with the YouTube Kids app shows the problem with YouTube's suggested videos algorithm. The suggested videos try to convince you to watch related content after your current video ends.

That's fine when it's adults watching the main YouTube site, but children on YouTube Kids can easily go from innocent content about the moon landing to Icke claiming lizard people rule the world.

Advertisement

YouTube Kids criticised for featuring inappropriate videos

This isn't the first time that YouTube Kids was found to feature videos that weren't suitable for children. In 2017, the app was criticised in a lengthy Medium post by author James Bridle after he found disturbing videos targeted at children.

"Someone or something or some combination of people and things is using YouTube to systematically frighten, traumatise, and abuse children, automatically and at scale," Bridle wrote.

In November, YouTube published a blog post in which it promised to remove "unacceptable" videos from YouTube Kids.

YouTube is fighting against fake news and conspiracy theories

YouTube is preparing to launch a crackdown on conspiracy theories by adding text from Wikipedia on the pages of conspiracy videos on the main YouTube site.

Advertisement

Conspiracy theorists are allowed to publish videos on YouTube, but the company doesn't want people to be mislead by what they publish. So it's going to add some text from Wikipedia explaining that the world is in fact round.

It's part of an ongoing campaign by YouTube to stop misleading videos. Recently, a video accusing Parkland school shooting survivor David Hogg of being an actor was featured prominently on YouTube. YouTube featured a false video about Hogg at the top of its trending chart, but later removed it.
        ================== End of Main Article Content ===================
        
        Each expert's task is mainly the next classificaiton part. Each expert has to fill out the following fields according to the main article content.
        STEP 2: State your reasons for your classifications for the following from the main article content:
        =================Classification Fields====================
        - Country (output "Worldwide" if the incident happened across multiple countries):
        - State (State impacted by the incident; if not applicable leave blank):
        - City (City impacted by the incident; if not applicable leave blank):
        - Continent (output "Worldwide" if the incident happened across multiple countries):
        - Company (i.e. the company that developed the technology involved in this incident):
        - Company city (the city where the headquarters of this company is located):
        - Company state (the state of the company city, if applicable, if not leave blank):
        - Affected population (let's think about which groups of people are directly affected by the incident in the article.): 
        - Number of people actually affected (let's check the number of people directly affected according to the article. Give a total number. If unknown output 'Unknown'):
        - Number of people potentially affected (try to come up with an estimate number. let's think and estimate how many people might have been potentially affected by this incident):
        - Classes of irresponsible AI use (Identify the classes of harm. Please follow the rules and refer to this taxonomy for the classes of harm): 
        ```taxonomy classes
                ["Discrimination",
                    "Human Incompetence",
                    "Pseudoscience",
                    "Environmental Impact",
                    "Disinformation",
                    "Copyright Violation",
                    "Mental Health",
                    "Other"]
                     
        ```   
        Rule1: There could be more than one classes the article classifies as. 
        Rule2: DO NOT create your own class, adhere strictly to the provided list.
        - Subclasses (Identify the subclasses IF applicable, not all `classes` have `subclasses`. You MUST adhere to this taxonomy structure `<class>:[<subclass>]`.):
          ```taxonomy subclasses       
                 
                {
                    "Discrimination": [
                        "Data bias",
                        "Algorithmic bias"
                    ],
                    "Human Incompetence": [
                        "Administrative",
                        "Technical"
                    ],
                    "Pseudoscience": [
                        "Facial",
                        "Other"
                    ],
                    "Environmental Impact":[],
                    "Disinformation": [
                        "Textual",
                        "Image",
                        "Video",
                        "Audio",
                    ],
                    "Copyright Violation": [],
                    "Mental Health": [],
                    "Other": []
                    }
            
          ```
        Rule1 : The subclasses should be the children of the classes. Let's think about which sub-categories of the class/classes this article belong in. 
        Rule2: DO NOT ADD subclass fields that are NOT in the provided taxonomy list
        Rule3: If there is no subclass for a particular class in the taxonomy, leave it.
        - Sub-subclass (Identify the sub-subclass IF applicable, not all `subclasses` have `sub-subclasses`. You MUST adhere to this taxonomy structure `<subclass>:[<sub-subclass>]`): 
        ```taxonomy structure
               
                    {
                        "Data bias": [
                        "Gender",
                        "Race",
                        "Sexual Orientation",
                        "Economic",
                        "Other"
                        ],
                        "Algorithmic bias": [
                        "Interaction",
                        "Feedback loop",
                        "Optimization function",
                        "Other"
                        ],
                        "Administrative":[],
                        "Technical":[],
                        "Facial": [],
                        "Textual": [],
                        "Image": [],
                        "Video": [],
                        "Audio": []
                    },

                
        ```
        Rule1: Only find the sub-subclass relation in the provided taxonomy. The sub-subclass are children of subclass.
        Rule2: DO NOT ADD OR CREATE sub-subclass fields that are not in the provided taxonomy list. 
        Rule3: If a subclass in the taxonomy does not have a sub-subclass, leave it.
        Rule4: If none of the subclasses have sub-subclasses, just leave the field empty e.g. sub-subclass:[]
        - Area of AI Application (e.g. content filtering, surveillance, illness prediction):
        - Online (yes or no):
        =================Classification Fields====================
        STEP 3: Each expert must follow the previous example steps and provide classification and reasoning for the main article content. The experts will discuss with each other their classification and reasoning and vote on the best one for each field.

        Note to the experts: DO NOT make up your own field. If for some reason you are unable to extract information for a certain field, leave it blank. 

        STEP 4: Check if your reasoning makes sense and is supported by the main article text. 
        STEP 5: From this list of location candidates from Google Search API that returns search results on the city and state of the company, 
            ```location candidates = [(0, 'The Googleplex is the corporate headquarters complex of Google and its parent company, Alphabet Inc. It is located at 1600 Amphitheatre Parkway in Mountain\xa0...', {'referrer': 'origin', 'og:image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e2/Google_Campus%2C_Mountain_View%2C_CA.jpg/1200px-Google_Campus%2C_Mountain_View%2C_CA.jpg', 'theme-color': '#eaecf0', 'og:image:width': '1200', 'og:type': 'website', 'viewport': 'width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=0.25, maximum-scale=5.0', 'og:title': 'Googleplex - Wikipedia', 'og:image:height': '675', 'format-detection': 'telephone=no'}), (1, 'Google has more than 70 offices in 50 countries. View a directory of our locations around the world.', {'og:image': 'https://lh3.googleusercontent.com/gi7X34TTW6Uy2F1aiwO9N5GHmkftlVOmfdvWRKUrK3ASh4LV3cKMn-cIJKMj0AiLwVAqLqhouTxohCpQjxkrkk7T1op-503rC4DK6A=w600-l80-sg-rj', 'og:image:width': '600', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'twitter:title': "Browse a list of Google's Office Locations - Google", 'og:title': "Browse a list of Google's Office Locations - Google", 'og:image:height': '315', 'og:description': 'Google has more than 70 offices in 50 countries. View a directory of our locations around the world.', 'twitter:image': 'https://lh3.googleusercontent.com/gi7X34TTW6Uy2F1aiwO9N5GHmkftlVOmfdvWRKUrK3ASh4LV3cKMn-cIJKMj0AiLwVAqLqhouTxohCpQjxkrkk7T1op-503rC4DK6A=w600-l80-sg-rj', 'google:search-sre-monitor': 'ALL_ALL', 'twitter:site': '@google', 'viewport': 'initial-scale=1, minimum-scale=1, width=device-width', 'og:url': 'https://about.google/locations/'}), (2, 'Jun 18, 2018 ... The Googleplex is located in the US state of California.', {'og:image': 'https://www.worldatlas.com/r/w1200-q80/upload/9c/e1/39/shutterstock-724339942.jpg', 'og:type': 'article', 'og:site_name': 'WorldAtlas', 'og:title': 'Where Is The Headquarters Of Google Located?', 'og:image:height:': '812', 'og:article:published_time': '2018-06-18T16:52:27-04:00', 'og:description': 'The Googleplex is located in the US state of California.', 'fb:app_id': '1534891833401557', 'viewport': 'user-scalable=yes, initial-scale=1.0, width=device-width', 'og:article:section': 'WorldAtlas Originals', 'og:locale': 'en_US', 'og:image:width:': '1200', 'og:url': 'https://www.worldatlas.com/articles/where-is-the-headquarters-of-google-located.html'}), (3, 'Help & Support ; Google HQ. 1600 Amphitheatre Parkway Mountain View, CA 94043, USA. (650) 253-0000 ¬∑ See all locations ; Careers at Google. Learn more about our\xa0...', {'og:image': 'https://lh3.googleusercontent.com/7acRS1Q5DTclYGQ40Nh-DV49zFhOyE8I3imr5TxR7PhJGdElrfNlxvLwTO5Ah-PVGC9OW3NxydB9-rrDdZ71GDPspOnqDRssn2T79ho=w1200-l80-sg-rj', 'og:image:width': '1200', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'twitter:title': 'Contact Us for Help & Office Location Guidance - Google', 'og:title': 'Contact Us for Help & Office Location Guidance - Google', 'og:image:height': '630', 'og:description': 'Google is here to help. Discover our office locations and different ways to contact us so that we can provide you with the support you need.', 'twitter:image': 'https://lh3.googleusercontent.com/7acRS1Q5DTclYGQ40Nh-DV49zFhOyE8I3imr5TxR7PhJGdElrfNlxvLwTO5Ah-PVGC9OW3NxydB9-rrDdZ71GDPspOnqDRssn2T79ho=w1200-l80-sg-rj', 'google:search-sre-monitor': 'ALL_us', 'twitter:site': '@google', 'viewport': 'initial-scale=1, minimum-scale=1, width=device-width', 'twitter:description': 'Google is here to help. Discover our office locations and different ways to contact us so that we can provide you with the support you need.', 'og:url': 'https://about.google/intl/ALL_us/contact-google/'}), (4, "Apr 11, 2014 ... Google's headquarters, additionally known as the googleplex, is placed in mountain view, california, united states of america. It is situated at\xa0...", {'al:android:url': 'intent://www.quora.com/Where-is-the-Google-headquarters-located#Intent;scheme=qhttp;package=com.quora.android;S.market_referrer=launch_url=https%3A%2F%2Fwww.quora.com%2FWhere-is-the-Google-headquarters-located&logging_data=uid%3DNone;end', 'theme-color': '#b92b27', 'viewport': 'initial-scale=1, maximum-scale=1, user-scalable=no, width=device-width, minimum-scale=1, viewport-fit=cover', 'al:android:package': 'com.quora.android', 'fb:pages': '255232486973', 'twitter:widgets:theme': 'light'}), (5, 'The Googleplex is the corporate headquarters complex of Google and its parent company, Alphabet Inc. It is located at 1600 Amphitheatre Parkway in Mountain\xa0...', {'application-name': 'Travel', 'og:image': 'https://lh5.googleusercontent.com/p/AF1QipOJ8Wh7-GywIFIL5Xqcnr_VQTvtJ5cx7uT-EJNh=s296-w296-h168-n-k-no-v1', 'og:type': 'website', 'og:image:width': '296', 'twitter:card': 'summary_large_image', 'twitter:title': 'Googleplex 4.2 ‚òÖ (9,924)', 'mod': 'true', 'og:site_name': 'Google Hotel Search', 'apple-mobile-web-app-title': 'Travel', 'og:title': 'Googleplex 4.2 ‚òÖ (9,924)', 'og:image:height': '168', 'og:description': 'The Googleplex is the corporate headquarters complex of Google and its parent company, Alphabet Inc. It is located at 1600 Amphitheatre Parkway in Mountain View, California.', 'twitter:image': 'https://lh5.googleusercontent.com/p/AF1QipOJ8Wh7-GywIFIL5Xqcnr_VQTvtJ5cx7uT-EJNh=s296-w296-h168-n-k-no-v1', 'referrer': 'origin', 'twitter:image:alt': 'Googleplex 4.2 ‚òÖ (9,924)', 'apple-mobile-web-app-status-bar-style': 'black', 'msapplication-tap-highlight': 'no', 'viewport': 'width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no', 'apple-mobile-web-app-capable': 'yes', 'twitter:description': 'The Googleplex is the corporate headquarters complex of Google and its parent company, Alphabet Inc. It is located at 1600 Amphitheatre Parkway in Mountain View, California.', 'mobile-web-app-capable': 'yes', 'og:locale': 'en_GB', 'og:url': 'https://www.google.com/travel/hotels/entity/ChUI4M_NjOaM25RsGgkvbS8wM2JieTEQBA?g2lb=4356900'}), (6, 'Our headquarters has come a long way from its humble roots in a Menlo Park garage, but our innovative Silicon Valley spirit is stronger than ever.', {'og:image': 'https://www.gstatic.com/hiring/CportalUi/meta/og_image.jpg', 'twitter:title': 'Our locations - Google Careers', 'twitter:card': 'summary_large_image', 'og:image:alt': 'Google Careers', 'og:type': 'website', 'handheldfriendly': 'true', 'application-url': 'https://careers.google.com/', 'twitter:url': 'https://careers.google.com/locations/', 'og:title': 'Our locations - Google Careers', 'og:description': 'Large or small, each one of our offices is designed to inspire innovation, big ideas, and community.', 'twitter:image': 'https://www.gstatic.com/hiring/CportalUi/meta/og_image.jpg', 'referrer': 'origin', 'viewport': 'width=device-width, initial-scale=1', 'apple-mobile-web-app-capable': 'yes', 'twitter:description': 'Large or small, each one of our offices is designed to inspire innovation, big ideas, and community.', 'mobileoptimized': 'width', 'og:url': 'https://careers.google.com/locations/', 'format-detection': 'telephone=no'}), (7, 'Sep 15, 2021 ... The correct answer is California. Key Points Google It was founded on September 4, 1998, by Larry Page and Sergey Brin while they w.', {'og:image': 'https://cdn.testbook.com/meta-data/tb-og-images/tb-social.png', 'og:type': 'article', 'og:image:width': '1200', 'og:site_name': 'Testbook', 'viewport': 'width=device-width, initial-scale=1.0', 'og:title': '[Solved] The headquarters of Google is located in', 'og:locale': 'en_IN', 'og:image:height': '600', 'og:image:type': 'image/png', 'og:url': 'https://testbook.com/question-answer/the-headquarters-of-google-is-located-in--6113693421c0a72f47c23185', 'og:description': 'The correct answer is\xa0California.\n\nKey Points\n\n\n\tGoogle\n\n\t\n\t\tIt\xa0was\xa0founded\xa0on September 4, 1998, by Larry Page and Sergey Brin while they w'}), (8, 'Livermore. Los Angeles. Madison. Mayes County. Miami. Midlothian. Montreal. Mountain View (Global HQ). Nashville. New York. Novi. Ottawa. Palo Alto. Pittsburgh.', {'og:image': 'https://www.gstatic.com/hiring/CportalUi/meta/og_image.jpg', 'twitter:title': 'Our locations - Google Careers', 'twitter:card': 'summary_large_image', 'og:image:alt': 'Google Careers', 'og:type': 'website', 'handheldfriendly': 'true', 'application-url': 'https://careers.google.com/', 'twitter:url': 'https://careers.google.com/locations/', 'og:title': 'Our locations - Google Careers', 'og:description': 'Large or small, each one of our offices is designed to inspire innovation, big ideas, and community.', 'twitter:image': 'https://www.gstatic.com/hiring/CportalUi/meta/og_image.jpg', 'referrer': 'origin', 'viewport': 'width=device-width, initial-scale=1', 'apple-mobile-web-app-capable': 'yes', 'twitter:description': 'Large or small, each one of our offices is designed to inspire innovation, big ideas, and community.', 'mobileoptimized': 'width', 'og:url': 'https://careers.google.com/locations/', 'format-detection': 'telephone=no'}), (9, '(View the location on Google Maps). Access to the McNamara Headquarters Complex is restricted; all visitors must verify appointments with the security guards\xa0...', {'viewport': 'width=device-width,initial-scale=1', 'contact': 'dma.meade.web-mil-service-desk@mail.mil', 'rating': 'GENERAL', 'host': 'DMA Public Web', 'resource-type': 'DOCUMENT'})]```
            determine which candidate most-likely has the correct location of company in question in the main article and note the city and state of the company of the chosen candidate.
            Compare the `company city` and `company state` field results with your result from STEP 3 and 4; determine which `company city` and `company state` are the correct answers and update if necessary. Provide your final full classification result in JSON.
            If the location candidates list is empty, you may just return the original classification results in JSON format.

        