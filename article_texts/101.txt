Last month, Prime Minister of the Netherlands Mark Rutte—along with his entire cabinet— Last month, Prime Minister of the Netherlands Mark Rutte—along with his entire cabinet— resigned after a year and a half of investigations revealed that since 2013, 26,000 innocent families were wrongly accused of social benefits fraud partially due to a discriminatory algorithm.

Advertisement

Forced to pay back money they didn’t owe, many families were driven to financial ruin, and some were torn apart. Others were left with Forced to pay back money they didn’t owe, many families were driven to financial ruin, and some were torn apart. Others were left with lasting mental health issues; people of color were disproportionately the victims.

After After relentless investigative reporting and a string of parliamentary hearings both preceding and following the mass resignations, the role of algorithms and automated systems in the scandal became clear, revealing how an austere and punitive war on low-level fraud was automated, leaving little room for accountability or basic human compassion. Even more, the automated system discriminated on the basis of nationality, flagging people with dual nationalities as being likely fraudsters.

The childcare benefits scandal (kinderopvangtoeslagaffaire in Dutch) is a cautionary tale of the havoc that black box algorithms can wreak, especially when they are weaponized to target society’s most vulnerable. It’s a problem that is not unique to the Netherlands: the Australian government faced its own “robodebt” scandal when its automated system for flagging benefits fraud The childcare benefits scandal (kinderopvangtoeslagaffaire in Dutch) is a cautionary tale of the havoc that black box algorithms can wreak, especially when they are weaponized to target society’s most vulnerable. It’s a problem that is not unique to the Netherlands: the Australian government faced its own “robodebt” scandal when its automated system for flagging benefits fraud stole nearly $1 billion from hundreds of thousands of innocent people . That case, too, came down to a poorly-designed algorithm without human oversight, an extension of cruel austerity politics with inexpressible collateral damage.

Advertisement

Here’s how the scandal unfolded. Here’s how the scandal unfolded.

Parents generally have to pay for childcare in the Netherlands. However, based on a parent’s income, the Dutch state reimburses a portion of the costs at the end of each month. Parents generally have to pay for childcare in the Netherlands. However, based on a parent’s income, the Dutch state reimburses a portion of the costs at the end of each month.

The fear of people gaming welfare systems is far from new and not particular to the Netherlands, but the rise of xenophobic far-right populism has placed it centerstage in the national political discourse. Anti-immigrant politics have become The fear of people gaming welfare systems is far from new and not particular to the Netherlands, but the rise of xenophobic far-right populism has placed it centerstage in the national political discourse. Anti-immigrant politics have become increasingly normalized, and immigrants are often painted as a threat to the Dutch welfare state.

Following this, a hardline stance regarding benefits fraud has become mostly politically uniform Following this, a hardline stance regarding benefits fraud has become mostly politically uniform (even among many left-wing parties) over the past decade.

“Who pays the bill?” asked Geert Wilders, leader of the anti-immigrant Dutch Party for Freedom (the second largest party in the country), during a “Who pays the bill?” asked Geert Wilders, leader of the anti-immigrant Dutch Party for Freedom (the second largest party in the country), during a speech in 2008. “It’s the people of the Netherlands, the people who work hard, who properly save money and properly pay their taxes. The regular Dutch person does not receive money as a gift. Henk and Ingrid pay for Mohammed and Fatima.”

Advertisement

What followed was essentially a take-no-prisoners war on benefits and welfare fraud. Like many nations, the Netherlands has long automated aspects of its welfare system paired with human oversight and review. From 2013 on (though these techniques could have been used earlier), authorities What followed was essentially a take-no-prisoners war on benefits and welfare fraud. Like many nations, the Netherlands has long automated aspects of its welfare system paired with human oversight and review. From 2013 on (though these techniques could have been used earlier), authorities used algorithms to create risk profiles of residents who were supposedly more likely to commit fraud and then used automated systems with little oversight to scan through benefits applicants and flag likely fraudsters who were then forced to pay money they didn’t owe in reality.

Before the increased use of automated systems, the decision to cut off a family from benefits payments would have to go through extensive review, said Marlies van Eck, an assistant professor at Radboud University who researches automated decision making in government agencies and who previously worked for the national benefits department. Now, such choices have increasingly been left to algorithms, or algorithms themselves have acted as their own form of review. Before the increased use of automated systems, the decision to cut off a family from benefits payments would have to go through extensive review, said Marlies van Eck, an assistant professor at Radboud University who researches automated decision making in government agencies and who previously worked for the national benefits department. Now, such choices have increasingly been left to algorithms, or algorithms themselves have acted as their own form of review.

“Suddenly, with technology in reach, benefits decisions were made in a really unprecedented manner,” she said. “In the past, if you worked for the government with paper files, you couldn’t suddenly decide from one moment to the next to stop paying people benefits.” “Suddenly, with technology in reach, benefits decisions were made in a really unprecedented manner,” she said. “In the past, if you worked for the government with paper files, you couldn’t suddenly decide from one moment to the next to stop paying people benefits.”

Advertisement

After years of denial, After years of denial, an investigation from the Dutch Data Protection Authority found that these algorithms were inherently discriminatory because they took variables such as whether someone had a second nationality into account.

Marc Schuilenburg is a professor in sociology and criminology at Vrije University in Amsterdam and author of the book Marc Schuilenburg is a professor in sociology and criminology at Vrije University in Amsterdam and author of the book Hysteria: Crime, Media, and Politics . Having spent a significant portion of his career studying the use of predictive policing algorithms, he argues that the child benefits scandal has to be seen within the context of this cultural and political shift towards punitive populism.

“The toeslagenaffaire [benefits scandal] is not an isolated problem,” Schuilenburg told Motherboard over Zoom. “It fits into a long tradition in the Netherlands of security policies that are designed to make clear that the days of tolerance are over, and that we are locked into this fight to the death with crimes such as welfare fraud. This fits into this whole notion of populist hysteria which I discuss in my book.” “The toeslagenaffaire [benefits scandal] is not an isolated problem,” Schuilenburg told Motherboard over Zoom. “It fits into a long tradition in the Netherlands of security policies that are designed to make clear that the days of tolerance are over, and that we are locked into this fight to the death with crimes such as welfare fraud. This fits into this whole notion of populist hysteria which I discuss in my book.”

Advertisement

“You see that these policies are spoken of in terms of war, in a hysterical military vocabulary—‘there is a war against fraud,’” he continued. “Through this language and these policies this brand of punishment populism prevails.” “You see that these policies are spoken of in terms of war, in a hysterical military vocabulary—‘there is a war against fraud,’” he continued. “Through this language and these policies this brand of punishment populism prevails.”

For those classified by the automated system as a fraudster, few properly-done follow-up investigations meant that there was little recourse. In some cases, even something as simple as For those classified by the automated system as a fraudster, few properly-done follow-up investigations meant that there was little recourse. In some cases, even something as simple as forgetting a signature landed families with the effectively irremovable label of having committed fraud. Once that label was there, they were forced to retroactively pay the government back for all the childcare benefits they had received, which amounted to thousands of euros for many and in some cases even tens of thousands of euros.

also found that parents accused of fraud were given the label of “intent / gross negligence”, meaning that they weren’t even eligible for a payment scheme to gradually pay off their already false debts. An investigation from Dutch daily newspaper Trouw also found that parents accused of fraud were given the label of “intent / gross negligence”, meaning that they weren’t even eligible for a payment scheme to gradually pay off their already false debts.

Victims were locked out of other benefits as well, such as the housing allowance and healthcare allowance. Many were forced to file for bankruptcy. The results were catastrophic. Victims were locked out of other benefits as well, such as the housing allowance and healthcare allowance. Many were forced to file for bankruptcy. The results were catastrophic.

Advertisement

“I believe in the States you have this saying ‘you’re one paycheck away from being homeless?’” van Eck told Motherboard over the phone. “Yeah, well that’s basically what we saw in this affair.” “I believe in the States you have this saying ‘you’re one paycheck away from being homeless?’” van Eck told Motherboard over the phone. “Yeah, well that’s basically what we saw in this affair.”

“If you miss two or three months of payments, especially for the child care benefits, you may have to quit your job,” she explained. If someone quit their job to care for their children as a result, she said, they’d end up having financial difficulties. “There was this huge snowball effect because everything is connected with each other in the Netherlands. It was horrible.” “If you miss two or three months of payments, especially for the child care benefits, you may have to quit your job,” she explained. If someone quit their job to care for their children as a result, she said, they’d end up having financial difficulties. “There was this huge snowball effect because everything is connected with each other in the Netherlands. It was horrible.”

In one of the more egregious examples of the lack of humanity in the authorities’ approach, a report from Trouw In one of the more egregious examples of the lack of humanity in the authorities’ approach, a report from Trouw revealed that the tax office had baselessly applied the mathematical Pareto principle to their punishments, assuming without evidence that 80 percent of the parents investigated for fraud were guilty and 20 percent were innocent.

The victims of the overzealous tax authorities were disproportionately people of color, highlighting how algorithms can perpetuate The victims of the overzealous tax authorities were disproportionately people of color, highlighting how algorithms can perpetuate discriminatory structures and institutional racism.

Advertisement

According to Nadia Benaissa—a policy advisor at the digital rights group According to Nadia Benaissa—a policy advisor at the digital rights group Bits of Freedom —, the fraud detection systems using variables like nationality can create problematic feedback loops similar to how predictive policing algorithms built on flawed assumptions can create a self-fulfilling prophecy that leads to the over-policing of minority groups.

Crucially, she said, we should place blame on the human individuals behind the creation and use of the algorithm rather than reify the technology as being the main driver. Crucially, she said, we should place blame on the human individuals behind the creation and use of the algorithm rather than reify the technology as being the main driver.

“Systems and algorithms are human-made, and do exactly what they’ve been instructed to do,” she said. “They can act as an easy and sort of cowardly way to take the blame off yourself. What’s important to understand is that often algorithms can use historical data without any proper interpretation of the context surrounding that data. With that pattern, you can only expect social problems like institutional racism to increase. The result is a sort of feedback loop that increases social problems that already exist.” “Systems and algorithms are human-made, and do exactly what they’ve been instructed to do,” she said. “They can act as an easy and sort of cowardly way to take the blame off yourself. What’s important to understand is that often algorithms can use historical data without any proper interpretation of the context surrounding that data. With that pattern, you can only expect social problems like institutional racism to increase. The result is a sort of feedback loop that increases social problems that already exist.”

While some efforts to increase algorithmic transparency have been made recently (such as an While some efforts to increase algorithmic transparency have been made recently (such as an algorithm register from the municipality of Amsterdam), many of the automated systems in use in society remain opaque, said van Eck, even for researchers.

“Transparency is certainly a major issue. As a researcher, it’s difficult because these algorithms remain invisible,” van Eck said. “If I want to know something, I have trouble finding a person who can talk to me about it. And, if they just buy a software system, it could be that nobody actually knows how it works. Transparency is important not just for citizens, but also on the administrative level.” “Transparency is certainly a major issue. As a researcher, it’s difficult because these algorithms remain invisible,” van Eck said. “If I want to know something, I have trouble finding a person who can talk to me about it. And, if they just buy a software system, it could be that nobody actually knows how it works. Transparency is important not just for citizens, but also on the administrative level.”

Beyond transparency, safeguards and accountability are especially important when algorithms are given enormous power over people's livelihoods, but as of now little of that exists in the Netherlands. And, in the meantime, smart algorithms and automated systems Beyond transparency, safeguards and accountability are especially important when algorithms are given enormous power over people's livelihoods, but as of now little of that exists in the Netherlands. And, in the meantime, smart algorithms and automated systems continue to take over a larger and larger share of administrative procedures.

For now, the families wrongly accused of fraud For now, the families wrongly accused of fraud are waiting to be given €30,000 each in compensation, but that won’t be enough to make up for the divorces, broken homes, and the psychological toll that resulted from the affair.

Meanwhile, despite the gravity of scandal, the resignation of Mark Rutte and his cabinet is largely symbolic. Though he resigned, he is still leading the government in the meantime and will be on the ballot in the national elections scheduled for next month. Rutte’s conservative VVD party is expected to win handily, meaning that both he and many of his ministers will likely return to their posts. Meanwhile, despite the gravity of scandal, the resignation of Mark Rutte and his cabinet is largely symbolic. Though he resigned, he is still leading the government in the meantime and will be on the ballot in the national elections scheduled for next month. Rutte’s conservative VVD party is expected to win handily, meaning that both he and many of his ministers will likely return to their posts.. Until recently, it wasn’t possible to say that AI had a hand in forcing a government to resign. But that’s precisely what happened in the Netherlands in January 2021, when the incumbent cabinet resigned over the so-called kinderopvangtoeslagaffaire: the childcare benefits affair.

When a family in the Netherlands sought to claim their government childcare allowance, they needed to file a claim with the Dutch tax authority. Those claims passed through the gauntlet of a self-learning algorithm, initially deployed in 2013. In the tax authority’s workflow, the algorithm would first vet claims for signs of fraud, and humans would scrutinize those claims it flagged as high risk.

In reality, the algorithm developed a pattern of falsely labeling claims as fraudulent, and harried civil servants rubber-stamped the fraud labels. So, for years, the tax authority baselessly ordered thousands of families to pay back their claims, pushing many into onerous debt and destroying lives in the process.

“When there is disparate impact, there needs to be societal discussion around this, whether this is fair. We need to define what ‘fair’ is,” says Yong Suk Lee, a professor of technology, economy, and global affairs at the University of Notre Dame, in the United States. “But that process did not exist.”

Postmortems of the affair showed evidence of bias. Many of the victims had lower incomes, and a disproportionate number had ethnic minority or immigrant backgrounds. The model saw not being a Dutch citizen as a risk factor.

“The performance of the model, of the algorithm, needs to be transparent or published by different groups,” says Lee. That includes things like what the model’s accuracy rate is like, he adds.

The tax authority’s algorithm evaded such scrutiny; it was an opaque black box, with no transparency into its inner workings. For those affected, it could be nigh impossible to tell exactly why they had been flagged. And they lacked any sort of due process or recourse to fall back upon.

“The government had more faith in its flawed algorithm than in its own citizens, and the civil servants working on the files simply divested themselves of moral and legal responsibility by pointing to the algorithm,” says Nathalie Smuha, a technology legal scholar at KU Leuven, in Belgium.

As the dust settles, it’s clear that the affair will do little to halt the spread of AI in governments—60 countries already have national AI initiatives. Private-sector companies no doubt see opportunity in helping the public sector. For all of them, the tale of the Dutch algorithm—deployed in an E.U. country with strong regulations, rule of law, and relatively accountable institutions—serves as a warning.

“If even within these favorable circumstances, such a dangerously erroneous system can be deployed over such a long time frame, one has to worry about what the situation is like in other, less regulated jurisdictions,” says Lewin Schmitt, a predoctoral policy researcher at the Institut Barcelona d’Estudis Internacionals, in Spain.

So, what might stop future wayward AI implementations from causing harm?

In the Netherlands, the same four parties that were in government prior to the resignation have now returned to government. Their solution is to bring all public-facing AI—both in government and in the private sector—under the eye of a regulator in the country’s data authority, which a government minister says would ensure that humans are kept in the loop.

On a larger scale, some policy wonks place their hope in the European Parliament’s AI Act, which puts public-sector AI under tighter scrutiny. In its current form, the AI Act would ban some applications, such as government social-credit systems and law enforcement use of face recognition, outright.

Something like the tax authority’s algorithm would abide, but due to its public-facing role in government functions, the AI Act would have marked it a high-risk system. That means that a broad set of regulations would apply, including a risk-management system, human oversight, and a mandate to remove bias from the data involved.

The tale of the Dutch algorithm—deployed in an E.U. country with strong regulations, rule of law, and relatively accountable institutions—serves as a warning.

“If the AI Act had been put in place five years ago, I think we would have spotted [the tax algorithm] back then,” says Nicolas Moës, an AI policy researcher in Brussels for the Future Society think tank.

Moës believes that the AI Act provides a more concrete scheme for enforcement than its overseas counterparts, such as the one that recently took effect in China—which focuses less on public-sector use and more on reining in private companies’ use of customers’ data—and proposed U.S. regulations that are currently floating in the legislative ether.

“The E.U. AI Act is really kind of policing the entire space, while others are still kind of tackling just one facet of the issue, very softly dealing with just one issue,” says Moës.

Lobbyists and legislators are still busy hammering the AI Act into its final form, but not everyone believes that the act—even if it’s tightened—will go far enough.

“We see that even the [General Data Protection Regulation], which came into force in 2018, is still not properly being implemented,” says Smuha. “The law can only take you so far. To make public-sector AI work, we also need education.”

That, she says, will need to come through properly informing civil servants of an AI implementation’s capabilities, limitations, and societal impacts. In particular, she believes that civil servants must be able to question its output, regardless of whatever temporal or organizational pressures they might face.

“It’s not just about making sure the AI system is ethical, legal, and robust; it’s also about making sure that the public service in which the AI system [operates] is organized in a way that allows for critical reflection,” she says.. A parliamentary report into the child care benefits scandal found several grave shortcomings, including ​​institutional biases and authorities hiding information or misleading the parliament about the facts. Once the full scale of the scandal came to light, Prime Minister Mark Rutte's government resigned, only to regroup 225 days later.

In addition to the penalty announced April 12, the Dutch data protection agency also fined the Dutch tax administration €2.75 million in December 2021 for the “unlawful, discriminatory and therefore improper manner” in which the tax authority processed data on the dual nationality of child care benefit applicants.

“There was a total lack of checks and balances within every organization of making sure people realize what was going on,” said Pieter Omtzigt, an independent member of the Dutch parliament who played a pivotal role in uncovering the scandal and grilling the tax authorities.

“What is really worrying me is that I’m not sure that we’ve taken even vaguely enough preventive measures to strengthen our institutions to handle the next derailment,” he continued.

The new Rutte government has pledged to create a new algorithm regulator under the country’s data protection authority. Dutch Digital Minister Alexandra van Huffelen — who was previously the finance minister in charge of the tax authority — told POLITICO that the data authority’s role will be “to oversee the creation of algorithms and AI, but also how it plays out when it’s there, how it’s treated, make sure that is human-centered, and that it does apply to all the regulations that are in use.” The regulator will scrutinize algorithms in both the public and private sectors.

Van Huffelen stressed the need to make sure humans are always in the loop. “What I find very important is to make sure that decisions, governmental decisions based on AI are also always treated afterwards by a human person,” she said.. On January 15, the Dutch government was forced to resign amidst a scandal around its child-care benefits scheme. Systems that were meant to detect misuse of the benefits scheme, mistakenly labelled over 20,000 parents as fraudsters. More crucially, a disproportionate amount of those labelled as fraudsters had an immigration background.

Amongst the upheaval, little attention was brought to the fact that the tax authority was making use of algorithms to guide its decision-making. In a report by the Dutch Data Protection Authority, it became clear that a ‘self-learning’ algorithm was used to classify the benefit claims. Its role was to learn which claims had the highest risk of being false. The risk-classification model served as a first filter; officials then scrutinized the claims with the highest risk label. As it turns out, certain claims by parents with double citizenship were systematically identified by the algorithm as high-risk, and officials then hastily marked those claims as fraudulent.

It is difficult to identify what led the algorithm to such a biased output, and that is precisely one of the core problems. This blogpost argues that the Dutch scandal should serve as a cautionary lesson for agencies who want to make use of algorithmic enforcement tools and stresses the need for dedicated governance structures within such agencies to prevent missteps.

The problem of fairness in machine learning

The Dutch scandal places us at the centre of the crucial debate around algorithmic fairness (Kleinberg et al., 2018). As this case shows, mistakes in the implementation of machine learning, can distort reality and lead to unfair outcomes. Such mistakes, like the so-called ‘class imbalance’, a common machine learning classification problem (Buda et al., 2018), may have been a factor which led the authority’s algorithm to find a greater distribution of errors in a minority population. The output of the algorithm consequently became biased and unfair.

We argue that this concept can easily be extended beyond this specific case. Algorithmic bias within agencies could adversely affect any stakeholder in any sector. Depending on various aspects such as the nature of the data that it processes (Lum, 2017), certain stakeholders could find themselves adversely labelled, while others are not. The algorithm of a competition agency, for instance, could disproportionally focus on firms in a particular region. Or food and safety controls could be unfairly directed to a specific type of food.

A tempting tool for supervisors

As the EU moves to more risk-based enforcement strategies (Blanc & Faure, 2020), it may be appealing for agencies to use algorithms to assess risk. This is particularly the case in sectors where the caseload would benefit from some form of automated data analysis. Though machine readable data is a prerequisite for such tools, most agencies have an abundance of it. The ECB, for instance, has expressed its interest in machine learning techniques to make supervision more efficient and proactive. It already attempted using algorithms to simplify fit and proper assessments of bank board appointees. Despite the appeal, the Dutch welfare scandal has highlighted the risks of using them in enforcement and the need for a dedicated governance structure.

The need for a dedicated governance structure

Recently, the European Commission focussed its AI concerns on the protection of fundamental rights and values. It also set out Ethics guidelines for trustworthy AI, in which fairness (or the absence of bias) is one of the key requirements of AI systems. Bias is described as ‘an inclination of prejudice towards or against a person, object, or position’. Fairness should therefore ensure that a risk-based model does not affect smaller groups of stakeholders disproportionately.

Beyond the broad strokes of principled commitments, agencies will need to devise meticulous plans to assess the operations of its machine learning algorithms. Removing bias from algorithm is an extremely difficult task (Hao, 2019) and requires true expertise. As the EBA notes, bias prevention and detection is a continuously evolving field of research. Ideally, one should ensure that algorithms are free of bias by design. This can sometimes only be achieved by hypothesising about the algorithm’s potential effects (Corbett-Davies & Goel, 2018). Agencies will therefore need to continuously adjust their algorithms, a task which may require dedicated departments and expertise.

Agencies may be inclined to hide behind the fact that, ultimately, competent humans control the input and output of the algorithm. But it is precisely because humans control the input and output of algorithms, that the mistakes they may make could ultimately transpire in the algorithm. The Dutch algorithm was ultimately trained by tax officials who, consciously or not, may have ‘taught’ the algorithm the bias they already had. Agencies therefore cannot escape the due diligence simply because the algorithm is merely used as a support tool.

Furthermore, the more complex the rules are, the more their enforcement requires flexibility. The rigidity of an algorithm may be useful as a classifier but will not grasp all the nuances of individual situations. There is a real risk of new types of false positives and false negatives. An algorithm may correctly estimate the risk in a very specific scenario, but this should not dictate enforcement policy.

Agencies may be more vulnerable

The ECB recently stated that : “supervisors are still responsible for their work, even if part of it is performed by a computer”. While this seems like a reasonable postulate, it overlooks the fact that the consequences of algorithmic enforcement are not limited to the responsibility of supervisors. It is likely that the more enforcement relies on algorithms, the more exposed it becomes to the risks associated with it.

From an agency design perspective, algorithmic enforcement may significantly reduce the accountability of agencies if management boards do not equip themselves with the necessary expertise. The deference to an algorithm may weaken the agency’s internal accountability, by attributing decisions to a less explainable machine. On the long run this could affect the effectiveness of enforcement.

The Dutch government could find solace in its political resignation, agencies are in a more delicate situation. Those which heavily rely on legitimacy to foster compliance (Tyler, 2003), could be the ones most hardly struck by an algorithmic mistake through undermining of their credibility. Such mistakes could also shatter strong deterrence-based enforcement strategies, which rely on the ability to identify risk accurately.

Legitimacy also comes paired with transparency, agencies must be clear about when they use algorithms and how they use them. Such transparency however comes with a risk. As stakeholders innovate to lower the cost of the regulatory burden, the most resourceful amongst them may exploit the functioning of enforcement algorithms to remain under the radar.

A cautionary tale

The Dutch benefits scandal teaches us a lesson about the risks of algorithmic enforcement. The difference lies in the fact that the tax authority had the luxury of political accountability, allowing it to mitigate long-term consequences of the scandal. An algorithmic scandal in an agency may be more prone to do lasting damage to its legitimacy, and thus, the effectiveness of its enforcement. Agencies should therefore put in place dedicated governance structures to ensure algorithms are free from bias and are used responsibly. A recently leaked version of the Commission’s draft AI regulation, expected to also apply to EU agencies (article 2(2)(d)), may be a good first step to guide them.. It was October 2021, and Imane, a 44-year-old mother of three, was still in pain from the abdominal surgery she had undergone a few weeks earlier. She certainly did not want to be where she was: sitting in a small cubicle in a building near the center of Rotterdam, while two investigators interrogated her. But she had to prove her innocence or risk losing the money she used to pay rent and buy food.

Imane emigrated to the Netherlands from Morocco with her parents when she was a child. She started receiving benefits as an adult, due to health issues, after divorcing her husband. Since then, she has struggled to get by using welfare payments and sporadic cleaning jobs. Imane says she would do anything to leave the welfare system, but chronic back pain and dizziness make it hard to find and keep work.

This story is part of a joint investigation between Lighthouse Reports and WIRED. To read other stories from the series, click here.

In 2019, after her health problems forced her to leave a cleaning job, Imane drew the attention of Rotterdam’s fraud investigators for the first time. She was questioned and lost her benefits for a month. “I could only pay rent,” she says. She recalls the stress of borrowing food from neighbors and asking her 16-year-old son, who was still in school, to take on a job to help pay other bills.

Now, two years later, she was under suspicion again. In the days before that meeting at the Rotterdam social services department, Imane had meticulously prepared documents: her rental contract, copies of her Dutch and Moroccan passports, and months of bank statements. With no printer at home, she had visited the library to print them.

“It took me two years to recover from this. I was destroyed mentally.” Imane, Rotterdam resident

In the cramped office she watched as the investigators thumbed through the stack of paperwork. One of them, a man, spoke loudly, she says, and she felt ashamed as his accusations echoed outside the thin cubicle walls. They told her she had brought the wrong bank statements and pressured her to log in to her account in front of them. After she refused, they suspended her benefits until she sent the correct statements two days later. She was relieved, but also afraid. “The atmosphere at the meetings with the municipality is terrible,” she says. The ordeal, she adds, has taken its toll. “It took me two years to recover from this. I was destroyed mentally.”

Imane, who asked that her real name not be used for fear of repercussions from city officials, isn’t alone. Every year, thousands of people across Rotterdam are investigated by welfare fraud officers, who search for individuals abusing the system. Since 2017, the city has been using a machine learning algorithm, trained on 12,707 previous investigations, to help it determine whether individuals are likely to commit welfare fraud.

The machine learning algorithm generates a risk score for each of Rotterdam’s roughly 30,000 welfare recipients, and city officials consider these results when deciding whom to investigate. Imane’s background and personal history meant the system ranked her as “high risk.” But the process by which she was flagged is part of a project beset by ethical issues and technical challenges. In 2021, the city paused its use of the risk-scoring model after external government-backed auditors found that it wasn’t possible for citizens to tell if they had been flagged by the algorithm and some of the data it used risked producing biased outputs.

In response to an investigation by Lighthouse Reports and WIRED, Rotterdam handed over extensive details about its system. These include its machine learning model, training data, and user operation manuals. The disclosures provide an unprecedented view into the inner workings of a system that has been used to classify and rank tens of thousands of people.. To revisit this article, visit My Profile, then View saved stories.