. The original Petition was accompanied by expert affidavits from Professor Linda Darling-Hammond (Stanford University), Professor Aaron Pallas (Columbia University Teachers College), Professor Audrey Amrein-Beardsley (Arizona State University), Carol Burris, (Long Island principal who has been recently recognized as both the Educator of the Year and the Principal of the Year), and Brad Lindell (Long Island research consultant and school psychologist who has conducted a detailed study of the Respondents’ teacher evaluation system). These individuals are nationally and locally renowned and respected in their respective fields, and are unaffiliated with Petitioner.

The original affidavit of Professor Linda Darling-Hammond of Stanford University, sworn to February 28, 2015, that the assessment being used in Respondents’ Growth Model does not allow measurement of growth for high-achieving and low achieving students: the learning of both high-achieving and low-achieving students is mis-measured because of the fact that the state tests pegged to grade-level standards do not include items that can measure growth for students who are already above grade level in their skills or who fall considerably below. This is not a problem that can be corrected by statistical adjustments.

(Original Darling-Hammond Aff. at ¶22.) Professor Darling-Hammond expressed a clear and specific opinion that the volatility in Petitioner’s ratings between school year 2012/13 and 2013/14 rendered the results irrational: The unexplained swing of Petitioner’s rating from 2012-2013 when she was identified as an “effective” teacher with a rating of 14 out of 20, to 2013-2014, when she was identified as an “ineffective” teacher with a rating of 1 out of 20, is clear proof the model and rating system, as applied to Petitioner, are irrational. These large swings demonstrate that, whatever it is the model is measuring, it is not measuring a stable construct we would recognize as reflecting teacher effectiveness, which should not and does not, in fact, change dramatically from year to year.

(Id. at ¶25.) Professor Darling-Hammond further expressed an expert opinion that the lack of any review process by Respondents was itself irrational. (Id. at ¶30.) Given the widely known challenges to accurate assessment of teachers using value-added methods, it is my further belief that the absence of any way for a particular teacher to challenge a VAM score on an individualized basis is irrational. For all the reasons outlined above, it is well-known and accepted among researchers that a particular individual score produced by a VAM procedure, even on the best developed and administrated model, may be wrong for a variety of reasons. It is my opinion that any teacher receiving an adverse score on a VAM model therefore should have the right to understand why he or she got the score awarded and appeal that score based upon the particular facts of any given case.

The original affidavit of Professor Aaron Pallas of Columbia University, sworn to February 25, 2015, explained, among other things, that the Respondents’ Growth Model was flawed because it pre-determined that only 7% of teachers could ever be rated as highly effective, and mandated that 7% of teachers would be rated as ineffective (the top and bottom ratings, respectively) without any scientific definition of effectiveness. (Original Pallas Aff. at ¶¶13, 14.) Professor Pallas also explained that the model did not provide a teacher, or his/her supervisor, with information at the beginning of the school year about what level of student

performance would result in the teacher being rated Highly Effective, Effective, Developing or Ineffective. Professor Pallas explained that, in New York, nobody knows before the end of the school year what was required of a teacher during the school year in order for that teacher to obtain a particular rating for that particular school year. (Id. at ¶¶s 20-22.)

The original affidavit of Professor Audrey Amrein-Beardsley, of Arizona State University, sworn to February 28, 2015, explained that VAM models, such as that being used by Respondent, have a 50% chance of showing that a teacher caused growth in students one year,

and a 50% chance of showing that the teacher detracted from student growth, so “the probabilitythat this teacher was truly effective or ineffective was no different that the flip of a coin.” (Amrein-Beardsley Aff. at ¶9.)

Dr. Carol Burris, a New York State Educator of the Year, and fellow of the National Education Policy Center, in her affidavit sworn to February 27, 2015, explained, among other things, that something is “seriously wrong if teachers, many of whom teach the same grade level, in the same school, using the same curriculum, could have their scores dramatically shift each year.”

Dr. Brad Lindell, a school psychologist with an advanced degree in applied research, in his affidavit sworn to March 1, 2015, provided the following example of why evaluations need to be reliable to be considered valid:

To understand the importance of reliability, I asked the Court to please consider the following. One of the most well-known measures of intelligence is the Wechsler Intelligence Scale for Children –V (WISC-V). One of the scores the WISC-V provides is a full scale IQ score that ranges from 40 to 160 with an IQ score of 100 being the average score. The reliability of this score with a one-year interval between two administrations of the test is generally in the .80 to .90 range. This indicates strong what is called test-retest reliability and the stability of the test over time. This means that if a student scored a 100 one year, it would be expected that when he is administered the test a year later, he would receive a similar score. If he received a score of 100 (average score) and then received a score of 120 (superior score) a year later and then a score of 85 (low average score) the following

year, the WISC-V would then not be considered reliable. If it was not reliable, then it could not be a valid measure of intelligence. The same argument is relevant to teacher assigned VAM scores. In order for the VAM scores to be a valid measure of teacher effectiveness, it needs to be reliable.. Just over half of New York City teachers were evaluated in the 2015–16 school year, in part, by tests in subjects or of students they didn’t teach, according to data obtained by Chalkbeat through a public records request.

At 53 percent of city teachers, it’s significant number, but substantially lower than in previous years, possibly thanks to a moratorium placed on using state tests, instituted mid-year.

That figure also highlights a key tension in evaluating all teachers by student achievement, even teachers who work with young students or in subjects like physical education. Being judged by other teachers’ students or subjects has long annoyed some educators and relieved others, who otherwise might have had to administer additional tests.

Supporters say evaluating teachers by group measures — often school-wide scores on standardized tests — helps create a sense of shared mission in a school. But the approach could also push teachers away from working in struggling schools.

RELATED: Do school vouchers really work?

“The key point around school-wide measures is that this could serve as a strong disincentive for these teachers in non-tested grades and subjects to stay in lower-performing schools,” said Matthew Steinberg at the University of Pennsylvania, who has studied teacher evaluation systems.

Will Mantell, a spokesperson for the New York City Department of Education, defended the district’s approach.

“Selecting school-wide [or] grade-wide … measures may better measure educators’ practice and support professional development,” he said. “For example, it makes sense for a social studies teacher who emphasizes writing in her classroom to be evaluated partially on an assessment of students’ ELA skills.”

New York’s evaluation system has gone through a number of substantial changes since it was first codified in state law in 2012, part of a nationwide push to connect teacher performance to student test scores, spurred by federal incentives.

Student assessments have comprised anywhere from 40 percent of the evaluation to essentially 50 percent, under a matrix system pushed by Governor Andrew Cuomo in 2015. Most recently, New York stopped using grades 3-8 English and math state tests as part of the system, but teachers must continue to be judged based on some assessment.

States across the country have struggled to evaluate teachers in traditionally non-tested grades and subjects. New York City has created a number of exams — known as performance assessments — in non-tested areas and given schools significant flexibility in which measures are used to judge their teachers.

In the 2015-16 school year, 53 percent of teachers were evaluated by a group metric, meaning one not focused on their subject or students. In the two previous years, the number was much higher — around 85 percent. It’s not clear why there was a substantial drop, but a spokesperson for the city’s education department notes that 2015-16 was an “outlier” due to the moratorium on state tests, instituted mid-year.

In all three years, most teachers were also evaluated by at least one individualized measure targeted to teachers’ grade, subject and students.

Data for the most recent school year are not yet available.

It’s also not clear what percentage of a teacher’s rating was based on group measures, and Mantell said this “varies from teacher to teacher.”

The United Federation of Teachers has pushed to give schools more individual options, including the use of more “authentic” assessments, not based on multiple choice questions.

“Right now, we don’t have enough options, which is why our most recent agreement with the DOE seeks to build more authentic assessments for additional grades and subjects,” said Michael Mulgrew, president of the UFT in a statement.

RELATED: De Blasio strikes deal with charter schools

Group measures offer an alternative to creating exams for each teacher in every grade and subject, which can lead to a proliferation of new tests, though in New York City teachers have often been judged by both group and individual metrics.

The challenge of evaluating teachers in traditionally untested areas is not unique to New York, and a number of states have embraced group or school-wide approaches. An analysis of 32 states, conducted by Steinberg, found that the average teacher in a non-tested grade or subject had about 7 percent of his or her evaluation based on school-wide achievement measures, though this averaged together substantial variation from place to place. Teachers in Tennessee and Florida have sued (unsuccessfully), arguing that it is unfair to evaluate them based on students they didn’t teach.

A more popular option, used in some districts in New York, has been student-learning objectives, in which teachers set goals for students often based on classroom exams. This approach has been praised for helping teachers set specific goals, but criticized as burdensome and easy to manipulate.

Research has found that using school-wide measures of performance tends to bring teachers closer to average performance. An analysis by the Brookings Institution showed that these group measures pulled down ratings of teachers with higher individual ratings at low-performing schools.

This story was originally posted on Chalkbeat, a nonprofit news site covering educational change in public schools.. Stories & Grievances

A NYC Math Teacher Fights Back After Receiving an Unfair 'Unsatisfactory' Rating from a Principal

Edmond Farrell uses the Freedom of Information Law and Department of Education/Teacher regulations in his fight to change an unfair 'unsatisfactory' rating. His rights in the administrative proceedings were violated, Chancellor Klein never answered his appeal, and now a Verified Petition has been filed with Commissioner Mills on the grounds of teacher abuse, abuse of discretion, and age discrimination. This case may change the practice of dumping good teachers in New York State.

Parents throughout New York City and across America are wondering why good teachers disappear from their child's schools while terrible - and sometimes even abusive - teachers are allowed to stay. We at parentadvocates.org believe that there is a "new" criteria for teachers that no one wants to talk about, because it is illegal. The criteria for obtaining employment at the Department of Education is no longer quality in teaching or "loves children". Instead, you will get hired if:(1) you are perceived as someone who will never ask questions or whistleblow what you see inside the secret hallways and offices of your school. In fact, you may have just moved to a new state because of crimes committed there, and you have a reason to maintain a school's secrecy.(2) you support the implementation of curricula that is unproven, unknown, and worthless, such as TERC math, Everyday Mathematics, "Balanced Literacy", etc., all of which are currently imposed on teachers throughout the US. The US government and educational researchers still have not found statistical research to support these programs. We all must see newspaper reports about success as part of the "Armstrong Williams" syndrome, or payola in the media. (See " Armstrong Williams: Education Propaganda, Payola, or Whatever You Call it, is Still False ADvertising and Political Misconduct ")This obviously "must keep confidential" criteria for teachers is harming our children and their future. They are not being given the skills and knowledge that they need to achieve and be successful in whatever they want to do because many kids drop out in frustration or are pushed out, suspended, arrested, and abused in order to leave. Principals do, we must not forget, want their school scores to go up rather than down, any way they can make this happen. Bonuses matter.Superintendents and principals have the power to "observe" subordinates and judge their performance based upon their "observations". Often the observation reports may be biased or prejudicial, and the teacher has little recourse. In New York City, an award-winning math teacher at John Adams High School, Mr. Edmond Farrell, was given six unsatisfactory observation reports after 9 years of satisfactory and excellent ratings. What changed was the administration: a new Principal, Mrs. Grace Zwillenberg, was appointed to the school in 2003-2004. She does not like Ed, who is over the age of 50. She wanted him out, and used observation reports to get him out of the school. He believes this is wrong, and is determined to prove this in court if necessary.In New York City observation reports have been designated in the courts as "opinions" ( Elentuck v Green, 202 AD2d 425, 608 NYS2d 701, 702; 1994 ). Will Mr. Farrell's U-rating hold up in court? He has filed a Verified Petition with New York State Education Commissioner Richard Mills to find out.Edmond Farrell, a math teacher in New York City who had the unfortunate experience of being, he believed, falsely accused of 'unsatisfactory' performance, fought back using references to the Freedom of Information Law and the Regulations of the Commissioner of Education. His letter, we believe, should be used as a model for anyone fighting an unfair assessment by someone higher up who is trying to get rid of/silence/harass him/her.I should add that at the Office of Appeals and Reviews Hearing for Edmond Farrell on November 8, 2004, I and Mr. Norman Scott, both listed as witnesses in the letter that follows, were barred from being witnesses or entering the Hearing at all. OAR Director Mrs. Virginia Caputo created quite a scene screaming at me that I could not, under any circumstances, be part of the Hearing, because "only Board of Education employees may be witnesses". When I asked to see this regulation in writing, she became even more angry, and told me she did have them available. Her Assistant in the OAR is Mr. Greg Brooks, who told me the same thing in the same way. Mr. Norman Scott was also barred because he currently does not work for the DOE and is retired. The other three witnesses listed in the letter below were dismissed before the Hearing. Ms. Caputo made a 'new' regulation for Mr. Scott, namely that he DID work for the DOE but not at John Adams High School. Mr. Farrell's UFT representative, Ms. Ritter, who accompanied him into the hearing, was also a retired teacher but had never taught at John Adams HS, was not an Attorney, had never seen Ed teach, and should therefore have been barred in a similar way. She was not.Mrs. Alvarez, the OAR Hearing Officer, told me, Mr. Norman Scott, Ms. Ritter, and Mr. Farrell after the Hearing had concluded and the tape machine turned off, "You cannot dispute what a Principal says".Betsy Combier, Editorconcerning the use of observation reports to fire teachers in NYC classrooms:September 16, 2004CERTIFIED MAILChancellor Joel I. KleinNew York City Department of Education52 Chambers StreetNew York, New York 10007Dr. Elizabeth AronsExecutive DirectorDivision of Human ResourcesNew York City Department of Education65 Court StreetBrooklyn, New York 11201Ms. Virginia CaputoDirectorOffice of Appeals and ReviewsNew York City Department of Education65 Court StreetBrooklyn, New York 11201Dear Chancellor Klein, Dr. Arons, and Ms. Caputo:I'm writing to you to complain that Mrs. Grace Zwillenberg, the principal of John Adams High School, gave me an arbitrary, capricious, irrational, and therefore, totally unjustified U-rating this past June, despite my teaching service for the 2003-2004 school year having been satisfactory. I'm requesting a full investigation of the matter, reversal of the U-rating to an S-rating, and that you take whatever additional action you deem appropriate.Page 2 of the Department's manual, The Appeal Process (a publication of the Division of Human Resources; See Rating Manuals ) in part:"The Appellant must file a full, written rebuttal to any of the reasons and documents furnished, and must notify the Office of Appeals and Reviews if any of the documents are being grieved with the Office of Labor Relations. If the Appellant so requests, the scheduling of the Review will be delayed until all grievance steps have been completed. The Appellant is required to file a Waiver Form with the Office of Appeals and Reviews to delay scheduling the Review."I wish to proceed as quickly as possible with my appeal of the U-rating. Therefore, I am not filing a waiver form.As a "full, written rebuttal to...the reasons and documents furnished," I wish to call the following matters to your personal attention:At the outset, it is appropriate to note that annual professional performance reviews of staff members in the New York City school system must be carried out in compliance with §100.2(o)(2)(iii)(a)(2) of the Regulations of the Commissioner of Education.As far as I have been able to determine, neither the UFT nor organizations representing parents were ever provided with an opportunity to comment on changes in the procedures for evaluating teachers, prior to their adoption. If so, then this is reflective of a violation of §100.2(o)(2)(iii)(a)(2) of the Regulations of the Commissioner of Education with respect to the formulation and implementation of the professional performance review plan.§100.2(o)(2)(iii)(a)(2) of the Commissioner's Regulations states:"Each superintendent and in the case of the City School District of the City of New York, the Chancellor, in collaboration with teachers, pupil personnel professionals, administrators and parents selected by the superintendent or in the case of the City School District of New York, the Chancellor, with the advice of their respective peers, shall develop the professional performance review plan, which shall be approved by the governing body of each school district or BOCES, filed in the district or BOCES office, as applicable, and available for review by any individual no later than September 10th of each year. The governing body of each school district and BOCES shall provide organizations representing parents and the recognized representative of the teachers' bargaining unit with an opportunity to comment on such plan prior to its adoption."The Part §100.2(o)(2) is available online On June 24, 2004, I was issued an Annual Professional Performance Review form (BE/DOP 9955B) which had been signed by Principal Grace Zwillenberg on June 18, 2004. I signed receipt for the form on the same date that it was issued to me.On the front of the form, Principal Zwillenberg conclusionally alleged that the following thirteen characteristics of my teaching service were "unsatisfactory":1) A. 4. Professional attitude and professional growth2) A. 5. Resourcefulness and initiative3) B. 1. Effect on character and personality growth of pupils4) B. 2. Control of class5) B. 3. Maintenance of wholesome classroom atmosphere6) B. 4. Planning and preparation of work7) B. 5. Skill in adapting instruction to individual needs and capacities8) B. 6. Effective use of appropriate methods and techniques9) B. 7. Skill in making class lessons interesting to pupils10) B. 8. Extent of pupil participation in the class and school program11) B. 9. Evidence of pupil growth in knowledge, skills, appreciations and attitude12) D. 2. Effort to establish and maintain good relationships with parents13) D. 3. Willingness to accept special assignments in connection with the school programOne indication that Principal Zwillenberg's allegations were conclusional is that she never presented me with a written statement of facts and statistics, with respect to any of the preceding thirteen categories, as a means of establishing that changes in the performance of my official duties must be made. Taking into account circumstances beyond my control, my position is that I was "sufficiently competent" with respect to every aspect of my teaching service that would properly form the basis of the annual professional performance review.To be "proper," in this instance, would mean that the review criteria were based on, but not limited to, the eight categories set forth in §100.2(o)(2)(iii)(b)(1) of the Commissioner's Regulations, and were developed collaboratively by teachers, pupil personnel professionals, administrators, and parents selected by the Chancellor, with the advice of their respective peers. The proposed professional performance review plan had to be submitted to parent groups and the UFT prior to going into effect so that they could offer comment. Finally, the plan had to be voted into place during a public meeting of the Panel for Educational Policy.A second indication that Principal Zwillenberg's allegations were conclusional is that she never presented me with a copy of the professional performance review plan currently in effect in the City School District of the City of New York, along with a factual or statistical statement demonstrating that my teaching service was not "sufficiently competent" with respect to each of the evaluation criteria set forth in the plan.Although Principal Zwillenberg's and Assistant Principal Aboughaida's written statements to me advised me to meet with AP Aboughaida on a weekly basis, I soon discovered that the meetings were not productive, and were designed to give the impression that the administration was offering assistance. Not once after any of the weekly meetings did APAboughaida ever issue me any written orders or mandates, directing that I change any aspect of my teaching service. Rather, APAboughaida kept suggesting that I "follow Region 5's Mathematics Prototype Investigation Model," and I kept informing him that teaching is essentially a function of the instructor's personality, and that there are no absolutes in teaching. As a result, the meetings became somewhat heated, rather than helpful in nature.On the back of the form, Principal Zwillenberg conclusionally alleged that each of six listed observation reports constituted "substantiating documentation" with respect to each of the thirteen preceding characteristics. Also, on the back of the form, she falsely alleged, within a listing of submitted records, that she (the principal) had observed an October 21, 2003 lesson. Rather, the assistant principal had observed the lesson.Page 3 of the Department's manual, Rating Pedagogical Staff Members (a publication of the Division of Human Resources), states, in part:"A Certification of Unsatisfactory or Doubtful work shall be accompanied by appropriate supporting data."Please be advised that my U-rating was not accompanied by "appropriate supporting data," because the records which did accompany it, did not contain statistical data, factual data, or instructions to staff that affect the public.Page 4 of the manual states, in part:"The characterization of an employee's service as Satisfactory indicates that he/she has demonstrated sufficient competence or improvement and a willingness to learn. If such has not been the case, ratings of Unsatisfactory or Doubtful should be used."Please be advised that my service was satisfactory, and, it goes without saying, "sufficiently competent." Neither Principal Zwillenberg nor anyone else issued me any instructions directing me to change any aspect, whatsoever, of my service.The manual's Foreword states, in part:"The admissibility of documents and written criticisms has been defined by contractual language, grievance/arbitration decisions and rulings adjudicated by both the legal system and the State Commissioner of Education. Hence, the principal must be aware of the type and nature of documents which are germane to the evaluation of staff and the need for clear, objectively written statements."It is reasonable to assume that Principal Zwillenberg was aware of "the need for clear, objectively written statements," i.e., statements that recited facts or statistics. Nonetheless, she inexplicably failed to ensure that AP, Regional Instructional Supervisor Ben Waxman, and she, herself, included any facts or statistics in the letters ("observation reports") that they wrote to me.Appendix B of the manual, as part of "SUGGESTED APPROACH TO A WRITTEN OBSERVATION," indicates that principals should include a:"Brief factual description of the lesson that would include aim, motivation, developmental procedures, summary, homework, and any major points of the lesson worth mentioning."Please be advised that none of the six observation reports that accompanied my U-rating contained a "factual description of the lesson."Appendix D of the manual, as part of "GUIDE FOR WRITTEN FORMAL OBSERVATION REPORT OF A LESSON," indicates that principals should:"Objectively state lesson development and activities."Please be advised that none of the six observation reports that accompanied my U-rating contained any "objective statements."The six observation reports consisted solely of advice, criticisms, evaluations, and recommendations.It is my position that the advice, criticisms, evaluations, and recommendations were not supported by a recitation of any facts or statistics.It is also my position that I was under no legal obligation to implement the recommendations, and that no teacher in the State of New York may be appropriately rated as "unsatisfactory" based on that teacher's alleged failure to implement recommendations, advice, suggestions, or the like.Principal Zwillenberg's determination to rate my service as "unsatisfactory" was not grounded on established legal principles. Her determination had no rational, factual basis in the record.On February 16, 1976, Arbitrator James J. Hill decided In the Matter of Arbitration between United Federation of Teachers and Board of Education of the City of New York, Cases Nos. 1330-0324-74 (Mae Leass) and 1330-0615-73 (Bertil Swanteson).On page 14, Arbitrator Hill stated:"A misstatement of fact, based on a misunderstanding of the supervisor, is of vital concern to the teacher, whether the error appears in an observation report or some other document, which both parties recognize as grievable.""As noted heretofore, the Board cites Union Counsel Kaufman on the non-grievability of professional evaluations of a teacher's performance and proficiency, but takes strong exception to his contention that factual allegations within such reports may be challenged under the grievance and arbitration provisions of the contract. Board's counsel states:"" 'Mr. Kaufman's claim that factual matters in such evaluations could be the subject of grievances is an impractical and illusory attempt to distinguish the indistinguishable. While such a distinction may appear reasonable at first, a thorough examination of an evaluatory report reveals that a rending of it into its factual and conclusory aspects is impossible. Such a report ordinarily sets forth those significant events which the supervisor has observed and then draws conclusions based upon his view of their educational soundness. The two are so intertwined and interrelated that to attempt to separate them would be analogous to Hercules' attempt to unravel the Gordian knot. ... The conclusions of the report, which Mr. Kaufman has admitted are not grievable, depend upon the factual matters set forth therein. To remove essential facts, which Mr. Kaufman claims is possible, would leave only unsupported conclusions, creating, in effect, a travesty of an observation report. As Hercules destroyed his problem rather than solving it, so this attempted separation of facts and conclusions would destroy an observation report.' (Brief, pp. 11-12)"" 'Mr. Kaufman's proposal for the handling of grievances concerning supervisory evaluations would leave, if the grievant were sustained in a claim of factual error, only baseless charges, not conclusions. Such a result was never agreed to by the Board.' (Brief, p. 14)"On page 15, Arbitrator Hill stated:"In my judgment, this argument is without merit. It seems to say that if an evaluator's conclusions rest on assertions of fact which are shown to be false, the assertions must remain lest the conclusion be seen as baseless. The argument is untenable. If the conclusions of an evaluator rest on assertions of fact which are shown to be false, the conclusions have no validity and should, in all fairness, be deleted. And the same result would obtain, it should be noted, if the Board were upheld in its original position that Article IV F 20 (5) was intended to provide for review of the accuracy and fairness of evaluative reports by some other appellate body, apart from the contract grievance procedures. Certainly if such evaluations should remain in the files, while the factual basis for such conclusions has been found to be false, the teacher should have the right to make it known, as a matter of record, that the judgment rests on factual allegations which have been deemed false and expunged from the record."However, the New York State Supreme Court, Appellate Division, Second Judicial Department, effectively reversed Arbitrator Hill's presumption that assertions of fact are to be found in observation reports. The (former) New York City Board of Education was aware of Elentuck v. Green (202 AD2d 425), a judicial decision involving (former) Chancellor Richard R. Green, and agreed with it.On May 13, 1998, Ron LeDonni, the Secretary of the (at the time) New York City Board of Education, wrote a letter (attached) to Mr. Irving Schachter informing him of a decision that had been issued by the New York City Board of Education's Freedom of Information Law Appeal Panel.Mr. LeDonni stated, in part:"You aver that observation reports contain factual data such as the date of the report, who was observed, the name of the school where the observation was held and depending on the subject matter, could contain mathematical problems, etc. You believe that based on relevant case law as stated in Gould v. NYC Police Department, 89 NY2d 267 (1996) and Mothers on the Move, Inc. v. Messer, 652 NYS 2d 773 (1997) that the reports should be disclosed because they contain factual information.""The Appeal Panel has reviewed your appeal, accompanying documents and applicable case law and makes the following determination.""In Elentuck v. Green, 202 A.D. 2d 425, 608 N.Y.S. 2d 701 (1994) the court held that lesson observation reports consist solely of advice, criticisms, evaluations, and recommendations prepared by a supervisor regarding lesson preparation and classroom performance and are exempt from disclosure. The court found that observation reports do not contain factual or statistical tabulations or data. This case is clearly applicable regarding the records you have requested. Although the cases you cite in your appeal address various types of documents containing factual information which should not be exempt from disclosure, they are not applicable to the records you request nor do they reverse the holding in Elentuck."In Gould v. NYC Police Department (89 NY2d 267), the case which the (at the time) New York City Board of Education rejected as permitting public access to observation reports, the Court of Appeals stated:"The question before us, then, is whether the complaint follow-up reports contain 'factual data.' Although the term 'factual data' is not defined by statute, the meaning of the term can be discerned from the purpose underlying the intra-agency exemption, which is 'to protect the deliberative process of the government by ensuring that persons in an advisory role (will) be able to express their opinions freely to agency decision makers' (Matter of Xerox Corp. v Town of Webster, 65 NY2d 131, 132 -quoting Matter of Sea Crest Constr. Corp. v Stubing, 82 AD2d 546, 549). Consistent with this limited aim to safeguard internal government consultations and deliberations, the exemption does not apply when the requested material consists of 'statistical or factual tabulations or data' (Public Officers Law §87 2 g i). Factual data, therefore, simply means objective information, in contrast to opinions, ideas, or advice exchanged as part of the consultative or deliberative process of government decision making (see, Matter of Johnson Newspaper Corp. v Stainkamp, 94 AD2d 825, 827, affd on op below, 61 NY2d 958; Matter of Miracle Mile Assocs. v Yudelson, 68 AD2d 176, 181-182)."In Gould, the Court of Appeals also stated:"However, the Police Department argues that any witness statements contained in the reports, in particular, are not 'factual' because there is no assurance of the statements' accuracy and reliability. We decline to read such a reliability requirement into the phrase 'factual data,' as the dissent would have us do, and conclude that a witness statement constitutes factual data insofar as it embodies a factual account of the witness's observations."A different Court of Appeals case demonstrated that only facts, and not opinions, could be assessed as accurate or inaccurate.In Elliot M. Gross v. The New York Times Company (82 NY2d 146), the Court of Appeals stated:"Thus, in determining whether a particular communication is actionable, we continue to recognize and utilize the important distinction between a statement of opinion that implies a basis in facts which are not disclosed to the reader or listener (see, Hotchner v Castillo-Puche, 551 F2d 910, 913, cert denied sub nom. Hotchner v Doubleday & Co., 434 US 834; Restatement - Second- of Torts § 566), and a statement of opinion that is accompanied by a recitation of the facts on which it is based or one that does not imply the existence of undisclosed underlying facts (see, Ollman v Evans, supra, p 976; Buckley v Littell, 539 F2d 882, 893, cert denied 429 US 1062; Restatement -Second- of Torts § 556 comment c). The former are actionable not because they convey 'false opinions' but rather because a reasonable listener or reader would infer that 'the speaker (or writer) knows certain facts, unknown to the audience, which support the opinion and are detrimental to the person toward whom the communication is directed' (Steinhilber v Alphonse, supra, p 290).In contrast, the latter are not actionable because, as was noted by the dissenting opinion in Milkovich v Lorain Journal Co. (supra, pp 26-27, p 28 n3 -Brennan, J.), a proffered hypothesis that is offered after a full recitation of the facts on which it is based is readily understood by the audience as conjecture (see, e.g., Potomac Valve & Fitting, Inc. v. Crawford Fitting Co., 829 F2d 1280, 1290). Indeed, this class of statements provides a clear illustration of situations in which the full context of the communication 'signal's readers or listeners that what is being read or heard is likely to be opinion, not fact' (Steinhilber v Alphonse, supra, p 292; quoting Ollman v Evans, supra, p 983)."To base my "unsatisfactory" rating, or any teacher's "unsatisfactory" rating, on records that do not contain statistical or factual tabulations or data, and instructions to staff that affect the public, is arbitrary, capricious, irrational, and flies in the face of the fundamental due process that is guaranteed to U.S. citizens by the Federal Constitution.In Canty v. New York City Board of Education (312 F. Supp. 254), the United States District Court of the Southern District of New York stated:"The terms 'arbitrary' and 'capricious' embrace a concept which emerges from the due process clauses of the Fifth and Fourteenth Amendments of the United States Constitution and operates to guarantee that the acts of government will be grounded on established legal principles and have a rational factual basis. A decision is arbitrary or capricious when it is not supported by evidence or when there is no reasonable justification for the decision."On February 2, 2000, Mr. Robert J. Freeman, Executive Director of the New York State Committee on Open Government issued Advisory Opinion #11936 to the Attorney for the Ellenville Central School District.Within his letter, Mr. Freeman offered his opinion concerning public access to "ratings of 'satisfactory' or 'unsatisfactory' given to teachers for classroom evaluations." Mr. Freeman pointed out "that the Appellate Division, Second Department, has determined that records apparently analogous to those requested may be withheld."In addition, Mr. Freeman referred to the precise same case that Mr. LeDonni did, Elentuck v. Green (202 AD2d 425), and quoted from the decision:"The lesson observation reports consist solely of advice, criticisms, evaluations, and recommendations prepared by the school assistant principal regarding lesson preparation and classroom performance. As such, these reports fall squarely within the protection of Public Officers Law §87(2)(g)."Since observation reports consist "solely of advice, criticisms, evaluations, and recommendations," teachers might wonder whether or not they are required to implement the advice and recommendations, and take the criticisms and evaluations to heart, lest they be rated "unsatisfactory" for not having "demonstrated sufficient competence or improvement and a willingness to learn."Throughout the State of New York, teachers are not required to implement advice, recommendations, suggestions, and the like, even if they originate from supervisory personnel. Furthermore, teachers may not be rated "unsatisfactory," or otherwise disciplined, for an alleged failure to implement such.On March 6, 1986, (at the time) Commissioner of Education Gordon Ambach determined Appeal of Board of Education of the Orchard Park Central School District (25 EDR 331).Commissioner Ambach stated on page 332:"Absent a violation of an established school rule or a direct order from a supervisor, respondent's behavior would not constitute neglect of duty. Although petitioner identifies certain provisions of the school building manual as the source of a rule against leaving students to work unsupervised in the hallway, those provisions are expressed in terms of suggestions to teachers rather than directives. Upon careful review of the record, I find that petitioner failed to establish that respondent's conduct breached any mandatory school rule or directive by a supervisor."An examination of each of the six observation reports that accompanied my U-rating will clearly reveal that no instructions or directives had been given to me, but rather suggestions.10/24/03 observation by Assistant Principal:"During future visits to your classes, I look forward to observing your implementation of the suggested strategies that we have discussed."12/1/03 joint observation by Principal and Assistant Principal:"During future visits to your classes, I look forward to observing your implementation of the suggested strategies that we have discussed."12/22/03 observation by Assistant Principal:"During future visits to your classes, I look forward to observing your implementation of the suggested strategies that we have discussed."3/15/04 observation by Assistant Principal:"During future visits to your classes, I look forward to observing your implementation of the suggested strategies that we have discussed."5/13/04 joint observation by Principal and Assistant Principal:"During future visits to your classes, I look forward to observing your implementation of the suggested strategies that we have discussed."6/7/04 observation by Ben Waxman Regional Instructional Supervisor:"During future visits to your classroom, I look forward to observing your implementation of the strategies and methodologies suggested and discussed."It is quite clear that if the Principal, AP, and Regional Instructional Supervisor were genuinely dissatisfied with my teaching service, then they had an obligation to give me directions or instructions as to what to do, but not a series of suggestions.On January 25, 1989, (at the time) Commissioner of Education Thomas Sobol determined Appeal of Board of Education of the City School District of the City of New York(28 EDR 302).Commissioner Sobol stated on page 303:"After 16 days of hearing, the hearing panel unanimously recommended dismissal of all charges. The panel found that while respondent is not a 'good' teacher; is boring; does not consistently follow district policy; and maintains a narrow structured point of view in conducting his classroom; his performance did not fall below the minimal level expected of a 'reasonable teacher.' The panel found that respondent had a minimal level of competency to communicate facts; that he had knowledge of subject matter content; that he could motivate and interest students; that he maintained a classroom environment reasonably conducive to learning; and that he had an ability to assess and evaluate student performance. Based upon its assessment of these factors, the panel unanimously agreed that respondent had met the minimum level of competency which should be expected of a 'reasonable teacher.' The panel also unanimously rejected the charges of neglect of duty. The panel determined that there was no evidence that respondent did not fulfill his obligations, nor that he was unprepared to teach or that his lesson-plans or records were in any form of disarray.""The panel, in a commentary on respondent's teaching and questioning techniques, stated that there were no absolutes in teaching, and that respondent had to be evaluated for effectiveness. The panel decision notes the fact that respondent's students fared at least as well or better than the students of other teachers in the science department during the period covered by the charges."At page 305, then Commissioner Sobol stated:"The issue of incompetency however is a more difficult one. In a classroom situation, incompetence in its simplest terms means that a teacher is unable to provide a valid educational experience for those students assigned to his classroom. The record before me supports the panel's conclusion that petitioner also failed to prove this charge by a preponderance of the credible evidence. While I do not feel that it should be the determining factor in every case, the performance of respondent's students on the regents examinations in both earth science and biology indicates that respondent's students did as well as or better than the students in similar classrooms in the same school. There is no evidence in the record that respondent's students were substantially different in academic ability than students assigned to other teachers in similar classes, or that any other factor outside of respondent's classroom influenced their performance on the examinations. This tends to support the panel's finding that respondent was able to educate his students. It is also my finding that respondent provided a plausible explanation for the decisions which he made concerning the conduct of his classes and his methods of instruction. Weighing all of these factors, it is my conclusion that petitioner has failed to prove by a preponderance of the credible evidence that respondent is an incompetent teacher."Chancellor Klein, on page 1 of the March 2004 issue of Connections (Volume 1, Issue 1), a Department of Education / Teacher Communication Exchange, you stated:"On the other hand, I don't support some of the practices that you told me about in some of your emails to me. Whether a 'mini-lesson' lasts for 10 or 12 minutes should be left to the professional judgment of the teacher, as should the decision about how and where children sit in their classrooms. Some of you have mentioned your concerns about bulletin boards. In my view, bulletin boards are important: by demonstrating student work, they set a tone in the school that can support learning. At the same time, we shouldn't become overly preoccupied with bulletin boards, and some of your emails suggest this may sometimes be happening. Effective management, like effective teaching, is a skill that can be learned and we are working to help all staff improve their management skills. Indeed, that is a key reason that we have developed our path-breaking Leadership Academy. I have also discussed these concerns with our Superintendents and I am confident that they will be reviewing management practices at the school level to ensure that teachers are given appropriate professional latitude."Chancellor Klein, if you will compare your statement against the advice, criticisms, evaluations, and recommendations that were set forth in the six observation reports that the Principal submitted as her allegedly "appropriate supporting data," you will surely realize that I was not being given "appropriate professional latitude."Chancellor Klein, on April 20, 2004, your opinion column entitled, "Give the Schools to the Mayor" was published in the Washington Post. In the column, you noted:"The ability to transform a public education system begins with a concept central to any successful enterprise, public or private -- accountability. And as we have seen, accountability can be little more than a cynical punch line in a system that disperses control of such critical educational functions as budget, management, and curriculum through a network of localized, politically driven, semi-independent school boards able to set, follow or ignore their own rules."Principal Zwillenberg demonstrated a remarkable lack of accountability when she made the conclusional allegation that my teaching service was "unsatisfactory." She demonstrated this same lack of accountability when she did not apply the Commissioner's Regulations to my annual professional performance review. Instead of following the Commissioner's Regulations, she appeared to have made up her own rule that a mathematics teacher is to be given a suggestion to follow the Region 5 Mathematics Prototype, and that if the suggestion isn't followed, the end result will be a U-rating.Mrs. Zwillenberg is known for sending out "mixed signals." On January 15, 2004, a date of bitterly cold, snowy weather, she sent me a letter which stated:"Please accept my appreciation for your attendance at school today. I am certainly aware of the tremendous hardship involved in traveling during this morning's severely inclement weather.""Once here, we witnessed a monumental effort of team cooperation by all.""On behalf of the Administration and the students whom we serve, thank you, Mr. Farrell. Your dedication and commitment to the students of John Adams are certainly admirable."It must be emphasized here that Mrs. Zwillenberg did not support her allegation that my teaching service was "unsatisfactory" by a recital of any statistical data or factual data. In addition, Mrs. Zwillenberg did not submit any instructions to staff that affect the public, provisions of the Education Law, Regulations of the Board of Regents, Commissioner's Regulations, Chancellor's Regulations, or Bylaws of the Panel for Educational Policy that I allegedly violated.§100.2(o)(2)(iii)(b)(5) of the Commissioner's Regulations states:"Training in performance evaluation. The plan shall describe how the school district or BOCES provides training in good practice for the conducting of performance evaluations to staff who perform such evaluations, or alternatively, shall state the fact that the school district or BOCES permits such personnel to participate in training in this subject offered by the department."I would like to know what training Grace Zwillenberg, Bahaa Aboughaida, and Ben Waxman received in the "conducting of performance evaluations," what their recent supervisory goals and objectives have been, and what the status is of labor relations at John Adams. I would also like to be able to compare the performance of my students on uniform departmental mathematics exams, to the performance of students in similar classrooms that were assigned to other teachers, and to be able to compare the behavior and attendance of my students (in the observed classes) to their behavior and attendance in their other subject classes.On page 2 of the September 2004 issue of CSA News, Jill S. Levy, the president of the Council of School Supervisors and Administrators, wrote an article entitled, "When the Office Bully is the Boss."Ms. Levy stated:"According to my research, workplace bullying is persistent, intrusive behavior exhibited by one or more individuals. It includes humiliating, unwarranted offensive behavior toward an individual or groups of employees. Such malicious attacks on personal or professional performance are typically unpredictable, unfair, irrational and often unseen.""Workplace bullying is an abuse of power or position that can cause such anxiety that people gradually lose all belief in themselves, and may suffer physical or mental illness as a result. Bullying has been identified as a more crippling and devastating problem than all the other work-related stresses put together.""The literature is clear about how to handle bullying. And I have said this again and again to you, my members. Do not be afraid. A bully works through lies and deception. You are not the problem, the bully is."After reading Ms. Levy's article, I realized that despite the Principal's having sent me a pleasant letter on January 15, 2004, her repeated suggestions to utilize the Region 5 Mathematics Prototype, which were reiterated by the AP and Regional IS, were typical of the methods employed by a "workplace bully."Should you wish to proceed with a formal administrative review of my U-rating, please direct Mrs. Zwillenberg, Mr. Aboughaida, and Mr. Waxman to bring along all records under their jurisdiction that reflect or reveal:1) the extent of their involvement in such training during any school year,2) each specific instance of their application of such training during the 2003-2004 school year relating to all staff members that they evaluated,3) the performance of all mathematics students enrolled at John Adams High School, during the 2003-2004 school year, on uniform departmental mathematics exams, and who were assigned to other teachers in similar classes to the ones observed,4) the academic, disciplinary, and attendance record of all my students (in the observed classes) in their other subject classes, during the entire time period that they have been enrolled at John Adams HS,5) the record that cited or mentioned Education Law §3210(1)(a) to students enrolled at John Adams HS and to their parents or guardians,6) all records revealing the Principal's, AP's, and Regional IS's supervisory goals and objectives during the 2003-2004 school year,7) all records revealing whether they didn't meet their 2003-2004 supervisory goals and objectives, met them, or exceeded them,8) all grievances filed at John Adams HS during the last three school years, and the ensuing decisions (at all levels), and9) the professional performance review plan in effect in the City School District of the City of New York during the 2003-2004 school year (as per §100.2(o)(2)(iii)(a)(2) of the Regulations of the Commissioner of Education).In addition, please direct Mrs. Zwillenberg, Mr. Aboughaida, and Mr. Waxman to provide me with one copy of the preceding records no later than twenty business days prior to the first scheduled date of the administrative review.Prof. James A. Gross of the New York State School of Industrial and Labor Relations, Cornell University, stated the following on pages 69 and 70 of Teachers on Trial -- Values Standards, & Equity in Judging Conduct & Competence, Copyright © 1988 by Cornell University:'Another pernicious subjective influence appearing in 3020-a incompetency cases has been termed the 'I'm-out-to-get-you' evaluation. (J. Rapp, Legal Aspects of Evaluation 5, pp. 17-18, Paper presented at the Annual Meeting of the American Association of School Administrators -Mar. 8-11, 1985.) Panels were not consistent in dealing with such biased evaluations. One panel adopted a prejudicial balancing test that permitted the use of biased evaluations to support charges of incompetence and then used that bias as a reason for mitigating the penalty imposed.Despite its finding that a new principal, the sole evaluator, was, with hostility, attempting to build a case against a teacher with nine years' experience because the teacher employed teaching methods not favored by the principal, the panel relied on the evaluations submitted in support of the district's charges of incompetence, concluding (for unexplained reasons) '(it had) no authority nor would it be appropriate for (it) to evaluate (the principal)' because 'she is a trained administrator).' In determining a penalty (a one-year suspension without pay), however, the panel said it 'would be inappropriate' to ignore the fact 'that an atmosphere was created between (the teacher and the principal)'.(Poland Central School Disrict v. Dain Faville, NYS Ed. Dept. at 11-12, 27-28 -1983.) Biased evaluations should never be used to assess competence. They must be rejected, and not with delicately vague language.""The identical approach was used by another panel in an even more flagrant example of evaluator bias. In this case, a new principal instructed an English department chairperson who had evaluated a teacher 'in highly approving terms' to change her evaluation to unsatisfactory. According to the department chairperson, the teacher 'had excellent rapport with the children. They trusted him. They felt at ease with him. Excellent.' (School District of New York v. Willard Gellis, NYS Ed. Dept. at 21-22, 25 -1985.) The chairperson testified that she was summoned to the office of (the principal). He asked her whether she had found the (teacher's) lesson satisfactory or not. She replied that she had found it satisfactory for the most part. There followed what the witness characterizes as a 'rather explosive' conversation with the principal. (The principal) instructed her to write up the observation as unsatisfactory. When she objected, (the principal) reacted as follows: '... He said to me do you want this job ... he repeated it over and over again ... when I didn't answer him, he said to me, I could get you out of this job next week if I have to.' (Id. at 22-23.)Subsequently, the principal and the superintendent met with the chairperson and told her that the teacher was unsatisfactory. The chairperson then resigned from her administrative position. (Id. at 24-25.)"Prof. Gross' book is very insightful, and is de rigueur reading for anyone involved in teacher evaluation. His biography on the back cover states:"James A. Gross is a professor in the New York State School of Industrial and Labor Relations, Cornell University, where he specializes in labor law and labor arbitration. He is a member of the National Academy of Arbitration and is on the labor arbitration panels of the American Arbitration Association, the Federal Mediation and Conciliation Service, and the New York State Public Employment Relations Board. Gross, who received his Ph.D. from the University of Wisconsin, has published extensively on labor relations, labor law, labor arbitration, and public policy."I am certain that this letter has more than adequately demonstrated that my U-rating is not sustainable. However, I am prepared to cite facts and statistics that would completely discredit the advice, criticisms, evaluations, and recommendations that Mrs. Zwillenberg, Mr. Aboughaida and Mr. Waxman offered in the observation reports that they wrote to me. I am also prepared to cite facts and statistics that would constitute competent evidence that my teaching performance was satisfactory (or better) at John Adams HS during the 2003-2004 school year.I can offer evidence from an outside, independent individual who found my teaching performance good enough to expand my teaching load for him on two different occasions, and to invite me back to teach for him again this semester.Further, I am prepared to offer as evidence that my requests for lesson videotaping and the presence of simultaneous independent observers were invariably denied without explanation. This was to ensure that Mrs. Zwillenberg, Mr. Aboughaida, and Mr. Waxman and others were free to list whatever advice, criticisms, evaluations, and recommendations they wanted to, and not be faced with the possibility that I would be able to produce something that, or someone who, would be able to demonstrate that the criticisms and evaluations were bogus, and that the advice and recommendations (which did not have to be followed, anyway) had no rational factual basis.Both Mr. Waxman's discussions with me, and the text of the observation report he issued, made it clear that he could not give me a "satisfactory" rating if I continued "bucking the system." His criticisms pretty clearly justified my main contention throughout the 2003-2004 school year, namely, that I have to be able to improvise on my lesson plans depending on how a lesson actually goes, and not follow in lockstep a hoped for plan.I am compelled to inform you that Mrs. Zwillenberg clearly was, and still is, out to get me. I can offer factual data to show this to be the case. One example of Mrs. Zwillenberg's hostility is the remark that she made to Ray Taruskin, the UFT Chapter Leader at Far Rockaway HS after a Step 2 grievance hearing concerning a joint observation report. Mr. Taruskin offered to try to arrange a transfer to a different school for me. In reply, Mrs. Zwillenberg stated, "I wouldn't inflict him on anyone else!" A second example occurred at an Article 24 conciliation hearing where Mrs. Zwillenberg repeatedly made derogatory comments regarding me.On more than one occasion, Mrs. Zwillenberg alleged orally that I was working for an outside company, and gathering data about my students to sell to outsiders for my financial gain. She repeated her suspicions on more than one occasion to the UFT Chapter Leader.Mr. Aboughaida was in Mrs. Zwillenberg's pocket, and did exactly what she wanted him to, in order to get and keep his job. At our various weekly meetings, Mr. Aboughaida frequently made reference to the fact that we had to do what Mrs. Zwillenberg wanted us to do, with respect to items such as the Region 5 Mathematics Prototype, lesson plans, and group work in every class, whether we agreed or not.With respect to the second of my six observations during the 2003-2004 school year, during a heated discussion, I told Mr. Aboughaida that he had to know that that lesson was definitely satisfactory. His response was that it was "her call, not mine" .Lastly, I offer my previous record under many different principals and assistant principals. Never once, prior to the 2003-2004 school year, did I ever receive a U-observation or U-rating. In fact, I received national recognition for outstanding performance in teaching math at the high school level at the national convention of Renaissance Learning, Inc. two years ago.I am still a tremendous asset to the New York City school system, bogus U-rating or not. I trust that you will listen to me, and will ensure that I am given the opportunity to expand my unique skills in dealing effectively with the problem of low-performing high school mathematics students, who perform at levels well below high school (fifth grade, for example), and whose skills must be brought up to a much higher level if they are to be able to deal successfully with high school math, and with life, in general. I ask you to ensure that I get this opportunity, and that you do not allow my current supervisors to drive me out of the school system, which is their obvious intention.Please inform me of your decision in this matter within ten business days of receipt of this letter. If you would like to put the City through the expense of holding a formal administrative review, I am agreeable, although it is obvious that Mrs. Zwillenberg's failure to submit statistical data, factual data, or instructions to staff that affect the public in support of my U-rating, or to submit any general agency or State rules, regulations, or policies, would warrant an immediate reversal of the rating, and the substitution of an S-rating in its place.Should you wish to proceed with the formal administrative review, I would like to call the following individuals as expert witnesses to testify as to the contents of observation reports:Robin S. Greenfield, Esq., and Susan W. Holtzman, Esq., of the Department of Education's Office of Legal Services.Celeste T. Segure, Esq, formerly of the Department of Education's Office of Legal Services, and currently employed by the New York City Housing Authority's Office of Equal Opportunity.Irving Schachter, retired teacher.Betsy Combier, a parent advocate, and who is the editor of www.parentadvocates.org and the president of the E-Accountability FoundationNorman Scott,, a retired teacher and former UFT Chapter LeaderChancellor Klein, since you are an attorney, I am hoping that you will respect the judicial decisions and Commissioner's decisions that I've cited in this letter, and will find them to be an adequate basis upon which to reverse my "unsatisfactory" rating since they clearly demonstrate that Mrs. Zwillenberg did not submit "appropriate supporting data." Indeed, she could not possibly have submitted "appropriate supporting data" for a U-rating since I am a "sufficiently competent" teacher. In addition, Mrs. Zwillenberg did not follow the procedures described in the Commissioner's Regulations for conducting an annual professional performance review. However, if, by any chance, you determine to sustain the U-rating, then I request that you issue me an expansive statement fully setting forth the rational factual basis for your decision, as I would certainly pursue the matter further.Thank you for your full consideration of the many important issues that I have raised.Sincerely,Edmond Farrellcc: Harolyn FritzRandi WeingartenGrace ZwillenbergElizabeth Arons, NYC BOE Director of Human Resources, doesn't know why good teachers are pushed out:June 1, 2005By MICHAEL WINERIP, NY TIMESTHEY knew from their children that there was trouble in the fifth grade at Public School 111 in Manhattan. But it wasn't until teacher conferences in late February that parents learned how badly the class had fallen apart.By then, the fifth graders were on their third teacher in seven months. The first left before Christmas to get married. And though her departure was widely known by November, the second teacher, a substitute, Emebet Shiferraw, says she wasn't told about taking over that class until she reported to P.S. 111 in early January.The sub left after two months, she says, because she got little support and things were not going well in class.The third teacher, Millie Rodriguez, was pulled off her job as a reading specialist at P.S. 111 and handed the fifth grade just days before conferences. Parents were told their fifth graders would not get second semester report cards. "Ms. Rodriguez told me she was just there a week and had no work whatsoever to go on," said Reyna Soriano, whose son Elmer is a fifth grader. "I was so upset. How could this happen? They don't have report cards? Maybe they don't learn nothing."Maria Adame described how her daughter Karla had stopped reading at home and had no homework anymore. "I said this could affect the tests coming up," recalled Ms. Adame. "Ms. Rodriguez agreed it's not fair if they're held back. These kids were cheated. Their education was at a pause."Massiel Fernandez believes that her daughter Catherine did not score high enough on the admissions test at a middle school for gifted students because of the disruptions this year. "There was work on the test that she never saw in class," she said.Teacher retention is a chronic problem at urban schools, and P.S. 111 - serving mostly poor Hispanic children in kindergarten to eighth grade - has one of the higher turnover rates in New York City. Only 21 percent of its teachers have more than five years' experience; 54 percent have been at P.S. 111 two years or less.Yesterday it was two years since New York City teachers began working without a contract, and for those hoping the impasse will break soon, P.S. 111 is a cautionary tale. For weeks now, the city and the teachers' union have been sniping at each other in the press. By far, the issue getting the most ink is the need to reduce the time it takes to dismiss bad teachers - a pet peeve of the mayor's.While this is clearly a problem, the far bigger problem is holding on to good teachers. Last year New York City had 3,567 "regular" teachers leave, the most in memory, 936 more than the year before, and 1,100 above the previous three-year average. These are not retirees or troubled teachers - they're certified teachers in good standing.In contrast, officially bad teachers - those given unsatisfactory ratings - last year totaled between 645 (union estimate) and 800 (city estimate) of the 80,000 city teachers.Union officials say the exodus reflects the lack of a contract, plus the modest pay increase being offered by the mayor - 5 percent over three years. Elizabeth Arons, the city schools' human resources director, says her office doesn't do exit interviews and doesn't know the reasons.But Ms. Arons says teacher retention is a top priority for Chancellor Joel I. Klein. She says a new mentoring program, pairing a veteran teacher with a new one, is aimed at reducing a young teacher's feeling of isolation and improving retention. And the Teaching Fellows program - which enables new teachers to get a master's degree paid for by the city - seems to have helped. Also, in 2002, the mayor raised the first-year salary to $39,000 from $31,000, almost instantly wiping out the annual new teacher shortage.These measures have improved retention of first- and second-year teachers. Last year, 14 percent quit after one year (it was 21 percent the year before the 2002 raise).But such gains are not being sustained. By the fourth year, 44 percent of city teachers still quit, a figure that has changed little over time. In contrast, Scarsdale still has 82 percent of its teachers after 5 years.Richard Ingersoll, a University of Pennsylvania professor, says teachers nationally give four main reasons for quitting: discipline problems; lack of administrative support; too little freedom to do their jobs; and, the biggest, money. That's a problem for New York City, where the salary range is $39,000 to $81,000, versus suburban Westchester County, where the median district's range is $44,000 to $98,000. As Amy Katz, a former P.S. 111 teacher said, "You can make $20,000 more in the suburbs for an easier day with better-prepared kids and more parent support."At P.S. 111, a lot of bad forces converged. The school had a history of high teacher turnover that worsened under the current principal, Sheryl Donovan. Teachers and parents say the principal put in long hours and cared about students, but was a weak manager who failed to instill discipline - in December three P.S. 111 girls were arrested and charged with beating up a seventh grader in a hallway. Most teachers interviewed mentioned disciplinary problems as a reason for quitting.Stephen Morello, a city spokesman, said Ms. Donovan, who is leaving this month, declined to be interviewed. He said it was her decision to go and she was proud of what she had done "to improve the performance of a chronically underperforming school."But teachers described a school that was often in crisis because of turnover.Eileen McCarthy, a middle school science teacher, taught at P.S. 111 for two and a half years. She says the middle school guidance counselor quit midyear, and she was given a few free periods a week to take over the job. "I was told to help eighth graders pick a high school, and I'm only in my second year," she said. "I didn't know high schools in New York City."After two years, she said, she was burned out and quit, then returned in the spring of 2003 as a sub, teaching eighth-grade math. "I was about the fifth sub in that math class," she said. "By then, they had no interest, no notebooks, no pencils, and even though it was the second half of the year, we had to start from scratch."Natalia Mehlman, a Columbia graduate who had been considering a teaching career, spent one year at P.S. 111, then left the field. She was assigned to teach middle school Spanish, but says she was given no curriculum and no reports on pupils' past performance. "The course? I just made it up," she said. "Every night I was pulling things off the Internet. There was no support from the principal. That school was so based on getting to minimum competency on the state English and math tests - Spanish was not on the radar.""Behavior was a big problem," said Ms. Mehlman, who is getting her doctorate in history and teaching at Stanford. "It got to the point, I'd see a few kids in back quietly playing cards - I'd ignore it."Glenda Pettiford, a first-year teacher, was assigned an in-school suspension class made up of the worst-behaved middle school students from Region 9 in Manhattan. She was put in a windowless room in the basement with up to 13 pupils. Even though, under city policy, Ms. Pettiford was supposed to have a second teacher, a security guard and guidance counselor to help, she was alone much of the fall. Only after the union filed grievances did she get the extra staff, and during Christmas break, she, too, left.CITY officials would not comment on problems at P.S. 111 or the in-school suspension program, except to acknowledge that it is rare for a first-year teacher to be given such an assignment.There was some improvement at P.S. 111 this year, and that, too, says a lot about the need to hold on to veteran teachers. On the fourth-grade state English test - the most closely watched by the public - the school went up 13 points, to 57 percent passing; citywide the average increase was 9.9 points to 59.5 percent passing. The staff believes a big reason was that the four teachers assigned to fourth grade are among the most senior at P.S. 111, all with at least six years experience.In contrast, turnover has been worse at P.S. 111's middle school, and the eighth grade results were abysmal. Citywide, 32.8 percent of eighth graders passed the English test; at P.S. 111, 15 percent passed, a drop of 9 points. As for the fifth graders with no report cards - test results are expected later this month.The exodus of experienced teachers from P.S. 111 has been noticed by fifth graders, including Karla Adame and Elmer Soriano. "Ms. Macken, our second grade teacher, left," said Karla. "And Ms. Rosado in kindergarten and Ms. Haddad, first grade.""Second grade Ms. Kline," said Elmer, "and the science teacher, Ms. Astor.""Third grade Ms. Myers," said Karla."Ms. Martinez left fourth," Elmer said."A lot of teachers coming and going," Karla said, "but it doesn't matter."But Ms. McCarthy, who was hired to teach science, then turned into a guidance counselor and math sub before quitting, thinks it does. "Kids get angry," she says. "There's no consistency - adults they care about keep leaving."It haunts teachers, too. Three years later and 3,000 miles away at Stanford, Ms. Mehlman has a recurring nightmare about her year at P.S. 111: "I wake up late. I have no lesson plan. The principal is standing at the door and I don't know what to teach."E-mail: edmike@nytimes.comTeachers throughout America are being threatened into silence about what they see and hear in our nation's public schools. We must put a stop to this violation of our freedom of speech.Related website: endteacherabuse.info. The New York City Department of Education released today a list of individual ratings of thousands of the city's schoolteachers, a move that concludes a lengthy legal battle waged by the local teachers' union and media.

The Teacher Data Reports rate more than 12,000 teachers who taught fourth through eighth grade English or math between 2007 and 2010 based on value-added analysis. Value-added analysis calculates a teacher's effectiveness in improving student performance on standardized tests -- based on past test scores. The forecasted figure is compared to the student's actual scores, and the difference is considered the "value added," or subtracted, by the teachers.

To some, the release means a step forward in using student data and improving transparency and accountability by giving parents access to information on teacher effectiveness. To others, it's a misguided over-reliance on incomplete or inaccurate data that publicly shames or praises educators, whether deserving or not.

Advertisement

In response, the union, the United Federation of Teachers, has launched a city-wide newspaper advertising campaign. The ad headlines, "This Is No Way To Rate A Teacher!" followed by a lengthy and complicated mathematical formula as well as a letter from UFT President Michael Mulgrew with a list of all the reasons he says the data reports are faulty and unreliable.

The ad will likely appear in the very publications being targeted for disseminating the Teacher Data Reports. Cynthia Brown, vice president of education policy at the Center for American Progress, issued a statement statement Friday drawing on findings from a November CAP report. The study concluded that publicly naming teachers tied to the performance and projected performance of their students actually undermines efforts to improve public schools, making it much harder to implement teacher evaluation systems that actually work.

"While we support next-generation evaluation systems that include student achievement as a component, we believe the public release of value-added data on individual teachers is irresponsible," Brown said Friday. "In this case, less disclosure is more reform."

Amid the report-releasing frenzy, GothamSchools is one news organizations that has stepped back from the crowd. It was one of the many news outlets that sought access to the Teacher Data Reports last year, but after internal deliberations, determined that they would not publish the raw database because "the data were flawed, that the public might easily be misled by the ratings, and that no amount of context would justify attaching teachers' names to the statistics."

Advertisement

The Times has publicly invited teachers to respond to their ratings, to be published side-by-side for readers to consider together: "If there were special circumstances that compromise the credibility of the numbers in particular cases, we want to know."

The reports were developed as a pilot program several years ago by then-Schools Chancellor Joel Klein as a part of the city's annual review of its teachers, and were later factored into tenure decisions. The ratings were intended for internal use and were not planned to be made public. Media organizations -- among them The Wall Street Journal, The New York Times and the New York Daily News -- sued for access to the data under the Freedom of Information Act. A court ruled in favor of the news organizations in August.

"When balancing the privacy interests at stake against the public interest in disclosure of the information ... we conclude that the requested reports should be disclosed," the court wrote, according to The Wall Street Journal. "Indeed, the reports concern information of a type that is of compelling interest to the public, namely, the proficiency of public employees in the performance of their job duties."

Because the New York teacher ratings are based on small amounts of data, there exist large margins of error. To add to that, the test scores the analyses are based on were determined by the state Department of Education to have been inflated because the exams had become predictable and easier to pass -- to the extent that students were told incorrectly they were proficient in certain subjects.

Advertisement

Teachers of students who took those tests, according to the Daily News, could find themselves penalized in their Teacher Data Report ratings for not teaching to the test. Conversely, those who narrowed their curricular focus catered to the exam could be rewarded.

Other omissions and errors, like failure to verify class size and assignment for each teacher, will also skew results of the analysis.

New York high school teacher Stephen Lazar expressed on his blog -- and on a comment in The New York Times -- that he's disappointed by the decision of many publications to release the data. He points to the shortcomings of value-added systems, writing about how he spent six weeks teaching students how to do college-level research that likely cost his students 5-10 points on the Regents exam for lost time preparing for the test, but the teacher ratings "don't tell you that when you ask my students who are now in college why they are succeeding when most of their urban public school peers are dropping out, they name that research project as one of their top three reasons every time."

The city has defended the ratings, saying that they give administrators an objective way to evaluate teacher effectiveness, creating a system that identifies successful model teachers, struggling teachers who need assistance and those who should be removed.

"The reports gave teachers and principals one useful perspective on how well teachers were doing in their most important job: helping students learn," Schools Chancellor Dennis Walcott wrote in a letter to educators Thursday, The New York Times reports. "However, these reports were never intended to be public or to be used in isolation. Although we can't control how reporters use this information, we will work hard to make sure parents and the public understand how to interpret the Teacher Data Reports."

Advertisement

In a piece for the Times this week, Bill Gates, co-chair of the Bill & Melinda Gates Foundation and former Microsoft CEO, writes that value-added ratings are important in overall evaluations, as the multifaceted nature of teaching means that student test scores alone aren't comprehensive enough to measure effective teaching or identify areas for improvement. A reliable evaluation system, he says, must factor in other measures of effectiveness like student feedback and classroom observations.

"Putting sophisticated personnel systems in place is going to take serious commitment," Gates writes. "Those who believe we can do it on the cheap -- by doing things like making individual teachers' performance reports public -- are underestimating the level of resources needed to spur real improvement.. Late last week and over the weekend, New York City newspapers, including the New York Times and Wall Street Journal, published the value-added scores (teacher data reports) for thousands of the city’s teachers. Prior to this release, I and others argued that the newspapers should present margins of error along with the estimates. To their credit, both papers did so.

In the Times’ version, for example, each individual teacher’s value-added score (converted to a percentile rank) is presented graphically, for math and reading, in both 2010 and over a teacher’s “career” (averaged across previous years), along with the margins of error. In addition, both papers provided descriptions and warnings about the imprecision in the results. So, while the decision to publish was still, in my personal view, a terrible mistake, the papers at least make a good faith attempt to highlight the imprecision.

That said, they also published data from the city that use teachers’ value-added scores to label them as one of five categories: low, below average, average, above average or high. The Times did this only at the school level (i.e., the percent of each school’s teachers that are “above average” or “high”), while the Journal actually labeled each individual teacher. Presumably, most people who view the databases, particularly the Journal's, will rely heavily on these categorical ratings, as they are easier to understand than percentile ranks surrounded by error margins. The inherent problems with these ratings are what I’d like to discuss, as they illustrate important concepts about estimation error and what can be done about it.

First, let’s quickly summarize the imprecision associated with the NYC value-added scores, using the raw datasets from the city. It has been heavily reported that the average confidence interval for these estimates – the range within which we can be confident the “true estimate” falls - is 35 percentile points in math and 53 in English Language Arts (ELA). But this oversimplifies the situation somewhat, as the overall average masks quite a bit of variation by data availability. Take a look at the graph below, which shows how the average confidence interval varies by the number of years of data available, which is really just a proxy for sample size (see the figure's notes).

When you’re looking at the single-year teacher estimates (in this case, for 2009-10), the average spread is a pretty striking 46 percentile points in math and 62 in ELA. Furthermore, even with five years of data, the intervals are still quite large – about 30 points in math and 48 in ELA. (There is, however, quite a bit of improvement with additional years. The ranges are reduced by around 25 percent in both subjects when you use five years data compared with one.)

Now, opponents of value-added have expressed a great deal of outrage about these high levels of imprecision, and they are indeed extremely wide – which is one major reason why these estimates have absolutely no business being published in an online database. But, as I’ve discussed before, one major, frequently-ignored point about the error – whether in a newspaper or an evaluation system – is that the problem lies less with how much there is than how you go about addressing it.

It’s true that, even with multiple years of data, the estimates are still very imprecise. But, no matter how much data you have, if you pay attention to the error margins, you can, at least to some degree, use this information to ensure that you’re drawing defensible conclusions based on the available information. If you don’t, you can't.

This can be illustrated by taking a look at the categories that the city (and the Journal) uses to label teachers (or, in the case of the Times, schools).

Here’s how teachers are rated: low (0-4th percentile); below average (5-24); average (25-74); above average (75-94); and high (95-99).

To understand the rocky relationship between value-added margins of error and these categories, first take a look at the Times’ “sample graph” below.

This is supposed to be a sample of one teacher’s results. This particular hypothetical teacher’s value-added score was at the 50th percentile, with a margin of error of plus or minus roughly 30 percentile points. What this tells you is that we can have a high level of confidence that this teacher’s “true estimate” is somewhere between the 20th and 80th percentile (that’s the confidence interval for this teacher), although it is more likely to be closer to 50 than 20 or 80.

One shorthand way to see whether teachers scores are, accounting for error, average, above average or below average is to see whether their confidence intervals overlap with the average (50th percentile, which is actually the median, but that's a semantic point in these data).

Let’s say we have a teacher with a value-added score at the 60th percentile, plus or minus 20 points, making for a confidence interval of 40-80. This crosses the average/median “border," since the 50th percentile is within the confidence interval. So, while this teacher, based on his or her raw percentile rank, may appear to be above the average, he or she cannot really be statistically distinguished from average (because the estimate is too imprecise for us to reject the possibility that random error is the only reason for the difference).

Now, let’s say we have another teacher, scoring at the 60th percentile, but with a much smaller error margin of plus or minus 5 points. In this case, the teacher is statistically above average, since the confidence interval (55-65) doesn’t cross the average/median border.

The big point here is that each teacher’s actual score (e.g., 60th percentile) must be interpreted differently, using the error margins, which vary widely between teachers. The fact that our first teacher had a wide margin of error (20 percentile points) did not preclude accuracy, since we used that information to interpret the estimate, and the only conclusion we could reach was the less-than-exciting but accurate characterization of the teacher as not distinguishable from average.

There’s always error, but if you pay attention to each teacher’s spread individually, you can try to interpret their scores more rigorously (though it's of course no guarantee).

Somewhat in contradiction with that principle, the NYC education department’s dataset sets its own uniform definitions of average (the five categories mentioned above), which are reported by the Times (at the school-level) and Journal (for each teacher). The problem is that the cutpoints are the same for all teachers – i.e., they largely ignore variability in margins of error.*

As a consequence, a lot of these ratings present misleading information.

Let’s see how this works out with the real data. For the sake of this illustration, I will combine the “high” and “above average” categories, as well as the “low” and “below average” categories. This means our examination of the NYC scheme’s "accuracy" – at least by the alternative standard accounting for error margins – is more lenient, since we’re ignoring any discrepancies among teachers assigned to categories at the very top and very bottom of the distribution.

When you rate teachers using the NYC scheme, about half of the city’s teachers receive “average," about a quarter received “below average” or “low," and the other quarter got “above average” or “high." This of course makes perfect sense – by definition, about half of teachers would come in between the 25th and 50th percentiles, so about half would be coded as “average."

Now, let’s look at the distribution of teacher ratings when you actually account for the confidence intervals – i.e., when you define average, above average, and below average in terms of whether estimates are statistically different from the average – i.e., whether the confidence intervals overlap with the average/median.

Since sample size is a big factor here, I once again present the distribution two different ways for each subject. One of them (“multi-year”) is based on teachers’ “career” estimates, limited to those that include between 2-5 years of data. The other is on their 2010 (single-year) estimates alone. In general, the former will be more accurate (i.e., will yield a more narrow confidence interval) than the latter.

These figures tell us the percentages of teachers’ scores that we can be confident are above average (green) or below average (red), as well as those that we cannot say are different from average (yellow). In many respects, only the estimates that are statistically above or below average are “actionable” – i.e., they’re the only ones that actually provide any information that might be used in policy (or by parents). All other teachers’ scores are best interpreted as no different from average – not in the negative sense of that term, but rather in the sense that the available data do not permit us to make a (test-based) determination that they are anything other than normally competent professionals.

There are three findings to highlight in the graph. First, because the number of years of data makes a big difference in precision (as shown in the first graph), it also influences how many teachers can actually be identified as above or below average (by the standard I’m using). In math, when you use the multi-year estimates (i.e., those based on 2-5 years of data), the distribution is quite similar to the modified NYC categories, with roughly half of teachers classified as indistinguishable from the average (this actually surprised me a bit, as I thought it would be higher).

When you use only one year of data, however, almost two-thirds cannot be distinguished from average. You get a similar decrease in ELA (from 82.1 percent to 69.6 percent).

In other words, when you have more years of data, more teachers’ estimates are precise enough, one way or the other, to draw conclusions. This illustrates the importance of sample size (years of data) in determining how much useful information these estimates can provide (and the potential risk of ignoring it).

The second thing that stands out from the graph, which you may have already noticed, is the difference between math and ELA. Even when you use multiple years of data, only about three in ten ELA teachers’ estimates are discernibly above or below average, whereas the proportion in math is much higher. This tells you that value-added scores in ELA are subject to higher levels of imprecision than those in math, and that even when there are several years of data, most teachers’ ELA estimates are statistically indistinguishable from the average.

This is important, not only for designing evaluation systems (discussed here), but also for interpreting the estimates in the newspapers’ databases. If you don't pay attention to error, the same percentile rank can mean something quite different in math versus ELA in terms of precision, even across groups of teachers (e.g., schools). And, beyond the databases, the manner in which the estimates are incorporated along with other measures into systems such as evaluations - for example, whether you use percentile ranks or actual scores (usually, in standard deviations) - is very important, and can influence the final results.

The third and final takeaway here – and the one that’s most relevant to the public databases – is that the breakdowns in the graph are in most cases quite different from the breakdowns using the NYC system (in which, again, about half of teachers are “average," and about a quarter are coded as “above-" and “below average”).

In other words, the NYC categories might in many individual cases be considered misleading. Many teachers who are labeled as “above average” or “below average” by the city are not necessarily statistically different from the average, once the error margins are accounted for. Conversely, in some cases, teachers who are rated average in the NYC system are actually statistically above or below average – i.e., their percentile ranks are above or below 50, and their confidence intervals don’t overlap with the average/median.

In the graph below, I present the percent of teachers whose NYC ratings would be different under the scheme that I use – that is, they are coded in the teacher data reports as “above average” or “below average” when they are (by my ratings, based on confidence intervals) no different from average, or vice-versa.

The discrepancies actually aren’t terrible in math – about eight percent with multiple years of data, and about 13 percent when you use a single year (2010). But they’re still noteworthy. About one in ten teachers is given a label – good or bad – that is based on what might be regarded as a misinterpretation of statistical estimates.

In ELA, on the other hand, the differences are rather high – one in five teachers gets a different rating using multiple years of data, compared with almost 30 percent based on the single-year estimates.

And it’s important to reiterate that I’m not even counting the estimates categorized as “low” (0-5th percentile) or “high” (95-99th percentile). If I included those, the rates in the graph would be higher.

(Side note: At least in the older versions of the NYC teacher data reports [sample here], there is actually an asterisk placed next to any teacher’s categorical rating that is accompanied by a particularly wide margin of error, along with a warning to “interpret with caution." I'm not sure - but hope - this practice continued in the more recent versions, but, either way, the asterisks do not appear in the newspaper databases, which I assume is because they are not included in the datasets that the papers received.)

I feel it was irresponsible to publish the NYC categories. I can give a pass to the Times, since they didn’t categorize each teacher individually (only across schools). The Journal, on the other hand, did choose to label each teacher, even using estimates that are based on only year of data – most of which are, especially for ELA, just too imprecise to be meaningful, regardless of whether or not the error margins were presented too. These categorical ratings, in my opinion, made a bad situation worse.

That said, you might argue that I’m being unfair here. Both papers did, to their credit, present and explain the error margins. Moreover, in choosing an alternative categorization scheme, the newspapers would have had to impose their own judgment on data that they likely felt a responsibility to provide to the public as they were received. Put differently, if the city felt the categorical ratings were important and valid, then it might be seen as poor journalistic practice, perhaps even unethical, to alter them.

That’s all fair enough – this is just my opinion, and the importance of this issue pales in comparison with the (in my view, wrong) decision to publish in the first place. But I would point out that the Times did make the choice to withhold the categories for individual teachers, so at least one newspaper felt omitting them was the right thing to do.

(On a related note, the Los Angeles Times value-added database, released in 2010, also applied labels to individual teachers, but they actually required that teachers have at least 60 students before publishing their names. Had the NYC papers applied the LA Times’ standard, they would have withheld almost 30 percent of teachers’ multi-year scores, and virtually all of their single-year scores.)

In any case, if there’s a bright spot to this whole affair, it is the fact that value-added error margins are finally getting the attention they are due.

And, no matter what you think of the published categorical ratings (or the decision to publish the database), all the issues discussed above – the need to account for error margins, the importance of sample size, the difference in results between subjects, etc. – carry very important implications for the use of value-added and other growth model estimates in evaluations, compensation, and other policy contexts. Although many systems are still on the drawing board, most of the new evaluations now being rolled out make the same "mistakes" as are reflected in the newspaper databases.**

So, I’m also hoping that more people will start asking the big question: If the error margins are important to consider when publishing these estimates in the newspaper, why are they still being ignored in most actual teacher evaluations?

In a future post, I’ll take a look at other aspects of the NYC data.

- Matt Di Carlo

*****

* It’s not quite accurate to say that these categories completely ignore the error margins, since they code teachers as average if they’re within 25 percentile points of the median (25-75). In other words, they don’t just call a teacher “below average” if he or she has a score below 50. They do allow for some spread, but it’s just the same for all teachers (the extremely ill-advised alternative is to code teachers as above average if they’re above 50, below if they’re below). One might also argue that there’s a qualitative difference between, on the hand, the city categorizing its teachers in reports that only administrators and teachers will see, and, on the other hand, publishing these ratings in the newspaper.

** In fairness, there are some (but very few) exceptions, such as Hillsborough County, Florida, which requires that teachers have at least 60 students for their estimates to count in formal evaluations. In addition, some of the models being used, such as D.C.’s, use a statistical technique called “shrinking," by which estimates are adjusted according to sample size. If any readers know of other exceptions, please leave a comment.. 