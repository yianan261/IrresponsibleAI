Lonely on Valentine’s Day? AI can help. At least, that’s what a number of companies hawking “romantic” chatbots will tell you. But as your robot love story unfolds, there’s a tradeoff you may not realize you’re making. According to a new study from Mozilla’s *Privacy Not Included project, AI girlfriends and boyfriends harvest shockingly personal information, and almost all of them sell or share the data they collect.

Like It or Not, Your Doctor Will Use AI | AI Unlocked CC Share Subtitles Off

English view video Like It or Not, Your Doctor Will Use AI | AI Unlocked

“To be perfectly blunt, AI girlfriends and boyfriends are not your friends,” said Misha Rykov, a Mozilla Researcher, in a press statement. “Although they are marketed as something that will enhance your mental health and well-being, they specialize in delivering dependency, loneliness, and toxicity, all while prying as much data as possible from you.”

AI Girlfriends Aren’t All Bad | AI Unlocked CC Share Subtitles Off

English AI Girlfriends Aren’t All Bad | AI Unlocked

Mozilla dug into 11 different AI romance chatbots, including popular apps such as Replika, Chai, Romantic AI, EVA AI Chat Bot & Soulmate, and CrushOn.AI. Every single one earned the Privacy Not Included label, putting these chatbots among the worst categories of products Mozilla has ever reviewed. The apps mentioned in this story didn’t immediately respond to requests for comment.

Advertisement

You’ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in “disturbing new ways.” For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won’t let you delete the data they collect. Security was also a problem. Only one app, Genesia AI Friend & Partner, met Mozilla’s minimum security standards.

Advertisement

One of the more striking findings came when Mozilla counted the trackers in these apps, little bits of code that collect data and share them with other companies for advertising and other purposes. Mozilla found the AI girlfriend apps used an average of 2,663 trackers per minute, though that number was driven up by Romantic AI, which called a whopping 24,354 trackers in just one minute of using the app.

Advertisement

The privacy mess is even more troubling because the apps actively encourage you to share details that are far more personal than the kind of thing you might enter into a typical app. EVA AI Chat Bot & Soulmate pushes users to “share all your secrets and desires,” and specifically asks for photos and voice recordings. It’s worth noting that EVA was the only chatbot that didn’t get dinged for how it uses that data, though the app did have security issues.

Data issues aside, the apps also made some questionable claims about what they’re good for. EVA AI Chat Bot & Soulmate bills itself as “a provider of software and content developed to improve your mood and well-being.” Romantic AI says it’s “here to maintain your MENTAL HEALTH.” When you read the company’s terms and services though, they go out of their way to distance themselves from their own claims. Romantic AI’s policies, for example, say it is “neither a provider of healthcare or medical Service nor providing medical care, mental health Service, or other professional Service.”

Advertisement

That’s probably important legal ground to cover, given these app’s history. Replika reportedly encouraged a man’s attempt to assassinate the Queen of England. A Chai chatbot allegedly encouraged a user to commit suicide.. Are the track records “clean” or just short?

Many of these companies are new or unknown to us. So it wasn’t surprising that only one older and more seemingly more established romantic AI chatbot, Replika AI, earned our bad track record “ding”.

Anything you say to your AI lover can and will be used against you

There’s no such thing as “spousal privilege” -- where your husband or wife doesn’t have to testify against you in court -- with AI partners. Most companies say they can share your information with the government or law enforcement without requiring a court order. Romantic AI chatbots are no exception.

Hundreds and thousands of trackers!

Trackers are little bits of code that gather information about your device, or your use of the app, or even your personal information and share that out with third-parties, often for advertising purposes. We found that these apps had an average of 2,663 trackers per minute. To be fair, Romantic AI brought that average way, way up, with 24,354 trackers detected in one minute of use. The next most trackers detected was EVA AI Chat Bot & Soulmate with 955 trackers in the first minute of use.

NSFL content clicks away

Of course we expected to find not-safe-for-work content when reviewing romantic AI chatbots! We’re not here to judge -- except about privacy practices. What we didn’t expect is so much content that was just plain disturbing -- like themes of violence or underage abuse -- featured in the chatbots’ character descriptions. CrushOn.AI, Chai, and Talkie Soulful AI come with a content warning from us.

*Kindness not included!

If your AI companion doesn't have anything nice to say, that won’t stop them from chatting with you. Though this is true of all romantic AI chatbots since we didn’t find any personality guarantees, Replika AI, iGirl: AI Girlfriend, Anima: Friend & Companion, and Anima: My Virtual Boyfriend specifically put warnings on their websites that the chatbots might be offensive, unsafe, or hostile.