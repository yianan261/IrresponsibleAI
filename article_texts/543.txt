On Monday, a tweeted AI-generated image suggesting a large explosion at the Pentagon led to brief confusion, which included a reported small drop in the stock market. It originated from a verified Twitter account named "Bloomberg Feed," unaffiliated with the well-known Bloomberg media company, and was quickly exposed as a hoax. However, before it was debunked, large accounts such as Russia Today had already spread the misinformation, The Washington Post reported.

The fake image depicted a large plume of black smoke alongside a building vaguely reminiscent of the Pentagon with the tweet "Large Explosion near The Pentagon Complex in Washington D.C. — Inital Report." Upon closer inspection, local authorities confirmed that the image was not an accurate representation of the Pentagon. Also, with blurry fence bars and building columns, it looks like a fairly sloppy AI-generated image created by a model like Stable Diffusion.

Before Twitter suspended the false Bloomberg account, it had a total post count of 224,000 tweets and had reached fewer than 1,000 followers overall, according to the Post, but it's unclear who ran it or the motives behind sharing the false image. In addition to Bloomberg Feed, other accounts that shared the false report include “Walter Bloomberg” and “Breaking Market News," both unaffiliated with the real Bloomberg organization.

This incident underlines the potential threats AI-generated images may present in the realm of hastily shared social media—and a paid verification system on Twitter. In March, fake images of Donald Trump's arrest created with Midjourney reached a wide audience. While clearly marked as fake, they sparked fears of mistaking them for real photos due to their realism. That same month, AI-generated images of Pope Francis in a white coat fooled many who saw them on social media.

Advertisement

The pope in puffy coats is one thing, but when someone features a government subject like the headquarters of the United States Department of Defense in a fake tweet, the consequences could potentially be more severe. Aside from general confusion on Twitter, the deceptive tweet may have affected the stock market. The Washington Post says that the Dow Jones Industrial Index dropped 85 points in four minutes after the tweet spread but rebounded quickly.

Much of the confusion over the false tweet may have been made possible by changes at Twitter under its new owner, Elon Musk. Musk fired content moderation teams shortly after his takeover and largely automated the account verification process, transitioning it to a system where anyone can pay to have a blue check mark. Critics argue that practice makes the platform more susceptible to misinformation.

While authorities easily picked out the explosion photo as a fake due to inaccuracies, the presence of image synthesis models like Midjourney and Stable Diffusion means it no longer takes artistic skill to create convincing fakes, lowering the barriers to entry and opening the door to potentially automated misinformation machines. The ease of creating fakes, coupled with the viral nature of a platform like Twitter, means that false information can spread faster than it can be fact-checked.

But in this case, the image did not need to be high quality to make an impact. Sam Gregory, the executive director of the human rights organization Witness, pointed out to The Washington Post that when people want to believe, they let down their guard and fail to look into the veracity of the information before sharing it. He described the false Pentagon image as a "shallow fake" (as opposed to a more convincing "deepfake").

"The way people are exposed to these shallow fakes, it doesn’t require something to look exactly like something else for it to get attention," he said. “People will readily take and share things that don’t look exactly right but feel right.”. Generative artificial intelligence is often considered a threat to white-collar workers employed in the knowledge sector. But what about capital markets?

Investors got a taste of what may be to come after fake images of smoke billowing next to U.S. Defense Department headquarters spread via Elon Musk’s social media platform Twitter.

“There was a very brief selloff in markets yesterday after unconfirmed reports circulated on Twitter about an explosion near the U.S. Pentagon,” wrote Jim Reid, Deutsche Bank’s head of global economics and thematic research, on Tuesday.

“Given the suggestions that the initial photo might have been A.I.-generated, it just shows the potential pitfalls for markets if fake news driven by A.I. can cause concrete movements in asset prices.”

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

No one knows just who exactly created the images or why, but they were released at a time coinciding with the last-minute game of chicken over the debt ceiling playing out within Washington, D.C.’s Beltway.

That means investors are already on edge as they attempt to price in contingencies of a default that Moody’s argues will not spare any corner of the global economy.

Yields on the one-month Treasury bill maturing in June hit a record high of 5.9% after bondholders demanded a higher compensation in light of a potential missed payment by the federal government.

Musk’s attempt to crack down on fake accounts

Deepfakes have proliferated of late as virtually anyone willing to pay money to generative A.I. platform Midjourney can now quickly and easily publish deceptive images such as the Pope sporting an all-white Balenciaga puffer jacket.

But Monday’s brief selloff could be the first time generative A.I. has had a demonstrable effect on financial markets.

The episode could prove damaging to Musk, who has sought to portray Twitter as the most accurate source of information on earth by charging customers for verification.

“The goal is to make this platform maximum truth-seeking or, said another way, the least untrue compared to everything else,” he wrote at the end of April.

Musk, who purchased the social media company last year for $44 billion, has argued his premium subscription service Twitter Blue serves as an effective deterrent for bots and fake accounts since it “significantly increases the cost.”

Twitter needs to become by far the most accurate source of information about the world. That’s our mission. — Elon Musk (@elonmusk) November 7, 2022

That however has not proved to be the case, as numerous accounts have successfully impersonated people and brands, angering advertisers in the process.

Critics argue it is in reality an attempt to milk Twitter customers for much-needed revenue since all legacy accounts already verified lost their status if they refused to pay Musk.

Even if Monday’s market reaction was brief after the Pentagon confirmed there had been no explosion and the tweet was quickly deleted, a potential bad actor had plenty of time to pocket ill-gotten gains from manipulating the market—if that had in fact been the plan all along.

This threat “could be a growing issue over the months and years ahead,” Deutsche Bank’s Reid argued on Monday, “particularly if the technology is able to provide increasingly convincing images.”. On Monday morning, a verified Twitter account called Bloomberg Feed shared an ominous tweet. Beneath the words, “Large Explosion near The Pentagon Complex in Washington, D.C. - Initial Report,” it showed an image of a huge plume of black smoke next to a vaguely Pentagon-like building. On closer inspection, the image was a fake, likely generated by artificial intelligence, and the report of an explosion was quickly debunked — though not before it was picked up by large accounts, including the Russian state media organ Russia Today. The tweet may have also briefly moved the stock market, as the Dow Jones Industrial Index dropped 85 points within four minutes, then rebounded just as quickly.

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

All in all, the hoax — the latest in a string of AI-generated images to fool some social media users — appears to have done little immediate damage. Twitter has since suspended the Bloomberg Feed account, which was not related to the real Bloomberg media organization, and within about 20 minutes, local authorities had debunked the report.

Advertisement

“Just looking at the image itself, that’s not the Pentagon,” said Nate Hiner, a captain with the fire department in Arlington, Va., where the Pentagon is located. “I have no idea what that building is. There’s no building that looks like that in Arlington.”

Yet the mechanisms involved, from the image’s amplification by large propaganda accounts to the almost instantaneous response from the stock market, suggest the potential for more such mischief if AI tools continue to make inroads in fields such as social media moderation, news writing and stock trading.

And Twitter is looking like an increasingly likely vector, as new owner Elon Musk has gutted its human workforce, laid off a team that used to fact-check viral trends, and changed account verification from a manual authentication process to one that’s largely automated and pay-for-play. The signature blue badges once indicated authority for public figures, large organizations, celebrities and others at risk of impersonation. Now, Twitter awards them to any one willing to pay $8 a month and confirm their phone number.

Advertisement

Twitter did not respond to a request for comment.

With experts predicting that AI will impact millions of human jobs, the concern becomes not just whether AI-generated misinformation might mislead people, but whether it might mislead its fellow automated systems.

“This isn’t an AI issue, per se,” said Renée DiResta, research manager at Stanford Internet Observatory and an expert on how misinformation circulates. “Anyone with Photoshop experience could have made that image — ironically, could probably have done it better. But it’s a look at how signals that help people decide whether information about breaking news is trustworthy on Twitter have been rendered useless, just as the capacity to create high-resolution unreality has been made available to everyone.”

Verified accounts spread the news

One of the first accounts to post about the fake event was a verified account called OSINTdefender, which tweeted the report of the explosion, along with the bogus image, to its 336,000 followers at 10:04 a.m. The building in the photograph looks little like the Pentagon, but it bears some the hallmarks of being AI-generated. The tweet has been deleted.

Advertisement

Reached via Twitter, the owner of the OSINTDefender account said they had first heard the report on the social platform Discord a few minutes earlier from a user who goes by the handle “Walter Bloomberg.” They said the image came from the Facebook page, since deleted, of a person who claimed to work in Arlington.

In the next few minutes, other large accounts posted similarly worded false reports on Twitter, including Walter Bloomberg, who goes by the same handle on both platforms. His 10:06 a.m. tweet garnered at least 730,000 views.

Many of the accounts that reshared it pose as aggregators of financial news, including a 386,000-follower account with the handle @financialjuice, named “Breaking Market News,” and another account called “Bloomberg Feed” that is unrelated to the real Bloomberg.

Some of the accounts had blue “verified” check marks, while legitimate organizations that shared the truth did not. The official account for the Pentagon Force Protection Agency, which polices the Pentagon, doesn’t pay for a blue check mark, and Twitter has not given it a gray check mark indicating it’s a verified institution. The agency retweeted a local law-enforcement message saying there was “NO explosion” at 10:27 a.m.; the tweet had only 78,000 views as of 4 p.m.

Local authorities scramble

Hiner, the Arlington Fire captain who handles the Northern Virginia department’s emergency communications, said it took about five minutes for him to realize the reports on Twitter were fake.

Advertisement

At 10:10 a.m., Hiner was in a meeting when he got the first call. He stepped out of the meeting to investigate.

The first sign something was off? He had not received any alerts from the department’s emergency software, First Due, which monitors dispatch and sends him a push notification when first responders are sent out for major incidents like fires.

Next, he checked his mobile data terminal — essentially a laptop that lists every active 911 incident in Arlington — and found no sign of anything going on near the Pentagon.

“There were no medical calls, no fire calls, no incidents whatsoever,” he said.

That’s when he finally pulled up social media himself, expecting to see some eyewitness accounts on Twitter. But again, there was nothing. All he saw was the doctored photo of the explosion.

At that point, he reached out to spokesmen at the Defense Department and at the Pentagon Force Protection Agency. By 10:27 a.m., he’d posted on Arlington Fire’s Twitter account that the reports were false.

Advertisement

“There is NO explosion or incident taking place at or near the Pentagon reservation,” the tweet said, “and there is no immediate danger or hazards to the public.”

Hiner said that he sometimes receives odd inquiries from Arlington residents after seeing a firetruck in their neighborhood or gets misguided calls based on scanner traffic. But he cannot recall another time, he said, “in which an emergency incident was being reported on social media that was just 100 percent inaccurate.”

New twist on an old problem

From Photoshopped images of a shark on a highway during Hurricane Sandy to false reports of celebrity deaths, viral lies are nothing new on Twitter. Generative AI tools, from chatbots such as ChatGPT that can pen fake news stories to AI art tools such as Midjourney and Stable Diffusion, are only the newest tools in the hoaxsters’ kit. They’ve been used in recent months to create other viral images, including one that appeared to show Donald Trump getting arrested and another depicting Pope Francis making a fashion statement.

Advertisement

For the most part, mainstream media outlets have successfully refuted the misinformation, and the world has marched on as before. Still, some hoaxes have wrought chaos, to varying degrees. In 2013, a fake tweet about an attack on the White House touched off a quick drop in financial markets.

Over time, social media users and the news media have learned to turn a skeptical eye on viral reports, especially from unverified sources. But Twitter’s new verification system means that the blue check mark, once a visual shortcut that conveyed a modicum of authority on an account, no longer serves that function.

Sam Gregory, the executive director of the human rights organization Witness, whose group has studied fake images and disinformation, said the Pentagon explosion image tweeted Monday carries multiple hallmarks of a fake, including visual glitches and an inaccurate view of the Pentagon. The challenge with such fakes, Gregory said, is the speed with which they can blast across the internet.

Advertisement

“These circulate rapidly, and the ability to do that fact-check or debunk or verification at an institutional level moves slower and doesn’t reach the same people,” he said.

Though the image may be obviously fake to some, the fact that it was attached to an authoritative-sounding claim made it that much more likely to gain attention, Gregory added.

“The way people are exposed to these shallow fakes, it doesn’t require something to look exactly like something else for it to get attention,” he said. “People will readily take and share things that don’t look exactly right but feel right.”

As for why the fakes were shared, it’s unclear. Some fakes have been shared to score political points, while others have been used to troll or build an audience that the account may hope to monetize.

“Sometimes they’re doing it maliciously, or sometimes they’re just doing it to get a lot of views,” he said. “You can get a lot of audience very quickly from this, and that is a powerful drug.”

Advertisement

Arlington County Board Chair Christian Dorsey (D) said local governments like Arlington’s face an increasingly steep challenge in responding to misinformation as AI makes it easier to rapidly generate plausible fakes. He said officials try to guide residents to follow local authorities on Twitter and turn to them for reliable information rather than “some random Twitter handle.” Arlington County, and its police and fire/EMS departments are all verified with a “silver check” on Twitter, indicating that they’re government-run accounts.

But he recognizes that may not be enough.

“Our number of followers pales in comparison to some of the most popular social media accounts out there. You always run the risk that they’re not going to penetrate as deeply,” Dorsey said. But “absent any magic bullet, where these platforms ensure only the best truthful information is relayed, I think it’s the best we can do.”. Media, technology and financial industries are not prepared for an onslaught of highly convincing fake images which could crash stock markets and spark political unrest, an artificial intelligence expert says.

An AI-generated image of an explosion next to what was claimed to be a building in the Pentagon was shared online on Monday, after being released on Twitter by a user posing as news organisation Bloomberg.

As it circulated, the S&P 500 stock exchange fell by about 0.3 per cent, to a session low, before rebounding when it was confirmed fake. RT, the Russian state-controlled TV news channel, tweeted the fake image then deleted the tweet shortly afterwareds.

Henry Ajder, an expert on deepfake images, told i the image was the “first really significant one to essentially spook the markets”.

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

Even something that was discovered to be false within a short timeframe could have a damaging impact. In the hands of bad actors, these images could be used to crash the market or make the stock price soar for a favourable return, said Mr Ajder, who presents BBC Radio 4’s The Future Will Be Synthesised.

“Moving forward, it’s just not going to be possible for even well-trained eyes in a lot of cases, from the image alone, to spot a fake.”

The photo was posted by a user under the name of “Bloomberg Feed” – which had a blue tick next to it, meaning the creators paid the required $8 per month to Twitter for a “verified” account.

The real Bloomberg news site actually has a gold tick beside it – which mean it is the official account of a business of other private organisation. However, judging whether an account is genuine would require the public to kow what a gold tick is – calling into question Twitter’s current verification system, Mr Ajder said.

An explosion at the Pentagon would be easy to quickly verify on the ground, but that might not be the case for an explosion for a village in Ukraine, he added, and the rapid creation and increasing sophistication of such images posed a challenge for those looking for news and for journalists seeking to verify events in a fast-paced environment.

“In a world where you can generate any kind of content in a realistic fashion with AI at scale and within minutes, you could create 200 different images of this fire from different perspectives,” said Mr Ajder. “I think it really speaks to how our information infrastructure and media infrastructure is not prepared for this AI-generated content.”

Last week, Open AI chief executive Sam Altman testified before US congress, agreeing with the need for AI regulation and saying, “I think if this technology goes wrong, it can go quite wrong,” The New York Times reported.

Mr Ajder said the current approach to AI risks was not fair as it is focused on how ordinary people ca spot fakes, rather than on demanding regulation from technology companies or spending more money on content moderation. News organisations and technology companies weren’t “prepared” for what might come, he added.

“It’s a perfect storm where traditional news media takes time to verify, the algorithmic nature of platform rewards controversial and sensational content and the platforms don’t have much in the way of strong content moderation to filter out these fakes,” he said.. It could be the first instance of an AI-generated image sowing enough confusion to move stock markets.
