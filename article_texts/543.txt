On Monday, a tweeted AI-generated image suggesting a large explosion at the Pentagon led to brief confusion, which included a reported small drop in the stock market. It originated from a verified Twitter account named "Bloomberg Feed," unaffiliated with the well-known Bloomberg media company, and was quickly exposed as a hoax. However, before it was debunked, large accounts such as Russia Today had already spread the misinformation, The Washington Post reported.

The fake image depicted a large plume of black smoke alongside a building vaguely reminiscent of the Pentagon with the tweet "Large Explosion near The Pentagon Complex in Washington D.C. — Inital Report." Upon closer inspection, local authorities confirmed that the image was not an accurate representation of the Pentagon. Also, with blurry fence bars and building columns, it looks like a fairly sloppy AI-generated image created by a model like Stable Diffusion.

Before Twitter suspended the false Bloomberg account, it had a total post count of 224,000 tweets and had reached fewer than 1,000 followers overall, according to the Post, but it's unclear who ran it or the motives behind sharing the false image. In addition to Bloomberg Feed, other accounts that shared the false report include “Walter Bloomberg” and “Breaking Market News," both unaffiliated with the real Bloomberg organization.

This incident underlines the potential threats AI-generated images may present in the realm of hastily shared social media—and a paid verification system on Twitter. In March, fake images of Donald Trump's arrest created with Midjourney reached a wide audience. While clearly marked as fake, they sparked fears of mistaking them for real photos due to their realism. That same month, AI-generated images of Pope Francis in a white coat fooled many who saw them on social media.

Advertisement

The pope in puffy coats is one thing, but when someone features a government subject like the headquarters of the United States Department of Defense in a fake tweet, the consequences could potentially be more severe. Aside from general confusion on Twitter, the deceptive tweet may have affected the stock market. The Washington Post says that the Dow Jones Industrial Index dropped 85 points in four minutes after the tweet spread but rebounded quickly.

Much of the confusion over the false tweet may have been made possible by changes at Twitter under its new owner, Elon Musk. Musk fired content moderation teams shortly after his takeover and largely automated the account verification process, transitioning it to a system where anyone can pay to have a blue check mark. Critics argue that practice makes the platform more susceptible to misinformation.

While authorities easily picked out the explosion photo as a fake due to inaccuracies, the presence of image synthesis models like Midjourney and Stable Diffusion means it no longer takes artistic skill to create convincing fakes, lowering the barriers to entry and opening the door to potentially automated misinformation machines. The ease of creating fakes, coupled with the viral nature of a platform like Twitter, means that false information can spread faster than it can be fact-checked.

But in this case, the image did not need to be high quality to make an impact. Sam Gregory, the executive director of the human rights organization Witness, pointed out to The Washington Post that when people want to believe, they let down their guard and fail to look into the veracity of the information before sharing it. He described the false Pentagon image as a "shallow fake" (as opposed to a more convincing "deepfake").

"The way people are exposed to these shallow fakes, it doesn’t require something to look exactly like something else for it to get attention," he said. “People will readily take and share things that don’t look exactly right but feel right.”. Generative artificial intelligence is often considered a threat to white-collar workers employed in the knowledge sector. But what about capital markets?

Investors got a taste of what may be to come after fake images of smoke billowing next to U.S. Defense Department headquarters spread via Elon Musk’s social media platform Twitter.

“There was a very brief selloff in markets yesterday after unconfirmed reports circulated on Twitter about an explosion near the U.S. Pentagon,” wrote Jim Reid, Deutsche Bank’s head of global economics and thematic research, on Tuesday.

“Given the suggestions that the initial photo might have been A.I.-generated, it just shows the potential pitfalls for markets if fake news driven by A.I. can cause concrete movements in asset prices.”

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

No one knows just who exactly created the images or why, but they were released at a time coinciding with the last-minute game of chicken over the debt ceiling playing out within Washington, D.C.’s Beltway.

That means investors are already on edge as they attempt to price in contingencies of a default that Moody’s argues will not spare any corner of the global economy.

Yields on the one-month Treasury bill maturing in June hit a record high of 5.9% after bondholders demanded a higher compensation in light of a potential missed payment by the federal government.

Musk’s attempt to crack down on fake accounts

Deepfakes have proliferated of late as virtually anyone willing to pay money to generative A.I. platform Midjourney can now quickly and easily publish deceptive images such as the Pope sporting an all-white Balenciaga puffer jacket.

But Monday’s brief selloff could be the first time generative A.I. has had a demonstrable effect on financial markets.

The episode could prove damaging to Musk, who has sought to portray Twitter as the most accurate source of information on earth by charging customers for verification.

“The goal is to make this platform maximum truth-seeking or, said another way, the least untrue compared to everything else,” he wrote at the end of April.

Musk, who purchased the social media company last year for $44 billion, has argued his premium subscription service Twitter Blue serves as an effective deterrent for bots and fake accounts since it “significantly increases the cost.”

Twitter needs to become by far the most accurate source of information about the world. That’s our mission. — Elon Musk (@elonmusk) November 7, 2022

That however has not proved to be the case, as numerous accounts have successfully impersonated people and brands, angering advertisers in the process.

Critics argue it is in reality an attempt to milk Twitter customers for much-needed revenue since all legacy accounts already verified lost their status if they refused to pay Musk.

Even if Monday’s market reaction was brief after the Pentagon confirmed there had been no explosion and the tweet was quickly deleted, a potential bad actor had plenty of time to pocket ill-gotten gains from manipulating the market—if that had in fact been the plan all along.

This threat “could be a growing issue over the months and years ahead,” Deutsche Bank’s Reid argued on Monday, “particularly if the technology is able to provide increasingly convincing images.”. On Monday morning, a verified Twitter account called Bloomberg Feed shared an ominous tweet. Beneath the words, “Large Explosion near The Pentagon Complex in Washington, D.C. - Initial Report,” it showed an image of a huge plume of black smoke next to a vaguely Pentagon-like building. On closer inspection, the image was a fake, likely generated by artificial intelligence, and the report of an explosion was quickly debunked — though not before it was picked up by large accounts, including the Russian state media organ Russia Today. The tweet may have also briefly moved the stock market, as the Dow Jones Industrial Index dropped 85 points within four minutes, then rebounded just as quickly.

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

All in all, the hoax — the latest in a string of AI-generated images to fool some social media users — appears to have done little immediate damage. Twitter has since suspended the Bloomberg Feed account, which was not related to the real Bloomberg media organization, and within about 20 minutes, local authorities had debunked the report.

Advertisement

“Just looking at the image itself, that’s not the Pentagon,” said Nate Hiner, a captain with the fire department in Arlington, Va., where the Pentagon is located. “I have no idea what that building is. There’s no building that looks like that in Arlington.”

Yet the mechanisms involved, from the image’s amplification by large propaganda accounts to the almost instantaneous response from the stock market, suggest the potential for more such mischief if AI tools continue to make inroads in fields such as social media moderation, news writing and stock trading.

And Twitter is looking like an increasingly likely vector, as new owner Elon Musk has gutted its human workforce, laid off a team that used to fact-check viral trends, and changed account verification from a manual authentication process to one that’s largely automated and pay-for-play. The signature blue badges once indicated authority for public figures, large organizations, celebrities and others at risk of impersonation. Now, Twitter awards them to any one willing to pay $8 a month and confirm their phone number.

Advertisement

Twitter did not respond to a request for comment.

With experts predicting that AI will impact millions of human jobs, the concern becomes not just whether AI-generated misinformation might mislead people, but whether it might mislead its fellow automated systems.

“This isn’t an AI issue, per se,” said Renée DiResta, research manager at Stanford Internet Observatory and an expert on how misinformation circulates. “Anyone with Photoshop experience could have made that image — ironically, could probably have done it better. But it’s a look at how signals that help people decide whether information about breaking news is trustworthy on Twitter have been rendered useless, just as the capacity to create high-resolution unreality has been made available to everyone.”

Verified accounts spread the news

One of the first accounts to post about the fake event was a verified account called OSINTdefender, which tweeted the report of the explosion, along with the bogus image, to its 336,000 followers at 10:04 a.m. The building in the photograph looks little like the Pentagon, but it bears some the hallmarks of being AI-generated. The tweet has been deleted.

Advertisement

Reached via Twitter, the owner of the OSINTDefender account said they had first heard the report on the social platform Discord a few minutes earlier from a user who goes by the handle “Walter Bloomberg.” They said the image came from the Facebook page, since deleted, of a person who claimed to work in Arlington.

In the next few minutes, other large accounts posted similarly worded false reports on Twitter, including Walter Bloomberg, who goes by the same handle on both platforms. His 10:06 a.m. tweet garnered at least 730,000 views.

Many of the accounts that reshared it pose as aggregators of financial news, including a 386,000-follower account with the handle @financialjuice, named “Breaking Market News,” and another account called “Bloomberg Feed” that is unrelated to the real Bloomberg.

Some of the accounts had blue “verified” check marks, while legitimate organizations that shared the truth did not. The official account for the Pentagon Force Protection Agency, which polices the Pentagon, doesn’t pay for a blue check mark, and Twitter has not given it a gray check mark indicating it’s a verified institution. The agency retweeted a local law-enforcement message saying there was “NO explosion” at 10:27 a.m.; the tweet had only 78,000 views as of 4 p.m.

Local authorities scramble

Hiner, the Arlington Fire captain who handles the Northern Virginia department’s emergency communications, said it took about five minutes for him to realize the reports on Twitter were fake.

Advertisement

At 10:10 a.m., Hiner was in a meeting when he got the first call. He stepped out of the meeting to investigate.

The first sign something was off? He had not received any alerts from the department’s emergency software, First Due, which monitors dispatch and sends him a push notification when first responders are sent out for major incidents like fires.

Next, he checked his mobile data terminal — essentially a laptop that lists every active 911 incident in Arlington — and found no sign of anything going on near the Pentagon.

“There were no medical calls, no fire calls, no incidents whatsoever,” he said.

That’s when he finally pulled up social media himself, expecting to see some eyewitness accounts on Twitter. But again, there was nothing. All he saw was the doctored photo of the explosion.

At that point, he reached out to spokesmen at the Defense Department and at the Pentagon Force Protection Agency. By 10:27 a.m., he’d posted on Arlington Fire’s Twitter account that the reports were false.

Advertisement

“There is NO explosion or incident taking place at or near the Pentagon reservation,” the tweet said, “and there is no immediate danger or hazards to the public.”

Hiner said that he sometimes receives odd inquiries from Arlington residents after seeing a firetruck in their neighborhood or gets misguided calls based on scanner traffic. But he cannot recall another time, he said, “in which an emergency incident was being reported on social media that was just 100 percent inaccurate.”

New twist on an old problem

From Photoshopped images of a shark on a highway during Hurricane Sandy to false reports of celebrity deaths, viral lies are nothing new on Twitter. Generative AI tools, from chatbots such as ChatGPT that can pen fake news stories to AI art tools such as Midjourney and Stable Diffusion, are only the newest tools in the hoaxsters’ kit. They’ve been used in recent months to create other viral images, including one that appeared to show Donald Trump getting arrested and another depicting Pope Francis making a fashion statement.

Advertisement

For the most part, mainstream media outlets have successfully refuted the misinformation, and the world has marched on as before. Still, some hoaxes have wrought chaos, to varying degrees. In 2013, a fake tweet about an attack on the White House touched off a quick drop in financial markets.

Over time, social media users and the news media have learned to turn a skeptical eye on viral reports, especially from unverified sources. But Twitter’s new verification system means that the blue check mark, once a visual shortcut that conveyed a modicum of authority on an account, no longer serves that function.

Sam Gregory, the executive director of the human rights organization Witness, whose group has studied fake images and disinformation, said the Pentagon explosion image tweeted Monday carries multiple hallmarks of a fake, including visual glitches and an inaccurate view of the Pentagon. The challenge with such fakes, Gregory said, is the speed with which they can blast across the internet.

Advertisement

“These circulate rapidly, and the ability to do that fact-check or debunk or verification at an institutional level moves slower and doesn’t reach the same people,” he said.

Though the image may be obviously fake to some, the fact that it was attached to an authoritative-sounding claim made it that much more likely to gain attention, Gregory added.

“The way people are exposed to these shallow fakes, it doesn’t require something to look exactly like something else for it to get attention,” he said. “People will readily take and share things that don’t look exactly right but feel right.”

As for why the fakes were shared, it’s unclear. Some fakes have been shared to score political points, while others have been used to troll or build an audience that the account may hope to monetize.

“Sometimes they’re doing it maliciously, or sometimes they’re just doing it to get a lot of views,” he said. “You can get a lot of audience very quickly from this, and that is a powerful drug.”

Advertisement

Arlington County Board Chair Christian Dorsey (D) said local governments like Arlington’s face an increasingly steep challenge in responding to misinformation as AI makes it easier to rapidly generate plausible fakes. He said officials try to guide residents to follow local authorities on Twitter and turn to them for reliable information rather than “some random Twitter handle.” Arlington County, and its police and fire/EMS departments are all verified with a “silver check” on Twitter, indicating that they’re government-run accounts.

But he recognizes that may not be enough.

“Our number of followers pales in comparison to some of the most popular social media accounts out there. You always run the risk that they’re not going to penetrate as deeply,” Dorsey said. But “absent any magic bullet, where these platforms ensure only the best truthful information is relayed, I think it’s the best we can do.”. Media, technology and financial industries are not prepared for an onslaught of highly convincing fake images which could crash stock markets and spark political unrest, an artificial intelligence expert says.

An AI-generated image of an explosion next to what was claimed to be a building in the Pentagon was shared online on Monday, after being released on Twitter by a user posing as news organisation Bloomberg.

As it circulated, the S&P 500 stock exchange fell by about 0.3 per cent, to a session low, before rebounding when it was confirmed fake. RT, the Russian state-controlled TV news channel, tweeted the fake image then deleted the tweet shortly afterwareds.

Henry Ajder, an expert on deepfake images, told i the image was the “first really significant one to essentially spook the markets”.

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

Even something that was discovered to be false within a short timeframe could have a damaging impact. In the hands of bad actors, these images could be used to crash the market or make the stock price soar for a favourable return, said Mr Ajder, who presents BBC Radio 4’s The Future Will Be Synthesised.

“Moving forward, it’s just not going to be possible for even well-trained eyes in a lot of cases, from the image alone, to spot a fake.”

The photo was posted by a user under the name of “Bloomberg Feed” – which had a blue tick next to it, meaning the creators paid the required $8 per month to Twitter for a “verified” account.

The real Bloomberg news site actually has a gold tick beside it – which mean it is the official account of a business of other private organisation. However, judging whether an account is genuine would require the public to kow what a gold tick is – calling into question Twitter’s current verification system, Mr Ajder said.

An explosion at the Pentagon would be easy to quickly verify on the ground, but that might not be the case for an explosion for a village in Ukraine, he added, and the rapid creation and increasing sophistication of such images posed a challenge for those looking for news and for journalists seeking to verify events in a fast-paced environment.

“In a world where you can generate any kind of content in a realistic fashion with AI at scale and within minutes, you could create 200 different images of this fire from different perspectives,” said Mr Ajder. “I think it really speaks to how our information infrastructure and media infrastructure is not prepared for this AI-generated content.”

Last week, Open AI chief executive Sam Altman testified before US congress, agreeing with the need for AI regulation and saying, “I think if this technology goes wrong, it can go quite wrong,” The New York Times reported.

Mr Ajder said the current approach to AI risks was not fair as it is focused on how ordinary people ca spot fakes, rather than on demanding regulation from technology companies or spending more money on content moderation. News organisations and technology companies weren’t “prepared” for what might come, he added.

“It’s a perfect storm where traditional news media takes time to verify, the algorithmic nature of platform rewards controversial and sensational content and the platforms don’t have much in the way of strong content moderation to filter out these fakes,” he said.. It could be the first instance of an AI-generated image sowing enough confusion to move stock markets.

ADVERTISEMENT

A fake image of an explosion next to the Pentagon spread on social media and caused a brief dip in the US stock market on Monday.

The now-debunked image showed a plume of smoke billowing beside the headquarters of the US Department of Defense. Experts say it bears the hallmarks of being generated by artificial intelligence (AI) and illustrates the threat the technology poses in terms of misinformation.

The image was shared by a number of Twitter accounts carrying a “verified” blue check mark, including one impersonating the Bloomberg news agency - called “@BloombergFeed” - which has since been suspended.

The fake news was subsequently spread by dozens of other Twitter accounts, including Russian state-controlled news network RT, which has been blocked in the EU but has a following of more than 3 million on the platform.

A number of the accounts that tweeted the fake image seem to be affiliated with conspiracies or Russia’s war in Ukraine, as well as a number of cryptocurrency accounts with blue check marks.

One of those, “WhaleChart”, tweeted the image along with the text: “BREAKING: Explosion near Pentagon”. That tweet has been viewed more than 500,000 times.

The fake news was rebutted by the Arlington County fire department, which said: “Pentagon Force Protection Agency and the ACFD are aware of a social media report circulating online about an explosion near the Pentagon. There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public.”

But several US news outlets noted that Wall Street took a noticeable dip in the moments after the image started circulating on Twitter. The real Bloomberg even labelled it “possibly the first instance of an AI-generated image moving the market”.

The incident adds to growing concerns about the potential misuse of AI tools to propagate fake news, destabilise societies, and even sway voters ahead of elections.

Twitter blue check chaos

Twitter initiated controversial changes to its verification system under the leadership of Elon Musk, who bought the company for $44 billion (€40.8 billion) last October.

Previously accounts that were deemed notable - due to them being recognised experts, celebrities, journalists, or organisations - could be verified with an identification check.

Under the new policy, anyone can get a blue verification checkmark by paying for it, and they don’t need to provide any evidence of identification.

This change led to warnings that impersonation could become rife on the platform - and there has been a big increase.

Under his leadership, Musk has said the company is now worth less than half what he bought it for, following a major drop in advertising revenue, and a number of lawsuits filed against the company.

Twitter responded to an emailed request for comment with a poo emoji.

‘Genie out the bottle’

The fake image was spread amid ongoing worries around generative AI and the potential dangers it could pose.

A Reuters/Ipsos poll published on May 17 showed more than two-thirds of Americans are worried about the negative effects of AI, while 61 per cent think it could threaten civilisation.

The CEO of OpenAI Sam Altman encouraged politicians last week to come up with a system of regulation for AI, at a hearing during which one senator, Cory Booker, said: "There's no way to put this genie in the bottle. Globally, this is exploding”.

ADVERTISEMENT

The European Union is currently drafting its own AI Act to regulate the technology.. A picture claiming to show an explosion near the Pentagon raises concerns about AI’s ability to produce misinformation.

A fake image appearing to show a large explosion near the Pentagon was shared on social media on Monday prompting a brief dip in the stock market.

Within minutes, a wave of social media accounts including some verified accounts shared the fake picture, further amplifying the confusion.

Officials later confirmed that no such incident had occurred.

Social media sleuths, including Nick Waters from Bellingcat, an online news verification group, were quick to point out some notable problems with the image, including:

That there were no other firsthand witnesses to corroborate the event, especially in a busy area like the Pentagon. “This is why it’s so difficult (I’d argue effectively impossible) to create a believable fake of such an event,” Waters tweeted.

That the building itself looks noticeably different from the Pentagon. This can easily be verified by using tools like Google Street View to compare the two images.

Other details including the odd-looking floating lamp post and black pole protruding from the pavement were another giveaway that the image was not what it seemed. Artificial intelligence still has a difficult time recreating locations without introducing random artefacts.

Confident that this picture claiming to show an "explosion near the pentagon" is AI generated. Check out the frontage of the building, and the way the fence melds into the crowd barriers. There's also no other images, videos or people posting as first hand witnesses. pic.twitter.com/t1YKQabuNL — Nick Waters (@N_Waters89) May 22, 2023

How to spot AI-generated and fake images

There are many generative AI tools like Midjourney, Dall-e 2 and Stable Diffusion that can create life-like images with very little effort.

These tools are trained by looking at large volumes of real images but fill in the gaps with their own interpretation when training data is missing. This can result in people having extra limbs and objects that are morphed with their surroundings.

When seeing images online that are purported to show a breaking news event, it is worth keeping the following in mind:

News doesn’t happen in a vacuum – In the case of a large explosion or big event, expect to see an influx of on-the-ground reports from different people and different angles.

In the case of a large explosion or big event, expect to see an influx of on-the-ground reports from different people and different angles. Who is uploading the content – Look at the post history of the user account. Does their location and the location of the event add up? Look at who they are following and who follows them. Can you reach out to them or talk to them?

Look at the post history of the user account. Does their location and the location of the event add up? Look at who they are following and who follows them. Can you reach out to them or talk to them? Use open-source intelligence tools – Reverse image search tools like Google Images and TinEye can allow you to upload an image and determine where and when it was first used. There are several other tools that you can use like looking at live public traffic camera footage to verify that an event is taking place.

– Reverse image search tools like Google Images and TinEye can allow you to upload an image and determine where and when it was first used. There are several other tools that you can use like looking at live public traffic camera footage to verify that an event is taking place. Analyse the image and its surroundings – Look for clues in the image like nearby landmarks, road signs and even the weather conditions to help you place where or when the purported event could have taken place.

Look for clues in the image like nearby landmarks, road signs and even the weather conditions to help you place where or when the purported event could have taken place. Hands, eyes and posture – When dealing with images of people, pay special attention to their eyes, hands and general posture. AI-generated videos that mimic people, known as deep fakes, tend to have problems blinking, as most training data sets do not contain faces with their eyes closed. Hands that are not correctly grasping objects or limbs that look unnaturally twisted can also help spot the fake.

For more information on news verification and open-source intelligence investigations, Al Jazeera’s Media Institute has published a handful of guidebooks available in multiple languages available to download below.

Finding the truth among the fakes [PDF]

News verification – A practical guide [PDF]

Open Source Investigations [PDF]. . It is arguable that the images had no impact on that, of course, but that would seem foolhardy ignorance given the fake Bloomberg tweet had been circulated by accounts with large followings, such as Russia Today, the country’s national broadcaster.

Counterfeit reality

Smart fakery is a pending problem that has vexed the minds of academics and technologists for years, but is rapidly becoming a real-world dilemma in the era of generative AI.

The potential for markets to be manipulated, societal unrest to be fomented, and individuals to be targeted by all kinds of horrible online hoaxes has arrived, and we are nowhere near prepared to deal with it.

Back in 2004 (yes, I was a child prodigy, honest), I interviewed an expert from US research firm Gartner called Daryl Plummer about a concept he was already warning about called “counterfeit reality”, whereby technology that was then being used to create convincing virtual worlds in movies, would be used to cause real-world havoc.

At the time there were already examples that had occurred. The previous year Piers Morgan had been forced to resign as editor of The Daily Mirror after splashing fake photographs of British soldiers torturing an Iraqi prisoner on its front page.

Advertisement

Meanwhile, photographs of Democrat Presidential candidate John Kerry speaking alongside Jane Fonda at an anti-Vietnam rally in 1970 appeared in the run-up to his tilt at the US presidential election.

Veterans called the image a “slap in the face” and, despite being proved fake, it succeeded in portraying Kerry as an anti-military service candidate, ahead of his very narrow loss to George W. Bush.

“Fake reality for sale will be the norm in the next decade and its growth will not be about technology, but society’s propensity for allowing the deviant to become the norm,” Plummer told me in 2004.

“As people come to expect the use of virtual technology instead of real-time human interaction, the psychological outrage expected with counterfeit reality will be diminished ... I suspect there will be some very notable situations where we are fooled and a lot of noise will happen. The existing examples are just the tip of the iceberg.”

While he got his timings wrong, we are now in the counterfeit reality era, whether we believe it or not.

Voice cloning technology is advanced enough to generate unique speech that sounds like the intended person, from just a few short samples of their genuine audio, and the ability to align this with faked video will weaponise it.

Advertisement

In the newsroom of a financial newspaper we will have to be more diligent than ever in vetting anonymous tips that come in. Leaked audio of the CEOs of Qantas and Virgin discussing a merger may sound like a huge scoop, but what if it is just a shareholder with an AI app trying to pump up the shares?

Current limitations

The conundrums for other industries will also be obvious to the people working within them.

Voice cloning currently has limitations in that it works far more effectively in a short burst of audio, and falls short of being convincing in a live scenario where a human may seek to interrogate it, but that is still a pretty big scope for skulduggery.

Earlier this year AI speech start-up ElevenLabs had to issue new terms and conditions after “an increasing number of misuse cases” emerged when it opened access to its voice-cloning platform.

Fake speeches by US President Joe Biden spread like wildfire, and it is all funny and good fun ... until it isn’t, of course. (Bad language warning for the end of the video in this tweet.)

Advertisement

Executive impersonation is something cybersecurity firms are already trying to sell related services to mitigate against to businesses, but there is an existential problem looming in financial markets, and society more broadly.

Is it the market operators responsibility to put shares in a halt because a tweet is going viral? What does Commonwealth Bank do when a video of Matt Comyn’s resignation speech goes up online, when he is really just on holiday in Byron Bay?

I certainly have no idea, but the worrying thing is, I suspect they don’t either.. The world paused briefly when the visuals of an explosion in Pentagon's premises began making rounds on social media on Monday. The image, which began circulating on Twitter on May 22, showed an explosion on a grass lawn outside the Pentagon.

The original post has since been removed.

Pentagon explosion visuals: Did an explosion really happen?

The US Department of Defense has clarified that there has been no explosion in and even in the vicinity of the Pentagon's premises.

A Department of Defense spokesperson was quoted as saying by Forbes that the image purportedly showing an explosion in Pentagon's premises is a piece of "misinformation". The Arlington Fire department shortly tweeted that there is "NO explosion or incident" at or near the Pentagon reservation.

'No immediate danger or hazards to public': Arlington Fire department

The Arlington fire department clarified that there is 'no immediate danger or hazards to the public' around the Pentagon.

So far, the source of the viral image remains undetermined.

AI-generated deep fakes in spotlight

The deep fakes generated by Artificial Intelligence have been in the spotlight for their life-like depictions of absolutely real people and subjects. They have spread on social media like wildfire.

The proliferation of such deep fakes has increased after a series of highly-powerful AI technologies, including OpenAI's ChatGPT, became public.

The AI's life-like depictions include Pope Francis wearing a Balenciaga coat as well as AI-generated viral images of former President Donald Trump resisting authorities during a fake arrest.

The image depicting a blast inside Pentagon's premises too has been described as a deepfake.

ALSO WATCH | AI-generated pic of Pope Francis in bougie puffer jacket goes viral × Meanwhile, last week, the words of caution related to the regulation of Artificial Intelligence came from none other than Samuel Altman, the CEO of most-successful A.I. startup of the day, ChatGPT-maker OpenAI.

In a US Senate hearing last week, Altman said:"If this technology goes wrong, it can go quite wrong."

Altman proposed the formation of a US or global agency that would license the most powerful AI systems and have the authority to "take that license away and ensure compliance with safety standards."

WATCH WION LIVE HERE. . . The image is fake but the consequences are real.

The image is fake but the consequences are real. Credit: Getty Images

A deepfake of an explosion at the Pentagon that caused the stock market to dip exemplified the misinformation risks of generative AI.

On Monday, a seemingly AI-generated image of what looked like an explosion outside of the Pentagon circulated on Twitter. The Arlington Police Department quickly debunked the image tweeting, "There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public."

But not before the stock market dipped by 0.26 percent before bouncing back, according to Insider.

It's unclear how the image was created, but it has the telltale signs of an AI-generated image. The fencing in front of the building is blurred and the columns appear to be different widths. Any social media sleuth accustomed to spotting photoshopped images of celebrities and influencers would have noticed this, but as generative AI continues to improve, deepfakes will be harder to spot.

Even with Arlington PD's quick response, Twitter's mess of a verification system compounded the issue. One of the accounts that tweeted the image was a verified account impersonating a Bloomberg news feed. That account, called @BloombergFeed, has since been suspended.

Other accounts that tweeted the image were @DeItaone and the account Russian state-media owned site RT. Now that anyone can pay to become verified on Twitter, situations like this are the perfect storm of misinformation.

A fake Twitter account shares a fake image that leads to real consequences. Welcome to 2023.. Fake viral images of an explosion at the Pentagon were probably created by AI

Enlarge this image toggle caption Daniel Slim/AFP via Getty Images Daniel Slim/AFP via Getty Images

A false report of an explosion at the Pentagon, accompanied by an apparently AI-generated image, spread on Twitter on Monday morning, sparking a brief dip in the stock market.

"There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public," the Department of Defense's Pentagon Force Protection Agency and the Arlington County fire department said in a joint statement on Twitter.

The fake image circulating on Twitter showed a black cloud of smoke near a building. The accounts posting it claimed it depicted the Pentagon.

But the image was likely generated by artificial intelligence, experts said, in an example of potential for misuse of the increasingly popular and prevalent technology that they have been worried about.

"Check out the frontage of the building, and the way the fence melds into the crowd barriers. There's also no other images, videos or people posting as first hand witnesses," Nick Waters of the open-source investigations group Bellingcat wrote on Twitter.

Soon, other apparently fake AI images purporting to show an explosion at the White House popped up.

Major stock market indices briefly dipped on the false reports before recovering.

Many of the Twitter accounts that spread the hoax carried blue checks, which used to signify that the social network had verified the account is who or what it claims to be. But under new owner Elon Musk, the company now gives a blue check to any account that pays for a monthly Twitter Blue subscription.

Among the blue-check accounts that shared the false Pentagon image were an one impersonating Bloomberg News and the real account of the Kremlin-linked Russian news service RT.

RT later deleted its post. The fake Bloomberg account has been suspended by Twitter.

Twitter responded to a request for comment with an auto-reply containing a poop emoji.. Between advances in AI technology and blue checks for anyone willing to pay the monthly subscription fee, a lot is happening on social media. Today (May 22), one of the most guarded government buildings in the nation fell victim to a hoax. A viral tweet claimed there was a massive explosion at the Pentagon.

This morning, multiple verified Twitter accounts, including one claiming to be affiliated with Bloomberg News, shared the AI-generated image via social media. The possible bombing of the ​​headquarters for the United States Department of Defense caused widespread panic that resulted in massive consequences. According to The New York Post, after the image of the Pentagon exploding was shared internationally, even the stock market saw the impact. The Dow Jones dropped 50 points, allegedly within minutes.

@PFPAOfficial and the ACFD are aware of a social media report circulating online about an explosion near the Pentagon. There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public. pic.twitter.com/uznY0s7deL — Arlington Fire & EMS (@ArlingtonVaFD) May 22, 2023

One major Indian television network even reported the incident and had a strategic expert join the broadcast to discuss the now-debunked bombing. In a statement to Forbes, a spokesperson for the United States Department of Defense called the image “misinformation.” The Arlington, Virginia Fire Department backed their claims, tweeting that they were “aware of a social media report circulating online about an explosion near the Pentagon. There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public.”

The photo caused many to fear the future implications that could stem from situations like this one. “I think AI-fueled [misinformation] is going to become an exponentially larger issue in the coming years. Today, it’s a fake picture of a Pentagon explosion. Only a matter of time till someone posts a deep fake of our president declaring war on Russia or something vastly more harmful than what we saw here today,” one person tweeted.

See what others are saying about the hoax below.

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

Earlier today an apparent AI-generated photo showed a fake explosion near the US Pentagon. The news was shared by Russian state-media RT on Twitter, which helped it go viral. It was also tweeted by a verified Twitter account called “BloombegFeed” which has now been suspended.… pic.twitter.com/KN1wOptlRb — Ed Krassenstein (@EdKrassen) May 22, 2023

So @republic aired a 'Live & Breaking' news of Pentagon explosion image. They even invited Prof. Madhav Nalapat "strategic expert" to discuss about the explosion.

BWT, It was an AI generated image. pic.twitter.com/8j1nfSJR6x — Mohammed Zubair (@zoo_bear) May 22, 2023

This morning, an AI generated image of an explosion at the US Pentagon surfaced. With multiple news sources reporting it as real, the S&P 500 fell 30 points in minutes. This resulted in a $500 billion market cap swing on a fake image. It then rebounded once the image was… pic.twitter.com/DpHgflkMXP — The Kobeissi Letter (@KobeissiLetter) May 22, 2023

Eyewitnesses refute reports of an explosion at the Pentagon Washington Post correspondent Yunus Paksoy posted a photo to prove that there was no explosion at the Pentagon. pic.twitter.com/E5mABXgFzR — Spriter (@Spriter99880) May 22, 2023

Confident that this picture claiming to show an "explosion near the pentagon" is AI generated. Check out the frontage of the building, and the way the fence melds into the crowd barriers. There's also no other images, videos or people posting as first hand witnesses. pic.twitter.com/t1YKQabuNL — Nick Waters (@N_Waters89) May 22, 2023

A fake Bloomberg account with a Verified Blue Check on Twitter posted an apparently AI-generated picture of an explosion at the Pentagon this morning 😳👀 pic.twitter.com/9sPkkAfIot — Daily Loud (@DailyLoud) May 22, 2023

Whoa, did you hear what happened this morning? An AI-generated image of an explosion at the US Pentagon spread like wildfire! The S&P 500 even took a hit, dropping 30 points in a matter of minutes. But get this – the image was fake! As in, totally bogus. And now people are… pic.twitter.com/KkYkT73xgw — Jewels Jones (@JewelsJonesLive) May 22, 2023

I think AI fueled disinformation is going to become an exponentially larger issue in the coming years. Today it's a fake picture of a Pentagon explosion. Only a matter of time till someone posts a deepfake of our president declaring war on Russia or something vastly more harmful… pic.twitter.com/eXbkuKcrtx — David Hanlin (@thelaptoplegend) May 22, 2023

Trending Stories. A purportedly AI-generated photo of a fake explosion at the Pentagon spread rapidly on social media on Monday – prompting mass confusion among users and a brief selloff in the US stock market.

The fake photo, which showed smoke billowing outside the Pentagon, was shared by Russian state media outlet and other accounts alongside claims that an explosion has occurred at the complex. RT later deleted the image.

The Arlington County Fire Department quickly tweeted a message debunking the hoax photo.

“@PFPAOfficial and the ACFD are aware of a social media report circulating online about an explosion near the Pentagon,” the tweet said. “There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public.”

The Post has reached out to the Department of Defense for comment.

US stocks appeared to briefly dip as the photo circulated, but quickly rebounded after the picture was exposed as a hoax.

The Dow Jones Industrial Average was down about 50 points as of noon ET, while the tech-heavy Nasdaq and the broad-based S&P 500 were both trading slightly higher.

Some users pointed out that the fake Pentagon photo showed clear signs of AI manipulation, including warped sections of fence.

The social media scare will add fuel to concerns among critics who say that advanced AI systems will allow bad actors around the world to spread misinformation and sow chaos online.

Previously, social media users were tricked by viral AI-generated images of Pope Francis wearing a Balenciaga puffer jacket and of former President Donald Trump clashing with the NYPD while being arrested.

In March, Elon Musk and more than 1,000 experts cited the spread of misinformation as a key motivation in their call for a six-month pause on the development of advanced AI until proper safety guidelines were in place.

The experts cited risks including the spread of “propaganda and untruth,” job losses, the development of “nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us,” and the risk of “loss of control of our civilization.”

Dr. Geoffrey Hinton, known as the “Godfather of AI,” grew so concerned by the technology’s potential risks that he quit his job at Google last month so that he could speak out without hurting his former employer.

An AI-generated picture showed smoke billowing near the Pentagon. Twitter/@FngFinancial

Several social media accounts shared the fake photo. Twitter/@FngFinancial

Hinton warned that AI will become more dangerous in the future — with “bad actors” potentially exploiting advanced systems “for bad things” that will be hard to prevent.. An AI-generated photo of an explosion near the Pentagon is being blamed for a significant drop in the U.S. Stock Market.

By Charlene Badasie |

An AI-generated photo of an explosion near the Pentagon in Washington, D.C., caused the United States stock market to crash. The image was shared by a verified account on Twitter called Bloomberg Feed, accompanied by a misleading caption that said, “Large Explosion Near the Pentagon Complex in Washington, DC – Initial Report.”

Prime example of the dangers in the pay-to-verify system: This account, which tweeted a (very likely AI-generated) photo of a (fake) story about an explosion at the Pentagon, looks at first glance like a legit Bloomberg news feed. pic.twitter.com/SThErCln0p — Andy Campbell (@AndyBCampbell) May 22, 2023

Although Twitter verification is meaningless since anyone can pay for the blue checkmark, the AI-generated Pentagon image had real-world consequences. According to The Byte, the “news” went viral after a user with over 650,000 followers shared the photo at 10:06 am. Four minutes later, the stock market fell by 0.26%.

The Arlington Police Department acted quickly to quell the panic by stating that the AI Pentagon image was indeed a fake. “There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public,” the department said on social media.

@PFPAOfficial and the ACFD are aware of a social media report circulating online about an explosion near the Pentagon. There is NO explosion or incident taking place at or near the Pentagon reservation, and there is no immediate danger or hazards to the public. pic.twitter.com/uznY0s7deL — Arlington Fire & EMS (@ArlingtonVaFD) May 22, 2023

While law enforcement hasn’t confirmed if the image was made using artificial intelligence tools, it does feature some of the hallmarks of AI-generated images. The columns on the building in the hoax photo vary in size and the fence blends into the sidewalk in some places. The post, which was also shared by a Russian state-media Twitter account with more than 3 million followers, has since been deleted.

Fortunately, the markets swiftly bounced back once the AI Pentagon photo was revealed to be a hoax. Similarly, the prominent cryptocurrency Bitcoin faced a short crash as the fake news circulated, causing it to drop to $26,500. The digital currency stabilized shortly after and is currently being traded at $26,882.

This form of online deception has sparked significant concerns among critics of unregulated AI advancement. Experts in the field have previously expressed their apprehension about malicious individuals’ potential misuse of advanced AI systems, enabling the dissemination of misinformation and triggering chaos within online communities.

The AI-generated Pentagon image isn’t the first instance of trickery. Other fake viral images that misled the public included faux photos of Pope Francis in a Balenciaga jacket, a picture of former President Donald Trump being arrested, and deepfake videos featuring celebrities like Elon Musk endorsing cryptocurrency scams. Fake X-rated video footage of Harry Potter star Emma Watson also surfaced online.

As a result, hundreds of tech experts have called for a six-month pause on advanced AI development until proper safety guidelines are established. Dr. Geoffrey Hinton, known as the Godfather of AI, voluntarily exited his role at Google to showcase his concerns about potential risks without harming his former employer’s reputation.

Instances of misinformation, like the AI Pentagon image, add fuel to the ongoing discourse about establishing a comprehensive ethical and regulatory framework for artificial intelligence. As this technology becomes an increasingly prominent tool in the hands of disinformation agents, the consequences can be more chaotic than a temporary stock market crash.

A lack of transparency, accountability, and ethical considerations could amplify these risks. But until some form of regulation is implemented worldwide, instances of fake news and other dangerous trends are bound to increase.. 