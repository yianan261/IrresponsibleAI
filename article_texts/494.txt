ARTICLE TITLE: Female Celebrities' Faces Shown in Sexually Suggestive Ads for Deepfake App
In the ad, a woman in a white lace dress makes suggestive faces at the camera, and then kneels. There’s something a bit uncanny about her; a quiver at the side of her temple, a peculiar stillness of her lip. But if you saw the video in the wild, you might not know that it’s a deepfake fabrication. It would just look like a video, like the opening shots of some cheesy, low-budget internet porn.

In top right corner, as the video loops, there is a still image of the actress Emma Watson, taken when she was a teenager, from a promotional shoot for the Harry Potter movies. It’s her face that has been pasted on to the porn performer’s. Suddenly, a woman who has never performed in pornography is featured in it.

The ads, which directed users to an app that makes deepfake videos, were discovered in more than 230 iterations across Facebook, Instagram and Meta’s Messenger, according to an NBC news investigation by Kat Tenbarge. Most of the ads featured Watson’s image; some others used the face of Scarlett Johansson. The same ads appeared in photo-editing and gaming apps available in the Apple App Store. Lest the message be lost on viewers, the ads make explicit that they are intended to help users create non-consensual porn of any women they like. “Swap ANY FACE in the video!” the ads read. “Replace face with anyone. Enjoy yourself with AI face swap technology.”

Similar ads for deepfake services appear directly next to explicit videos on PornHub. Though deepfake technology can theoretically be used for any kind of content – anything from joking satire to malicious political disinformation campaigns – overwhelmingly, the tech is being used to create nonconsensual porn. According to a 2019 report, 96% of deepfake material online is pornographic.

That figure might well be increasing. The ads on Meta and Apple platforms appeared as consumer demand for deepfake pornography is exploding. The surge comes on the heels of a controversy that rocked online video game communities in January, when a popular streamer, Brandon Ewing – who calls himself “Atrioc” – displayed deepfake pornography of several popular women streamers in one of his online broadcasts. He later admitted to having paid for the artificial porn of the women, who were his colleagues and friends, after seeing an ad similar to those that appeared on Meta and Apple platforms. The women whose images were commandeered for Ewing’s pornography issued angry and hurt responses; Ewing himself apologized. But the controversy seems to have only made the streamer’s overwhelmingly young and male follower base more aware of the availability of deepfake content – and eager to use it themselves.

Genevieve Oh, a researcher who studies livestreaming, told NBC that after Ewing’s apology, web traffic to the top deepfake porn sites exploded. That rapid increase over the past few weeks has followed a slower, but still alarming, growth of the deepfake revenge porn sector over the past several years. In 2018, fewer than 2,000 videos had been uploaded to the best-known deepfake streaming site; by 2022, that number had ballooned to 13,000, with a monthly view count of 16m. As deepfake revenge porn becomes more popular, the barrier to access is quite low: the app that misused Watson’s face in its ads charges just $8 per week.

The rapid increase in the number and availability of nonconsensual deepfake porn videos raises alarming questions about privacy and consent in the digital future. How will the huge numbers of women – and the smaller, but significant, numbers of men – who are affected by this new AI-enabled revenge porn manage their reputations and lives? As the technology improves, how will viewers know the difference between fact and AI-generated fiction? How can nonconsensual material be removed when the internet moves so much faster than regulation?

But the example of these apps – and of the men, like Ewing and his fans, who use them – also illuminates something older, and more uncomfortable, about the nature of porn: that men often use it as an expression of their contempt for women, and feel that the sexual depiction of women degrades and violates them. This is, in fact, much of mainstream porn’s appeal, at least according to the sentiments of many of the men who consume it: that it enables men to imagine themselves in control of women, and of inflicting pain and degradation on them. Deepfake revenge porn, then, merely fulfills with technology what mainstream porn has offered men in fantasy: the assurance that any woman can be made lesser, degraded and humiliated, through sexual force. The non-consent is the point; the humiliation is the point; the cruelty is the point.

There is no other way, really, to understand deepfake pornography’s appeal: it is not as if the internet lacks sexual content depicting real and consenting adults. What these apps offer their users is specifically and explicitly the opportunity to hurt women by forcing them into pornography against their will. After Ewing exposed his deepfake pornography to his streaming audience in January, one of the women depicted issued her own tearful video, describing how the malice and violation of the deepfake had wounded her. In response, a man sent her a picture of her own crying face appearing on his tablet. The screen was covered in semen.

For now, the women and others who are targeted by deepfake revenge porn have few avenues of legal recourse. Most states have laws punishing revenge porn, but only four – California, New York, Georgia and Virginia – ban nonconsensual deepfakes. Companies hosting the apps are often based overseas, mostly beyond the reach of legal enforcement – the company whose app was advertised on Meta appears to be owned by a parent company based in China. Meanwhile, more and more men will begin to use the technology against more and more women. “I was on fucking Pornhub … and there was an ad [for the deepfake site],” Ewing said in his apology video, by way of explaining how he discovered the AI revenge porn site. “There’s an ad on every fucking video for this so I know other people must be clicking it.”. In a Facebook ad, a woman with a face identical to actor Emma Watson’s face smiles coyly and bends down in front of the camera, appearing to initiate a sexual act. But the woman isn’t Watson, the “Harry Potter” star. The ad was part of a massive campaign this week for a deepfake app, which allows users to swap any face into any video of their choosing.

Deepfakes are content where faces or sounds are switched out or manipulated. Commonly, deepfake creators make videos in which celebrities are made to look like they are willingly appearing in them, even though they are not. Increasingly, the technology has been used to make nonconsensual pornography featuring the faces of celebrities, influencers or any person, including children.

The ad campaign on Meta nods to the fact that this once-advanced technology has rapidly spread to readily available consumer applications being advertised on mainstream parts of the internet. Despite many platforms prohibiting manipulative and malicious deepfake content, apps like the ones reviewed by NBC News have been able to slip through the cracks.

Stills from the ads appearing to feature Watson's face.

On Sunday and Monday, an app for creating “DeepFake FaceSwap” videos rolled out more than 230 ads on Meta’s services, including Facebook, Instagram and Messenger, according to a review of Meta’s ad library. Some of the ads showed what looked like the beginning of pornographic videos with the well-known sound of the porn platform Pornhub’s intro track playing. Seconds in, the women’s faces were swapped with those of famous actors.

When Lauren Barton, a journalism student in Tennessee, saw the same ad on a separate application, she was shocked enough to screen-record it and tweet it out, where it received over 10 million views, according to Twitter’s views counter.

“This could be used with high schoolers in public schools who are bullied,” Barton said. “It could ruin somebody’s life. They could get in trouble at their job. And this is extremely easy to do and free. All I had to do was upload a picture of my face and I had access to 50 free templates.”

NBC News / Getty Images

Of the Meta ads, 127 featured Watson’s likeness. Another 74 featured the actor Scarlett Johansson’s face swapped with those of women in similarly provocative videos. Neither actor responded to a request for comment.

“Replace face with anyone,” the captions on 80 of the ads read. “Enjoy yourself with AI swap face technology.”

On Tuesday, after NBC News asked Meta for comment, all of the app’s ads were removed from Meta’s services.

While no sexual acts were shown in the videos, their suggestive nature illustrates how the application can potentially be used to generate faked sexual content. The app allows users to upload videos to manipulate and also includes dozens of video templates, many of which appear to be taken from TikTok and similar social media platforms.

The preset categories include “Fashion,” “Bride,” “For Men,” “For Women,” and “TikTok,” while the category with the most options is called “Hot.” It features videos of scantily clad women and men dancing and posing. After selecting a video template or uploading their own, users can input a single photo of anyone’s face, and receive a face-swapped version of the video in seconds.

The terms of service for the app, which costs $8 per week, say it does not allow users to impersonate others via their services or upload sexually explicit content. The app developer listed on the App Store is called Ufoto Limited, owned by a Chinese parent company, Wondershare. Neither company responded to a request for comment.

Meta banned most deepfake content in 2020, and the company prohibits adult content in ads, including nudity, depictions of people in explicit or suggestive positions, or activities that are sexually provocative.

“Our policies prohibit adult content regardless of whether it is generated by AI or not, and we have restricted this Page from advertising on our platform,” a Meta spokesperson said in a statement.

The same ads were also spotted on free photo-editing and gaming apps downloaded from Apple’s App Store, where the app first appeared in 2022 for free for ages 9 and up.

An Apple representative said that the company does not have specific rules about deepfakes but that it prohibits apps that include pornography and defamatory content. Apple said it removed the app from the App Store after having been contacted by NBC News.

The app was also previously available to download for free on Google Play, where it was rated "Teen" for "suggestive themes." Google said Wednesday it removed the app from Google Play after having been contacted by NBC News.

Apple and Google have taken action against similar AI face-swap apps, including a different app that was the subject of a Reuters investigation in December 2021. Reuters found that the app was advertising the creation of “deepfake porn” on pornographic websites. At the time, Apple said it didn’t have specific guidelines around deepfake apps but prohibited content that was defamatory, discriminatory or likely to intimidate, humiliate or harm anyone. While its ratings and ad campaigns have been adjusted, the app that Reuters reported on is still available to download free on Apple’s App Store and Google Play.

The app NBC News reviewed is one of the latest in a boom of freely accessible consumer deepfake products.

When searching “deepfake” on app stores, dozens of apps with similar technological capabilities appear, including ones that promote making “hot” content.

Mainstream examples of the technology show celebrities and politicians doing and saying things they’ve never actually said or done. Sometimes the effects are comical.

However, deepfake technology has overwhelmingly been used to make pornography with nonconsenting stars. As the technology has improved and become more widespread, the market for nonconsensual sexual imagery has ballooned. Some websites allow users to sell nonconsensual deepfake porn from behind a paywall.

A 2019 report from DeepTrace, an Amsterdam-based company monitoring synthetic media online, found that 96% of deepfake material online is of a pornographic nature.

In January, female Twitch streamers spoke out after a popular male streamer apologized for consuming deepfake porn of his peers.

Livestreaming research conducted by the independent analyst Genevieve Oh found that the top website for consuming deepfake porn exploded in traffic following the Twitch streamer’s apology. Oh’s research also found that the number of deepfake pornographic videos has nearly doubled every year since 2018. February had the largest number of deepfake porn videos uploaded in a month ever, Oh said.

While the nonconsensual sharing of sexually explicit photos and videos is illegal in most states, laws addressing deepfake media are in effect only in California, Georgia, New York and Virginia.