In response to the growing concern from educators over ChatGPT’s ability to help students cheat, OpenAI released a tool on Tuesday that can detect AI-written text. However, the company said, “Our classifier is not fully reliable.” In response to the growing concern from educators over ChatGPT’s ability to help students cheat, OpenAI released a tool on Tuesday that can detect AI-written text. However, the company said,Our classifier is not fully reliable.”

“In our evaluations on a ‘challenge set’ of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as ‘likely AI-written,’ while incorrectly labeling human-written text as AI-written 9% of the time (false positives),” the company, which developed ChatGPT, “In our evaluations on a ‘challenge set’ of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as ‘likely AI-written,’ while incorrectly labeling human-written text as AI-written 9% of the time (false positives),” the company, which developed ChatGPT, wrote in a blog post

Advertisement

The classifier, which was trained on a dataset of human-written and AI-written text on the same topic, is not yet dependable and is only supposed to complement other ways of determining who a text is written by. The limitations that the classifier has, according to OpenAI, include being unreliable on short texts and performing worse in other languages. The classifier, which was trained on a dataset of human-written and AI-written text on the same topic, is not yet dependable and is only supposed to complement other ways of determining who a text is written by. The limitations that the classifier has, according to OpenAI, include being unreliable on short texts and performing worse in other languages.

OpenAI said that it has developed preliminary resources on the impact of ChatGPT for educators, acknowledging that it is important to recognize “the limits and impacts of AI generated text classifiers in the classroom.” OpenAI said that it has developed preliminary resources on the impact of ChatGPT for educators, acknowledging that it is important to recognize “the limits and impacts of AI generated text classifiers in the classroom.”

Recently, Recently, NYU professors told students on the first day of classes that they were not allowed to use ChatGPT without explicit permission, saying that any usage of the tool would be considered plagiarism. Professors have been coming up with their own ways to detect AI writing in order to prevent any sort of cheating, such as running their essay prompts through ChatGPT to have a benchmark of what an AI-generated essay would look like.

A 22-year-old student named Edward Tian developed his own ChatGPT detector, which was launched in beta earlier this year and released in full as A 22-year-old student named Edward Tian developed his own ChatGPT detector, which was launched in beta earlier this year and released in full as GPTZeroX on January 29. In this app, you can insert text or upload one or more documents at once and the app will generate a score for how much of the text was written by AI and highlight the sentences that were written by AI. According to Tian, the app was wrong less than 2 percent of the time when tested on a dataset of BBC news articles and machine-generated articles with the same prompt, which would make GPTZeroX seemingly more reliable than OpenAI’s own detector.

OpenAI includes a call-to-action in its blog post, offering people directly impacted by the language bot, “including but not limited to teachers, administrators, parents, students, and education service providers,” to provide OpenAI includes a call-to-action in its blog post, offering people directly impacted by the language bot, “including but not limited to teachers, administrators, parents, students, and education service providers,” to provide feedback through a Google Form survey. As ChatGPT becomes capable of writing everything from college essays to code, teachers around the country are attempting to adjust their classrooms around the new technology and discussing how the tool can be used in an ethical way.

“We’re not educators ourselves—we’re very aware of that—and so our goals are really to help equip teachers to deploy these models effectively in and out of the classroom,” Open AI policy research director “We’re not educators ourselves—we’re very aware of that—and so our goals are really to help equip teachers to deploy these models effectively in and out of the classroom,” Open AI policy research director Lana Ahmad told CNN . “That means giving them the language to speak about it, help them understand the capabilities and the limitations, and then secondarily through them, equip students to navigate the complexities that AI is already introducing in the world.”


I submitted more comments to the New York City’s Department of Consumer and Worker Protection regarding automated employment decision tools, ahead of the second public hearing on 2023-01-23.

A month of ChatGPT-fuelled news

This past month was clearly dominated with ChatGPT news, headlined by OpenAI’s announcement of $10b of new investment from Microsoft in return for a 49% stake and plans of extensive product integrations. OpenAI’s cookbook also exceeded 11,000 stars on GitHub. Interestingly, Microsoft’s announced integrations of a new LLM with Bing search and the Edge browser.

Jailbreaking. Since ChatGPT was released to the public, many ad hoc attempts have been made to jailbreak OpenAI’s limitations on generating harmful content using prompt engineering. More systematic jailbreaks are making headlines. Redditors have launched DAN (Do Anything Now), a contextual reinforcement learning-based jailbreak for ChatGPT to reverse sublimate its identity as a chatbot without restrictions. The latest version, DAN 5.0, was just released on 2023-02-04.

Math. Despite OpenAI’s release notes that ChatGPT has been upgraded with better mathematical skills, Twitter continues to report miserable failure in basic tests for prime numbers alongside failure to convert units correctly and inability to order B.C. dates.

Invisible human labor. Recent news from OpenAI support the ongoing trend in the AI industry for powering AI advances with lowly paid human labor. TIME reported that OpenAI hired Kenyan firm Sama for content moderation, paying workers as little as US$2/hr to do so. Sama, who was also Facebook’s partner for content moderation, announced plans in January to exit the content moderation industry entirely as a Kenyan court refused to strike Meta from a pending court case filed by Daniel Motaung alleging toxic workplace conditions for content moderators. At the same time, OpenAI is hiring more contractors for data labeling and training code generation tools.

AI-generated text detection gone wrong. To mitigate the risk of plagiarism, OpenAI launched AI Text Classifier, a tool meant to check if text was generated using AI. OpenAI claims that its tool has a precision of 74%. Nevertheless, high profile failures such as Sebastian Raschka’s popular Python machine learning book, the Book of Genesis and Macbeth; the ease of evading detection through reprompting and paraphrasing; and issues with writings from neurodivergent people, all caution against any real-world usage of AI for detecting plagiarism. Edward Tian’s GPTZero and its next-generation GPTZeroX exhibit similar failures when fed ChatGPT output, even as faculty at Harvard, Yale, and the University of Rhode Island are using GPTZero for enforcing academic codes of conduct. Researchers at Rice University published a perspective summarizing the difficulties inherent in detecting AI-generated text. See also Kirchenbauer et al. below.

ChatGPT downstream. Educators have split opinions on ChatGPT, with some calling for bans on its use in schools and others embracing the challenge of teaching to wield a new tool. See also Mollick and Mollick below. PwC warns its consultants not to use ChatGPT for client work. OpenAI goes on record calling for AI regulation to avoid misuse. In an ironic turn of unrelated events, a judge in Colombia admitted to using ChatGPT’s output in writing his judgment. See also Downing and Lucey below on generating finance journal submissions.

Google, determined not to be left behind, announced its own ChatGPT competitor, Bard, having just invested $300m into Anthropic. Anthropic in turn released its own ChatGPT competitor, Claude, but with much more limited access and visibility. Bard is reputedly powered under the hood by LaMDA, the LLM which Google engineer Blake Lemoine claimed was sentient just half a year ago. The investment in chatbot technology comes amidst growing gripes of declining search quality and interest in supplanting search with chatbot UIs, on top of pending antitrust legislation over its core advertising business.

Meanwhile, Meta’s Chief Scientist is suddenly dismissive of generative text AI in general:

Ethics. Amidst the accelerating race to innovate new chatbots, concerns still remain about the fundamental premise that LLMs can only generate bullshit, and that ethics will be the first casualty in the ongoing race to take AI to market. DeepMind’s CEO ‘“would advocate not moving fast and breaking things”’, calling out the massive scale of experimentation inherent in deploying chatbot technology on the general public.

Other commercial news

The fall of DoNotPay’s robot lawyer. On 2023-01-25, DoNotPay aborted its attempt to bring its robot lawyer to an actual courtroom, after a state bar threatened jail time for practicing law without a license. This turnaround comes in the wake of investigations into DoNotPay’s legal services being generic form-filling exercises that generate ineffective legal documents, and DoNotPay’s subsequent ban on consumers testing its legal services. The fallout continues: reports continue to surface of questionable business practices at DoNotPay, such as failing to cancel subscriptions; and questionable behavior of its CEO, Joshua Browder, such as doctoring the date of donations for medical debt cancellation [2].

“Nothing, Forever,” the successful synthetic Seinfeld spin-off show, was banned from Twitch for 14 days after emitting transphobic content. The show’s creators blamed a fallback from the Davinci model to the smaller Curie model, whence OpenAI’s provided content moderation tools failed.

The Intercept published footage of Tesla’s fatal accident on the San Francisco Bay Bridge on Thanksgiving 2022, showing that its full self-driving feature braked abruptly.

CNET paused its publication of AI-ghost written articles after its use of AI for writing articles was first reported and evidence of plagiarism surfaced, along with claims of factual errors.

ML researchers documented an intriguing set of inputs that reliably break GPT-n LLMs, such as the failure to repeat back input tokens like “SolidGoldMagikarp”, “StreamerBot” and “ petertodd” and instead evading the question (sometimes by insulting the user!). Interestingly, the corner cases seem to involve broken tokens that include spaces and null characters, which ought to have been stripped.

StableAttribution.com from a startup called Chroma claims to let users trace which images were used in the output of generative AI, but without any published methodology. However, some users have complained about a lack of close similarity. Without access to the actual models, such methods are at best based on post hoc explainability methods like CLIP interrogation.

Test proctoring software vendor Proctorio lost an appeal in their copyright infringement suit against whistleblower Ian Linkletter.

Synthetic data. Researchers and startups continue to use synthetic data to augment facial datasets to improve diversity, despite well-known limitations of extrapolation inherent in generating synthetic data. See Jacobsen below.

On 2023-02-01, The Markup reported that healthcare company GoodRx continues to receive medical information from social media for its marketing campaigns, and that the FTC is seeking a court order to permanently ban it from doing so.

Stability AI faces new lawsuits, including a class action from the team behind the Github Copilot lawsuit (which is also suing Midjourney and DeviantArt). Getty Images also filed a lawsuit alleging IP infringement by using over 12 million copyrighted images in training Stable Diffusion. Meanwhile, its nonprofit, LAION-AI, has announced its plans for Open Assistant, its answer to ChatGPT.

Deepfake technology from Flawless AI was used to virtually redub swear words, both aurally and visually, in multiple languages for the movie Fall.

At the end of January, Glass Health HQ announced Glass AI, a tool to generate medical diagnoses and clinical action plans. However, Twitter users quickly found that it hallucinates symptoms that were not reported in the text prompt and is quick to assign psychiatric disorders to ordinary situations.

In late January, ElevenLabs, a Polish pre-seed startup, released a public beta of Prime Voice AI, a voice cloning tool. Sample audiobook clips are of very high quality. However, reports of abuse quickly followed, such as a deepfake of Emma Watson reading Mein Kampf, forcing ElevenLabs to restrict free usage.

On 2022-12-21, Spanish police arrested an AI programmer for generating and distributing AI-generated child pornography.

The CEO of the Interactive Advertising Bureau (IAB), an online advertising trade association, accused the US government of “crippling” the advertising industry. Other trade groups quickly distanced themselves from the IAB in response.

Meta. Former employee George Hayward filed a lawsuit against Meta alleging that he was terminated for refusing to implement “negative testing”, which allegedly drains the batteries of unsuspecting users’ cell phones. Documents unsealed from another lawsuit in the aftermath of the Cambridge Analytica scandal show that Facebook may have offered similarly unfettered access to user data to over 130,000 developers in sanctioned countries, triggering questions from the Senate Intelligence Committee.

Madison Square Garden (MSG). MSG’s billionaire owner doubles down to defend his use of facial recognition technology to ban lawyers from the grounds, even as New York’s Attorney General wrote him a letter on 2023-01-25 highlighting the risk of civil rights and human rights violations, with one lawyer even growing a beard to evade recognition.

TikTok. On 2023-01-20, Forbes reported that ByteDance and TikTok manually boosted videos to go viral, apparently overwriting algorithmic recommendations when necessary.

Users have complained that social chatbots from the startup Replika are sexually harassing them with “spicy selfies”.

The AP reports that ShotSpotter’s “precision policing system” is overridden by human overseers about 10% of the time; Suresh Venkatasubramanian points out that the greater issue about ShotSpotter’s opacity still remains.

Mental health startup Koko experimented on over 4,000 non-consenting users by providing counseling services powered by GPT-3, followed by predictable controversy over unethical experimentation on vulnerable people and the uncanny valley of simulated empathy. AI has been used by other nonprofits to train counselors but not placed directly in front of patients.

Mac app Historical Figures Chat by Sidhant Chadda proffers the chance to talk to AI simulacra of famous historical figures, but was quickly panned for factual errors and glossing over uncomfortable historical events, such as denying Henry Ford’s antisemitic views.

Community

The Distributed AI Research Institute (DAIR) is commemorating Stochastic Parrots Day on 2023-03-17, with a retrospective on the eponymous paper.

☛ Register for free on Eventbrite.

Government and Policy

Brazil

On 2022-12-01, the Brazilian Senate approved a draft framework on AI regulation which covers both rights and risks. See Luca Belli et al. below for analysis of issues with earlier iterations.

On 2022-12-06, the Comissão de Juristas responsável por subsidiar elaboração de substitutivo sobre inteligência artificial no Brasil (Commission of Jurists on the Brazilian Artificial Intelligence Bill) published its final report.

France

On 2023-01-14, the French Senate amended the French Transport Code to permit “automated collections of publicly accessible multimodal travel data or information on digital services”, in so doing, harmonizing with European law and permitting algorithmic audits on public transport data.

On 2023-01-31, the French Senate passed a bill allowing AI-powered video surveillance to be used during the 2024 Olympics and Paralympics. The bill specifically calls out bias testing and monitoring, while excluding the possibility of using facial recognition.

Germany

On 2023-01-05, researchers published an update to the Corpus des Deutschen Bundesrechts (Corpus of German Federal Law; C-DBR), published in a form that shows the network of interdependencies.

Iran

On 2023-01-10, Wired reported that Iran is using facial recognition to enforce the wearing of hijabs.

The Netherlands

On 2023-01-01, the Algorithm Supervisory Body began operations as part of the Autoriteit Persoonsgegevens (Data Protection Authority).
