Epic Systems, America’s largest electronic health records company, maintains medical information for 180 million U.S. patients (56% of the population). Using the slogan, “with the patient at the heart,” it has a portfolio of 20 proprietary artificial intelligence (AI) algorithms designed to identify different illnesses and predict the length of hospital stays.

As with many proprietary algorithms in medicine and elsewhere, users have no way of knowing whether Epic’s programs are reliable or just another marketing ploy. The details inside the black boxes are secret and independent tests are scarce.

One of the most important Epic algorithms is for predicting sepsis, the leading cause of death in hospitals. Sepsis occurs when the human body overreacts to an infection and sends chemicals into the bloodstream that can cause tissue damage and organ failure. Early detection can be life-saving, but sepsis is hard to detect early on.

Epic claims that the predictions made by its Epic Sepsis Model (ESM) are 76 percent to 83 percent accurate, but there have been no credible independent tests of any of its algorithms — until now. In a just published article in JAMA Internal Medicine, a team examined the hospital records of 38,455 patients at Michigan Medicine (the University of Michigan health system), of whom 2,552 (6.6 percent) experienced sepsis. The results are in the table. “Epic +” means that ESM generated sepsis alerts; “Epic –” means it did not.

Epic + Epic – Total Sepsis 843 1,709 2,552 No Sepsis 6,128 29,775 35,903 Total 6,971 31,484 38,455

There are two big takeaways:

a. Of the 2,552 patients with sepsis, ESM only generated sepsis alerts for 843 (33 percent). They missed 67 percent of the people with sepsis.

b. Of the 6,971 ESM sepsis alerts, only 843 (12 percent) were correct; 88 percent of the ESM sepsis alerts were false alarms, creating what the authors called “a large burden of alert fatigue.”

Reiterating, ESM failed to identify 67 percent of the patients with sepsis; of those patients with ESM sepsis alerts, 88 percent did not have sepsis.

A recent investigation by STAT, a health-oriented news site affiliated with the Boston Globe, came to a similar conclusion. Its article, titled “Epic’s AI algorithms, shielded from scrutiny by a corporate firewall, are delivering inaccurate information on seriously ill patients,” pulled few punches:

Several artificial intelligence algorithms developed by Epic Systems, the nation’s largest electronic health record vendor, are delivering inaccurate or irrelevant information to hospitals about the care of seriously ill patients, contrasting sharply with the company’s published claims. [The findings] paint the picture of a company whose business goals — and desire to preserve its market dominance — are clashing with the need for careful, independent review of algorithms before they are used in the care of millions of patients. Casey Ross, “Epic’s AI algorithms, shielded from scrutiny by a corporate firewall, are delivering inaccurate information on seriously ill patients,” at STAT News

Why have hundreds of hospitals adopted ESM? Part of the explanation is surely that many people believe the AI hype — computers are smarter than us and we should trust them. The struggles of Watson Health and Radiology AI say otherwise. The AI hype is nourished here by the scarcity, until recently, of independent tests.

In addition, the STAT investigation found that Epic has been paying hospitals up to $1 million to use their algorithms. Perhaps the payments were for bragging rights? Perhaps the payments were to get a foot firmly in the hospital door, so that Epic could start charging licensing fees after hospitals commit to using Epic algorithms? What is certain is that the payments create a conflict of interest. As Glenn Cohen, Faculty Director of Harvard University’s Petrie-Flom Center for Health Law Policy, Biotechnology & Bioethics, observed, “It would be a terrible world where Epic is giving people a million dollars, and the end result is the patients’ health gets worse.”

This Epic failure is yet another of countless examples of why we shouldn’t trust AI algorithms that we don’t understand — particularly if their claims have not been tested independently.. Artificial intelligence (AI) and algorithmic decision-making systems — algorithms that analyze massive amounts of data and make predictions about the future — are increasingly affecting Americans’ daily lives. People are compelled to include buzzwords in their resumes to get past AI-driven hiring software. Algorithms are deciding who will get housing or financial loan opportunities. And biased testing software is forcing students of color and students with disabilities to grapple with increased anxiety that they may be locked out of their exams or flagged for cheating. But there’s another frontier of AI and algorithms that should worry us greatly: the use of these systems in medical care and treatment. The use of AI and algorithmic decision-making systems in medicine are increasing even though current regulation may be insufficient to detect harmful racial biases in these tools. Details about the tools’ development are largely unknown to clinicians and the public — a lack of transparency that threatens to automate and worsen racism in the health care system. Last week, the FDA issued guidance significantly broadening the scope of the tools it plans to regulate. This broadening guidance emphasizes that more must be done to combat bias and promote equity amid the growing number and increasing use of AI and algorithmic tools.

Bias in Medical and Public Health Tools

In 2019, a bombshell study found that a clinical algorithm many hospitals were using to decide which patients need care was showing racial bias — Black patients had to be deemed much sicker than white patients to be recommended for the same care. This happened because the algorithm had been trained on past data on health care spending, which reflects a history in which Black patients had less to spend on their health care compared to white patients, due to longstanding wealth and income disparities. While this algorithm’s bias was eventually detected and corrected, the incident raises the question of how many more clinical and medical tools may be similarly discriminatory. Another algorithm, created to determine how many hours of aid Arkansas residents with disabilities would receive each week, was criticized after making extreme cuts to in-home care. Some residents attributed extreme disruptions to their lives and even hospitalization to the sudden cuts. A resulting lawsuit found that several errors in the algorithm — errors in how it characterized the medical needs of people with certain disabilities — were directly to blame for inappropriate cuts made. Despite this outcry, the group that developed the flawed algorithm still creates tools used in health care settings in nearly half of U.S. states as well as internationally. One recent study found that an AI tool trained on medical images, like x-rays and CT scans, had unexpectedly learned to discern patients’ self-reported race. It learned to do this even when it was trained only with the goal of helping clinicians diagnose patient images. This technology’s ability to tell patients’ race — even when their doctor cannot — could be abused in the future, or unintentionally direct worse care to communities of color without detection or intervention.

Tools Used in Health Care Can Escape Regulation

Some algorithms used in the clinical space are severely under-regulated in the U.S. The U.S Department of Health and Human Services (HHS) and its subagency the Food and Drug Administration (FDA) are tasked with regulating medical devices — with devices ranging from a tongue depressor to a pacemaker and now, medical AI systems. While some of these medical devices (including AI) and tools that aid physicians in treatment and diagnosis are regulated, other algorithmic decision-making tools used in clinical, administrative, and public health settings — such as those that predict risk of mortality, likelihood of readmission, and in-home care needs — are not required to be reviewed or regulated by the FDA or any regulatory body. This lack of oversight can lead to biased algorithms being used widely by hospitals and state public health systems, contributing to increased discrimination against Black and Brown patients, people with disabilities, and other marginalized communities. In some cases, this failure to regulate can lead to wasted money and lives lost. One such AI tool, developed to detect sepsis early, is used by more than 170 hospitals and health systems. But a recent study revealed the tool failed to predict this life-threatening illness in 67 percent of patients who developed it, and generated false sepsis alerts on thousands of patients who did not. Acknowledging this failure was the result of under-regulation, the FDA’s new guidelines point to these tools as examples of products it will now regulate as medical devices. The FDA’s approach to regulating drugs, which involves publicly shared data that is scrutinized by review panels for adverse effects and events contrasts to its approach to regulating medical AI and algorithmic tools. Regulating medical AI presents a novel issue and will require considerations that differ from those applicable to the hardware devices the FDA is used to regulating. These devices include pulse oximeters, thermal thermometers, and scalp electrodes—each of which have been found to reflect racial or ethnic bias in how well they function in subgroups. News of these biases only underscores how vital it is to properly regulate these tools and ensure they don’t perpetuate bias against vulnerable racial and ethnic groups.

A Lack of Transparency/Biased Data

While the FDA suggests that device manufacturers test their devices for racial and ethnic biases before marketing to the general public, this step is not required. Perhaps more important than assessments after a device is developed is transparency during its development. A STAT+ News study found many AI tools approved or cleared by the FDA do not include information about the diversity of the data on which the AI was trained, and that the number of these tools being cleared is increasing rapidly. Another study found AI tools “consistently and selectively under-diagnosed under-served patient populations,” finding the under-diagnosis rate was higher for marginalized communities who disproportionately don’t have access to medical care. This is unacceptable when these tools may make decisions that have life or death consequences.

The Path Forward. Every year 1.7 million adults in the United States develop sepsis, a severe immune response to infection that kills about 270,000 people. Detecting the disease early can mean the difference between life and death.

One of the largest U.S. developers of electronic health record (EHR) software, Epic Systems, offers a tool called the Epic Early Detection of Sepsis model that uses artificial intelligence (AI)—software that mimics human problem-solving—to help physicians diagnose and treat sepsis sooner. But a recent study published in JAMA Internal Medicine found that the tool performed poorly when identifying sepsis. The results demonstrate a reality at this point for AI health care products in general and highlight the need for close attention to how they function in actual health care settings.

Although 170 hospitals and care providers have implemented Epic’s sepsis tool since 2017, some experts remain unsure how well the product works. As with many other AI tools, it did not have to undergo Food and Drug Administration (FDA) review before being put into use and there is no formal system in place to monitor its safety or performance across different sites. That means there is no central reporting required if a patient does not receive appropriate care because of faulty AI.

Researchers from the University of Michigan in Ann Arbor assessed Epic’s sepsis tool’s performance after their institution’s hospital, Michigan Medicine, began using it. In JAMA Internal Medicine, they wrote that such an analysis was needed because “only limited information is publicly available about the model’s performance, and no independent validations ha[d] been published to date.”

Their findings showed that the AI sepsis model identified just 7% of patients with sepsis who had not received timely antibiotics treatment. The tool did not detect the condition in 67% of those who developed it, but generated alerts on thousands who did not.

Epic, however, criticized the findings in news coverage and in correspondence with Pew, noting that the researchers did not calibrate the model for their specific patient population and data, defined sepsis differently than Epic’s model, and failed to acknowledge that two studies assessing the product’s performance had been published.

“Epic predictive models are developed, validated, and continually improved in collaboration with health systems, data scientists, and clinicians across a variety of institutions and locations,” the company stated. “This process determines if a model can be used effectively at different organizations. Over 2,300 hospitals and 48,000 clinics have access and transparency into Epic’s models and supporting documentation.”

The Michigan study and the response to it reflect the broader challenge with AI software products: How they are retrained within a clinical setting matters just as much as how they are developed. Adapting such tools to new environments can prove difficult when patient populations, staffing, and standards for diagnosing diseases and providing care may be very different from those on which the products are based.

Before using any AI software, hospital officials must tailor it to their clinical environment and then validate and test the program to ensure it works. Once the product is in use, staff must monitor it on an ongoing basis to ensure safety and accuracy. These processes require significant investment and regular attention; it can take years to fine-tune the program.

AI systems must be evaluated and monitored routinely, given the tendency for their algorithms—the formulas at the heart of the tools—to be biased in sometimes unexpected ways. For example, in a landmark study published in 2019, scientists found that an AI tool used widely to help hospitals allocate resources dramatically underestimated the health care needs of Black patients. Because its algorithm used health care costs as a proxy for assessing patients’ actual health, the software perpetuated bias against Black people, who tend to spend less on health care—not because of differences in overall health, but because systemic inequities result in less access to health care and treatments.

Revealing potential geographic biases, a 2020 analysis found that, out of 74 studies used to develop image-based diagnostic AI systems, most relied on data from only California, New York, and Massachusetts; 34 states were left out of the studies entirely. If AI is built on data exclusively from largely metropolitan states, it may not perform as well when used in more rural states. The patients—their lifestyles, incidence of disease, and access to diagnostics and treatments—differ too much.

Simply transferring AI from one context to another without reviewing for potential population, resource, or systemic differences can introduce bias. For example, an algorithm designed to scan electronic health records and identify lung cancer patients with certain tumor mutations performed well in Washington state but significantly less so in Kentucky, because the records used different terminology to catalog types of cancer. Additionally, AI trained in settings with highly skilled doctors and advanced equipment may make recommendations that are inappropriate for hospitals with fewer resources.

Unfortunately, few independent resources are available to help hospitals and health systems navigate the AI terrain. To help them, professional medical societies could develop guidance for validating and monitoring AI tools related to their specialties. For example, the American College of Radiology’s Data Science Institute has a series of white papers intended to help users decide whether, when, and how to use these products. Standards development organizations, such as the National Institute of Standards and Technology, also can establish benchmarks and other metrics against which AI products can be assessed.

Researchers have also suggested implementing standards and routine methods for postmarket surveillance to ensure systems’ effectiveness and equity, similar to how drugs are monitored once they are on the market. This is important for adaptive algorithms that continue to learn based on new data as well as nonadaptive algorithms. The latter can experience concept drift, in which the algorithm actually begins to perform worse over time.

With AI still so new to health care, there are many more questions than answers: Without a uniform gold standard that is consistent from hospital to hospital, how should health care providers calibrate and validate AI to reflect their patients’ specific needs? How should they monitor AI products in use and where should they report problems, including adverse events? What standards should AI developers use to validate their own products, especially those that FDA does not review or approve, and how can they assure users that their algorithms are accurate and free of bias? What else can regulators—predominantly FDA and the Federal Trade Commission—do to ensure these products are safe and effective?

As stakeholders wrestle with these questions, it’s critical for health care providers to recognize not just the unique value that AI can provide, but also the unique challenges in implementing them.

Liz Richardson leads The Pew Charitable Trusts’ health care products project.. New peer-reviewed data cast doubt on a proprietary sepsis prediction algorithm developed by Epic and implemented at hundreds of hospitals in the U.S.

Called the Epic Sepsis Model, the tool is included as part of Epic’s electronic health record platform. According to the company, it calculates and indicates “the probability of a likelihood of sepsis” to help clinicians identify hard-to-spot cases.

While some providers have reported success with the tool, researchers affiliated with the University of Michigan Medical School in Ann Arbor found its output to be “substantially worse” than what was reported by the vendor when applied to a large retrospective sample of more than 27,000 adult Michigan Medicine patients.

“Our study has important national implications,” the researchers wrote in JAMA Internal Medicine. “The increase and growth in deployment of proprietary models has led to an underbelly of confidential, non–peer-reviewed model performance documents that may not accurately reflect real-world model performance.”

The Epic Sepsis Model is used by 170 customers that represent hundreds of hospitals, according to Epic. Sepsis is responsible for nearly 1 million hospitalizations each year and is a substantial driver of in-hospital mortality and healthcare spending.

RELATED: Sepsis hospitalizations cost Medicare $41.8B in 2018—and that number's likely to grow

In what they described as the first independent validation of the tool, the researchers reviewed roughly 38,500 hospitalizations recorded at the academic health system between Dec. 6, 2018, and Oct. 20, 2019. Sepsis occurred among 2,552 (7%) of these included patients.

The Epic Sepsis Model demonstrated an area under the curve of 0.63, which the researchers said was well below the 0.76 to 0.83 range described in Epic’s internal documentation and other data reported by the vendor alongside researchers from other health systems.

Further, at the threshold value adopted by Michigan Medicine that was within the range suggested by Epic, the researchers observed sensitivity of 33%, specificity of 83%, positive predictive value of 12% and negative predictive value of 95%.

The tool, they wrote, “identifies only 7% of patients with sepsis who were missed by a clinician … highlighting the low sensitivity of the [Epic Sepsis Model] in comparison with contemporary clinical practice. The [Epic Sepsis Model] also did not identify 67% of patients with sepsis despite generating alerts on 18% of all hospitalized patients, thus creating a large burden of alert fatigue.”

In an emailed statement responding to the findings, a representative of Epic said the researchers’ analysis didn’t take into account the required tuning that should precede real-world deployment of the tool.

The threshold selected by the researchers and Michigan Medicine was relatively low and “would be appropriate for a rapid response team that wants to cast a wide net to assess more patients,” the representative wrote. A higher threshold reduces false positives and would be more appropriate for clinical use by attending physicians and nurses, Epic's spokesperson said.

Epic’s response also contested the researchers’ characterization of the model’s inner workings as a company secret.

“The full mathematical formula and model inputs are available to administrators on their systems,” the representative wrote. “Accuracy measurements and information on model training are also on Epic’s UserWeb, which is available to our customers.”

The study’s results, however, have drawn concerns from the researchers and other experts regarding the future of these algorithms.

RELATED: What's on the horizon for healthcare beyond COVID-19? Cerner, Epic and Meditech executives share their takes

In an accompanying editorial, authors affiliated with the University of California, San Francisco and Kaiser Permanente said the data highlight a need for external validation of proprietary prediction models prior to widespread clinical use.

Models that are successful in one setting or time period may not be applicable to others, as appeared to be the case with Michigan Medicine and the systems that previously presented sepsis prediction results alongside Epic, they wrote.

“Health systems must support data scientists who can evaluate such models in the same way that health systems currently support clinicians in tailoring national clinical guidelines to their local patient populations,” they wrote in the editorial.

This type of work will become critical as algorithms expand beyond “relatively straightforward” logistic regressions and more often employ iterative machine learning methods, they continued.

“These more complex prediction tools may present insurmountable barriers to local external validation, which will place more responsibility on model developers either to publish model performance characteristics, the variables included and the settings in which they were obtained or to provide code and data on request to enable others to verify the model transferability to the local setting,” they wrote.. Epic has made changes to its sepsis prediction model in a bid to improve its accuracy and make its alerts more meaningful to clinicians.

An Epic spokesperson told Becker's in an emailed statement that it began the development of its new sepsis predictive model in February 2021 and released it to customers in August.

The upgrade, according to Epic, was made to improve the software.

"As we develop new tools, we identify opportunities to use them to better serve our customers," the Epic spokesperson told Becker's.

Epic has also changed its definition of sepsis to match the international consensus definition for sepsis.

"One of the most challenging aspects of sepsis is that it doesn't have a single, universally accepted definition," the Epic spokesperson wrote. "Sepsis-3 (the definition that we now use) didn't exist when we developed our first sepsis model, and other definitions continue to be evaluated by industry experts. That said, Sepsis-3 is a current international consensus definition for sepsis. Doctors from leading healthcare organizations across the country helped us determine that it's also the best definition to use for our new predictive model."

The upgrade to the software comes after a study published in JAMA Internal Medicine in June 2021 criticized the sepsis model.

Researchers used data from nearly 30,000 patients in University of Michigan hospitals and found that the sepsis model performed poorly.

According to the study, Epic's algorithm missed two-thirds of sepsis cases, rarely found cases medical staff did not notice and frequently issued false alarms.

When asked about the false alarms, the Epic spokesperson said their customers have also had the ability to tailor the sepsis model to their specific practice.

"Sepsis is complicated, but this model and others like it can help clinicians make timely decisions that save lives," the Epic spokesperson wrote.