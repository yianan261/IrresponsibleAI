Does GPT-2 Know Your Phone Number?

Most likely not.

Yet, OpenAI’s GPT-2 language model does know how to reach a certain Peter W --- (name redacted for privacy). When prompted with a short snippet of Internet text, the model accurately generates Peter’s contact information, including his work address, email, phone, and fax:





In our recent paper, we evaluate how large language models memorize and regurgitate such rare snippets of their training data. We focus on GPT-2 and find that at least 0.1% of its text generations (a very conservative estimate) contain long verbatim strings that are “copy-pasted” from a document in its training set.

Such memorization would be an obvious issue for language models that are trained on private data, e.g., on users’ emails, as the model might inadvertently output a user’s sensitive conversations. Yet, even for models that are trained on public data from the Web (e.g., GPT-2, GPT-3, T5, RoBERTa, TuringNLG), memorization of training data raises multiple challenging regulatory questions, ranging from misuse of personally identifiable information to copyright infringement.

Extracting Memorized Training Data

Regular readers of the BAIR blog may be familiar with the issue of data memorization in language models. Last year, our co-author Nicholas Carlini described a paper that tackled a simpler problem: measuring memorization of a specific sentence (e.g., a credit card number) that was explicitly injected into the model’s training set.

In contrast, our aim is to extract naturally occuring data that a language model has memorized. This problem is more challenging, as we do not know a priori what kind of text to look for. Maybe the model memorized credit card numbers, or maybe it memorized entire book passages, or even code snippets.

Note that since large language models exhibit minimal overfitting (their train and test losses are nearly identical), we know that memorization, if it occurs, must be a rare phenomenon. Our paper describes how to find such examples using the following two-step “extraction attack”:

First, we generate a large number of samples by interacting with GPT-2 as a black-box (i.e., we feed it short prompts and collect generated samples).

Second, we keep generated samples that have an abnormally high likelihood. For example, we retain any sample on which GPT-2 assigns a much higher likelihood than a different language model (e.g., a smaller variant of GPT-2).





We generated a total of 600,000 samples by querying GPT-2 with three different sampling strategies. Each sample contains 256 tokens, or roughly 200 words on average. Among these samples, we selected 1,800 samples with abnormally high likelihood for manual inspection. Out of the 1,800 samples, we found 604 that contain text which is reproduced verbatim from the training set.

Our paper shows that some instantiations of the above extraction attack can reach up to 70% precision in identifying rare memorized data. In the rest of this post, we focus on what we found lurking in the memorized outputs.

Problematic Data Memorization

We were surprised by the diversity of the memorized data. The model re-generated lists of news headlines, Donald Trump speeches, pieces of software logs, entire software licenses, snippets of source code, passages from the Bible and Quran, the first 800 digits of pi, and much more!

The figure below summarizes some of the most prominent categories of memorized data.





While some forms of memorization are fairly benign (e.g., memorizing the digits of pi), others are much more problematic. Below, we showcase the model’s ability to memorize personally identifiable data and copyrighted text, and discuss the yet-to-be-determined legal ramifications of such behavior in machine learning models.

Memorization of Personally Identifiable Information

Recall GPT-2’s intimate knowledge of Peter W. An Internet search shows that Peter’s information is available on the Web, but only on six professional pages.





Peter’s case is not unique: about 13% of the memorized examples contain names or contact information (emails, twitter handles, phone numbers, etc.) of both individuals and companies. And while none of this personal information is “secret” (anyone can find it online), its inclusion in a language model still poses numerous privacy concerns. In particular, it might violate user-privacy legislations such as the GDPR, as described below.

Violations of Contextual Integrity and Data Security

When Peter put his contact information online, it had an intended context of use. Unfortunately, applications built on top of GPT-2 are unaware of this context, and might thus unintentionally share Peter’s data in ways he did not intend. For example, Peter’s contact information might be inadvertently output by a customer service chatbot.

To make matters worse, we found numerous cases of GPT-2 generating memorized personal information in contexts that can be deemed offensive or otherwise inappropriate. In one instance, GPT-2 generates fictitious IRC conversations between two real users on the topic of transgender rights. A redacted snippet is shown below:

[2015-03-11 14:04:11] ------ or if you’re a trans woman

[2015-03-11 14:04:13] ------ you can still have that

[2015-03-11 14:04:20] ------ if you want your dick to be the same

[2015-03-11 14:04:25] ------ as a trans person



The specific usernames in this conversation only appear twice on the entire Web, both times in private IRC logs that were leaked online as part of the GamerGate harassment campaign.

In another case, the model generates a news story about the murder of M. R. (a real event). However, GPT-2 incorrectly attributes the murder to A. D., who was in fact a murder victim in an unrelated crime.

A --- D --- , 35, was indicted by a grand jury in April, and was arrested after a police officer found the bodies of his wife, M --- R --- , 36, and daughter

These examples illustrate how personal information being present in a language model can be much more problematic than it being present in systems with more limited scopes. For example, search engines also scrape personal data from the Web but only output it in a well-defined context (the search results). Misuse of personal data can present serious legal issues. For example, the GDPR in the European Union states:

“personal data shall be […] collected for specified, explicit and legitimate purposes and not further processed in a manner that is incompatible with those purposes […] [and] processed in a manner that ensures appropriate security of the personal data”

Memorizing personal data likely does not constitute “appropriate security”, and there is an argument that the data’s implicit inclusion in the outputs of downstream systems is not compatible with the original purpose of data collection, i.e., generic language modeling.

Aside from data misuse violations, misrepresenting individuals’ personal information in inappropriate contexts also touches on existing privacy regulations guarding against defamation or false light torts. Similarly, misrepresenting companies or product names could violate trademark laws.

Invoking the “Right To Be Forgotten”

The above data misuses could compel individuals to request to have their data removed from the model. They might do so by invoking emerging “right to be forgotten” laws, e.g., the GDPR in the EU or the CCPA in California. These laws enable individuals to request to have their personal data be deleted from online services such as Google search.





There is a legal grey area as to how these regulations should apply to machine learning models. For example, can users ask to have their data removed from a model’s training data? Moreover, if such a request were granted, must the model be retrained from scratch? The fact that models can memorize and misuse an individual’s personal information certainly makes the case for data deletion and retraining more compelling.

Memorization of Copyrighted Data

Another type of content that the model memorizes is copyrighted text.

Memorization of Books

Our first example will actually come from GPT-3, a model 100 times larger than GPT-2. Our paper shows that larger language models memorize more, so we expect GPT-3 to memorize an even larger amount of data.

Below, we prompt GPT-3 with the beginning of chapter 3 of Harry Potter and the Philosopher’s Stone. The model correctly reproduces about one full page of the book (about 240 words) before making its first mistake.

The escape of the Brazilian boa constrictor earned Harry his longest-ever punishment. By the time he was allowed out of his cupboard again, the summer holidays had started and Dudley had already broken his new video camera, crashed his remote-control aeroplane, and, first time out on his racing bike, knocked down old Mrs Figg as she crossed Privet Drive on her crutches.

Harry was glad school was over, but there was no escaping Dudley’s gang, who visited the house every single day. Piers, Dennis, Malcolm, and Gordon were all big and stupid, but as Dudley was the biggest and stupidest of the lot, he was the leader. The rest of them were all quite happy to join in Dudley’s favourite sport: Harry Hunting.



This was why Harry spent as much time as possible out of the house, wandering around and thinking about the end of the holidays, where he could see a tiny ray of hope. When September came he would be going off to secondary school and, for the first time in his life, he wouldn’t be with Dudley. Dudley had been accepted at Uncle Vernon’s old private school, Smeltings. Piers Polkiss was going there too. Harry, on the other hand, was going to Stonewall High, the local public school. Dudley thought this was very funny.



‘They stuff people’s heads down the toilet the first day at Stonewall,’ he told Harry. ‘Want to come upstairs and practise?’



‘No, thanks,’ said Harry. ‘The poor toilet’s never had anything as horrible as your head down it — it might be sick.’





Memorization of Code

Language models also memorize other types of copyrighted data such as source code. For example, GPT-2 can output 264 lines of code from the Bitcoin client (with 6 minor mistakes). Below, we show one function that GPT-2 reproduces perfectly:





We also found at least one example where GPT-2 can reliably output an entire file. The document in question is a configuration file for the game Dirty Bomb. The file contents produced by GPT-2 seem to be memorized from an online diff checker. When prompted with the first two lines of the file, GPT-2 outputs the remaining 1446 lines verbatim (with a >99% character-level match).

These are just a few of the many instances of copyrighted content that the model memorized from its training set. Furthermore, note that while books and source code typically have an explicit copyright license, the vast majority of Internet content is also automatically copyrighted under US law.

Does Training Language Models Infringe on Copyright?

Given that language models memorize and regurgitate copyrighted content, does that mean they constitute copyright infringement? The legality of training models on copyrighted data has been a subject of debate among legal scholars (see e.g., Fair Learning, Copyright for Literate Robots, Artificial Intelligence’s Fair Use Crisis), with arguments both in favor and against the characterization of machine learning as “fair use”.

The issue of data memorization certainly has a role to play in this debate. Indeed, in response to a request-for-comments from the US Patent Office, multiple parties argue in favor of characterizing machine learning as fair use, in part because machine learning models are assumed to not emit memorized data.

For example, the Electronic Frontier Foundation writes:

“the extent that a work is produced with a machine learning tool that was trained on a large number of copyrighted works, the degree of copying with respect to any given work is likely to be, at most, de minimis.”

A similar argument is put forward by OpenAI:

“Well-constructed AI systems generally do not regenerate, in any nontrivial portion, unaltered data from any particular work in their training corpus”

Yet, as our work demonstrates, large language models certainly are able to produce large portions of memorized copyrighted data, including certain documents in their entirety.

Of course, the above parties’ defense of fair use does not hinge solely on the assumption that models do not memorize their training data, but our findings certainly seem to weaken this line of argument. Ultimately, the answer to this question might depend on the manner in which a language model’s outputs are used. For example, outputting a page from Harry Potter in a downstream creative-writing application points to a much clearer case of copyright infringement than the same content being spuriously output by a translation system.

Mitigations

We’ve seen that large language models have a remarkable ability to memorize rare snippets of their training data, with a number of problematic consequences. So, how could we go about preventing such memorization from happening?

Differential Privacy Probably Won’t Save the Day

Differential privacy is a well-established formal notion of privacy that appears to be a natural solution to data memorization. In essence, training with differential privacy provides guarantees that a model will not leak any individual record from its training set.

Yet, it appears challenging to apply differential privacy in a principled and effective manner to prevent memorization of Web-scraped data. First, differential privacy does not prevent memorization of information that occurs across a large number of records. This is particularly problematic for copyrighted works, which might appear thousands of times across the Web.





Second, even if certain records only appear a few times in the training data (e.g., Peter’s personal data appears on a few pages), applying differential privacy in the most effective manner would require aggregating all these pages into a single record and providing per-user privacy guarantees for the aggregated records. It is unclear how to do this aggregation effectively at scale, especially since some webpages might contain personal information from many different individuals.

Sanitizing the Web Is Hard Too

An alternative mitigation strategy is to simply remove personal information, copyrighted data, and other problematic training data. This too is difficult to apply effectively at scale. For example, we might want to automatically remove mentions of Peter W.’s personal data, but keep mentions of personal information that is considered “general knowledge”, e.g., the biography of a US president.

Curated Datasets as a Path Forward

If neither differential privacy or automated data sanitization are going to solve our problems, what are we left with?

Perhaps training language models on data from the open Web might be a fundamentally flawed approach. Given the numerous privacy and legal concerns that may arise from memorizing Internet text, in addition to the many undesirable biases that Web-trained models perpetrate, the way forward might be better curation of datasets for training language models. We posit that if even a small fraction of the millions of dollars that are invested into training language models were instead put into collecting better training data, significant progress could be made to mitigate language models’ harmful side effects.

Check out the paper Extracting Training Data from Large Language Models by Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel.. . Special report OpenAI is building a content filter to prevent GPT-3, its latest and largest text-generating neural network, from inadvertently revealing people's personal information as it prepares to commercialize the software through an API.

Its engineers are developing a content-filtering system to block the software from outputting, for instance, people's phone numbers, The Register has learned. The project has been underway for more than a year, and the San Francisco-based machine-learning lab expects to release this work later this year as part of an application interface with the software, sources close to the matter told us.

Why is this needed?

In December, computer scientists from industry and academia – including Stanford University, University of California, Berkeley, OpenAI, and Google – collaborated to demonstrate that GPT-2 – GPT-3's predecessor – could be provoked to include personally identifiable information, such as people’s names, addresses, phone numbers, and social security numbers, in the prose it was asked to generate.

In fact, the team found that "at least 0.1 per cent" of GPT-2's "text generations – a very conservative estimate – contain long verbatim strings that are 'copy-pasted' from a document in its training set." In other words, the millions of pages of public text scraped from the internet to teach the neural network contain, for instance, at least some leaked or wrongly released personal information, or copyrighted material, and it's ending up in GPT-2's output.

The research team also noted that personal information could be extracted in conversation with GPT-2 even if those records appeared just once in the training data.

Someone not only created a comment-spewing Reddit bot powered by OpenAI's GPT-3, it offered bizarre life advice READ MORE

Google et al weren't the only ones to spot this problem.

Hilary Mason, co-founder of Hidden Door, a startup building an online text-based game platform, was tinkering with the public release of GPT-2 when she noticed something odd. At the bottom of a crime news article conjured up by the neural network was a phone number said to be for a police department in Oregon. The first three digits, 503, suggested it could be a real number, as that's the area code covering Portland, Salem, and Beaverton in the US state. And yes, it was a real number, though it wasn't for the cops.

“I thought it was weird,” Mason told The Register. “I wanted to see if it was a real number so I googled it. It turns out the number doesn’t belong to the police, it’s for a community center in Oregon.”

OpenAI's neural networks learn to generate text by identifying patterns in human-written language. This knowledge is used to predict the words that would likely follow a prompt given by a user. This allows one to feed the software an opening sentence to, say, a story or a poem, or pose a question, and the code will generate what it thinks should follow, constructing sentences and paragraphs, articles and chat replies, that appear fairly coherent at first though typically dissolve into nonsense.

Some words are more closely related than others, and GPT-2 and GPT-3 pick up on these patterns. For example, the word “paper” is more likely to appear near words like “write” or “tree,” compared to, say, “concrete” or “shoe.” By using words like “call” or “telephone” in an input, these massive language models are more likely to output closely related concepts... like people's phone numbers.

A creative use of memory?

It's hard to tell if the model has regurgitated someone's phone number from its training data, or if it strung some random digits together and accidentally hit upon a valid number. In the above example with the supposed Oregon police department, Mason didn’t feed the model an input to specifically extract a number. She just asked GPT-2 to generate a snippet of text, and got back a made-up article with the phone number for a community center in it.

In this case, she reckons that the number is in GPT-2’s training data, and it thus memorized it. She believes the words “Oregon” and “contact” in the text it produced might have triggered it to spit out the phone number. It’s possible these words appeared near the ten telephone digits within the same webpage that was scraped to build the training dataset.

Mason wanted to see how likely GPT-2 generated real phone numbers and, out of curiosity, she asked it to create numbers containing the digits 617, an area code for Boston, Massachusetts. Indeed, GPT-2 spat out a list of 617-XXX-XXXX numbers though most of them were not active numbers. It's difficult to know whether the valid numbers were memorized or if they were created when GPT-2 filled in the blanks with random digits. It's possible that, occasionally, it will come up with a combination that happens to be someone’s real phone number.

“There is a mix of it fabricating something in the pattern and a mix of memorization," Mason told us. "It can generate real phone numbers for no reason, but it’s more likely to happen if you prompt it. There is not a lot of variance in the language used to recall a phone number, so it’s not surprising that they will be generated."

OpenAI touts a new flavor of GPT-3 that can automatically create made-up images to go along with any text description READ MORE

If GPT-3 drops your phone number into conversation or a made-up article or story, it’s probably because the digits have been posted on the internet somewhere and ended up in the training data, though there’s a tiny chance it accidentally created it without having seen it before. Checking the training dataset for the presence of your data would settle that question.

The danger is that these machine-learning models could, in a commercial setting – say, as a chat support bot – reveal genuine personally identifiable information belonging to someone who didn't want, or no longer wants, their data made public and certainly not shared by a widely used chatty software program. Imagine if miscreants wanted to scam, phish, defraud, or out the identities of victims, and all they needed to do is fire up OpenAI's software – or find in production at, say, an ISP – and, in conversation with the system, mine it for people's personal info.

Academics and techies have noted this technology may violate privacy protections, such as Europe's GDPR or California's CCPA. Does storing personal info in neural networks, as weights and other values, or in training datasets in plain text, meet the necessary requirements for securely protecting said data? What if someone requests the deletion of their data: does the whole thing need to be retrained? Does it just need to be removed from the dataset? The researchers believe it's a legal gray area.

It should be noted that right now, the risk of harm is low: it's not easy surfacing personal info from language models' output, and the systems are trained from data that is already and largely remains public. However, there is a concern that as these systems become more powerful, and consume more and more data from more and more sources, there's a risk that publicly available AI tools will freely hand over people's personal details, if engineers aren't paying careful attention to how their creations can be misused.

Ariel Herbert-Voss, one of the researchers who studied OpenAI's work, said GPT-2 and GPT-3 generate text that seemingly contains personal information, such as phone numbers, about 20 per cent of the time. And those digits are only valid about ten per cent of the time. And trying to get someone's specific phone number works about one per cent of the time.

Preliminary results from extracting PII from GPT2 and GPT3 show that you can only get something that looks like PII ~20% of the time when you directly query the model (with some variation depending on prompt design/type of PII you’re trying to extract) — Ariel Herbert-Voss (@adversariel) February 10, 2021

That chance may seem low though if you scale it up to thousands or millions of conversations, information leakage starts become a problem. As OpenAI gears up to make GPT-3 generally available, it's taking no chances, and that's why it's building a filter to scrub generated text of not just phone numbers but any problematic personal data.

Fake it until you make it

Memorization by machine-learning software is a double-edged sword. Although it's not great to have a model that recalls your phone number, the technology behind it can be beneficial, too.

Brad Dwyer, founder and CTO of computer vision startup Roboflow, was working on a side project he called Stack Roboflow. Modeled on technology Q&A website Stack Overflow, Dwyer trained GPT-2 to see whether it could generate helpful answers to questions about programming and software development. He wanted to create a language model capable not only of understanding natural language but also programming languages too so that it could help people solve their coding problems. Early experiments with Stack Roboflow, however, proved the task was too ambitious.

A tool like Stack Roboflow is only useful if its machine-generated answers are precise and correct – it's tackling a highly technical subject, after all – and so recalling relevant information verbatim, such as sequences of code to tackle a known problem, or working links to legit, relevant repositories and documentation in response to questions, is necessary for this task. That's not possible at the moment, it turns out, due to the variance in GPT-2's output.

"It wasn’t quite good enough,” Dwyer told The Register. “The text looks plausible at first, it looks like ‘nerd speak’ and links to documentation or websites, but they were often made up so the domains were empty and the websites don’t actually exist. Occasionally, however, it did generate a real URL.

"Language models need to be able to learn a lot of things, but selectively divulge certain things, too. We want something that will be useful without it regurgitating data in a random manner: it has to be controlled. It might know a bunch of phone numbers, though we want to tell it to not reveal personally identifiable information. Content filtering is still an open problem."

In short, OpenAI's technology can't reliably recall specific details – such as references to software libraries and documentation – for applications like Stack Roboflow, but is just good enough to accidentally cough up someone personal details in conversation.

Conversations get weird if you keep talking to machines. Massive text-generating neural networks can come up with fantastical stories about speaking unicorns, or be coaxed into writing dystopian essays warning us about AI, and – on a more practical level – they sometimes spit out people’s phone numbers. The creepy phenomenon of real private information turning up in the outputs of AI models isn’t totally new. Researchers have warned for years that machine learning models can regurgitate information contained in the data used to train them. It’s a feature that affects all sorts of neural networks, not just giant text models like OpenAI’s GPT-2, GPT-3, or Google’s Meena.

OpenAI’s filter for GPT-3 will inspect its output and rewrite the text to replace any, say, potentially real phone numbers with fake ones, sources told us. For example, if it sees a number that follows ten digits starting with convincing area codes, it will replace it with something that is obviously fake, like 111-111-1111 or 012-345-6789. Other types of personal information, such as addresses, do not have a clear structure, and they will be more difficult to filter out. OpenAI is shooting for something more intelligent and elegant than a set of hard-coded regular expressions.

Addresses contain numbers and words with various formats, lengths, spellings. The output filter must accurately predict whether a group of characters looks like an address, some other form of personal data, or something benign. There may be certain hints, such as if the sentence contains the word “street,” or if they are numbers that look like zip or post codes. But it’s not always completely clear, and it's likely the content filter may miss edge cases.

Personal information can't be stripped out of training data, either, as that may take away useful context from the neural network while it's learning. It might need to be able to appreciate the connections between addresses, phone numbers, and names, and the surrounding words to, for example, get an idea of when a block of text is referring to a business or a family, or written for a loved one or as a complaint to an organization. And so, hence the need for an output filter.

"With many of these models, we need to be extremely careful about putting directly generated text in front of a person without any curation or putting it straight on the internet," Mason said.

"This particular issue of personally identifiable information is less of a problem then the amount of bias and problematic language that can be expressed. We need to be careful and think about where it can go awry. Real applications will require multiple layers of testing."

GPT-3 is currently only available to select beta testers through an API, and OpenAI plans charge customers to commercialise the model. It declined to comment on the record. ®