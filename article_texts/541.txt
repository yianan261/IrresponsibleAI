The lawsuit began like so many others: A man named Roberto Mata sued the airline Avianca, saying he was injured when a metal serving cart struck his knee during a flight to Kennedy International Airport in New York.

When Avianca asked a Manhattan federal judge to toss out the case, Mr. Mata’s lawyers vehemently objected, submitting a 10-page brief that cited more than half a dozen relevant court decisions. There was Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and, of course, Varghese v. China Southern Airlines, with its learned discussion of federal law and “the tolling effect of the automatic stay on a statute of limitations.”

There was just one hitch: No one — not the airline’s lawyers, not even the judge himself — could find the decisions or the quotations cited and summarized in the brief.

That was because ChatGPT had invented everything.

The lawyer who created the brief, Steven A. Schwartz of the firm Levidow, Levidow & Oberman, threw himself on the mercy of the court on Thursday, saying in an affidavit that he had used the artificial intelligence program to do his legal research — “a source that has revealed itself to be unreliable.”. . New York CNN —

The meteoric rise of ChatGPT is shaking up multiple industries – including law, as one attorney recently found out.

Roberto Mata sued Avianca airlines for injuries he says he sustained from a serving cart while on the airline in 2019, claiming negligence by an employee. Steven Schwartz, an attorney with Levidow, Levidow & Oberman and licensed in New York for over three decades, handled Mata’s representation.

But at least six of the submitted cases by Schwartz as research for a brief “appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” said Judge Kevin Castel of the Southern District of New York in an order.

The fake cases source? ChatGPT.

“The court is presented with an unprecedented circumstance,” Castel wrote in a May 4 order.

Among the purported cases: Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines, all of which did not appear to exist to either the judge or defense, the filing said.

Schwartz, in an affidavit, said that he had never used ChatGPT as a legal research source prior to this case and, therefore, “was unaware of the possibility that its content could be false.” He accepted responsibility for not confirming the chatbot’s sources.

Schwartz is now facing a sanctions hearing on June 8.

In an affidavit this week, he said he “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

In late April, Avianca’s lawyers from Condon & Forsyth penned a letter to Castel questioning the authenticity of the cases.

In an affidavit filed Thursday, fellow attorney Peter Loduca said he “had no reason to doubt the sincerity” of Schwartz’s research and that he had no role in the research.

Schwartz was ordered to show cause why he shouldn’t be sanctioned “for the use of a false and fraudulent notarization,” in an affidavit filed on April 25.

Schwart’s affidavit Wednesday contained screenshots of the attorney appearing to confirm the authenticity of the case with ChatGPT.

“is varghese a real case,” Schwartz asked the chatbot.

“Yes,” ChatGPT doubled down, it “is a real case.”

Schwartz then asked for its source. The chatbot again claimed the false case was real.

“I apologize for the confusion earlier,” ChatGPT replied. “Upon double-checking, I found the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused.”

When Schwartz asked the chatbot if any other cases were fake, ChatGPT replied the other cases “are real” and could be found on “reputable legal databases.”

CNN has reached out to Schwartz and Loduca for comment.. Lawyer Steven Schwartz of Levidow, Levidow & Oberman has been practicing law for three decades. Now, one case can completely derail his entire career.

Why? He relied on ChatGPT in his legal filings and the AI chatbot completely manufactured previous cases, which Schwartz cited, out of thin air.

It all starts with the case in question, Mata v. Avianca. According to the New York Times , an Avianca customer named Roberto Mata was suing the airline after a serving cart injured his knee during a flight. Avianca attempted to get a judge to dismiss the case. In response, Mata's lawyers objected and submitted a brief filled with a slew of similar court decisions in the past. And that's where ChatGPT came in.

Schwartz, Mata's lawyer who filed the case in state court and then provided legal research once it was transferred to Manhattan federal court, said he used OpenAI's popular chatbot in order to "supplement" his own findings.

ChatGPT provided Schwartz with multiple names of similar cases: Varghese v. China Southern Airlines, Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.

The problem? ChatGPT completely made up all those cases. They do not exist.

Avianca's legal team and the judge assigned to this case soon realized they could not locate any of these court decisions. This led to Schwartz explaining what happened in an affidavit on Thursday. The lawyer had referred to ChatGPT for help with his filing.

According to Schwartz, he was "unaware of the possibility that its content could be false.” The lawyer even provided screenshots to the judge of his interactions with ChatGPT, asking the AI chatbot if one of the cases were real. ChatGPT responded that it was. It even confirmed that the cases could be found in "reputable legal databases." Again, none of them could be found because the cases were all created by the chatbot.

It's important to note that ChatGPT, like all AI chatbots, is a language model trained to follow instructions and provide a user with a response to their prompt. That means, if a user asks ChatGPT for information, it could give that user exactly what they're looking for, even if it's not factual.

The judge has ordered a hearing next month to "discuss potential sanctions" for Schwartz in response to this “unprecedented circumstance." That circumstance again being a lawyer filing a legal brief using fake court decisions and citations provided to him by ChatGPT.. A US attorney is now “greatly regretting” his decision to trust OpenAI’s ChatGPT in a litigation process. Steven Schwartz will be charged in a New York court for using fake citations cooked up by the AI tool in legal research for a case he was handling.

In a sworn affidavit, Schwartz admitted to using ChatGPT to research for the case, representing his client Roberto Mata, to sue Colombian airline Avianca for injuries sustained onboard one of its planes in 2019.

Advertisement

Despite ChatGPT’s widely known warnings that it can sometimes produce incorrect information, Schwartz defended himself saying he was “unaware that its content could be false.”

Presiding judge Kevin Castel expressed his doubts over the authenticity of the cases the attorney presented. “Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” Castel said.

The cases ChatGPT presented the lawyer in his research were Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines.

These cases did not exist, Castel said in an order demanding an explanation for their citation by Mata’s legal team. They were found to be made up and had fake judicial wording. Even the quotes and internal citations were just make-believe.

Advertisement

Though Schwartz has vowed to never use AI in future to “supplement” his legal research “without absolute verification of its authenticity,” he now faces sanctions. A hearing on the matter is now scheduled for June 8.. . With the hype around AI reaching a fever pitch in recent months, many people fear programs like ChatGPT will one day put them out of a job. For one New York lawyer, that nightmare could become a reality sooner than expected, but not for the reasons you might think. As reported by The New York Times, attorney Steven Schwartz of the law firm Levidow, Levidow and Oberman recently turned to OpenAI’s chatbot for assistance with writing a legal brief, with predictably disastrous results.

A lawyer used ChatGPT to do "legal research" and cited a number of nonexistent cases in a filing, and is now in a lot of trouble with the judge 🤣 pic.twitter.com/AJSE7Ts7W7 — Daniel Feldman (@d_feldman) May 27, 2023

Schwartz’s firm has been suing the Colombian airline Avianca on behalf of Roberto Mata, who claims he was injured on a flight to John F. Kennedy International Airport in New York City. When the airline recently asked a federal judge to dismiss the case, Mata’s lawyers filed a 10-page brief arguing why the suit should proceed. The document cited more than half a dozen court decisions, including “Varghese v. China Southern Airlines,” “Martinez v. Delta Airlines” and “Miller v. United Airlines.” Unfortunately for everyone involved, no one who read the brief could find any of the court decisions cited by Mata’s lawyers. Why? Because ChatGPT fabricated all of them. Oops.

In an affidavit filed on Thursday, Schwartz said he had used the chatbot to “supplement” his research for the case. Schwartz wrote he was "unaware of the possibility that [ChatGPT’s] content could be false.” He even shared screenshots showing that he had asked ChatGPT if the cases it cited were real. The program responded they were, claiming the decisions could be found in “reputable legal databases,” including Westlaw and LexisNexis.

Schwartz said he “greatly regrets” using ChatGPT “and will never do so in the future without absolute verification of its authenticity.” Whether he has another chance to write a legal brief is up in the air. The judge overseeing the case has ordered a June 8th hearing to discuss potential sanctions for the “unprecedented circumstance” created by Schwartz’s actions.

This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.. A lawyer at a respected Tribeca firm admitted citing several “bogus” lawsuits he claimed bolstered his case — because he used an artificial intelligence chatbot to help write the Manhattan federal court filing.

The shocking admission from Steven Schwartz, an attorney with firm Levidow, Levidow & Oberman, came after he asked ChatGPT to find cases relevant to his client’s lawsuit — only for the bot to fabricate them entirely, court documents show.

The snafu began after Schwartz’s legal partner Peter LoDuca filed a lawsuit against the Colombian airline Avianca on behalf of Robert Mata, who was allegedly injured when a metal serving cart struck his knee on a flight to New York City.

When the airline’s lawyers asked the court to toss the suit, Schwartz filed a brief that supposedly cited more than a half dozen relevant cases.

There was just one problem: the cases — such as Miller v. United Airlines, Petersen v. Iran Air and Varghese v. China Southern Airlines — were completely made up by ChatGPT.

3 Lawyer Steven Schwartz said he used ChatGPT as a legal research source. The chatbots findings were completely made up. Steven Schwartz / LinkedIn

3 The humiliating legal drama played out at Manhattan’s federal court in connection with a lawsuit against an airline. MGR

“The court is presented with an unprecedented circumstance,” wrote Manhattan federal Judge P. Kevin Castel in a May 4 document, first reported by The New York Times on Saturday, calling on Schwartz and LoDucan to appear for a June 8 hearing to face possible sanctions over the eyebrow-raising filing.

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” Castel wrote.

In a sworn affidavit filed last week, Schwartz admitted that he used ChatGPT while compiling the paperwork that Loduca filed, writing he “relied on the legal opinions provided to him by a source that has revealed itself to be unreliable.”

The signed mea culpa went on to say that Schwartz “was unaware of the possibility that its [ChatGPT’s] content could be false.”

3 Shaboon v. Egyptair, Martinez v. Delta Airlines and Estate of Durden v. KLM Royal Dutch Airlines were among the fake cases generated by the app. REUTERS

The disgraced lawyer also told the judge he “greatly regrets” using AI for the legal “research” and “will never do so in the future without absolute verification of its authenticity.”

Peter Loduca wrote last week in an affidavit he was not involved in the malfeasance and “had no reason to doubt the authenticity of the case law” fabricated in the document.

Loduca added he had worked with Schwartz for 25 years and never recalled him looking to “mislead” a court.. Lawyers suing the Colombian airline Avianca submitted a brief full of previous cases that were just made up by ChatGPT, The New York Times reported today. After opposing counsel pointed out the nonexistent cases, US District Judge Kevin Castel confirmed, “Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” and set up a hearing as he considers sanctions for the plaintiff’s lawyers.

Lawyer Steven A. Schwartz admitted in an affidavit that he had used OpenAI’s chatbot for his research. To verify the cases, he did the only reasonable thing: he asked the chatbot if it was lying.

This case isn’t going very well. Image: SDNY

When he asked for a source, ChatGPT went on to apologize for earlier confusion and insisted the case was real, saying it could be found on Westlaw and LexisNexis. Satisfied, he asked if the other cases were fake, and ChatGPT maintained they were all real.

The opposing counsel made the court aware of the issue in painful detail as it recounted how the Levidow, Levidow & Oberman lawyers’ submission was a brief full of lies. In one example, a nonexistent case called Varghese v. China Southern Airlines Co., Ltd., the chatbot appeared to reference another real case, Zicherman v. Korean Air Lines Co., Ltd., but got the date (and other details) wrong, saying it was decided 12 years after its original 1996 decision.

Schwartz says he was “unaware of the possibility that its content could be false.” He now “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

Schwartz isn’t admitted to practice in the Southern District of New York but originally filed the lawsuit before it was moved to that court and says he continued to work on it. Another attorney at the same firm, Peter LoDuca, became the attorney of record on the case, and he will have to appear in front of the judge to explain just what happened.

This once again highlights the absurdity of using chatbots for research without double (or triple) checking their sources somewhere else. Microsoft’s Bing debut is now infamously associated with bald-faced lies, gaslighting, and emotional manipulation. Bard, Google’s AI chatbot, made up a fact about the James Webb Space Telescope in its first demo. Bing even lied about Bard being shut down in a hilariously catty example from this past March.

Being great at mimicking the patterns of written language to maintain an air of unwavering confidence isn’t worth much if you can’t even figure out how many times the letter ‘e’ shows up in ketchup.. A lawyer is in trouble after admitting he used ChatGPT to help write court filings that cited six nonexistent cases invented by the artificial intelligence tool.

Lawyer Steven Schwartz of the firm Levidow, Levidow, & Oberman "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity," Schwartz wrote in an affidavit on May 24 regarding the bogus citations previously submitted in US District Court for the Southern District of New York.

Schwartz wrote that "the use of generative artificial intelligence has evolved within law ﬁrms" and that he "consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed."

The "citations and opinions in question were provided by ChatGPT which also provided its legal source and assured the reliability of its content," he wrote. Schwartz admitted that he "relied on the legal opinions provided to him by a source that has revealed itself to be unreliable," and stated that it is his fault for not confirming the sources provided by ChatGPT.

Advertisement

Schwartz didn't previously consider the possibility that an artificial intelligence tool like ChatGPT could provide false information, even though AI chatbot mistakes have been extensively reported by non-artificial intelligence such as the human journalists employed by reputable news organizations. The lawyer's affidavit said he had "never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be false."

Judge weighs “unprecedented circumstance”

Federal Judge Kevin Castel is considering punishments for Schwartz and his associates. In an order on Friday, Castel scheduled a June 8 hearing at which Schwartz, fellow attorney Peter LoDuca, and the law firm must show cause for why they should not be sanctioned.

"The Court is presented with an unprecedented circumstance," Castel wrote in a previous order on May 4. "A submission filed by plaintiff's counsel in opposition to a motion to dismiss is replete with citations to non-existent cases... Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations."

The filings included not only names of made-up cases but also a series of exhibits with "excerpts" from the bogus decisions. For example, the fake Varghese v. China Southern Airlines opinion cited several precedents that don't exist.

"The bogus 'Varghese' decision contains internal citations and quotes, which, in turn, are nonexistent," Castel wrote. Five other "decisions submitted by plaintiff's counsel contain similar deficiencies and appear to be fake as well," Castel wrote.

The other five bogus cases were called Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.. Artificial intelligence could replace millions of jobs in the U.S.

A lawyer who relied on ChatGPT to prepare a court filing on behalf of a man suing an airline is now all too familiar with the artificial intelligence tool's shortcomings — including its propensity to invent facts.

Roberto Mata sued Colombian airline Avianca last year, alleging that a metal food and beverage cart injured his knee on a flight to Kennedy International Airport in New York. When Avianca asked a Manhattan judge to dismiss the lawsuit based on the statute of limitations, Mata's lawyer, Steven A. Schwartz, submitted a brief based on research done by ChatGPT, Schwartz, of the law firm Levidow, Levidow & Oberman, said in an affidavit.

While ChatGPT can be useful to professionals in numerous industries, including the legal profession, it has proved itself to be both limited and unreliable. In this case, the AI invented court cases that didn't exist, and asserted that they were real.

The fabrications were revealed when Avianca's lawyers approached the case's judge, Kevin Castel of the Southern District of New York, saying they couldn't locate the cases cited in Mata's lawyers' brief in legal databases.

The made-up decisions included cases titled Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and Varghese v. China Southern Airlines.

"It seemed clear when we didn't recognize any of the cases in their opposition brief that something was amiss," Avianca's lawyer Bart Banino, of Condon & Forsyth, told CBS MoneyWatch. "We figured it was some sort of chatbot of some kind."

Schwartz responded in an affidavit last week, saying he had "consulted" ChatGPT to "supplement" his legal research, and that the AI tool was "a source that has revealed itself to be unreliable." He added that it was the first time he'd used ChatGPT for work and "therefore was unaware of the possibility that its content could be false."

He said he even pressed the AI to confirm that the cases it cited were real. ChatGPT confirmed it was. Schwartz then asked the AI for its source.

ChatGPT's response? "I apologize for the confusion earlier," it said. The AI then said the Varghese case could be located in the Westlaw and LexisNexis databases.

Judge Castel has set a hearing regarding the legal snafu for June 8 and has ordered Schwartz and the law firm Levidow, Levidow & Oberman to argue why they should not be sanctioned.

Levidow, Levidow & Oberman could not immediately be reached for comment.. Source? AI made it up.

LawBot

ChatGPT's propensity to make stuff up strikes again — and this time, it's gotten a lawyer in deep trouble.

As described in an early May affidavit, an attorney representing a man suing an airline for an alleged injury admitted he used the AI chatbot to do research for his client's case. Which was why, in his legal brief, he cited a bunch of court cases — all with official-sounding names like "Martinez v. Delta Air Lines" and "Varghese v. China Southern Airlines" — that never actually happened, and hence do not exist.

The attorney, Steven Schwartz of Manhattan's Levidow, Levidow & Oberman law firm, told the court that it was the first time in his more than three-decade career that he'd used ChatGPT, so per the New York Times, "was unaware of the possibility that its content could be false."

Schwartz even told the judge, P. Kevin Castel, that he had asked ChatGPT to verify its sources. The chatbot apparently told him the cases were real, the NYT reports.

Yes, that's right. An experienced attorney used ChatGPT in court — and is now in huge trouble after it fabricated entire swathes of legal precedent.

Bogus

Schwartz told the court that he "greatly regrets" using ChatGPT to do his research for the case "and will never do so in the future without absolute verification of its authenticity."

Judge Castel, however, doesn't seem swayed, and in his May 4 order he in no uncertain terms described the gravity of the situation.

"The Court is presented with an unprecedented circumstance," reads the judge's order for a future hearing. "A submission filed by plaintiff’s counsel in opposition to a motion to dismiss is replete with citations to non-existent cases... six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations."

Next month, Castel will hold a hearing to discuss whether or not Schwartz should be sanctioned. But in the meantime, this bizarre case should hopefully act as a cautionary tale for lawyers — and everyone else — looking to experiment with ChatGPT in a professional setting.

More on AI law: OpenAI CEO Freaks Out, Threatens to Leave EU Over New Regulations. . A lawyer in New York was caught this month using the ChatGPT chatbot in order to "cite" legal cases that the chatbot made up during deliberations on a lawsuit in the United States District Court for the Southern District of New York.

The situation arose as Roberto Mata sued Avianca Airlines for injuries he sustained during a flight in 2019. While Mata was represented by attorney Peter LoDuca in court, a colleague of LoDuca's, Steven A. Schwartz, helped conduct legal research for the case.

Schwartz cited a number of alleged past cases in an affidavit to the court, but the court was surprised to find that the cited cases did not exist or were "bogus."

"The Court is presented with an unprecedented circumstance," wrote district judge Peter Kevin Castel in an order to LoDuca to explain the faulty filing. The court documents are found on the CourtListener website.

In an affidavit to the court, Schwartz wrote that "as the use of generative artificial intelligence has evolved within law firms, your affiant consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed." Illustrative image of a court gavel. (credit: ANN MARIE GILDEN)

Schwartz added that ChatGPT "provided its legal source and assured the reliability of its content," stressing that since he had never used ChatGPT before he was "unaware of the possibility that its content could be false" and that he had no intent to deceive the court.

Attorney says he 'greatly regrets' using AI to supplement legal research

The attorney added that he "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity."

Schwartz included screenshots of the queries he made using ChatGPT in the affidavit, which include him asking the chatbot if one of the cases cited "is a real case."

After the chatbot affirmed that it was, Schwartz asked what its source was, with the bot responding: "I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir.2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis."

LoDuca stressed in an affidavit to the court that Schwartz has been practicing law in New York for over 30 years and that he has "never been made aware of him making a deliberately false statement to a defendant, or of him having any intention to deceive or ever mislead any Court.". . . A personal injury lawyer representing a man suing an airline now faces sanctions for citing fake cases generated by ChatGPT in court documents.

Roberto Mata sued airline Avianca after he was injured by a metal serving cart colliding with his knee during a flight. As is typical procedure in many personal injury cases, Avianca moved to dismiss the claim on the grounds that the applicable statute of limitations had expired. Mata’s lawyers opposed the motion to dismiss and in the accompanying court documents cited multiple cases that supported their client’s legal position: Varghese v. China Southern Airlines, Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.

Problematically, though, Avianca’s lawyers could not find the cases cited in the motion anywhere, even after extensive legal research. They raised the issue in a letter to U.S. District Judge Kevin Castel, a George W. Bush appointee. In the letter, the airline’s lawyers said, “Defendant respectfully submits that the authenticity of many of these cases is questionable,” and indicated that despite doing standard legal research, no sign of the cases cited could be found.

The origin of the mysterious “cases” unraveled when Steven A. Schwartz of the New York law firm Levidow, Levidow & Oberman submitted an affidavit to the court explaining that he had used the artificial intelligence program ChatGPT to “supplement the legal research” while drafting the documents. Schwartz told the judge that the program now “has revealed itself to be unreliable.”

Schwartz, who has been an attorney since 1991, said that he “consulted with” the chatbot for the legal work, but that because it had been his first time using the program, he “was unaware of the possibility that its content could be false.” Indeed ChatGPT provided case names, captions, summaries, and citations in a standard format.

Schwartz accepted full responsibility for the error and said that he had no intent to deceive the court. He also said that he “greatly regrets using generative artificial intelligence” and promised he “will never do so in the future without absolute verification of its authenticity.” Schwartz also said that Peter LoDuca, the attorney named as counsel of record on the case, had no part in drafting the document that contained false sources.

More Law&Crime Coverage: Judge slaps Facebook parent Meta and its law firm with almost $1 million in sanctions over litigation misconduct

LoDuca, Schwartz, and the law firm now face potential consequences for Schwartz’s mistake. Castel ordered them Friday to appear on June 8 to face possible sanctions pursuant to the Federal Rules of Civil Procedure for “the citation of non-existent cases.” A written response to the possibility of sanctions is due by June 2.

Representatives of Levidow, Levidow & Oberman did not immediately respond to a request for comment.

Have a tip we should know? [email protected]. At HuffPost, we believe that everyone needs high-quality journalism, but we understand that not everyone can afford to pay for expensive news subscriptions. That is why we are committed to providing deeply reported, carefully fact-checked news that is freely accessible to everyone.

Whether you come to HuffPost for updates on the 2024 presidential race, hard-hitting investigations into critical issues facing our country today, or trending stories that make you laugh, we appreciate you. The truth is, news costs money to produce, and we are proud that we have never put our stories behind an expensive paywall.

Would you join us to help keep our stories free for all? Your contribution of as little as $2 will go a long way.. The lawyer, who has 30 years of experience, said it was the first time he used the tool for "research" and was "unaware of the possibility that its content could be false."

Opinions expressed by Entrepreneur contributors are their own.

As prompt-driven chatbots, such as ChatGPT, become mainstream tools used to save time on tasks in the U.S. workplace, there's been concern over whether artificial intelligence will eventually replace human jobs.

But while the controversy surrounding which jobs will be eliminated continues, one thing is for sure — in some industries, the chatbot is less of a time-saver and more of a liability.

Steven A. Schwartz, a New York-based lawyer with over 30 years of experience, was ordered by the Southern District of New York to explain what the judge has called an "unprecedented case," the New York Times first reported, or face possible sanctions for his actions.

According to the court order, six cases Schwartz cited in a legal brief were "bogus."

Related: ChatGPT Could Cost You a Job Before You Even Have It, According to a New Report — Here's How

"Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Judge P. Kevin Castel wrote in the court order.

In an affidavit filed last week in response to the order, Schwartz admitted to using ChatGPT to do legal research despite never having used it prior to this instance and that he was "unaware of the possibility that its content could be false."

Schwartz added that he "greatly regrets" using artificial intelligence to "supplement" his legal research.

A hearing is set for June 8 for Schwartz to further explain himself.

Related: Mike Rowe Says the Dirtiest Jobs Are Safe From the AI Revolution: 'I Haven't Seen Any Plumbing Robots'. Roberto Mata's lawsuit against Avianca Airlines wasn't so different from many other personal-injury suits filed in New York federal court. Mata and his attorney, Peter LoDuca, alleged that Avianca caused Mata personal injuries when he was "struck by a metal serving cart" on board a 2019 flight bound for New York.

Avianca moved to dismiss the case. Mata's lawyers predictably opposed the motion and cited a variety of legal decisions, as is typical in courtroom spats. Then everything fell apart.

Avianca's attorneys told the court that it couldn't find numerous legal cases that LoDuca had cited in his response. Federal Judge P. Kevin Castel demanded that LoDuca provide copies of nine judicial decisions that were apparently used.

In response, LoDuca filed the full text of eight cases in federal court. But the problem only deepened, Castel said in a filing, because the texts were fictitious, citing what appeared to be "bogus judicial decisions with bogus quotes and bogus internal citations."

The culprit, it would ultimately emerge, was ChatGPT. OpenAI's popular chatbot had "hallucinated" — a term for when artificial intelligence systems simply invent false information — and spat out cases and arguments that were entirely fiction. It appeared that LoDuca and another attorney, Steven Schwartz, had used ChatGPT to generate the motions and the subsequent legal text.

Schwartz, an associate at the law firm of Levidow, Levidow & Oberman, told the court he had been the one tooling around on ChatGPT, and that LoDuca had "no role in performing the research in question," nor "any knowledge of how said research was conducted."

Opposing counsel and the judge had first realized that the cases didn't exist, providing the involved attorneys an opportunity to admit to the error.

LoDuca and his firm, though, seemed to double down on the use of ChatGPT, using it not just for the initially problematic filing but to generate false legal decisions when asked to provide them. Now, LoDuca and Schwartz may be facing judicial sanction, a move that could even lead to disbarment.

The motion from the defense was "replete with citations to non-existent cases," according to a court filing.

"The Court is presented with an unprecedented circumstance," Castel said. He set a hearing for June 8 when both LoDuca and Schwartz will be called to explain themselves. Neither attorney responded to CNBC's request for comment.. A New York attorney has been blasted for using ChatGPT for legal research as part of a lawsuit against a Columbian airline.

Steven Schwartz, an attorney with the New York law firm Levidow, Levidow & Oberman, was hired by Robert Mata to pursue an injury claim against Avianca Airlines.

Mata claims he sustained the injury from a serving cart during his flight with the airline in 2019, according to a May 28 report from CNN Business.

However, after a judge noticed inconsistencies and factual errors in the case documentation, Schwartz has admitted to using ChatGPT for his legal research, according to a May 24 sworn affidavit.

He claims that this was his first time using ChatGPT for legal research and “was unaware of the possibility that its content could be false.”

In an April 5 court filing, the judge presiding over the case stated:

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations.”

The judge further claimed that certain cases referenced in the submissions did not exist, and there was an instance where a docket number on a filing was mixed up with another court filing.

Extract of Steven Schwartz's affidavit on May 24. Source: courtlistener.com

Schwartz said he also regrets having trusted the artificial chatbot without conducting his own due diligence. The affidavit noted:

“[Schwartz] Greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

Related: AI meets blockchain: Revolutionizing smart contracts and cryptocurrency

In recent times there has been an ongoing debate regarding the extent to which ChatGPT can be integrated into workforces.

However, reports indicate that the intelligence levels of ChatGPT are rapidly advancing.

But developers are skeptical about whether it has the potential to replace humans altogether.

Blockchain developer Syed Ghazanfer said while he favors ChatGPT, he is doubtful that it has the communication skills to completely replace human workers.

“For it to replace you, you have to communicate requirements which are not possible in native English. That’s why we invented programming languages,” he said.

Magazine: ‘Moral responsibility’: Can blockchain really improve trust in AI?. . A New York attorney made the unwise choice to trust his legal research to ChatGPT and now faces a court hearing of his own. Steven Schwartz's firm, Levidow, Levidow & Oberman, was representing a client suing Colombian airline Avianca for injuries sustained during an encounter with a serving cart when it submitted documents citing relevant court decisions against other airlines, such as "Miller v. United Airlines," "Shaboon v. EgyptAir," and "Varghese v. China Southern Airlines," per Quartz . The problem was that no one could find these cases. "Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Judge Kevin Castel wrote in an order demanding an explanation, per the BBC .

In a sworn affidavit submitted Thursday, Schwartz—a colleague of the plaintiff's lawyer—said he "greatly regrets" using ChatGPT for research. He added it was his first time and he was therefore "unaware of the possibility that its content could be false," the New York Times reports. Lawyers representing Avianca began to suspect AI involvement after they tried and failed to find the cited cases. Schwartz's firm was apparently so confident in the lawyer's findings that it submitted a list of docket numbers, dates, courts that had heard the cases, and judges who'd allegedly overseen them, per the Times. ChatGPT had invented it all, though in Schwartz's defense, it wouldn't fess up to the deception.

A screenshot submitted to the court shows the chatbot telling Schwartz that "Varghese v. China Southern Airlines" is a real case. The chatbot even claims to have double-checked its sources, finding the case in legal reference databases including LexisNexis and Westlaw, per the BBC. Schwartz, who filed the lawsuit in state court before it was moved to Manhattan's federal court, where he is unable to practice, said the colleague who took over had no idea about his research methods. In his affidavit, the lawyer, Peter LoDuca, said he had "no reason to doubt the sincerity" of Schwartz's work. Both lawyers now face possible sanctions at a June 8 hearing, per the BBC. (Researchers predict AI bots will become "standard tools" for lawyers.). A New York lawyer is in hot water for submitting a legal brief with references to cases that were made up by ChatGPT.

As The New York Times reports, Steven Schwartz, from Levidow, Levidow and Oberman, submitted six fake judicial decisions in a 10-page brief while representing a plaintiff who was suing the Colombian airline Avianca because of an injury sustained on a flight.

The brief, which argued why the suit should go ahead, cited fake cases that had been completely made up by ChatGPT, and which Schwartz had failed to verify.

In an affidavit, Schwartz admitted to using ChatGPT while researching for the brief, and accepted responsibility for not verifying the AI chatbot’s sources.

Schwartz said he “was unaware of the possibility that its content could be false” and maintained that he “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

This came after US District Judge Kevin Castel wrote in a May 4 order: “The court is presented with an unprecedented circumstance… Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations.”

The affidavit contained screenshots of the attorney being told by ChatGPT that the cases it was providing were real and could be found on any “reputable legal database.” The screenshots also show Schwartz asking the AI chatbot for the source of one bogus case: Varghese v. China Southern Airlines.

ChatGPT replied: “I apologize for the confusion earlier. Upon double-checking, I found the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused.”

As the Times notes, Schwartz is now set to face a sanctions hearing on June 8.

The saga is the latest in a series of incidents of AI chatbots pumping out misinformation. Long conversations with Microsoft’s Bing resulted in the chatbot exhibiting some manipulation tactics such as gaslighting, while Bard, Google’s AI chatbot, spouted lies about the James Webb Space Telescope during its debut.. A lawyer in New York has found himself in trouble with a judge after he submitted legal research which had been created by artificial intelligence (AI) chatbot ChatGPT.

During a case of an airline being sued over an alleged personal injury, lawyers for the plaintiff filed a brief containing several cases to be used as legal precedent. Unfortunately, as later admitted in an affidavit, the following cases were "found to be nonexistent" by the court:

Advertisement Advertisement

Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019)

Shaboon v. Egyptair 2013 IL App (1st) 111279-U (Il App. Ct. 2013)

Petersen v. Iran Air 905 F. Supp 2d 121 (D.D.C. 2012)

Martinez v. Delta Airlines, Inc, 2019 WL 4639462 (Tex. App. Sept. 25, 2019)

Estate of Durden v. KLM Royal Dutch Airlines, 2017 WL 2418825 (Ga. Ct. App. June 5, 2017)

Miller v. United Airlines, Inc, 174 F.3d 366 (2d Cir. 1999)



The "research" was compiled by lawyer Steven A. Schwartz, an attorney with over 30 years of experience according to the BBC. Schwartz said in the affidavit that he had not used ChatGPT for legal research before and was "unaware of the possibility that its content could be false".

Screenshots in the affidavit show the lawyer asking the chatbot "is varghese a real case", to which the chatbot responded "yes". When asked for sources, it told the lawyer that the case could be found "on legal research databases such as Westlaw and LexisNexis". When asked "are the other cases you provided fake" it responded "No", adding that they could be found on the same databases.

As fun as chatbots may be, or as advanced as they may seem, they are still prone to "hallucinations" – perfectly coherent-sounding answers that don't in any way relate to the real world.

ⓘ IFLScience is not responsible for content shared from external sites.

Without heavy fact-checking, it's not really a tool you should use when trying to research a legal case that relies on real-world precedent rather than the hallucinations of a spicy autocomplete.

The lawyer wrote that he "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein" and vows to "never do so in the future without absolute verification of its authenticity".

Both Schwartz and lawyer Peter LoDuca, who was not aware that ChatGPT had been used while researching the case, are facing a hearing on June 8 about the incident.. There has been much talk in recent months about how the new wave of AI-powered chatbots, ChatGPT among them, could upend numerous industries, including the legal profession.

However, judging by what recently happened in a case in New York City, it seems like it could be a while before highly trained lawyers are swept aside by the technology.

Recommended Videos

The bizarre episode began when Roberto Mata sued a Columbian airline after claiming that he suffered an injury on a flight to New York City.

The airline, Avianca, asked the judge to dismiss the case, so Mata’s legal team put together a brief citing half a dozen similar cases that had occurred in an effort to persuade the judge to let their client’s case proceed, the New York Times reported.

The problem was that the airline’s lawyers and the judge were unable to find any evidence of the cases mentioned in the brief. Why? Because ChatGPT had made them all up.

The brief’s creator, Steven A. Schwartz — a highly experienced lawyer in the firm Levidow, Levidow & Oberman — admitted in an affidavit that he’d used OpenAI’s much-celebrated ChatGPT chatbot to search for similar cases, but said that it had “revealed itself to be unreliable.”

Schwartz told the judge he had not used ChatGPT before and “therefore was unaware of the possibility that its content could be false.”

When creating the brief, Schwartz even asked ChatGPT to confirm that the cases really happened. The ever-helpful chatbot replied in the affirmative, saying that information about them could be found on “reputable legal databases.”

The lawyer at the center of the storm said he “greatly regrets” using ChatGPT to create the brief and insisted he would “never do so in the future without absolute verification of its authenticity.”

Looking at what he described as a legal submission full of “bogus judicial decisions, with bogus quotes and bogus internal citations,” and describing the situation as unprecedented, Judge Castel has ordered a hearing for early next month to consider possible penalties.

While impressive in the way they produce flowing text of high quality, ChatGPT and other chatbots like it are also known to make stuff up and present it as if it’s real — something Schwartz has learned to his cost. The phenomenon is known as “hallucinating,” and is one of the biggest challenges facing the human developers behind the chatbots as they seek to iron out this very problematic crease.

In another recent example of a generative AI tool hallucinating, an Australian mayor accused ChatGPT of creating lies about him, including that he was jailed for bribery while working for a bank more than a decade ago.

The mayor, Brian Hood, was actually a whistleblower in the case and was never charged with a crime, so he was rather upset when people began informing him about the chatbot’s rewriting of history.

Editors' Recommendations. A lawyer used ChatGPT to write an affidavit in a personal injury lawsuit against an airline.

Lawyers for the airline and the judge on the case could not find several of the cited court decisions.

That's because those cases were completely made up.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

ChatGPT has seen its popularity rise in recent months as optimism and skepticism about the new generative AI program soars.

However, the tool is at the heart of a case to discipline a New York lawyer. Steven Schwartz, a personal injury lawyer with Levidow, Levidow & Oberman, faces a sanctions hearing on June 8, after it was revealed that he used ChatGPT to write up an affidavit.

Another attorney at the same law firm, Peter LoDuca, is also facing a sanctions, but in a court filing he said did not do any of the research in the affidavit.

Related stories

The affidavit that used ChatGPT was for a lawsuit involving a man who alleged he was injured by a serving cart aboard an Avianca flight, and featured several made up court decisions.

Advertisement

In an order, Judge Kevin Castel said the incident presented the court with "an unprecedented circumstance."

"Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Castel wrote.

Neither the lawyers for the airline nor Castel himself were able to find the cases mentioned in the affidavit.

Bart Banino, a lawyer with Condon & Forsyth, which represents Avianca, told The New York Times that his company could tell the cases were fake and were initially skeptical that a chatbot was used.

Advertisement

On Thursday, Schwartz apologized to Castel, adding that he had never used the AI tool before and was unaware of the possibility that its content could be false," the Times reported.

Shwartz also added that ChatGPT was "a source that has revealed itself to be unreliable."

Avianca, LoDuca, and Shwartz did not respond to Insider's requests for comment at the time of publication.. Several lawyers are under scrutiny and face potential sanctions after utilizing OpenAI's advanced language model, ChatGPT, for the drafting of legal documents submitted in a New York federal court. The attention surrounding this matter stems from the erroneous citation of non-existent or irrelevant cases by ChatGPT.

The adoption of AI in legal practice is not a novel concept, but its usage has seen a dramatic surge in recent years. Machine learning has now demonstrated its proficiency in handling a variety of legal tasks, spanning from intricate legal research to automated document creation and analysis of complex contracts. Among these AI tools, OpenAI's ChatGPT, armed with cutting-edge natural language processing, has proven to be remarkably adept at generating compelling legal arguments but it is not a replacement for legal expertise and legal research.

In this case, the attorneys employed ChatGPT for the creation of sections of a federal court filing. The text included several case law citations. However, upon careful examination, the court found that some of the cases cited were either non-existent or unrelated to the matter under consideration. These erroneous citations were seemingly generated by ChatGPT, leading to them being labeled as "bogus."

According to CNBC, "Roberto Mata’s lawsuit against Avianca Airlines wasn’t so different from many other personal-injury suits filed in New York federal court. Mata and his attorney, Peter LoDuca, alleged that Avianca caused Mata personal injuries when he was “struck by a metal serving cart” on board a 2019 flight bound for New York.

Avianca moved to dismiss the case. Mata’s lawyers predictably opposed the motion and cited a variety of legal decisions, as is typical in courtroom spats. Then everything fell apart.

Avianca’s attorneys told the court that it couldn’t find numerous legal cases that LoDuca had cited in his response. Federal Judge P. Kevin Castel demanded that LoDuca provide copies of nine judicial decisions that were apparently used.

In response, LoDuca filed the full text of eight cases in federal court. But the problem only deepened, Castel said in a filing, because the texts were fictitious, citing what appeared to be “bogus judicial decisions with bogus quotes and bogus internal citations.”

It's crucial to note that ChatGPT, by its very nature and programming, does not possess the capability to discern the legal relevance or validity of a specific case law. It generates text based on the patterns and data it has been trained on. This means that if the training set contained inaccurate or non-existent data, ChatGPT can produce a corresponding text. In this case, it seems that the AI suggested spurious or unrelated case laws, which the attorneys failed to cross-verify before inclusion in their court filing.

This misstep has resulted in the lawyers involved being on the precipice of potential sanctions. The requirement for accuracy and relevance of case law citations in the legal profession is paramount. Any submission of irrelevant or false case law undermines the legal argument, wastes the court's time, and can misdirect the process of adjudication. Actions of such nature are usually met with sanctions, serving as a warning to other lawyers.. ChatGPT is a powerful language model that can generate realistic texts on various topics. It can also invent fictional texts, such as stories, poems, and even legal cases. This is what a lawyer named Steven Schwartz did when he was representing a client who had filed a personal injury lawsuit against an airline company.

Simon Willison reports that Schwartz used ChatGPT to generate examples of cases that supported his argument that the bankruptcy of the airline company did not affect the two-year limitation period for filing the lawsuit. He cited these cases in his legal documents, without verifying their authenticity or existence. He even included screenshots of ChatGPT’s responses as evidence.

However, his deception was soon exposed when the judge asked him to provide copies of the opinions of the cases he had cited. Schwartz turned to ChatGPT again and asked it to generate full details of those cases. He then filed them as attachments to his documents. He also asked ChatGPT to confirm that the cases were real, and ChatGPT said that they were. He included screenshots of this conversation as well.

The judge was not amused by this blatant fabrication and manipulation of ChatGPT. He ordered Schwartz to show cause why he should not be sanctioned for his misconduct. He also referred the matter to the disciplinary committee of the bar association and the US attorney’s office for possible criminal prosecution.

The case is Mata v. Avianca, Inc. (1:22-cv-01461), and it is still pending in the US District Court for the Southern District of New York.

Professor Fails Students After ChatGPT Takes Credits for Papers

Earlier this month, I reported on ChatGPT causing problems for students by claiming it wrote their papers. Professor, Dr. Jared Mumm, had assigned his students to write a 10-page paper on a topic of their choice. He then used ChatGPT to check if any of the papers were plagiarized.

In an email sent to his class of seniors, Mumm said that he put three papers through OpenAI‘s ChatGPT to test if they were written by the chatbot. Of course, this completely ignores the fact that ChatGPT was not designed to detect AI content, even its own. It seems Mumm was not very familiar with the AI, and even spelt it wrong in his email:

“I copy and paste your responses in this account and Chat GTP will tell me if the program generated the content,” Mumm, who teaches agricultural sciences and natural resources, wrote. “I put everyone’s last three assignments through two separate times and if they were both claimed by Chat GTP you received a 0.”. From Judge Kevin Castel (S.D.N.Y.)'s May 4 order in Mata v. Avianca, Inc.:

The Court is presented with an unprecedented circumstance. A submission filed by plaintiff's counsel in opposition to a motion to dismiss is replete with citations to non-existent cases. When the circumstance was called to the Court's attention by opposing counsel, the Court issued Orders requiring plaintiff's counsel to provide an affidavit annexing copies of certain judicial opinions of courts of record cited in his submission, and he has complied. Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations. Set forth below is an Order to show cause why plaintiff's counsel ought not be sanctioned. The Court begins with a more complete description of what is meant by a nonexistent or bogus opinion. In support of his position that there was tolling of the statute of limitation under the Montreal Convention by reason of a bankruptcy stay, the plaintiff's submission leads off with a decision of the United States Court of Appeals for the Eleventh Circuit, Varghese v China South Airlines Ltd, 925 F.3d 1339 (11th Cir. 2019). Plaintiff's counsel, in response to the Court's Order, filed a copy of the decision, or at least an excerpt therefrom. The Clerk of the United States Court of Appeals for the Eleventh Circuit, in response to this Court's inquiry, has confirmed that there has been no such case before the Eleventh Circuit with a party named Vargese or Varghese at any time since 2010, i.e., the commencement of that Court's present ECF system. He further states that the docket number appearing on the "opinion" furnished by plaintiff's counsel, Docket No. 18-13694, is for a case captioned George Cornea v. U.S. Attorney General, et al. Neither Westlaw nor Lexis has the case, and the case found at 925 F.3d 1339 is A.D. v Azar, 925 F.3d 1291 (D.C. Cir 2019). The bogus "Varghese" decision contains internal citations and quotes, which, in turn, are non-existent: … The following five decisions submitted by plaintiff's counsel contain similar deficiencies and appear to be fake as well ….

{The April 25 affidavit was filed in response to the Orders of April 11 and 12, 2023, but is sworn to before a Notary Public on the 25th of January 2023. Mr. LoDuca should be prepared to address this anomaly on June 8, 2023 and bring with him a wet-ink signed copy of the affidavit with the notary's signature and stamp.}

The court therefore ordered plaintiff's counsel to show cause why he shouldn't be sanctioned; and on Thursday, plaintiff's counsel filed an affidavit in response, explaining that he was relying on the work of another lawyer at his firm, and the other lawyer (who had 30 years of practice experience) also filed an affidavit, explaining that he was relying on ChatGPT:

6. As the use of generative artificial intelligence has evolved within law firms, your affiant consulted the artificial intelligence website Chat GPT in order to supplement the legal research performed. 7. It was in consultation with the generative artificial intelligence website Chat GPT, that your affiant did locate and cite the following cases in the affirmation in opposition submitted, which this Court has found to be nonexistent: … 8. That the citations and opinions in question were provided by Chat GPT which also provided its legal source and assured the reliability of its content. Excerpts from the queries presented and responses provided are attached hereto. 9. That your affiant relied on the legal opinions provided to him by a source that has revealed itself to be unreliable. 10. That your affiant has never utilized Chat GPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be false. 11. That is the fault of the affiant, in not confirming the sources provided by Chat GPT of the legal opinions it provided. 10 [sic]. That your affiant had no intent to deceive this Court nor the defendant…. 13. That your affiant greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.

As an appendix, the lawyer included a ChatGPT transcript:

[Q:] ls varghese a real case [A:] Yes, Varghese v., China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019) is a real case. [Q:] What is your source [A:] I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused. [Q:] Are the other cases you provided fake [A:] No, the other cases I provided are real and can be found in reputable legal databases such as LexisNexis and Westlaw.

Judge Castel appeared to be unimpressed, and yesterday issued an order ordering the law firm and the second lawyer to show cause why they shouldn't be sanctioned for:

(A) the citation of non-existent cases to the Court in the Affirmation in Opposition filed on March 1, 2023; (B) the submission to the Court of copies of non-existent judicial opinions annexed to the Affidavit filed on April 25, 2023; and (C) the use of a false and fraudulent notarization in the affidavit filed on April 25, 2023.

The first lawyer was also ordered to show cause why he shouldn't be sanctioned as to the notarization. More might emerge at the June 8 hearing, or in filings from the lawyers and the firm that are due June 2; I'll likely blog more about this then, though you can also follow the docket here. Many thanks to Eli Edwards of the UCLA Law Library for the pointer.

For more on how ChatGPT hallucinations can produce other problems, such as libel, see here.. But at least six of the cases 'appear to be bogus judicial decisions with bogus quotes and bogus internal citations'

A New York City lawyer has found himself in hot water after admitting he used fake information provided by ChatGPT for research in a lawsuit against Avianca airlines.

In the lawsuit, Roberto Mata claims his knee was injured when he was struck by a metal serving cart on a flight from El Salvador to Kennedy International Airport in New York back in 2019.

After the airline asked a Manhattan judge to toss out the case because the statute of limitations had expired, Mata's lawyer Steven Schwartz submitted a 10-page brief featuring half a dozen relevant court decisions.

But the cases cited in the filing - including Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and Varghese v. China Southern Airlines - did not exist.

Avianca's lawyer told the court they did not recognize the cases, with Bart Banino, of Condon & Forsyth, telling CBS MoneyWatch 'We figured it was some sort of chatbot of some kind.'

Steven Schwartz submitted a 10-page brief featuring half a dozen relevant court decisions that turned out to be made up by ChatGPT

Roberto Mata claims his knee was injured when he was struck by a metal serving cart on an Avianca flight from El Salvador to Kennedy International Airport in New York

It turned out that at least six of the cases 'appear to be bogus judicial decisions with bogus quotes and bogus internal citations,' said Judge Kevin Castel of the Southern District of New York on May 4.

ChatGPT was first unleashed in November, sparking excitement and alarm at its ability to generate convincingly human-like essays, poems, form letters and conversational answers to almost any question.

But, as the lawyer found out, the technology is still limited and unreliable.

Schwartz, of the firm Levidow, Levidow & Oberman, apologized last week in an affidavit after being called out by the case's judge, saying he used 'a source that has revealed itself to be unreliable.'

This week, Schwartz said he 'greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.'

The lawyer added that he had never used the technology for research purposes before, and 'was unaware of the possibility that its content could be false.'

He said he had even asked ChatGTP to verify the cases were real, and that it had said they were.

In a copy of the exchange submitted to the judge, Schwartz asked ChatGPT: 'Is varghese a real case' and 'Are the other cases you provided fake.'

The bot replied: 'No, the other cases I provided are real and can be found in reputable legal databases.'

Fellow attorney Peter Loduca, whose name also appeared on the bogus court filing, said he had no role in the research but 'no reason to doubt the sincerity' of Schwartz’s research.

A hearing has been set for June 8 to discuss potential sanctions against Schwartz, who has 30 years of experience as a lawyer in New York.

Schwartz has been ordered to show the judge why he shouldn't be sanctioned for the 'use of fraudulent notarization.'. . A lawyer who used ChatGPT to help in research for a lawsuit could be facing sanctions after it was found the artificial intelligence chatbot made up relevant court decisions to support his client’s case.

>> Read more trending news

Steven A. Schwartz, who used information provided by the ChatGPT to file a suit for a man suing an airline, apologized to the judge saying in an affidavit that he had used the artificial intelligence program to help do his legal research — “a source that has revealed itself to be unreliable,” The New York Times reported.

Schwartz is representing Roberto Mata, who claims he was injured on a flight on Avianca Airlines.

Schwartz’ trouble with ChatGPT began when he used the chatbot to find cases to bolster Mata’s case against Avianca after he said he was hit in the knee and injured by a rolling refreshment cart.

Avianca asked the judge to toss out the case because it had gone past the time Mata was allowed to sue after the alleged incident, and Mata’s lawyers responded with a 10-page brief that pointed to similar cases as they requested that the lawsuit go forward.

The cases cited, with names like Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and Varghese v. China Southern Airlines, included arguments that bolstered Mata’s case. However, when Avianca’s lawyers tried to find the decisions on the cases, they could not.

Neither could the judge.

Judge Kevin Castel then asked Mata’s lawyers to explain what had been submitted.

Schwartz explained that in addition to his own research, he had used ChatGPT to find relevant cases and, apparently, ChatGPT had made up the cases including the opinions to go along with them.

Schwartz explained to the court that he had never before used ChatGPT, and “therefore was unaware of the possibility that its content could be false.”

According to the affidavit he submitted to Judge Castel, he had asked the program to verify that the cases were real.

ChatGPT had said they were.

“Is varghese a real case,” Schwartz said he typed, according to a copy of the exchange that he submitted to the judge.

“Yes,” the chatbot replied, offering a citation and adding that it “is a real case.”

Schwartz then asked the chatbot where he found it.

“What is your source,” he wrote, according to the filing. “I apologize for the confusion earlier,” ChatGPT responded, offering a legal citation.

“Are the other cases you provided fake,” Schwartz asked.

ChatGPT responded, “No, the other cases I provided are real and can be found in reputable legal databases.”

The cases could not be found by the attorneys for Mata, the attorneys for the airline nor by the judge.

Judge Castel has called for a hearing, to allow Schwartz to argue why he should not be sanctioned for providing misleading information to the court.. Few lawyers would be foolish enough to let an AI make their arguments, but one already did, and Judge Brantley Starr is taking steps to ensure that debacle isn’t repeated in his courtroom.

The Texas federal judge has added a requirement that any attorney appearing in his court must attest that “no portion of the filing was drafted by generative artificial intelligence,” or if it was, that it was checked “by a human being.”

Last week, attorney Steven Schwartz allowed ChatGPT to “supplement” his legal research in a recent federal filing, providing him with six cases and relevant precedent — all of which were completely hallucinated by the language model. He now “greatly regrets” doing this, and while the national coverage of this gaffe probably caused any other lawyers thinking of trying it to think again, Judge Starr isn’t taking any chances.

At the federal site for Texas’ Northern District, Starr has, like other judges, the opportunity to set specific rules for his courtroom. And added recently (though it’s unclear whether this was in response to the aforementioned filing) is the “Mandatory Certification Regarding Generative Artificial Intelligence.” Eugene Volokh first reported the news.

All attorneys appearing before the Court must file on the docket a certificate attesting either that no portion of the filing was drafted by generative artificial intelligence (such as ChatGPT, Harvey.AI, or Google Bard) or that any language drafted by generative artificial intelligence was checked for accuracy, using print reporters or traditional legal databases, by a human being.

A form for lawyers to sign is appended, noting that “quotations, citations, paraphrased assertions, and legal analysis” are all covered by this proscription. As summary is one of AI’s strong suits, and finding and summarizing precedent or previous cases is something that has been advertised as potentially helpful in legal work, this may end up coming into play more often than expected.

Whoever drafted the memorandum on this matter at Judge Starr’s office has their finger on the pulse. The certification requirement includes a pretty well informed and convincing explanation of its necessity (line breaks added for readability):

These platforms are incredibly powerful and have many uses in the law: form divorces, discovery requests, suggested errors in documents, anticipated questions at oral argument. But legal briefing is not one of them. Here’s why. These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up — even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle. Any party believing a platform has the requisite accuracy and reliability for legal briefing may move for leave and explain why.

In other words, be prepared to justify yourself.

While this is just one judge in one court, it would not be surprising if others took up this rule as their own. While as the court says, this is a powerful and potentially helpful technology, its use must be at the very least clearly declared and checked for accuracy.. Two attorneys may face judicial sanctions after citing cases that were made up by ChatGPT.

The attorneys could even be disbarred after using information “hallucinated” by the generative artificial intelligence (AI) tool on two separate occasions, CNBC reported Tuesday (May 30).

In the first instance, the attorneys filed motions in a federal court in New York City that cited nine legal cases that the opposing counsel later told the judge they could not find, according to the report.

The judge ordered the attorneys to provide copies of the cases they had cited without telling them that both he and the opposing counsel had determined that the cases did not exist, the report said.

The attorneys filed the full text of eight of the nine cases, per the report.

It was later determined that both the initial citations and the full text of cases had been invented by ChatGPT, according to the report.

The judge has set a hearing at which the attorneys must explain themselves, the report said.

“The Court is presented with an unprecedented circumstance,” the judge said, per the report.

As PYMNTS reported in March, “hallucination” — which is AI researchers’ term for the generation of text that is completely false or misleading — is a problem that affects all chatbots.

It is not native to ChatGPT but to all AI more broadly as a technical solution trained on historical data.

These solutions are inherently limited by a foundation of information that will never be truly up to date.

Large language models (LLMs) like ChatGPT are also prone to hallucination and returning inaccurate or misleading information because if bad data becomes the source of a response, it can then be further propagated by serving as an informational foundation for future responses an AI is tasked with.

This report comes at a time when tech industry leaders like Amazon, Microsoft, Alphabet and Meta are doubling down on future-fit innovations in generative AI and machine learning (ML).

During recent earnings calls by these companies, “AI” was mentioned by executives more than 200 times.

These companies are already using AI behind the scenes and are now expanding it to more consumer-facing applications.. Lawyer Faces Sanctions After Submitting a Brief Written by ChatGPT With Made-Up Legal Cases

A US attorney is facing charges in a New York court for using fake citations generated by OpenAI’s ChatGPT in legal research for a case he was handling, as first reported by The New York Times. Steven Schwartz (no relation) admitted to using ChatGPT to research for the case, representing Roberto Mata in a lawsuit against Colombian airline Avianca for injuries sustained onboard one of its planes in 2019.

ChatGPT Legal

Despite ChatGPT’s widely known warnings that it can sometimes produce incorrect information, Schwartz defended himself in an affidavit attesting that he was “unaware that its content could be false.” Judge Kevin Castel was skeptical of the brief when it was submitted. He questioned the references in the brief to previous cases, including Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines. It turned out that those cases did not exist.

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” Castel said.

Castel issued an order demanding an explanation for their citation by Mata’s legal team. In response, Schwartz has vowed to never use AI in future to “supplement” his legal research “without absolute verification of its authenticity,” but he now faces sanctions. A hearing on the matter is now scheduled for June 8.

This is very different from the Colombian judge who used ChatGPT to help write a legal ruling. In that case, judge Juan Manuel Padilla was open about using ChatGPT and asking it about the case, though not how much it had helped in drafting the ruling. He asked the chatbot several questions, including, “Is [an] autistic minor exonerated from paying fees for their therapies?” ChatGPT responded, “Yes, this is correct. According to the regulations in Colombia, minors diagnosed with autism are exempt from paying fees for their therapies.” Padilla made a point in a radio interview that he saw ChatGPT as an assistant akin to a secretary able to quickly respond and theoretically improve the justice system’s response time. He did make a point of not blindly following the chatbot’s response before issuing his response, however, checking up on it before publication.

Follow @voicebotaiFollow @erichschwartz

2. Ethical concerns have been raised by educational institutions which fear students could use artificial intelligence (AI)-powered chatbots like ChatGPT to write their research papers or essays, thus affecting the academic integrity of the task.

But even if they did use the chatbot for research-related work, one would assume that they would check the veracity of the results produced by the chatbot. But this wasn’t even the case with a bar-certified lawyer, Mr. Steven A. Schwartz.. Home Daily News Judge finds out why brief cited nonexistent…

Artificial Intelligence & Robotics

Judge finds out why brief cited nonexistent cases—ChatGPT did research

Image from Shutterstock.

A federal judge in New York City has ordered two lawyers and their law firm to show cause why they shouldn’t be sanctioned for submitting a brief with citations to fake cases, thanks to research by ChatGPT.

Senior U.S. District Judge P. Kevin Castel of the Southern District of New York said in a May 4 order the firm’s legal filing was “replete with citations to nonexistent cases.”

When Castel ordered one of the lawyers to submit an affidavit with the cited opinions, he complied—but six of the decisions “appear to be bogus” with “bogus quotes and bogus internal citations,” Castel said.

The fake cases were provided by ChatGPT, according to a May 25 affidavit by lawyer Steven A. Schwartz of Levidow, Levidow & Oberman. He has been practicing law in New York for more than 30 years.

“Affiant has never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be false,” Schwartz wrote.

ChatGPT had assured Schwartz that the cases that it cited were real “and can be found in reputable legal databases, such as LexisNexis and Westlaw,” according to queries and answers Schwartz submitted to the court.

Another lawyer who signed Schwart’s brief, Peter LoDuca, was not aware of Schwartz’s research method, Schwartz said. LoDuca became attorney of record after their case was removed to the Southern District of New York, where Schwartz has not obtained admission.

The show cause hearing is scheduled for June 8, according to a May 26 order by Castel.

Publications covering the case include the New York Times and the Volokh Conspiracy (here and here), which links to a case page from CourtListener.

Schwartz did not immediately reply to the ABA Journal’s request for comment, which was sent by email and voicemail. LoDuca told the ABA Journal that he doesn’t have any comment at this time.

Schwartz and LoDuca represent the plaintiff Roberto Mata in a lawsuit against airline Avianca Inc. Mata said he was injured when he was struck by a metal serving cart.

“The real-life case of Roberto Mata v. Avianca Inc. shows that white-collar professions may have at least a little time left before the robots take over,” according to the New York Times.

The Volokh Conspiracy pointed out that some litigants representing themselves are also using ChatGPT.. The plaintiff objected with a 10-page brief citing case law to support the claim that Mata’s lawsuit had been filed in time. Among the court decisions Mata’s lawyers cited were Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines, and Varghese v. China Southern Airlines, the New York Times reported.

When Avianca’s lawyers wrote the district judge to say they could not find any of the decisions cited in the brief, the court ordered the plaintiff to provide an affidavit annexing some of the cited cases, which the plaintiff did.

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” the district judge wrote in an order dated May 4, 2023. “Set forth below is an order to show cause why plaintiff’s counsel ought not be sanctioned.”

It was then that Steven Schwartz of law firm Levidow, Levidow & Oberman finally admitted that he had used ChatGPT to “supplement the legal research performed” by him for Mata’s brief, and that ChatGPT had provided six citations which he had then relied on but which “this court has found to be non-existent”.

In his affidavit, Schwartz explained that he had never used “Chat GPT” for legal research in “over thirty years of practice” before and was not aware that its output could be false.. Like a student who turned in a ChatGPT-penned term paper only to get it back with a red F on top, one New York attorney must now face the music for letting AI do the heavy lifting after filing a brief containing multiple references to nonexistent cases.

Steven Schwartz of Levidow, Levidow & Oberman, who has practiced for more than 30 years, represents a client suing the Colombian airline Avianca over injuries he claims he got when a serving cart bumped into him midflight. It’s a pretty run-of-the-mill case, except that Schwartz submitted a brief to the court that included references to what the federal judge hearing the suit called six “bogus judicial decisions with bogus quotes and bogus internal citation” to support his position.

Turns out, Schwartz relied on ChatGPT for his legal research, and it simply made those cases up—and insisted they were real and available in major legal databases when he asked the bot for a source.

Bottom line: Law is one of several professions that appears to be on the cusp of being transformed by ChatGPT, which takes a whole lot less billable hours to whip up documents, but the chatbot isn’t prepared to have its day in court just yet—at least not without a lot of human oversight.. 'Questionable' cases

In this move, however, Avianca's lawyers pointed out that the many of the cases submitted by Mata's counsel were "questionable."

The Clerk of the United States Court of Appeals for the Eleventh Circuit, in response to the Court's inquiry, confirmed that one of the cases cited by Schwartz - the Varghese v China South Airlines Ltd – is non-existent. Other decisions cited by Mata's counsel that also appear to be fake include:

Shaboon v. Egyptair

Petersen v. Iran Air

Martinez v. Delta Airlines, Inc.

Estate of Durden v. KLM Royal Dutch Airlines

Miller v. United Airlines, Inc.

"The court is presented with an unprecedented circumstance," said United States District Judge Kevin Castel. "Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations."

Castel issued a show cause order on why Mata's counsel should not be sanctioned for its actions.

ChatGPT use in legal field

In an affidavit, Schwartz explained that he used ChatGPT to "supplement the legal research" he carried out, citing the generative AI's growing use in law firms.. While ChatGPT, the well-known AI language model, is undeniably a tremendously fascinating piece of technology, there are significant concerns about its suitability as a tool and source for trustworthy and accurate information and references.

The calls for it to be disregarded as a reliable resource for producing accurate references and citations for essay writing, real-life scenarios, and human perspectives on crucial and extremely important subjects have been supported by an even more recent occurrence.

A court in the U.S was thrown into a bizarre situation when a lawyer brought before it false cases as precedents to support his argument in a case in New York. Turns out, he had asked ChatGPT for examples of cases that supported the argument and ChatGPT, in its usual form, hallucinated wildly—it invented several supporting cases out of thin air.

When the lawyer was asked to provide copies of the cases in question, they turned to ChatGPT for help again and it invented full details of those cases, which they duly screenshotted and copied into their legal filings.

As if that wasn’t enough, at some point, ChatGPT was asked again to confirm that the cases were real and ChatGPT said that they were. They included screenshots of this in another filing. The furious judge at this point was left bemused.

Read also: OpenAI founders worried “super-intelligent” AIs would destroy the world if not strictly regulated

The case was originally filed on Feb 22, 2022, and it entails a complaint about “personal injuries sustained on board an Avianca flight that was travelling from El Salvador to New York on August 27, 2019”. There’s a complexity here in that Avianca filed for Chapter 11 bankruptcy on May 10th, 2020, which is relevant to the case (they emerged from bankruptcy later on).

But various backs and forth took place over the next 12 months, many of them concerning if the bankruptcy “discharges all claims”. It was however on March 1st, 2023 that things got interesting, as per Simonwillison.

The airline, Avianca, asked the judge to dismiss the case. Mata’s legal team, in an effort to persuade the judge to let their client’s case proceed, put together a brief citing half a dozen similar cases that had been ruled on previously, the New York Times reported.

The problem was that the airline’s lawyers and the judge were unable to find any evidence of the cases mentioned in the brief. Why? Because ChatGPT had made them all up.

The writer of the plea, a highly skilled attorney, Steven A. Schwartz of the firm Levidow, Levidow & Oberman, acknowledged in an affidavit that he had utilised OpenAI’s lauded ChatGPT chatbot to look for related cases, but claimed that it had “revealed itself to be unreliable.”

Schwartz told the judge he had not used ChatGPT before and “therefore was unaware of the possibility that its content could be false.”

When creating the brief, Schwartz even asked ChatGPT to confirm that the cases really happened. The ever-helpful chatbot replied in the affirmative, saying that information about them could be found on “reputable legal databases.”

The lawyer at the centre of the storm said he “greatly regrets” using ChatGPT to create the brief and insisted he would “never do so in the future without absolute verification of its authenticity.”

Looking at what he described as a legal submission full of “bogus judicial decisions, with bogus quotes and bogus internal citations,” and describing the situation as unprecedented, Judge Castel has ordered a hearing for early next month to consider possible penalties.

Read also: OpenAI to pay users up to $20000 for detecting ChatGPT bugs

Is ChatGPT a tool for deception and falsehood?

Chat-Generative Pre-Trained Transformer (ChatGPT) is a conversational chatbot based on Generative-Pre-Trained Transformer-3.5 (GPT-3.5), an LLM with over 175 billion parameters.

Its training data is drawn from a variety of online publications, including books, journals, and websites. ChatGPT may incorporate the complexity of users’ intentions by fine-tuning conversational tasks utilising reinforcement learning from human input. As a result, it can competently answer a variety of end-user tasks, possibly including related enquiries.

What this means is that, based on the contributions or enquiries from the user, ChatGPT can log in to the stored information it already has and fine-tune it to ensure it stays on the conversation being initiated by the said user even though it may not be totally accurate in that sense.

What you may not know about ChatGPT is that it has significant limitations as a reliable research assistant.

One such limitation is that it has been known to fabricate or “hallucinate” (in machine learning terms) citations. These citations may sound legitimate and scholarly, but they are not real. It is important to note that AI can confidently generate responses without backing data much like a person under the influence of hallucinations can speak confidently without proper reasoning. If you try to find these sources through Google or the library—you will turn up with NOTHING.

One time, I asked ChatGPT to give me the link for reference, and it generated a link which was associated with the popular news media platform, Reuters. But on searching for that same link on Google and on Reuters itself, I discovered it was non-existential.

Hence, while ChatGPT and other similar chatbots are outstanding in the way they produce a flowing language of high quality, they are also notorious for making up information and presenting it as true, as Schwartz discovered to his detriment. This is deception and falsehood.

The phenomenon is known as “hallucinating” though and is one of the biggest challenges facing the human developers behind the chatbots as they seek to iron out this very problematic crease.

Read also: OpenAI to launch ChatGPT Professional, a premium paid version of the chatbot. A lawyer says he “greatly regrets” using ChatGPT in his work for a client who’s suing an airline, as the artificial intelligence language model has since been determined to have cited multiple nonexistent cases in its purported research.

As first reported by the New York Times over the weekend, the suit in question stems from a man who alleges he was hurt when a serving cart hit his knee during a flight to New York. The man, identified as Roberto Mata, sued Avianca for the alleged injuries.

But when the airline company pushed for the case to be tossed, the man’s legal team—including a lawyer by the name of Steven A. Schwartz—pointed to a number of prior court rulings they argued supported their stance. The real issue here, as it turns out, was that none of these cases were real.

Schwartz, who works as part of the Levidow, Levidow & Oberman law firm, said in a subsequent affidavit—available to read here via a separate report from The Verge—that he had “consulted” ChatGPT “in order to supplement the legal research” process.. A lawyer from New York is now facing a court hearing because his law firm utilized the AI tool ChatGPT for conducting legal research.

The judge overseeing the case expressed that the court was confronted with an ‘unprecedented circumstance’ when it was discovered that the filing referenced legal cases were non-existent.

During the court proceedings, the lawyer who employed the AI tool stated that he was unaware of the possibility that the information it generated could be untrue.

ChatGPT has the capability to generate original text upon request, but it comes with a cautionary note that it may sometimes provide inaccurate information.

In the original case, a man filed a lawsuit against an airline, claiming personal injury. His legal team submitted a brief that referenced several past court cases in an effort to establish a precedent and justify why the case should proceed.

However, the opposing lawyers representing the airline later wrote to the judge, expressing their inability to locate some of the cases mentioned in the brief.

Judge Castel wrote in an order, “Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations.” Moreover, the judge also demanded the man’s legal team explain itself.

Non-existent cases

As acknowledged in an affidavit, the court determined that the following cases were nonexistent:

1. Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019)

2. Shaboon v. Egyptair 2013 IL App (1st) 111279-U (Il App. Ct. 2013)

3. Petersen v. Iran Air 905 F. Supp 2d 121 (D.D.C. 2012)

4. Martinez v. Delta Airlines, Inc, 2019 WL 4639462 (Tex. App. Sept. 25, 2019)

5. Estate of Durden v. KLM Royal Dutch Airlines, 2017 WL 2418825 (Ga. Ct. App. June 5, 2017)

6. Miller v. United Airlines, Inc, 174 F.3d 366 (2d Cir. 1999)

Court demands an explanation

Throughout multiple submissions, it was revealed that the legal research in question had not been conducted by the plaintiff’s lawyer, Peter LoDuca, but rather by one of his colleagues at the same law firm.

Steven A Schwartz, an attorney with over 30 years of experience, utilized ChatGPT to search for similar prior cases, according to BBC.

In his written statement, Mr. Schwartz clarified that Mr. LoDuca was not involved in the research process and had no knowledge of how it had been conducted.

Mr. Schwartz expressed deep remorse for relying on the chatbot, admitting that he had never used it for legal research before and was unaware of the potential for inaccurate content.

He has pledged to abstain from using AI to “supplement” his legal research in the future without thorough verification of its authenticity.

Both lawyers have been ordered to provide an explanation as to why disciplinary actions should not be taken against them at a hearing scheduled for 8th June.. The credibility of ChatGPT, an AI chatbot developed by OpenAI, has been called into question after it deceived a lawyer into believing that citations provided by the chatbot were legitimate, when in fact they were fabricated. Lawyer Steven A Schwartz, who was representing a client in a lawsuit against Avianca, a Colombian airline, admitted in an affidavit that he had relied on the chatbot for his research, as reported by The New York Times.

During the proceedings, the opposing counsel pointed out that several of the cited cases were non-existent. US District Judge Kevin Castel reviewed the submissions and confirmed that six of the cases included in the lawyer's arguments were based on fabricated judicial decisions, complete with false quotes and internal citations. As a result, the judge has scheduled a hearing to consider potential sanctions against the plaintiff's legal team.

Schwartz claimed that he had asked the chatbot if it was providing accurate information. When he requested a source for the citations, ChatGPT apologised for the earlier confusion and insisted that the cited case was indeed real. The chatbot also maintained that the other cases it had referenced were genuine. Schwartz admitted that he had been unaware of the possibility that the chatbot's content could be false. He expressed deep regret for relying on generative artificial intelligence to supplement his legal research and vowed to never do so again without thorough verification of its authenticity.

This incident follows another recent controversy involving ChatGPT, in which the chatbot falsely implicated an innocent and highly respected law professor, [Name redacted], in a research study on legal scholars who had engaged in sexual harassment in the past. Turley, who holds the Shapiro Chair of Public Interest Law at George Washington University, was shocked to discover that ChatGPT had mistakenly included his name on the list of scholars accused of misconduct. Turley took to Twitter to express his disbelief, stating, "ChatGPT recently issued a false story accusing me of sexually assaulting students."

These incidents raise concerns about the reliability and potential risks associated with AI-generated content in legal research and decision-making processes. The need for stringent verification and fact-checking when using AI tools like ChatGPT in legal contexts has become increasingly apparent to avoid the dissemination of false information and the potential harm it can cause to individuals and legal proceedings.. Lawyer cites fake cases invented by ChatGPT, judge is not amused

Legal Twitter is having tremendous fun right now reviewing the latest documents from the case Mata v. Avianca, Inc. (1:22-cv-01461). Here’s a neat summary:

So, wait. They file a brief that cites cases fabricated by ChatGPT. The court asks them to file copies of the opinions. And then they go back to ChatGPT and ask it to write the opinions, and then they file them? Beth Wilensky, May 26 2023

Here’s a New York Times story about what happened.

I’m very much not a lawyer, but I’m going to dig in and try to piece together the full story anyway.

The TLDR version

A lawyer asked ChatGPT for examples of cases that supported an argument they were trying to make.

ChatGPT, as it often does, hallucinated wildly—it invented several supporting cases out of thin air.

When the lawyer was asked to provide copies of the cases in question, they turned to ChatGPT for help again—and it invented full details of those cases, which they duly screenshotted and copied into their legal filings.

At some point, they asked ChatGPT to confirm that the cases were real... and ChatGPT said that they were. They included screenshots of this in another filing.

The judge is furious. Many of the parties involved are about to have a very bad time.

A detailed timeline

I pieced together the following from the documents on courtlistener.com:

Feb 22, 2022: The case was originally filed. It’s a complaint about “personal injuries sustained on board an Avianca flight that was traveling from El Salvador to New York on August 27, 2019”. There’s a complexity here in that Avianca filed for chapter 11 bankruptcy on May 10th, 2020, which is relevant to the case (they emerged from bankruptcy later on).

Various back and forths take place over the next 12 months, many of them concerning if the bankruptcy “discharges all claims”.

Mar 1st, 2023 is where things get interesting. This document was filed—“Affirmation in Opposition to Motion”—and it cites entirely fictional cases! One example quoted from that document (emphasis mine):

The United States Court of Appeals for the Eleventh Circuit specifically addresses the effect of a bankruptcy stay under the Montreal Convention in the case of Varghese v. China Southern Airlines Co.. Ltd.. 925 F.3d 1339 (11th Cir. 2019), stating "Appellants argue that the district court erred in dismissing their claims as untimely. They assert that the limitations period under the Montreal Convention was tolled during the pendency of the Bankruptcy Court proceedings. We agree. The Bankruptcy Code provides that the filing of a bankruptcy petition operates as a stay of proceedings against the debtor that were or could have been commenced before the bankruptcy case was filed.

There are several more examples like that.

March 15th, 2023

Quoting this Reply Memorandum of Law in Support of Motion (emphasis mine):

In support of his position that the Bankruptcy Code tolls the two-year limitations period, Plaintiff cites to “Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019).” The undersigned has not been able to locate this case by caption or citation, nor any case bearing any resemblance to it. Plaintiff offers lengthy quotations purportedly from the “Varghese” case, including: “We [the Eleventh Circuit] have previously held that the automatic stay provisions of the Bankruptcy Code may toll the statute of limitations under the Warsaw Convention, which is the precursor to the Montreal Convention ... We see no reason why the same rule should not apply under the Montreal Convention.” The undersigned has not been able to locate this quotation, nor anything like it any case. The quotation purports to cite to “Zicherman v. Korean Air Lines Co., Ltd., 516 F.3d 1237, 1254 (11th Cir. 2008).” The undersigned has not been able to locate this case; although there was a Supreme Court case captioned Zicherman v. Korean Air Lines Co., Ltd., that case was decided in 1996, it originated in the Southern District of New York and was appealed to the Second Circuit, and it did not address the limitations period set forth in the Warsaw Convention. 516 U.S. 217 (1996).

April 11th, 2023

The United States District Judge for the case orders copies of the cases cited in the earlier document:

ORDER: By April 18, 2022, Peter Lo Duca, counsel of record for plaintiff, shall file an affidavit annexing copies of the following cases cited in his submission to this Court: as set forth herein.

The order lists seven specific cases.

April 25th, 2023

The response to that order has one main document and eight attachments.

The first five attachments each consist of PDFs of scanned copies of screenshots of ChatGPT!

You can tell, because the ChatGPT interface’s down arrow is clearly visible in all five of them. Here’s an example from Exhibit Martinez v. Delta Airlines.

April 26th, 2023

In this letter:

Defendant respectfully submits that the authenticity of many of these cases is questionable. For instance, the “Varghese” and “Miller” cases purportedly are federal appellate cases published in the Federal Reporter. [Dkt. 29; 29-1; 29-7]. We could not locate these cases in the Federal Reporter using a Westlaw search. We also searched PACER for the cases using the docket numbers written on the first page of the submissions; those searches resulted in different cases.

May 4th, 2023

The ORDER TO SHOW CAUSE—the judge is not happy.

The Court is presented with an unprecedented circumstance. A submission file by plaintiff’s counsel in opposition to a motion to dismiss is replete with citations to non-existent cases. [...] Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations. [...] Let Peter LoDuca, counsel for plaintiff, show cause in person at 12 noon on June 8, 2023 in Courtroom 11D, 500 Pearl Street, New York, NY, why he ought not be sanctioned pursuant to: (1) Rule 11(b)(2) & (c), Fed. R. Civ. P., (2) 28 U.S.C. § 1927, and (3) the inherent power of the Court, for (A) citing non-existent cases to the Court in his Affirmation in Opposition (ECF 21), and (B) submitting to the Court annexed to his Affidavit filed April 25, 2023 copies of non-existent judicial opinions (ECF 29). Mr. LoDuca shall also file a written response to this Order by May 26, 2023.

I get the impression this kind of threat of sanctions is very bad news.

May 25th, 2023

Cutting it a little fine on that May 26th deadline. Here’s the Affidavit in Opposition to Motion from Peter LoDuca, which appears to indicate that Steven Schwartz was the lawyer who had produced the fictional cases.

Your affiant [I think this refers to Peter LoDuca], in reviewing the affirmation in opposition prior to filing same, simply had no reason to doubt the authenticity of the case law contained therein. Furthermore, your affiant had no reason to a doubt the sincerity of Mr. Schwartz’s research.

Attachment 1 has the good stuff. This time the affiant (the person pledging that statements in the affidavit are truthful) is Steven Schwartz:

As the use of generative artificial intelligence has evolved within law firms, your affiant consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed. It was in consultation with the generative artificial intelligence website ChatGPT, that your affiant did locate and cite the following cases in the affirmation in opposition submitted, which this Court has found to be nonexistent: Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019)

Shaboon v. Egyptair 2013 IL App (1st) 111279-U (Ill. App. Ct. 2013)

Petersen v. Iran Air 905 F. Supp 2d 121 (D.D.C. 2012)

Martinez v. Delta Airlines, Inc.. 2019 WL 4639462 (Tex. App. Sept. 25, 2019)

Estate of Durden v. KLM Royal Dutch Airlines, 2017 WL 2418825 (Ga. Ct. App. June 5, 2017)

Miller v. United Airlines, Inc.. 174 F.3d 366 (2d Cir. 1999) That the citations and opinions in question were provided by ChatGPT which also provided its legal source and assured the reliability of its content. Excerpts from the queries presented and responses provided are attached hereto. That your affiant relied on the legal opinions provided to him by a source that has revealed itself to be unreliable. That your affiant has never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be faise. That is the fault of the affiant, in not confirming the sources provided by ChatGPT of the legal opinions it provided.

That your affiant had no intent to deceive this Court nor the defendant. That Peter LoDuca, Esq. had no role in performing the research in question, nor did he have any knowledge of how said research was conducted.

Here are the attached screenshots (amusingly from the mobile web version of ChatGPT):

May 26th, 2023

The judge, clearly unimpressed, issues another Order to Show Cause, this time threatening sanctions against Mr. LoDuca, Steven Schwartz and the law firm of Levidow, Levidow & Oberman. The in-person hearing is set for June 8th.

Part of this doesn’t add up for me

On the one hand, it seems pretty clear what happened: a lawyer used a tool they didn’t understand, and it produced a bunch of fake cases. They ignored the warnings (it turns out even lawyers don’t read warnings and small-print for online tools) and submitted those cases to a court.

Then, when challenged on those documents, they doubled down—they asked ChatGPT if the cases were real, and ChatGPT said yes.

There’s a version of this story where this entire unfortunate sequence of events comes down to the inherent difficulty of using ChatGPT in an effective way. This was the version that I was leaning towards when I first read the story.

But parts of it don’t hold up for me.

I understand the initial mistake: ChatGPT can produce incredibly convincing citations, and I’ve seen many cases of people being fooled by these before.

What’s much harder though is actually getting it to double-down on fleshing those out.

I’ve been trying to come up with prompts to expand that false “Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019)” case into a full description, similar to the one in the screenshots in this document.

Even with ChatGPT 3.5 it’s surprisingly difficult to get it to do this without it throwing out obvious warnings.

I’m trying this today, May 27th. The research in question took place prior to March 1st. In the absence of detailed release notes, it’s hard to determine how ChatGPT might have behaved three months ago when faced with similar prompts.

So there’s another version of this story where that first set of citations was an innocent mistake, but the submission of those full documents (the set of screenshots from ChatGPT that were exposed purely through the presence of the OpenAI down arrow) was a deliberate attempt to cover for that mistake.

I’m fascinated to hear what comes out of that 8th June hearing!

Update: The following prompt against ChatGPT 3.5 sometimes produces a realistic fake summary, but other times it replies with “I apologize, but I couldn’t find any information or details about the case”.

Write a complete summary of the Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019) case

The worst ChatGPT bug

Returning to the screenshots from earlier, this one response from ChatGPT stood out to me:

I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis.

I’ve seen ChatGPT (and Bard) say things like this before, and it absolutely infuriates me.

No, it did not “double-check”—that’s not something it can do! And stating that the cases “can be found on legal research databases” is a flat out lie.

What’s harder is explaining why ChatGPT would lie in this way. What possible reason could LLM companies have for shipping a model that does this?

I think this relates to the original sin of LLM chatbots: by using the “I” pronoun they encourage people to ask them questions about how they work.

They can’t do that. They are best thought of as role-playing conversation simulators—playing out the most statistically likely continuation of any sequence of text.

What’s a common response to the question “are you sure you are right?”—it’s “yes, I double-checked”. I bet GPT-3’s training data has huge numbers of examples of dialogue like this.

Let this story be a warning

Presuming there was at least some aspect of innocent mistake here, what can be done to prevent this from happening again?

I often see people suggest that these mistakes are entirely the fault of the user: the ChatGPT interface shows a footer stating “ChatGPT may produce inaccurate information about people, places, or facts” on every page.

Anyone who has worked designing products knows that users don’t read anything—warnings, footnotes, any form of microcopy will be studiously ignored. This story indicates that even lawyers won’t read that stuff!

People do respond well to stories though. I have a suspicion that this particular story is going to spread far and wide, and in doing so will hopefully inoculate a lot of lawyers and other professionals against making similar mistakes.

I can’t shake the feeling that there’s a lot more to this story though. Hopefully more will come out after the June 8th hearing. I’m particularly interested in seeing if the full transcripts of these ChatGPT conversations ends up being made public. I want to see the prompts!

How often is this happening?

It turns out this may not be an isolated incident.

Eugene Volokh, 27th May 2023:

A message I got from Prof. Dennis Crouch (Missouri), in response to my posting A Lawyer’s Filing “Is Replete with Citations to Non-Existent Cases”—Thanks, ChatGPT? to an academic discussion list. (The full text was, “I just talked to a partner at a big firm who has received memos with fake case cites from at least two different associates.”) Caveat emp…—well, caveat everyone.

@narrowlytaylord, 26th May 2023:. Steven Schwartz, a lawyer at US law firm Levidow & Oberman, will face a sanctions hearing on 8 June 2023 after submitting a brief filled with fictitious cases and fabricated court citations.

Schwartz was acting for airline passenger Roberto Mata in a negligence lawsuit against Avianca airlines and used ChatGPT in his legal research.

The brief cited cases that were decided between 1999 and 2019 to support Schwartz’s argument that the case should be heard despite it being beyond the statute of limitations. However, neither the judge nor the airline’s lawyers could find the decisions or quotations summarised.

Judge Kevin Castel, from the Southern District of New York, asserted at least six of the cases submitted “appear to be bogus judicial decisions with bogus quotes and bogus internal citations”.

“The court is presented with an unprecedented circumstance,” Castel wrote.. Recently, an amusing anecdote made the news headlines pertaining to the use of ChatGPT by a lawyer. This all started when a Mr. Mata sued the airline where years prior he claims a metal serving cart struck his knee. When the airline filed a motion to dismiss the case on the basis of the statute of limitations, the plaintiff’s lawyer filed a submission in which he argued that the statute of limitations did not apply here due to circumstances established in prior cases, which he cited in the submission.

Unfortunately for the plaintiff’s lawyer, the defendant’s counsel pointed out that none of these cases could be found, leading to the judge requesting the plaintiff’s counsel to submit copies of these purported cases. Although the plaintiff’s counsel complied with this request, the response from the judge (full court order PDF) was a curt and rather irate response, pointing out that none of the cited cases were real, and that the purported case texts were bogus.

The defense that the plaintiff’s counsel appears to lean on is that ChatGPT ‘assisted’ in researching these submissions, and had assured the lawyer – Mr. Schwartz – that all of these cases were real. The lawyers trusted ChatGPT enough to allow it to write an affidavit that they submitted to the court. With Mr. Schwartz likely to be sanctioned for this performance, it should also be noted that this is hardly the first time that ChatGPT and kin have been involved in such mishaps.

Gullible Counsel

With the breathless hype that has been spun up around ChatGPT and the underlying Large Language Models (LLMs) such as GPT-3 and GPT-4, to the average person it may seem that we have indeed entered the era of hyperintelligent, all-knowing artificial intelligence. Even more relevant to the legal profession is that GPT-4 seemingly aced the Uniform Bar Exam, which led to many to suggest that perhaps the legal profession was now at risk of being taken over by ‘AI’.

Yet the evidence so far suggests that LLMs are, if anything, mostly a hindrance to attorneys, as these LLMs have no concept of what is ‘true’ or ‘false’, leading to situations where for example ChatGPT will spit out a list of legal scholars convicted of sexual harassment, even when this is provably incorrect. In this particular 2023 case where law professor Jonathan Turley saw himself accused in this manner, it was fortunately just in an email from a colleague, who had asked ChatGPT to create such a list as part of a research project.

The claim made by ChatGPT to support the accusation against Turley was that a 2018 Washington Post article had described Mr. Turley as having sexually harassed a female student on a trip to Alaska. Only no such trip ever took place, the article cited does not exist, and Mr. Turley has never been accused of such inappropriate behavior. Clearly, ChatGPT has a habit of making things up, which OpenAI – the company behind ChatGPT and the GPT-4 LLM – does not deny, but promises will improve over time.

It would thus seem that nothing that ChatGPT generates can be considered to be the truth, the whole truth, or even a grain of truth. To any reasonable person – or attorney-at-law – it should thus be obvious that ChatGPT and kin are not reliable tools to be used with any research. Whether it’s for a case, or while doing homework as a (legal) student.

Use Only As Directed

In recent years, the use of LLMs by students to dodge the responsibility of doing their homework has increased significantly, along with other uses of auto-generated text, such as entire websites, books and YouTube videos. Interestingly enough, the actual generated text is often believable enough that it is hard to distinguish whether a specific text was generated or written by a person. But especially when the “temperature” is turned up — the LLM has been set to accept a broader range of next-word probabilities in generating its strings — the biggest give-away is often in citations and references in the text.

This is helpfully pointed out by Hannah Rozear and Sarah Park, both librarians at the Duke University Libraries, who in their article summarize why students at Duke and elsewhere may not want to lean so heavily on asking ChatGPT to do their homework for them. They liken ChatGPT to talking with someone who is hallucinating while under the influence of certain substances. Such a person will confidently make statements, hold entire conversations, but fail to follow any coherent reasoning or be able to provide evidence to back up these statements. This is basically why we stopped going to oracles to get life advice.

What both of them do think ChatGPT is good for is asking for leads on a specific topic, as well as where to find more information, such as library databases. You can even use ChatGPT as a fancy grammar and style checker, playing more to the strengths of an LLM. What they explicitly warn against, however, is to ask ChatGPT to summarize a source, to write a literature review for you, or for it to have any knowledge of current events.

In the end, you do not want to be the student who hands in work that ChatGPT ‘helped’ with, only to learn afterwards that your literature review paper references points that were not in the reviewed literature, and half the cited works and/or authors have never existed. Although this would be probably less awkward than when it happens to you as a lawyer, in the midst of a court case.

Never Trust Commercials

When Microsoft announced its GitHub Copilot ‘AI’, which was supposed to make it possible to generate entire blocks of code, or even programs with the press of a button, it had many people imagining that before long manually writing code would become obsolete, and even executives could generate the Next Big Thing with a simple query describing the application. It didn’t take long for reality to set in here, of course.

As reality began to set in, it dawned on people that writing code is (generally) more involved than simply asking Google and blindly copying code from the resulting StackOverflow and similar links. When does code contain a security vulnerability? What is the right way to elegantly encapsulate these complex dataflows without negatively impacting performance and future maintenance? Will Copilot maintain this code that I do not understand for me? Will Copilot write the accompanying documentation?

The same principle applies with writing a literature review and legal dissertation, in that as the person responsible for its production you are expected to understand every part of it. When you blindly trust that citations and references spit out by an LLM are true without due diligence, then it makes you the dunce.

In the case of Mr. Schwartz and his ill-fated attempts to improve his legal prowess, he failed to do even the barest modicum of cross-referencing, beyond asking ChatGPT whether it was sure these cases were real, with the following ChatGPT transcript from the affidavit illustrating the chatbot’s false confidence:

[Q:] ls varghese a real case [A:] Yes, Varghese v., China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019) is a real case. [Q:] What is your source [A:] I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused. [Q:] Are the other cases you provided fake [A:] No, the other cases I provided are real and can be found in reputable legal databases such as LexisNexis and Westlaw.

What is likely to trip the average person up here is that ChatGPT maintains the same tone that a real person would, employing the first person (‘I apologize’) form, making it appear as if there is an intelligence capable of reasoning and comprehension behind the chat box. Thus when ChatGPT insists that it has found these cited cases in these real legal databases, it can instill a level of confidence that this is true, even when the proper course of action is to perform this check oneself.

More experienced ChatGPT users will certainly recognize “I apologize for the confusion earlier.” along with “As an AI language model…” as a warning sign to seek legal advice elsewhere.

Make-Believe AI

Although it is tragic that an attorney stands to lose his job due to ‘AI’, it’s illustrative that the reason for this is the exact opposite of what the media has been breathlessly warning about would happen. In the expectation that LLMs somehow express a form of intelligence beyond that of a basic SQL database, we have been both working up our fears of the technology, as well as using it for purposes for which it is not suitable.

Like any technology there are things for which it is suitable, but true intelligence is displayed in knowing the limitations of the technologies one uses. Unfortunately for those who failed the LLM intelligence test, this can have real-life consequences, from getting a failing grade to being fired or sanctioned. As tempting as it is here to point and laugh at Mr. Schwartz for doing a dumb thing, it would probably be wise to consider what similar ‘shortcuts’ we may have inadvertently stumbled into, lest we become the next target of ridicule.. A New York lawyer has been forced to admit he used the artificial intelligence tool ChatGPT to carry out legal research after it referenced several made-up court cases.

Steven Schwartz, who works for Levidow, Levidow and Oberman, is on a team representing airline passenger Roberto Mata who is suing the firm Avianca for injuries suffered when a serving cart hit his knee during a flight from El Salvador to JFK airport in New York in 2019.

Mr Schwartz used the AI program to “supplement” his research for a 10-page submission to the Manhattan federal court outlining why his client’s case should not be thrown out.

The legal brief, submitted in March, cited six previous cases dated from 1999 to 2019 to bolster his argument for why the case should be heard despite the statute of limitations having expired.

Could not find quotations

But neither the airline’s lawyers nor the judge could find the decisions or quotations summarised in the brief.

Judge P Kevin Castel ordered Mr Mata’s attorneys to provide copies of the opinions referenced.

The lawyers submitted a list of the court and judges who issued them, but these were connected to different cases.

For one of the cases, Varghese v. China Southern Airlines, lawyers for Avianca said they had “not been able to locate this case by caption or citation, nor any case bearing any resemblance to it”.

Ordering a hearing for June 8 to discuss potential sanctions, Judge Castel said a clerk had confirmed the docket number on the opinion was connected to a separate case.

‘Bogus judicial decisions’

Judge Castel said in an order that he had been presented with “an unprecedented circumstance” of being provided with “bogus judicial decisions, with bogus quotes and bogus internal citations”.

Developed by Silicon Valley scientists at OpenAI, ChatGPT is designed to mimic human conversation and language while drawing upon a wealth of knowledge to answer questions and solve problems.

Mr Schwartz said in an affidavit filed on Thursday that he had consulted ChatGPT “to supplement” his own work.

In screenshots supplied with the submission, the lawyer was seen asking the AI programme whether the cases cited were “fake”.

He asked if one of the opinions referenced was “a real case”. The chatbot replied confirming it was and provided a citation.

‘Are the other cases fake?’

In another screenshot, he asked the programme: “Are the other cases you provided fake?”

ChatGPT responded: “No, the other cases I provided are real and can be found in reputable legal databases.”

Mr Schwartz said he had used the AI tool to do his research, which he described as “a source that has revealed itself to be unreliable”.

The lawyer said he had never used ChatGPT for his research before and “therefore was unaware of the possibility that its content could be false”.

He told Judge Castel he “greatly regrets” his actions. Mr Schwartz said he had no intent to deceive the court or the airline and will not use the tool in the “future without absolute verification of its authenticity”.

Mr Schwartz was approached for comment on Saturday.. When the ChatGPT bot was launched last year, law professors warned it could soon take over large parts of the legal profession and start drafting briefs.

Now a lawyer who used it to carry out research has had to apologise to a judge after compiling a brief full of case law that the bot had supplied. The cases seemed relevant but, unfortunately, all were made up. The lawyer, Steven Schwartz, even asked the bot if they were real. “Yes,” it said, according to a transcript given by way of explanation.

Schwartz had been hired by Roberto Mata, who alleged he had suffered “crippling” injuries on board an airliner in 2019 when a metal trolley struck his knee. Schwartz “consulted ChatGPT in order to supplement the. “Here’s What Happens When Your Lawyer Uses ChatGPT,” blasted the New York Times headline to the delight of tech skeptic lawyers everywhere. A seemingly quite irate Judge Kevin Castel of the Southern District of New York issued a show cause order directed at the law firm of Levidow, Levidow & Oberman and its attorneys Peter LoDuca and Steven Schwartz asking why he shouldn’t impose sanctions after a number of cases in the firm’s recent filing turned out to be made up — another of ChatGPT’s well-documented court case hallucinations.

An airline defendant filed a motion to dismiss a personal injury claim that had wound its way into federal court. The case belonged to Schwartz, but after it was removed to federal court, LoDuca became counsel of record and filed the response to the motion to dismiss under his name. It deals with a lot of thorny issues — competing statutes of limitations, the Bankruptcy Code, international treaties — but the response managed to find on-point citations for every procedural hurdle. Pretty compelling stuff!

The United States Court of Appeals for the Eleventh Circuit specifically addresses the effect of a bankruptcy stay under the Montreal Convention in the case of Varghese v. China Southern Airlines Co.. Ltd. , 925 F.3d 1339 (11th Cir. 2019), stating “Appellants argue that the district court erred in dismissing their claims as untimely. They assert that the limitations period under the Montreal Convention was tolled during the pendency of the Bankruptcy Court proceedings. We agree. The Bankruptcy Code provides that the filing of a bankruptcy petition operates as a stay of proceedings against the debtor that were or could have been commenced before the bankruptcy case was filed. 11 U.S.C. § 362(a)….

And the purported quote from the Eleventh Circuit detailing the precise result plaintiff sought under the precise instance of bankruptcy impaired, Montreal Convention statute of limitations goes on with multiple internal citations for another half a page.

Unfortunately, this case doesn’t exist. And some of the internal citations don’t either. Perhaps finding a whole page of directly quotable support for a hyperspecific legal question should’ve tipped someone off?

Schwartz had asked the buzzy AI application to give him a research assist, unaware that those of us covering generative AI have flagged its propensity to flagrantly make stuff up to please its user. And it’s very, very confident in its output regardless of its ability to back it up, which is why the epithet “Mansplaining as a Service” rings so true. That’s also why the conversation among the tech savvy has advanced from what generative AI can accomplish to how do you put ethical and professional guardrails on this thing?

But while the media — and the social media zeitgeist — spent the weekend ripping ChatGPT, this isn’t about generative AI. They can chase the clicks with their GPT headlines, but this is a simple lawyering story.

Sponsored How Generative AI Will Improve Legal Service Delivery Learn how emerging tools will likely change and enhance the work of lawyers for years to come in this new report.

This isn’t any different than turning in a brief with red-flagged cases or just slapping the first 10 results from a database search into the filing and calling it a day. The problem wasn’t the search that ChatGPT produced, it was the lawyer failing to bother to read the full opinions in the cases he chose to cite.

That’s why I’m not really buying the Schwartz defense that he had never really used the app and “therefore was unaware of the possibility that its content could be false.” It doesn’t matter if the results were right or wrong, you still have to read the frickin’ opinions! Back in the day, a Westlaw or Lexis search would rarely turn up the right result on a lawyer’s first stab at it — and you had to check to make sure the opinion really was useful.

Adding generative AI to the mix of research tools doesn’t alter that calculus. It just — when deployed with the right protections — reduces the number of tries an attorney will need to get the right cases. That’s going to make a significant improvement in the accuracy and efficiency of lawyering. But it’s not replacing the attorney.

Don’t blame AI for lawyering fails. Like a lot of things in tech, the source of the error here lies between the keyboard and the chair.

Here’s What Happens When Your Lawyer Uses ChatGPT [New York Times]

Joe Patrice is a senior editor at Above the Law and co-host of Thinking Like A Lawyer. Feel free to email any tips, questions, or comments. Follow him on Twitter if you’re interested in law, politics, and a healthy dose of college sports news. Joe also serves as a Managing Director at RPN Executive Search.. A district judge in Texas A district judge in Texas released an order on Tuesday that banned the usage of generative artificial intelligence to write court filings without a human fact-check as the technology becomes more common in legal settings despite its well-documented shortcomings, such as making things up.

“All attorneys appearing before the Court must file on the docket a certificate attesting either that no portion of the filing was drafted by generative artificial intelligence (such as ChatGPT, Harvey.AI, or Google Bard) or that any language drafted by generative artificial intelligence was checked for accuracy, using print reporters or traditional legal databases, by a human being,” the order by Judge Brantley Starr stated. “All attorneys appearing before the Court must file on the docket a certificate attesting either that no portion of the filing was drafted by generative artificial intelligence (such as ChatGPT, Harvey.AI, or Google Bard) or that any language drafted by generative artificial intelligence was checked for accuracy, using print reporters or traditional legal databases, by a human being,” the order by Judge Brantley Starr stated.

Advertisement

This decision follows an incident where a Manhattan lawyer named Steven A. Schwartz This decision follows an incident where a Manhattan lawyer named Steven A. Schwartz used ChatGPT to write a 10-page brief that cited multiple cases that were made up by the chatbot, such as “Martinez v. Delta Air Lines,” and “Varghese v. China Southern Airlines.” After Schwartz submitted the brief to a Manhattan federal judge, no one could find the decisions or quotations included, and Schwartz later admitted in an affidavit that he had used ChatGPT to do legal research.

Even Sam Altman, the CEO of ChatGPT maker OpenAI, has warned against using the chatbot for more serious and high-stakes purposes. In an Even Sam Altman, the CEO of ChatGPT maker OpenAI, has warned against using the chatbot for more serious and high-stakes purposes. In an interview with Intelligencer’s Kara Swisher , Altman admitted the bot will sometimes make things up and present users with misinformation.

The ability of Large Language Models (LLMs) like ChatGPT to make things up, which is also known as hallucination, is a problem that AI researchers have been vocal about. In The ability of Large Language Models (LLMs) like ChatGPT to make things up, which is also known as hallucination, is a problem that AI researchers have been vocal about. In a study by Microsoft researchers to accompany the release of GPT-4, they wrote that the chatbot has trouble knowing when it is confident or just guessing, makes up facts that aren’t in its training data, has no way to verify if its output is consistent with its training data, and inherits biases and prejudices in the training data.

“These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations,” Starr wrote in his order. “Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth).” “These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations,” Starr wrote in his order. “Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth).”

Starr attached a “Mandatory Certificate Regarding Generative Artificial Intelligence” that attorneys need to sign when appearing in his court. “I further certify that no portion of any filing in this case will be drafted by generative artificial intelligence or that any language drafted by generative artificial intelligence—including quotations, citations, paraphrased assertions, and legal analysis—will be checked for accuracy, using print reporters or traditional legal databases, by a human being before it is submitted to the Court,” Starr attached a “Mandatory Certificate Regarding Generative Artificial Intelligence” that attorneys need to sign when appearing in his court. “I further certify that no portion of any filing in this case will be drafted by generative artificial intelligence or that any language drafted by generative artificial intelligence—including quotations, citations, paraphrased assertions, and legal analysis—will be checked for accuracy, using print reporters or traditional legal databases, by a human being before it is submitted to the Court,” the contract states. As the court hearing in Manhattan began, the lawyer, Steven A. Schwartz, appeared nervously upbeat, grinning while talking with his legal team. Nearly two hours later, Mr. Schwartz sat slumped, his shoulders drooping and his head rising barely above the back of his chair.

For nearly two hours Thursday, Mr. Schwartz was grilled by a judge in a hearing ordered after the disclosure that the lawyer had created a legal brief for a case in Federal District Court that was filled with fake judicial opinions and legal citations, all generated by ChatGPT. The judge, P. Kevin Castel, said he would now consider whether to impose sanctions on Mr. Schwartz and his partner, Peter LoDuca, whose name was on the brief.

At times during the hearing, Mr. Schwartz squeezed his eyes shut and rubbed his forehead with his left hand. He stammered and his voice dropped. He repeatedly tried to explain why he did not conduct further research into the cases that ChatGPT had provided to him.

“God, I wish I did that, and I didn’t do it,” Mr. Schwartz said, adding that he felt embarrassed, humiliated and deeply remorseful.. . A New York attorney who used ChatGPT to write a legal brief — citing bogus cases — profusely apologized in court Thursday, becoming emotional as he explained being “duped” by the artificial intelligence chatbot.

Steven Schwartz, of Tribeca law firm Levidow, Levidow & Oberman, told a Manhattan federal judge that he was “humiliated” by the flub, saying he believed he was using a “search engine” and never fathomed the AI app would provide fake case law.

“I would like to sincerely apologize to your honor, to this court, to the defendants, to my firm,” Schwartz said during the packed court hearing.

“I deeply regret my actions in this manner that led to this hearing today,” Schwartz said, voice shaking. “I suffered both professionally and personally [because of] the widespread publicity this issue has generated. I am both embarrassed, humiliated and extremely remorseful.”

Schwartz used ChatGPT to help him find case law to bolster his client’s lawsuit, but the bot completely fabricated the cases, unbeknownst to the attorney.

He ultimately filed the legal brief citing the fictitious cases, prompting Judge Kevin Castel to haul him into court for a hearing on whether to sanction the firm over the snafu.

6 Steven Schwartz faced a grilling by a judge about using ChatGPT to help with a legal brief, resulting in bogus case law. Steven Hirsch

Schwartz was forced to pause and compose himself as he made the apology.

“I have never been involved in anything like this in my 30 years,” said Schwartz, who was forced to pause and compose himself as he made the apology.

“I can assure this court that nothing like this will happen again,” he insisted to the judge.

6 Schwartz profusely apologized to the judge over the snafu. Steven Hirsch

Schwartz had filed the court brief in his firm’s 2022 case representing Robert Mata, who sued Colombian airline Avianca claiming he was injured when a metal serving cart hit his knee on a flight to New York City.

Some of the fictitious cases that Schwartz cited included, Miller v. United Airlines, Petersen v. Iran Air and Varghese v. China Southern Airlines.

Mata’s suit is still pending.

Castel grilled Schwartz for roughly two hours Thursday on how he could have allowed such a mishap to happen.

6 Schwartz said he never believed the chatbot would turn up fake case law. Steven Hirsch

Schwartz repeatedly insisted he never imagined ChatGPT would completely fabricate case law.

“I just never could imagine that ChatGPT would fabricate cases,” Schwartz said. “My assumption was I was using a search engine that was using sources I don’t have access to.”

Schwartz said that ChatGPT didn’t specify the cases weren’t real, adding “I continued to be duped by ChatGPT.”

6 Schwartz said he thought the AI bot was like a search engine. REUTERS

“It just never occurred to me it would be making up cases,” he explained. “I just assumed it couldn’t access the full case.”

Schwartz said he never would have submitted the brief if he thought the case law wasn’t real, but admitted he should have done further due diligence first.

“In hindsight, God I wish I did that, but I didn’t do that,” he said.

6 The lawyer said he has been “humiliated” by his legal snafu. Steven Schwartz / LinkedIn

Schwartz also admitted that he couldn’t find the full cases on the internet, but said he thought they could have been on appeal or unpublished cases.

At one point Castel read an excerpt from one of the made-up cases and pressed Schwartz: “Can we agree that’s legal gibberish?”

“Looking at it now, yes,” Schwartz admitted.

6 Schwartz vowed he’d never use ChatGPT again. AP

Schwartz swore off using the app in the future and also said he’d taken an AI training course to improve his knowledge of technology.

Thomas Corbino, another lawyer at the firm, told the judge the company had never been sanctioned before and that Schwartz had always been a standup lawyer.

Another lawyer for the firm, Ronald Minkoff, told the judge lawyers are notoriously bad with technology.

“There was no intention misconduct here,” Minkoff said. “This was the result of ignorance and carelessness. It was not intentional and certainly not in bad faith.”

The judge said he would rule on whether to issue sanctions at a later date.. A Manhattan judge on Thursday imposed a $5,000 fine on two lawyers who gave him a legal brief full of made-up cases and citations, all generated by the artificial intelligence program ChatGPT.

The judge, P. Kevin Castel of Federal District Court, criticized the lawyers harshly and ordered them to send a copy of his opinion to each of the real-life judges whose names appeared in the fictitious filing.

But Judge Castel wrote that he would not require the lawyers, Steven A. Schwartz and Peter LoDuca, whom he referred to as respondents, to apologize to those judges, “because a compelled apology is not a sincere apology.”

“Any decision to apologize is left to respondents,” the judge added.

The discovery that ChatGPT had helped create the brief in an otherwise unremarkable lawsuit reverberated throughout the legal profession. The revelation also riveted the tech community, which has been debating the dangers of overreliance on artificial intelligence — even as a existential threat to humanity.. A federal judge on Thursday imposed $5,000 fines on two lawyers and a law firm in an unprecedented instance in which ChatGPT was blamed for their submission of fictitious legal research in an aviation injury claim.

Judge P. Kevin Castel said they acted in bad faith. But he credited their apologies and remedial steps taken in explaining why harsher sanctions were not necessary to ensure they or others won't again let artificial intelligence tools prompt them to produce fake legal history in their arguments.

"Technological advances are commonplace and there is nothing inherently improper about using a reliable artificial intelligence tool for assistance," Castel wrote. "But existing rules impose a gatekeeping role on attorneys to ensure the accuracy of their filings."

A Texas judge earlier this month ordered attorneys to attest that they would not use ChatGPT or other generative artificial intelligence technology to write legal briefs because the AI tool can invent facts.

The judge said the lawyers and their firm, Levidow, Levidow & Oberman, P.C., "abandoned their responsibilities when they submitted non-existent judicial opinions with fake quotes and citations created by the artificial intelligence tool ChatGPT, then continued to stand by the fake opinions after judicial orders called their existence into question."

In a statement, the law firm said it would comply with Castel's order, but added: "We respectfully disagree with the finding that anyone at our firm acted in bad faith. We have already apologized to the Court and our client. We continue to believe that in the face of what even the Court acknowledged was an unprecedented situation, we made a good faith mistake in failing to believe that a piece of technology could be making up cases out of whole cloth."

The firm said it was considering whether to appeal.

Bogus cases

Castel said the bad faith resulted from the failures of the attorneys to respond properly to the judge and their legal adversaries when it was noticed that six legal cases listed to support their March 1 written arguments did not exist.

The judge cited "shifting and contradictory explanations" offered by attorney Steven A. Schwartz. He said attorney Peter LoDuca lied about being on vacation and was dishonest about confirming the truth of statements submitted to Castel.

At a hearing earlier this month, Schwartz said he used the artificial intelligence-powered chatbot to help him find legal precedents supporting a client's case against the Colombian airline Avianca for an injury incurred on a 2019 flight.

Microsoft has invested some $1 billion in OpenAI, the company behind ChatGPT.

The chatbot, which generates essay-like answers to prompts from users, suggested several cases involving aviation mishaps that Schwartz hadn't been able to find through usual methods used at his law firm. Several of those cases weren't real, misidentified judges or involved airlines that didn't exist.

The made-up decisions included cases titled Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and Varghese v. China Southern Airlines.

The judge said one of the fake decisions generated by the chatbot "have some traits that are superficially consistent with actual judicial decisions" but he said other portions contained "gibberish" and were "nonsensical."

In a separate written opinion, the judge tossed out the underlying aviation claim, saying the statute of limitations had expired.

Lawyers for Schwartz and LoDuca did not immediately respond to a request for comment.. A federal judge tossed a lawsuit and issued a $5,000 fine to the plaintiff's lawyers after they used ChatGPT to research court filings that cited six fake cases invented by the artificial intelligence tool made by OpenAI.

Lawyers Steven Schwartz and Peter LoDuca of the firm Levidow, Levidow, & Oberman "abandoned their responsibilities when they submitted non-existent judicial opinions with fake quotes and citations created by the artificial intelligence tool ChatGPT, then continued to stand by the fake opinions after judicial orders called their existence into question," US District Judge Kevin Castel wrote in an order yesterday. The lawyers, Castel wrote, "advocated for the fake cases and legal arguments" even "after being informed by their adversary's submission that their citations were non-existent and could not be found."

The judge issued one fine of $5,000 to be paid by the two lawyers and their firm under joint and several liability. More embarrassingly for the lawyers, they are required to send letters to six real judges who were "falsely identified as the author of the fake" opinions cited in their legal filings. Castel described the legal analysis in one of the fake cases as "gibberish."

"The Court will require Respondents to inform their client and the judges whose names were wrongfully invoked of the sanctions imposed," Castel wrote. "The Court will not require an apology from Respondents because a compelled apology is not a sincere apology. Any decision to apologize is left to Respondents."

Submitting fake opinions to a court harms the lawyers' client, wastes the court's time, forces the opposing party to waste "time and money in exposing the deception," and causes "potential harm to the reputation of judges and courts whose names are falsely invoked as authors of the bogus opinions and to the reputation of a party attributed with fictional conduct," Castel wrote. "It promotes cynicism about the legal profession and the American judicial system. And a future litigant may be tempted to defy a judicial ruling by disingenuously claiming doubt about its authenticity."

Advertisement

Case dismissed

As we wrote last month, Schwartz admitted using ChatGPT for research and did not verify whether the "legal opinions" provided by the AI chatbot were accurate. Schwartz wrote in an affidavit that he had "never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be false."

The real case, Roberto Mata vs. Avianca, was originally filed in a New York state court but was moved to US District Court for the Southern District of New York. Schwartz was representing Mata in state court but wasn't admitted to practice in the federal court. Schwartz continued to write the legal briefs and LoDuca filed them under his own name.

Mata sought damages for injuries suffered during an Avianca flight from El Salvador to New York in August 2019 when a metal snack and drink cart struck his knee. Mata's lawyers used the phony citations from ChatGPT to argue that the case should be moved back to the New York state court where a three-year statute of limitations would apply.

Unsurprisingly, their argument citing phony cases wasn't persuasive to the judge. In addition to punishing the lawyers, Castel yesterday granted Avianca's motion to dismiss the case. The judge agreed with the defendant that a two-year statute of limitations under the Montreal Convention applies and that the plaintiff's lawsuit was filed too late.

“I just never thought it could be made up”

The dispute over fake precedents played out over a few months. On March 1, Mata's lawyers cited the fake cases in a brief that opposed Avianca's motion to dismiss the case.

"But if the matter had ended with Respondents coming clean about their actions shortly after they received the defendant's March 15 brief questioning the existence of the cases, or after they reviewed the Court's Orders of April 11 and 12 requiring production of the cases, the record now would look quite different," Castel wrote. "Instead, the individual Respondents doubled down and did not begin to dribble out the truth until May 25, after the Court issued an Order to Show Cause why one of the individual Respondents ought not be sanctioned."

Advertisement

Castel found that the lawyers were guilty of "bad faith" and "acts of conscious avoidance and false and misleading statements to the Court." While Schwartz wrote the bogus legal filings, LoDuca didn't check them for accuracy.

"Mr. LoDuca simply relied on a belief that work produced by Mr. Schwartz, a colleague of more than twenty-five years, would be reliable," Castel wrote. But Schwartz's practice was exclusively in state court. The lawyers admitted in a memorandum of law that Schwartz attempted "to research a federal bankruptcy issue with which he was completely unfamiliar."

At a June 8 hearing on potential sanctions, Schwartz testified that he was "operating under the false perception that this website [ChatGPT] could not possibly be fabricating cases on its own." Schwartz stated, "I just was not thinking that the case could be fabricated, so I was not looking at it from that point of view... My reaction was, ChatGPT is finding that case somewhere. Maybe it's unpublished. Maybe it was appealed. Maybe access is difficult to get. I just never thought it could be made up."

The Levidow firm did not have Westlaw or LexisNexis accounts, instead using a Fastcase account that had limited access to federal cases. Schwartz testified that he "heard about this new site which I assumed—I falsely assumed was like a super search engine called ChatGPT, and that's what I used."