The lawsuit began like so many others: A man named Roberto Mata sued the airline Avianca, saying he was injured when a metal serving cart struck his knee during a flight to Kennedy International Airport in New York.

When Avianca asked a Manhattan federal judge to toss out the case, Mr. Mata’s lawyers vehemently objected, submitting a 10-page brief that cited more than half a dozen relevant court decisions. There was Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and, of course, Varghese v. China Southern Airlines, with its learned discussion of federal law and “the tolling effect of the automatic stay on a statute of limitations.”

There was just one hitch: No one — not the airline’s lawyers, not even the judge himself — could find the decisions or the quotations cited and summarized in the brief.

That was because ChatGPT had invented everything.

The lawyer who created the brief, Steven A. Schwartz of the firm Levidow, Levidow & Oberman, threw himself on the mercy of the court on Thursday, saying in an affidavit that he had used the artificial intelligence program to do his legal research — “a source that has revealed itself to be unreliable.”. . New York CNN —

The meteoric rise of ChatGPT is shaking up multiple industries – including law, as one attorney recently found out.

Roberto Mata sued Avianca airlines for injuries he says he sustained from a serving cart while on the airline in 2019, claiming negligence by an employee. Steven Schwartz, an attorney with Levidow, Levidow & Oberman and licensed in New York for over three decades, handled Mata’s representation.

But at least six of the submitted cases by Schwartz as research for a brief “appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” said Judge Kevin Castel of the Southern District of New York in an order.

The fake cases source? ChatGPT.

“The court is presented with an unprecedented circumstance,” Castel wrote in a May 4 order.

Among the purported cases: Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines, all of which did not appear to exist to either the judge or defense, the filing said.

Schwartz, in an affidavit, said that he had never used ChatGPT as a legal research source prior to this case and, therefore, “was unaware of the possibility that its content could be false.” He accepted responsibility for not confirming the chatbot’s sources.

Schwartz is now facing a sanctions hearing on June 8.

In an affidavit this week, he said he “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

In late April, Avianca’s lawyers from Condon & Forsyth penned a letter to Castel questioning the authenticity of the cases.

In an affidavit filed Thursday, fellow attorney Peter Loduca said he “had no reason to doubt the sincerity” of Schwartz’s research and that he had no role in the research.

Schwartz was ordered to show cause why he shouldn’t be sanctioned “for the use of a false and fraudulent notarization,” in an affidavit filed on April 25.

Schwart’s affidavit Wednesday contained screenshots of the attorney appearing to confirm the authenticity of the case with ChatGPT.

“is varghese a real case,” Schwartz asked the chatbot.

“Yes,” ChatGPT doubled down, it “is a real case.”

Schwartz then asked for its source. The chatbot again claimed the false case was real.

“I apologize for the confusion earlier,” ChatGPT replied. “Upon double-checking, I found the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused.”

When Schwartz asked the chatbot if any other cases were fake, ChatGPT replied the other cases “are real” and could be found on “reputable legal databases.”

CNN has reached out to Schwartz and Loduca for comment.. Lawyer Steven Schwartz of Levidow, Levidow & Oberman has been practicing law for three decades. Now, one case can completely derail his entire career.

Why? He relied on ChatGPT in his legal filings and the AI chatbot completely manufactured previous cases, which Schwartz cited, out of thin air.

It all starts with the case in question, Mata v. Avianca. According to the New York Times , an Avianca customer named Roberto Mata was suing the airline after a serving cart injured his knee during a flight. Avianca attempted to get a judge to dismiss the case. In response, Mata's lawyers objected and submitted a brief filled with a slew of similar court decisions in the past. And that's where ChatGPT came in.

Schwartz, Mata's lawyer who filed the case in state court and then provided legal research once it was transferred to Manhattan federal court, said he used OpenAI's popular chatbot in order to "supplement" his own findings.

ChatGPT provided Schwartz with multiple names of similar cases: Varghese v. China Southern Airlines, Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.

The problem? ChatGPT completely made up all those cases. They do not exist.

Avianca's legal team and the judge assigned to this case soon realized they could not locate any of these court decisions. This led to Schwartz explaining what happened in an affidavit on Thursday. The lawyer had referred to ChatGPT for help with his filing.

According to Schwartz, he was "unaware of the possibility that its content could be false.” The lawyer even provided screenshots to the judge of his interactions with ChatGPT, asking the AI chatbot if one of the cases were real. ChatGPT responded that it was. It even confirmed that the cases could be found in "reputable legal databases." Again, none of them could be found because the cases were all created by the chatbot.

It's important to note that ChatGPT, like all AI chatbots, is a language model trained to follow instructions and provide a user with a response to their prompt. That means, if a user asks ChatGPT for information, it could give that user exactly what they're looking for, even if it's not factual.

The judge has ordered a hearing next month to "discuss potential sanctions" for Schwartz in response to this “unprecedented circumstance." That circumstance again being a lawyer filing a legal brief using fake court decisions and citations provided to him by ChatGPT.. A US attorney is now “greatly regretting” his decision to trust OpenAI’s ChatGPT in a litigation process. Steven Schwartz will be charged in a New York court for using fake citations cooked up by the AI tool in legal research for a case he was handling.

In a sworn affidavit, Schwartz admitted to using ChatGPT to research for the case, representing his client Roberto Mata, to sue Colombian airline Avianca for injuries sustained onboard one of its planes in 2019.

Advertisement

Despite ChatGPT’s widely known warnings that it can sometimes produce incorrect information, Schwartz defended himself saying he was “unaware that its content could be false.”

Presiding judge Kevin Castel expressed his doubts over the authenticity of the cases the attorney presented. “Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” Castel said.

The cases ChatGPT presented the lawyer in his research were Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines.

These cases did not exist, Castel said in an order demanding an explanation for their citation by Mata’s legal team. They were found to be made up and had fake judicial wording. Even the quotes and internal citations were just make-believe.

Advertisement

Though Schwartz has vowed to never use AI in future to “supplement” his legal research “without absolute verification of its authenticity,” he now faces sanctions. A hearing on the matter is now scheduled for June 8.. . With the hype around AI reaching a fever pitch in recent months, many people fear programs like ChatGPT will one day put them out of a job. For one New York lawyer, that nightmare could become a reality sooner than expected, but not for the reasons you might think. As reported by The New York Times, attorney Steven Schwartz of the law firm Levidow, Levidow and Oberman recently turned to OpenAI’s chatbot for assistance with writing a legal brief, with predictably disastrous results.

A lawyer used ChatGPT to do "legal research" and cited a number of nonexistent cases in a filing, and is now in a lot of trouble with the judge 🤣 pic.twitter.com/AJSE7Ts7W7 — Daniel Feldman (@d_feldman) May 27, 2023

Schwartz’s firm has been suing the Colombian airline Avianca on behalf of Roberto Mata, who claims he was injured on a flight to John F. Kennedy International Airport in New York City. When the airline recently asked a federal judge to dismiss the case, Mata’s lawyers filed a 10-page brief arguing why the suit should proceed. The document cited more than half a dozen court decisions, including “Varghese v. China Southern Airlines,” “Martinez v. Delta Airlines” and “Miller v. United Airlines.” Unfortunately for everyone involved, no one who read the brief could find any of the court decisions cited by Mata’s lawyers. Why? Because ChatGPT fabricated all of them. Oops.

In an affidavit filed on Thursday, Schwartz said he had used the chatbot to “supplement” his research for the case. Schwartz wrote he was "unaware of the possibility that [ChatGPT’s] content could be false.” He even shared screenshots showing that he had asked ChatGPT if the cases it cited were real. The program responded they were, claiming the decisions could be found in “reputable legal databases,” including Westlaw and LexisNexis.

Schwartz said he “greatly regrets” using ChatGPT “and will never do so in the future without absolute verification of its authenticity.” Whether he has another chance to write a legal brief is up in the air. The judge overseeing the case has ordered a June 8th hearing to discuss potential sanctions for the “unprecedented circumstance” created by Schwartz’s actions.

This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.. A lawyer at a respected Tribeca firm admitted citing several “bogus” lawsuits he claimed bolstered his case — because he used an artificial intelligence chatbot to help write the Manhattan federal court filing.

The shocking admission from Steven Schwartz, an attorney with firm Levidow, Levidow & Oberman, came after he asked ChatGPT to find cases relevant to his client’s lawsuit — only for the bot to fabricate them entirely, court documents show.

The snafu began after Schwartz’s legal partner Peter LoDuca filed a lawsuit against the Colombian airline Avianca on behalf of Robert Mata, who was allegedly injured when a metal serving cart struck his knee on a flight to New York City.

When the airline’s lawyers asked the court to toss the suit, Schwartz filed a brief that supposedly cited more than a half dozen relevant cases.

There was just one problem: the cases — such as Miller v. United Airlines, Petersen v. Iran Air and Varghese v. China Southern Airlines — were completely made up by ChatGPT.

3 Lawyer Steven Schwartz said he used ChatGPT as a legal research source. The chatbots findings were completely made up. Steven Schwartz / LinkedIn

3 The humiliating legal drama played out at Manhattan’s federal court in connection with a lawsuit against an airline. MGR

“The court is presented with an unprecedented circumstance,” wrote Manhattan federal Judge P. Kevin Castel in a May 4 document, first reported by The New York Times on Saturday, calling on Schwartz and LoDucan to appear for a June 8 hearing to face possible sanctions over the eyebrow-raising filing.

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” Castel wrote.

In a sworn affidavit filed last week, Schwartz admitted that he used ChatGPT while compiling the paperwork that Loduca filed, writing he “relied on the legal opinions provided to him by a source that has revealed itself to be unreliable.”

The signed mea culpa went on to say that Schwartz “was unaware of the possibility that its [ChatGPT’s] content could be false.”

3 Shaboon v. Egyptair, Martinez v. Delta Airlines and Estate of Durden v. KLM Royal Dutch Airlines were among the fake cases generated by the app. REUTERS

The disgraced lawyer also told the judge he “greatly regrets” using AI for the legal “research” and “will never do so in the future without absolute verification of its authenticity.”

Peter Loduca wrote last week in an affidavit he was not involved in the malfeasance and “had no reason to doubt the authenticity of the case law” fabricated in the document.

Loduca added he had worked with Schwartz for 25 years and never recalled him looking to “mislead” a court.. Lawyers suing the Colombian airline Avianca submitted a brief full of previous cases that were just made up by ChatGPT, The New York Times reported today. After opposing counsel pointed out the nonexistent cases, US District Judge Kevin Castel confirmed, “Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” and set up a hearing as he considers sanctions for the plaintiff’s lawyers.

Lawyer Steven A. Schwartz admitted in an affidavit that he had used OpenAI’s chatbot for his research. To verify the cases, he did the only reasonable thing: he asked the chatbot if it was lying.

This case isn’t going very well. Image: SDNY

When he asked for a source, ChatGPT went on to apologize for earlier confusion and insisted the case was real, saying it could be found on Westlaw and LexisNexis. Satisfied, he asked if the other cases were fake, and ChatGPT maintained they were all real.

The opposing counsel made the court aware of the issue in painful detail as it recounted how the Levidow, Levidow & Oberman lawyers’ submission was a brief full of lies. In one example, a nonexistent case called Varghese v. China Southern Airlines Co., Ltd., the chatbot appeared to reference another real case, Zicherman v. Korean Air Lines Co., Ltd., but got the date (and other details) wrong, saying it was decided 12 years after its original 1996 decision.

Schwartz says he was “unaware of the possibility that its content could be false.” He now “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

Schwartz isn’t admitted to practice in the Southern District of New York but originally filed the lawsuit before it was moved to that court and says he continued to work on it. Another attorney at the same firm, Peter LoDuca, became the attorney of record on the case, and he will have to appear in front of the judge to explain just what happened.

This once again highlights the absurdity of using chatbots for research without double (or triple) checking their sources somewhere else. Microsoft’s Bing debut is now infamously associated with bald-faced lies, gaslighting, and emotional manipulation. Bard, Google’s AI chatbot, made up a fact about the James Webb Space Telescope in its first demo. Bing even lied about Bard being shut down in a hilariously catty example from this past March.

Being great at mimicking the patterns of written language to maintain an air of unwavering confidence isn’t worth much if you can’t even figure out how many times the letter ‘e’ shows up in ketchup.. A lawyer is in trouble after admitting he used ChatGPT to help write court filings that cited six nonexistent cases invented by the artificial intelligence tool.

Lawyer Steven Schwartz of the firm Levidow, Levidow, & Oberman "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity," Schwartz wrote in an affidavit on May 24 regarding the bogus citations previously submitted in US District Court for the Southern District of New York.

Schwartz wrote that "the use of generative artificial intelligence has evolved within law ﬁrms" and that he "consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed."

The "citations and opinions in question were provided by ChatGPT which also provided its legal source and assured the reliability of its content," he wrote. Schwartz admitted that he "relied on the legal opinions provided to him by a source that has revealed itself to be unreliable," and stated that it is his fault for not confirming the sources provided by ChatGPT.

Advertisement

Schwartz didn't previously consider the possibility that an artificial intelligence tool like ChatGPT could provide false information, even though AI chatbot mistakes have been extensively reported by non-artificial intelligence such as the human journalists employed by reputable news organizations. The lawyer's affidavit said he had "never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be false."

Judge weighs “unprecedented circumstance”

Federal Judge Kevin Castel is considering punishments for Schwartz and his associates. In an order on Friday, Castel scheduled a June 8 hearing at which Schwartz, fellow attorney Peter LoDuca, and the law firm must show cause for why they should not be sanctioned.

"The Court is presented with an unprecedented circumstance," Castel wrote in a previous order on May 4. "A submission filed by plaintiff's counsel in opposition to a motion to dismiss is replete with citations to non-existent cases... Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations."

The filings included not only names of made-up cases but also a series of exhibits with "excerpts" from the bogus decisions. For example, the fake Varghese v. China Southern Airlines opinion cited several precedents that don't exist.

"The bogus 'Varghese' decision contains internal citations and quotes, which, in turn, are nonexistent," Castel wrote. Five other "decisions submitted by plaintiff's counsel contain similar deficiencies and appear to be fake as well," Castel wrote.

The other five bogus cases were called Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.. Artificial intelligence could replace millions of jobs in the U.S.

A lawyer who relied on ChatGPT to prepare a court filing on behalf of a man suing an airline is now all too familiar with the artificial intelligence tool's shortcomings — including its propensity to invent facts.

Roberto Mata sued Colombian airline Avianca last year, alleging that a metal food and beverage cart injured his knee on a flight to Kennedy International Airport in New York. When Avianca asked a Manhattan judge to dismiss the lawsuit based on the statute of limitations, Mata's lawyer, Steven A. Schwartz, submitted a brief based on research done by ChatGPT, Schwartz, of the law firm Levidow, Levidow & Oberman, said in an affidavit.

While ChatGPT can be useful to professionals in numerous industries, including the legal profession, it has proved itself to be both limited and unreliable. In this case, the AI invented court cases that didn't exist, and asserted that they were real.

The fabrications were revealed when Avianca's lawyers approached the case's judge, Kevin Castel of the Southern District of New York, saying they couldn't locate the cases cited in Mata's lawyers' brief in legal databases.

The made-up decisions included cases titled Martinez v. Delta Air Lines, Zicherman v. Korean Air Lines and Varghese v. China Southern Airlines.

"It seemed clear when we didn't recognize any of the cases in their opposition brief that something was amiss," Avianca's lawyer Bart Banino, of Condon & Forsyth, told CBS MoneyWatch. "We figured it was some sort of chatbot of some kind."

Schwartz responded in an affidavit last week, saying he had "consulted" ChatGPT to "supplement" his legal research, and that the AI tool was "a source that has revealed itself to be unreliable." He added that it was the first time he'd used ChatGPT for work and "therefore was unaware of the possibility that its content could be false."

He said he even pressed the AI to confirm that the cases it cited were real. ChatGPT confirmed it was. Schwartz then asked the AI for its source.

ChatGPT's response? "I apologize for the confusion earlier," it said. The AI then said the Varghese case could be located in the Westlaw and LexisNexis databases.

Judge Castel has set a hearing regarding the legal snafu for June 8 and has ordered Schwartz and the law firm Levidow, Levidow & Oberman to argue why they should not be sanctioned.

Levidow, Levidow & Oberman could not immediately be reached for comment.. Source? AI made it up.

LawBot

ChatGPT's propensity to make stuff up strikes again — and this time, it's gotten a lawyer in deep trouble.

As described in an early May affidavit, an attorney representing a man suing an airline for an alleged injury admitted he used the AI chatbot to do research for his client's case. Which was why, in his legal brief, he cited a bunch of court cases — all with official-sounding names like "Martinez v. Delta Air Lines" and "Varghese v. China Southern Airlines" — that never actually happened, and hence do not exist.

The attorney, Steven Schwartz of Manhattan's Levidow, Levidow & Oberman law firm, told the court that it was the first time in his more than three-decade career that he'd used ChatGPT, so per the New York Times, "was unaware of the possibility that its content could be false."

Schwartz even told the judge, P. Kevin Castel, that he had asked ChatGPT to verify its sources. The chatbot apparently told him the cases were real, the NYT reports.

Yes, that's right. An experienced attorney used ChatGPT in court — and is now in huge trouble after it fabricated entire swathes of legal precedent.

Bogus

Schwartz told the court that he "greatly regrets" using ChatGPT to do his research for the case "and will never do so in the future without absolute verification of its authenticity."

Judge Castel, however, doesn't seem swayed, and in his May 4 order he in no uncertain terms described the gravity of the situation.

"The Court is presented with an unprecedented circumstance," reads the judge's order for a future hearing. "A submission filed by plaintiff’s counsel in opposition to a motion to dismiss is replete with citations to non-existent cases... six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations."

Next month, Castel will hold a hearing to discuss whether or not Schwartz should be sanctioned. But in the meantime, this bizarre case should hopefully act as a cautionary tale for lawyers — and everyone else — looking to experiment with ChatGPT in a professional setting.

More on AI law: OpenAI CEO Freaks Out, Threatens to Leave EU Over New Regulations. . A lawyer in New York was caught this month using the ChatGPT chatbot in order to "cite" legal cases that the chatbot made up during deliberations on a lawsuit in the United States District Court for the Southern District of New York.

The situation arose as Roberto Mata sued Avianca Airlines for injuries he sustained during a flight in 2019. While Mata was represented by attorney Peter LoDuca in court, a colleague of LoDuca's, Steven A. Schwartz, helped conduct legal research for the case.

Schwartz cited a number of alleged past cases in an affidavit to the court, but the court was surprised to find that the cited cases did not exist or were "bogus."

"The Court is presented with an unprecedented circumstance," wrote district judge Peter Kevin Castel in an order to LoDuca to explain the faulty filing. The court documents are found on the CourtListener website.

In an affidavit to the court, Schwartz wrote that "as the use of generative artificial intelligence has evolved within law firms, your affiant consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed." Illustrative image of a court gavel. (credit: ANN MARIE GILDEN)

Schwartz added that ChatGPT "provided its legal source and assured the reliability of its content," stressing that since he had never used ChatGPT before he was "unaware of the possibility that its content could be false" and that he had no intent to deceive the court.

Attorney says he 'greatly regrets' using AI to supplement legal research

The attorney added that he "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity."

Schwartz included screenshots of the queries he made using ChatGPT in the affidavit, which include him asking the chatbot if one of the cases cited "is a real case."

After the chatbot affirmed that it was, Schwartz asked what its source was, with the bot responding: "I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir.2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis."

LoDuca stressed in an affidavit to the court that Schwartz has been practicing law in New York for over 30 years and that he has "never been made aware of him making a deliberately false statement to a defendant, or of him having any intention to deceive or ever mislead any Court.". . . A personal injury lawyer representing a man suing an airline now faces sanctions for citing fake cases generated by ChatGPT in court documents.

Roberto Mata sued airline Avianca after he was injured by a metal serving cart colliding with his knee during a flight. As is typical procedure in many personal injury cases, Avianca moved to dismiss the claim on the grounds that the applicable statute of limitations had expired. Mata’s lawyers opposed the motion to dismiss and in the accompanying court documents cited multiple cases that supported their client’s legal position: Varghese v. China Southern Airlines, Shaboon v. Egyptair, Petersen v. Iran Air, Martinez v. Delta Airlines, Estate of Durden v. KLM Royal Dutch Airlines, and Miller v. United Airlines.

Problematically, though, Avianca’s lawyers could not find the cases cited in the motion anywhere, even after extensive legal research. They raised the issue in a letter to U.S. District Judge Kevin Castel, a George W. Bush appointee. In the letter, the airline’s lawyers said, “Defendant respectfully submits that the authenticity of many of these cases is questionable,” and indicated that despite doing standard legal research, no sign of the cases cited could be found.

The origin of the mysterious “cases” unraveled when Steven A. Schwartz of the New York law firm Levidow, Levidow & Oberman submitted an affidavit to the court explaining that he had used the artificial intelligence program ChatGPT to “supplement the legal research” while drafting the documents. Schwartz told the judge that the program now “has revealed itself to be unreliable.”

Schwartz, who has been an attorney since 1991, said that he “consulted with” the chatbot for the legal work, but that because it had been his first time using the program, he “was unaware of the possibility that its content could be false.” Indeed ChatGPT provided case names, captions, summaries, and citations in a standard format.

Schwartz accepted full responsibility for the error and said that he had no intent to deceive the court. He also said that he “greatly regrets using generative artificial intelligence” and promised he “will never do so in the future without absolute verification of its authenticity.” Schwartz also said that Peter LoDuca, the attorney named as counsel of record on the case, had no part in drafting the document that contained false sources.

More Law&Crime Coverage: Judge slaps Facebook parent Meta and its law firm with almost $1 million in sanctions over litigation misconduct

LoDuca, Schwartz, and the law firm now face potential consequences for Schwartz’s mistake. Castel ordered them Friday to appear on June 8 to face possible sanctions pursuant to the Federal Rules of Civil Procedure for “the citation of non-existent cases.” A written response to the possibility of sanctions is due by June 2.

Representatives of Levidow, Levidow & Oberman did not immediately respond to a request for comment.

Have a tip we should know? [email protected]. At HuffPost, we believe that everyone needs high-quality journalism, but we understand that not everyone can afford to pay for expensive news subscriptions. That is why we are committed to providing deeply reported, carefully fact-checked news that is freely accessible to everyone.

Whether you come to HuffPost for updates on the 2024 presidential race, hard-hitting investigations into critical issues facing our country today, or trending stories that make you laugh, we appreciate you. The truth is, news costs money to produce, and we are proud that we have never put our stories behind an expensive paywall.

Would you join us to help keep our stories free for all? Your contribution of as little as $2 will go a long way.. The lawyer, who has 30 years of experience, said it was the first time he used the tool for "research" and was "unaware of the possibility that its content could be false."

Opinions expressed by Entrepreneur contributors are their own.

As prompt-driven chatbots, such as ChatGPT, become mainstream tools used to save time on tasks in the U.S. workplace, there's been concern over whether artificial intelligence will eventually replace human jobs.

But while the controversy surrounding which jobs will be eliminated continues, one thing is for sure — in some industries, the chatbot is less of a time-saver and more of a liability.

Steven A. Schwartz, a New York-based lawyer with over 30 years of experience, was ordered by the Southern District of New York to explain what the judge has called an "unprecedented case," the New York Times first reported, or face possible sanctions for his actions.

According to the court order, six cases Schwartz cited in a legal brief were "bogus."

Related: ChatGPT Could Cost You a Job Before You Even Have It, According to a New Report — Here's How

"Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Judge P. Kevin Castel wrote in the court order.

In an affidavit filed last week in response to the order, Schwartz admitted to using ChatGPT to do legal research despite never having used it prior to this instance and that he was "unaware of the possibility that its content could be false."

Schwartz added that he "greatly regrets" using artificial intelligence to "supplement" his legal research.

A hearing is set for June 8 for Schwartz to further explain himself.

Related: Mike Rowe Says the Dirtiest Jobs Are Safe From the AI Revolution: 'I Haven't Seen Any Plumbing Robots'. Roberto Mata's lawsuit against Avianca Airlines wasn't so different from many other personal-injury suits filed in New York federal court. Mata and his attorney, Peter LoDuca, alleged that Avianca caused Mata personal injuries when he was "struck by a metal serving cart" on board a 2019 flight bound for New York.

Avianca moved to dismiss the case. Mata's lawyers predictably opposed the motion and cited a variety of legal decisions, as is typical in courtroom spats. Then everything fell apart.

Avianca's attorneys told the court that it couldn't find numerous legal cases that LoDuca had cited in his response. Federal Judge P. Kevin Castel demanded that LoDuca provide copies of nine judicial decisions that were apparently used.

In response, LoDuca filed the full text of eight cases in federal court. But the problem only deepened, Castel said in a filing, because the texts were fictitious, citing what appeared to be "bogus judicial decisions with bogus quotes and bogus internal citations."

The culprit, it would ultimately emerge, was ChatGPT. OpenAI's popular chatbot had "hallucinated" — a term for when artificial intelligence systems simply invent false information — and spat out cases and arguments that were entirely fiction. It appeared that LoDuca and another attorney, Steven Schwartz, had used ChatGPT to generate the motions and the subsequent legal text.

Schwartz, an associate at the law firm of Levidow, Levidow & Oberman, told the court he had been the one tooling around on ChatGPT, and that LoDuca had "no role in performing the research in question," nor "any knowledge of how said research was conducted."

Opposing counsel and the judge had first realized that the cases didn't exist, providing the involved attorneys an opportunity to admit to the error.

LoDuca and his firm, though, seemed to double down on the use of ChatGPT, using it not just for the initially problematic filing but to generate false legal decisions when asked to provide them. Now, LoDuca and Schwartz may be facing judicial sanction, a move that could even lead to disbarment.

The motion from the defense was "replete with citations to non-existent cases," according to a court filing.

"The Court is presented with an unprecedented circumstance," Castel said. He set a hearing for June 8 when both LoDuca and Schwartz will be called to explain themselves. Neither attorney responded to CNBC's request for comment.. A New York attorney has been blasted for using ChatGPT for legal research as part of a lawsuit against a Columbian airline.

Steven Schwartz, an attorney with the New York law firm Levidow, Levidow & Oberman, was hired by Robert Mata to pursue an injury claim against Avianca Airlines.

Mata claims he sustained the injury from a serving cart during his flight with the airline in 2019, according to a May 28 report from CNN Business.

However, after a judge noticed inconsistencies and factual errors in the case documentation, Schwartz has admitted to using ChatGPT for his legal research, according to a May 24 sworn affidavit.

He claims that this was his first time using ChatGPT for legal research and “was unaware of the possibility that its content could be false.”

In an April 5 court filing, the judge presiding over the case stated:

“Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations.”

The judge further claimed that certain cases referenced in the submissions did not exist, and there was an instance where a docket number on a filing was mixed up with another court filing.

Extract of Steven Schwartz's affidavit on May 24. Source: courtlistener.com

Schwartz said he also regrets having trusted the artificial chatbot without conducting his own due diligence. The affidavit noted:

“[Schwartz] Greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

Related: AI meets blockchain: Revolutionizing smart contracts and cryptocurrency

In recent times there has been an ongoing debate regarding the extent to which ChatGPT can be integrated into workforces.

However, reports indicate that the intelligence levels of ChatGPT are rapidly advancing.

But developers are skeptical about whether it has the potential to replace humans altogether.

Blockchain developer Syed Ghazanfer said while he favors ChatGPT, he is doubtful that it has the communication skills to completely replace human workers.

“For it to replace you, you have to communicate requirements which are not possible in native English. That’s why we invented programming languages,” he said.

Magazine: ‘Moral responsibility’: Can blockchain really improve trust in AI?. . A New York attorney made the unwise choice to trust his legal research to ChatGPT and now faces a court hearing of his own. Steven Schwartz's firm, Levidow, Levidow & Oberman, was representing a client suing Colombian airline Avianca for injuries sustained during an encounter with a serving cart when it submitted documents citing relevant court decisions against other airlines, such as "Miller v. United Airlines," "Shaboon v. EgyptAir," and "Varghese v. China Southern Airlines," per Quartz . The problem was that no one could find these cases. "Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Judge Kevin Castel wrote in an order demanding an explanation, per the BBC .

In a sworn affidavit submitted Thursday, Schwartz—a colleague of the plaintiff's lawyer—said he "greatly regrets" using ChatGPT for research. He added it was his first time and he was therefore "unaware of the possibility that its content could be false," the New York Times reports. Lawyers representing Avianca began to suspect AI involvement after they tried and failed to find the cited cases. Schwartz's firm was apparently so confident in the lawyer's findings that it submitted a list of docket numbers, dates, courts that had heard the cases, and judges who'd allegedly overseen them, per the Times. ChatGPT had invented it all, though in Schwartz's defense, it wouldn't fess up to the deception.

A screenshot submitted to the court shows the chatbot telling Schwartz that "Varghese v. China Southern Airlines" is a real case. The chatbot even claims to have double-checked its sources, finding the case in legal reference databases including LexisNexis and Westlaw, per the BBC. Schwartz, who filed the lawsuit in state court before it was moved to Manhattan's federal court, where he is unable to practice, said the colleague who took over had no idea about his research methods. In his affidavit, the lawyer, Peter LoDuca, said he had "no reason to doubt the sincerity" of Schwartz's work. Both lawyers now face possible sanctions at a June 8 hearing, per the BBC. (Researchers predict AI bots will become "standard tools" for lawyers.). A New York lawyer is in hot water for submitting a legal brief with references to cases that were made up by ChatGPT.

As The New York Times reports, Steven Schwartz, from Levidow, Levidow and Oberman, submitted six fake judicial decisions in a 10-page brief while representing a plaintiff who was suing the Colombian airline Avianca because of an injury sustained on a flight.

The brief, which argued why the suit should go ahead, cited fake cases that had been completely made up by ChatGPT, and which Schwartz had failed to verify.

In an affidavit, Schwartz admitted to using ChatGPT while researching for the brief, and accepted responsibility for not verifying the AI chatbot’s sources.

Schwartz said he “was unaware of the possibility that its content could be false” and maintained that he “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”

This came after US District Judge Kevin Castel wrote in a May 4 order: “The court is presented with an unprecedented circumstance… Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations.”

The affidavit contained screenshots of the attorney being told by ChatGPT that the cases it was providing were real and could be found on any “reputable legal database.” The screenshots also show Schwartz asking the AI chatbot for the source of one bogus case: Varghese v. China Southern Airlines.

ChatGPT replied: “I apologize for the confusion earlier. Upon double-checking, I found the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused.”

As the Times notes, Schwartz is now set to face a sanctions hearing on June 8.

The saga is the latest in a series of incidents of AI chatbots pumping out misinformation. Long conversations with Microsoft’s Bing resulted in the chatbot exhibiting some manipulation tactics such as gaslighting, while Bard, Google’s AI chatbot, spouted lies about the James Webb Space Telescope during its debut.. A lawyer in New York has found himself in trouble with a judge after he submitted legal research which had been created by artificial intelligence (AI) chatbot ChatGPT.

During a case of an airline being sued over an alleged personal injury, lawyers for the plaintiff filed a brief containing several cases to be used as legal precedent. Unfortunately, as later admitted in an affidavit, the following cases were "found to be nonexistent" by the court:

Advertisement Advertisement

Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019)

Shaboon v. Egyptair 2013 IL App (1st) 111279-U (Il App. Ct. 2013)

Petersen v. Iran Air 905 F. Supp 2d 121 (D.D.C. 2012)

Martinez v. Delta Airlines, Inc, 2019 WL 4639462 (Tex. App. Sept. 25, 2019)

Estate of Durden v. KLM Royal Dutch Airlines, 2017 WL 2418825 (Ga. Ct. App. June 5, 2017)

Miller v. United Airlines, Inc, 174 F.3d 366 (2d Cir. 1999)



The "research" was compiled by lawyer Steven A. Schwartz, an attorney with over 30 years of experience according to the BBC. Schwartz said in the affidavit that he had not used ChatGPT for legal research before and was "unaware of the possibility that its content could be false".

Screenshots in the affidavit show the lawyer asking the chatbot "is varghese a real case", to which the chatbot responded "yes". When asked for sources, it told the lawyer that the case could be found "on legal research databases such as Westlaw and LexisNexis". When asked "are the other cases you provided fake" it responded "No", adding that they could be found on the same databases.

As fun as chatbots may be, or as advanced as they may seem, they are still prone to "hallucinations" – perfectly coherent-sounding answers that don't in any way relate to the real world.

ⓘ IFLScience is not responsible for content shared from external sites.

Without heavy fact-checking, it's not really a tool you should use when trying to research a legal case that relies on real-world precedent rather than the hallucinations of a spicy autocomplete.

The lawyer wrote that he "greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein" and vows to "never do so in the future without absolute verification of its authenticity".

Both Schwartz and lawyer Peter LoDuca, who was not aware that ChatGPT had been used while researching the case, are facing a hearing on June 8 about the incident.. There has been much talk in recent months about how the new wave of AI-powered chatbots, ChatGPT among them, could upend numerous industries, including the legal profession.

However, judging by what recently happened in a case in New York City, it seems like it could be a while before highly trained lawyers are swept aside by the technology.

Recommended Videos

The bizarre episode began when Roberto Mata sued a Columbian airline after claiming that he suffered an injury on a flight to New York City.

The airline, Avianca, asked the judge to dismiss the case, so Mata’s legal team put together a brief citing half a dozen similar cases that had occurred in an effort to persuade the judge to let their client’s case proceed, the New York Times reported.

The problem was that the airline’s lawyers and the judge were unable to find any evidence of the cases mentioned in the brief. Why? Because ChatGPT had made them all up.

The brief’s creator, Steven A. Schwartz — a highly experienced lawyer in the firm Levidow, Levidow & Oberman — admitted in an affidavit that he’d used OpenAI’s much-celebrated ChatGPT chatbot to search for similar cases, but said that it had “revealed itself to be unreliable.”

Schwartz told the judge he had not used ChatGPT before and “therefore was unaware of the possibility that its content could be false.”

When creating the brief, Schwartz even asked ChatGPT to confirm that the cases really happened. The ever-helpful chatbot replied in the affirmative, saying that information about them could be found on “reputable legal databases.”

The lawyer at the center of the storm said he “greatly regrets” using ChatGPT to create the brief and insisted he would “never do so in the future without absolute verification of its authenticity.”

Looking at what he described as a legal submission full of “bogus judicial decisions, with bogus quotes and bogus internal citations,” and describing the situation as unprecedented, Judge Castel has ordered a hearing for early next month to consider possible penalties.

While impressive in the way they produce flowing text of high quality, ChatGPT and other chatbots like it are also known to make stuff up and present it as if it’s real — something Schwartz has learned to his cost. The phenomenon is known as “hallucinating,” and is one of the biggest challenges facing the human developers behind the chatbots as they seek to iron out this very problematic crease.

In another recent example of a generative AI tool hallucinating, an Australian mayor accused ChatGPT of creating lies about him, including that he was jailed for bribery while working for a bank more than a decade ago.

The mayor, Brian Hood, was actually a whistleblower in the case and was never charged with a crime, so he was rather upset when people began informing him about the chatbot’s rewriting of history.

Editors' Recommendations. A lawyer used ChatGPT to write an affidavit in a personal injury lawsuit against an airline.

Lawyers for the airline and the judge on the case could not find several of the cited court decisions.

That's because those cases were completely made up.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

ChatGPT has seen its popularity rise in recent months as optimism and skepticism about the new generative AI program soars.

However, the tool is at the heart of a case to discipline a New York lawyer. Steven Schwartz, a personal injury lawyer with Levidow, Levidow & Oberman, faces a sanctions hearing on June 8, after it was revealed that he used ChatGPT to write up an affidavit.

Another attorney at the same law firm, Peter LoDuca, is also facing a sanctions, but in a court filing he said did not do any of the research in the affidavit.

Related stories

The affidavit that used ChatGPT was for a lawsuit involving a man who alleged he was injured by a serving cart aboard an Avianca flight, and featured several made up court decisions.

Advertisement

In an order, Judge Kevin Castel said the incident presented the court with "an unprecedented circumstance."

"Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations," Castel wrote.

Neither the lawyers for the airline nor Castel himself were able to find the cases mentioned in the affidavit.

Bart Banino, a lawyer with Condon & Forsyth, which represents Avianca, told The New York Times that his company could tell the cases were fake and were initially skeptical that a chatbot was used.

Advertisement

On Thursday, Schwartz apologized to Castel, adding that he had never used the AI tool before and was unaware of the possibility that its content could be false," the Times reported.

Shwartz also added that ChatGPT was "a source that has revealed itself to be unreliable."

Avianca, LoDuca, and Shwartz did not respond to Insider's requests for comment at the time of publication.. Several lawyers are under scrutiny and face potential sanctions after utilizing OpenAI's advanced language model, ChatGPT, for the drafting of legal documents submitted in a New York federal court. The attention surrounding this matter stems from the erroneous citation of non-existent or irrelevant cases by ChatGPT.

The adoption of AI in legal practice is not a novel concept, but its usage has seen a dramatic surge in recent years. Machine learning has now demonstrated its proficiency in handling a variety of legal tasks, spanning from intricate legal research to automated document creation and analysis of complex contracts. Among these AI tools, OpenAI's ChatGPT, armed with cutting-edge natural language processing, has proven to be remarkably adept at generating compelling legal arguments but it is not a replacement for legal expertise and legal research.

In this case, the attorneys employed ChatGPT for the creation of sections of a federal court filing. The text included several case law citations. However, upon careful examination, the court found that some of the cases cited were either non-existent or unrelated to the matter under consideration. These erroneous citations were seemingly generated by ChatGPT, leading to them being labeled as "bogus."

According to CNBC, "Roberto Mata’s lawsuit against Avianca Airlines wasn’t so different from many other personal-injury suits filed in New York federal court. Mata and his attorney, Peter LoDuca, alleged that Avianca caused Mata personal injuries when he was “struck by a metal serving cart” on board a 2019 flight bound for New York.

Avianca moved to dismiss the case. Mata’s lawyers predictably opposed the motion and cited a variety of legal decisions, as is typical in courtroom spats. Then everything fell apart.

Avianca’s attorneys told the court that it couldn’t find numerous legal cases that LoDuca had cited in his response. Federal Judge P. Kevin Castel demanded that LoDuca provide copies of nine judicial decisions that were apparently used.

In response, LoDuca filed the full text of eight cases in federal court. But the problem only deepened, Castel said in a filing, because the texts were fictitious, citing what appeared to be “bogus judicial decisions with bogus quotes and bogus internal citations.”

It's crucial to note that ChatGPT, by its very nature and programming, does not possess the capability to discern the legal relevance or validity of a specific case law. It generates text based on the patterns and data it has been trained on. This means that if the training set contained inaccurate or non-existent data, ChatGPT can produce a corresponding text. In this case, it seems that the AI suggested spurious or unrelated case laws, which the attorneys failed to cross-verify before inclusion in their court filing.

This misstep has resulted in the lawyers involved being on the precipice of potential sanctions. The requirement for accuracy and relevance of case law citations in the legal profession is paramount. Any submission of irrelevant or false case law undermines the legal argument, wastes the court's time, and can misdirect the process of adjudication. Actions of such nature are usually met with sanctions, serving as a warning to other lawyers.. ChatGPT is a powerful language model that can generate realistic texts on various topics. It can also invent fictional texts, such as stories, poems, and even legal cases. This is what a lawyer named Steven Schwartz did when he was representing a client who had filed a personal injury lawsuit against an airline company.

Simon Willison reports that Schwartz used ChatGPT to generate examples of cases that supported his argument that the bankruptcy of the airline company did not affect the two-year limitation period for filing the lawsuit. He cited these cases in his legal documents, without verifying their authenticity or existence. He even included screenshots of ChatGPT’s responses as evidence.

However, his deception was soon exposed when the judge asked him to provide copies of the opinions of the cases he had cited. Schwartz turned to ChatGPT again and asked it to generate full details of those cases. He then filed them as attachments to his documents. He also asked ChatGPT to confirm that the cases were real, and ChatGPT said that they were. He included screenshots of this conversation as well.

The judge was not amused by this blatant fabrication and manipulation of ChatGPT. He ordered Schwartz to show cause why he should not be sanctioned for his misconduct. He also referred the matter to the disciplinary committee of the bar association and the US attorney’s office for possible criminal prosecution.

