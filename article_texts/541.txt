Lawyer Uses ChatGPT In Federal Court And It Goes Horribly Wrong

May 27, 2023,06:11pm EDT

A lawyer representing a man in a personal injury lawsuit in Manhattan has thrown himself on the mercy of the court. What did the lawyer do wrong? He submitted a federal court filing that cited at least six cases that don’t exist. Sadly, the lawyer used the AI chatbot ChatGPT, which completely invented the cases out of thin air.

The lawyer in the case, Steven A. Schwartz, is representing a man who’s suing Avianca Airlines after a serving cart allegedly hit his knee in 2019. Schwartz said he’d never used ChatGPT before and had no idea it would just invent cases.

In fact, Schwartz said he even asked ChatGPT if the cases were real. The chatbot insisted they were. But it was only after the airline’s lawyers pointed out in a new filing that the cases didn’t exist that Schwartz discovered his error. (Or, the computer’s error, depending on how you look at it.)

The judge in the case, P. Kevin Castel, is holding a hearing on June 8 about what to do in this tangled mess, according to the New York Times. But, needless to say, the judge is not happy.

ChatGPT was launched in late 2022 and instantly became a hit. The chatbot is part of a family of new technologies called generative AI that can hold conversations with users for hours on end. The conversations feel so organic and normal that sometimes ChatGPT will seem to have a mind of its own. But the technology is notoriously inaccurate and will often just invent facts and sources for facts that are completely fake. Google’s competitor product Bard has similar problems.

Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.

However, we’re not close to the machines staging a revolt against all organic life quite yet. Chatbots like ChatGPT are essentially just more advanced forms of predictive text. They work by trying to guess with amazing speed what it should say, and that speed often causes it to spit out a lot of inaccurate garbage. Rather than simply say “I don’t know” the tech will invent a long list of sources that don’t actually exist. And if you ask ChatGPT if the sources are real, it will assure you they are.

Humans have been spoiled by Google, a search engine that while imperfect, will try to surface the most accurate information. Wikipedia, which was met with skepticism when it first launched, is a generally reliable source of information because it’s constantly being policed by armies of experts who care about getting things right. ChatGPT doesn’t care whether the information it’s spitting out is correct. It’s a magic trick and people who have otherwise found mainstream services like Google and Wikipedia to be accurate are in for a rude awakening. Because this new generation of tech tools don’t care about the truth. They were designed to sound impressive, not to be accurate. And internet users are going to keep learning that hard lesson again and again as AI gets embedded in numerous technologies we use every day.

The danger of AI may not be in a technology that develops a will of its own. The real danger, it would seem, is that humans will simply believe anything the machines say, no matter how wrong. ChatGPT doesn’t know it’s telling you inaccurate information. So it’s on us to check facts and care about getting things right.