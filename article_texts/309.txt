. This kind of biometric technology (Facial Recognition Technology) has the potential to be a really useful crime fighting tool but we are not there yet. It needs to be properly tested and evaluated if it is going to be effective and it will need to be handled carefully by the police and the government if it is going to be trusted by the public.

The previous Biometrics Commissioner made similar points as have others, such as the Science and Technology Select Committee, and we have yet to see what the government proposes as their Biometrics Strategy has been delayed for some time.

Police forces need to work together to agree on a single facial recognition system that has been proved to work in the field and government needs to create a legislative framework for its use, with independent oversight to provide public assurance, as it has done for DNA and fingerprints.

The police already hold over 20 million facial images but there is as yet no single, shared policing system for storing and searching police held images nor an evaluation of its accuracy and usefulness.

Tests of facial matching for spotting individuals in large crowds have so far had very poor success – hence the Metropolitan Police’s trial. It is good that they have made their trial public but they must carry out a proper evaluation and publish the results.

There is a public benefit in the use of such technology if it can be shown to help prevent the problems that there have been at previous carnivals by assisting the police to catch offenders or prevent crime.

Police already hold over 20 million facial images on both the Police National Database and in separate force systems. Her Majesty’s Inspector of Constabulary (Scotland) recently commented unfavourably on this situation since it means that different standards are being applied across the UK. Most of these facial images are custody images. The courts held in 2012 that these holdings were unlawful and the Home Office responded to that judgment in 2017. I have commented on this response elsewhere.

Current police interest in facial matching has moved on from custody images to whether it can be used to identify individual offenders in public places. The capability to do this is still unproven since tests in such situations have shown very poor match rates unlike match rates in controlled environments.[1]

The police are conducting a number of trials to see if facial searching and matching technology can be employed effectively in crowded public places. Such experiments should be properly designed and evaluated, preferably involving external experts, and the results published. The police should also evaluate their use of facial images generally in order to demonstrate that they have a useful and cost-effective purpose, based on adequate matching quality. They also need to explain how they will deal with potential false matches.

There is limited research on this area and most of it has been conducted in the USA. Evaluations should include not just the behaviour of the matching algorithms but how they work in the total criminal justice system and how human decision making in such systems affects the accuracy of match rates.[2]

Facial matching systems have improved significantly recently and the use being explored by the Metropolitan Police may, at some point, reach acceptable quality for operational use but presently that remains to be demonstrated.. London cops' facial recognition kit has only correctly identified two people to date – neither of whom were criminals – and the UK capital's police force has made no arrests using it, figures published today revealed.

According to information released under Freedom of Information laws, the Metropolitan Police's automated facial recognition (AFR) technology has a 98 per cent false positive rate.

That figure is the highest of those given by UK police forces surveyed by the campaign group Big Brother Watch as part of a report that urges the the police to stop using the tech immediately.

Forces use facial recognition in two ways: one is after the fact, while cross-checking of images against mugshots held in national databases; the other involves real-time scanning of people's faces in a crowd to compare against a "watch list" that is freshly drawn up for each event.

Big Brother Watch's report focused on the latter, which it said breaches human rights laws as it surveils people without their knowledge and might dissuade them from attending public events.

London cops urged to scrap use of 'biased' facial recognition at Notting Hill Carnival READ MORE

And, despite cops' insistence that it works, the report showed an average false positive rate – where the system "identifies" someone not on the list – of 91 per cent across the country. That doesn't mean nine out of ten people seen on camera are wrongly flagged up, instead it means that 91 per cent of people flagged up turned out to be not on the watch list.

The Met has the highest percentage, at 98 per cent, with 35 false positives recorded in one day alone, at the Notting Hill Carnival 2017.

However, the Met Police claimed that this figure is misleading because there is human intervention after the system flags up the match.

"We do not not consider these as false positive matches because additional checks and balances are in place to confirm identification following system alerts," a spokesperson told The Register.

The system, though, hasn't had much success in positive identifications either: the report showed there have been just two accurate matches, and neither person was a criminal.

The first was at Notting Hill, but the person identified was no longer wanted for arrest because the information used to generate the watch list was out of date.

The second such identification took place during last year's Remembrance Sunday event, but this was someone known as a "fixated individual" – these are people known to frequently contact public figures – but who was not a criminal and not wanted for arrest.

Typically people on this list have mental health issues, and Big Brother Watch expressed concern that the police said there had not been prior consultation with mental health professionals about cross-matching against people in this database.

The group described this as a "chilling example of function creep" and an example of the dangerous effect it could have on the rights of marginalised people.

It also raised concerns about racial bias in the kit used, criticising the Met Police for saying it would not record ethnicity figures for the number of individuals identified, either correctly or not.

As a result, it said, "any demographic disproportionately in this hi-tech policing will remain unaccountable and hidden from public view".

This is compounded by the fact that the commercial software used by the Met – and also South Wales Police (SWP) – has yet to be tested for demographic accuracy biases.

"We have been extremely disappointed to encounter resistance from the police in England and Wales to the idea that such testing is important or necessary," Big Brother Watch said in the report.

SWP – which has used AFR at 18 public places since it was first introduced in May 2017 – has fared only slightly better. Its false positive rate is 91 per cent, and the matches led to 15 arrests – equivalent to 0.005 per cent of matches.

For example, the Welsh force's AFR that scanned Brits during the UEFA Champions League week in Cardiff in 2017 had poor results.

The SWP said that false positives were to be expected while the technology develops, but that the accuracy was improving, and added that no one had been arrested after a false match – again because of human intervention.

"Officers can quickly establish if the person has been correctly or incorrectly matched by traditional policing methods, either by looking at the person or through a brief conversation," a spokesperson said.

"If an incorrect match has been made, officers will explain to the individual what has happened and invite them to see the equipment along with providing them with a Fair Processing Notice.”

Underlying the concerns about the poor accuracy of the kit are complaints about a lack of clear oversight – an issue that has been raised by a number of activists, politicians and independent commissioners in related areas.

Government minister Susan Williams – who once described the use of AFR as an "operational" decision for the police – said earlier this year the government is to create a board comprised of the information, biometrics and surveillance camera commissioners to oversee the tech.

Further details are expected in the long-awaited biometrics strategy, which is slated to appear in June.

Big Brother Watch also reiterated its concerns about the mass storage of custody images of innocent people on the Police National Database, which has more than 12.5 million photos on it that can be scanned biometrically.

Despite a 2012 High Court ruling that said keeping images of presumed innocent people on file was unlawful, the government has said it isn't possible to automate removal. This means that they remain on the system unless a person asks for them to be removed.

In March, Williams said that because images can only be deleted manually, weeding out innocent people "will have significant costs and be difficult to justify given the off-setting reductions forces would be required to find to fund it".

The group had little patience with this, stating in the report the government should provide funding for administrative staff to deal with this problem – one person per force employed for a full year at £35,000 would be a total of £1.5m, it said.

"'Costs' are not an acceptable reason for the British Government not to comply with the law," it said. Big Brother Watch said that given the Home Office had forked out £2.6m to SWP for its AFR kit, they were also "hardly a convincing reason".

Big Brother Watch is launching its campaign against AFR today in Parliament. ®. The controversial trial of facial recognition equipment at Notting Hill Carnival resulted in roughly 35 false matches and an 'erroneous arrest', highlighting questions about police use of the technology.

The system only produced a single accurate match during the course of Carnival, but the individual had already been processed by the justice system between the time police compiled the suspect database and deployed it.

In the days before Carnival, Sky News revealed that police have more than 20 million facial recognition images on the British public, including hundreds of thousands on innocent people.

There are a number of legal questions surrounding the police's databases, especially following a High Court ruling in 2012 which said that the retention of those images was unlawful.

Image: Notting Hill Carnival attracts over a million visitors annually

Silkie Carlo, the technology policy officer for human rights group Liberty, observed the trial and described it as a "worryingly inaccurate and painfully crude facial recognition operation".

Ms Carlo explained that the 'erroneously arrested' individual was flagged up as being wanted on warrant for a rioting offence.

However, betweem the construction of the suspect database and Carnival, the individual had already been arrested and was no longer wanted.

"The project leads viewed this as a resounding success - not a failure," wrote Ms Carlo.

Independent sources have confirmed details in a blog post by Ms Carlo noting that five members of the public were flagged as suspects by the system and approached by officers to prove their identities.

Advertisement

The individuals had identification documents on them, but had they not they could potentially have been wrongfully arrested.

A spokesperson for the Metropolitan Police Service explained to Sky News that the "erroneously arrested" individual had not been arrested, which it defined as meaning they were taken in custody to a police station and questioned.

They said that the individual was wanted on suspicion of a public order offence and "would have been stopped by officers explaining why they were being stopped" before a radio check against the Police National Computer established that they had already been dealt with.

The Met said: "We have always maintained that it was a continued trial to test the technology and assess if it could assist police in identifying known offenders in large events, in order to protect the wider public."

"A full analysis of its deployments and a wider consultation will take place at the conclusion of the trial," the spokesperson added, although no date was given for this.

Sky sources have suggested that the Met is planning to trial the technology at other events in the future.