New York CNN —

In its annual “worldwide threat assessment,” top US intelligence officials have warned in recent years of the threat posed by so-called deepfakes – convincing fake videos made using artificial intelligence.

“Adversaries and strategic competitors,” they warned in 2019, might use this technology “to create convincing—but false—image, audio, and video files to augment influence campaigns directed against the United States and our allies and partners.”

The scenarios are not difficult to imagine; a faked video showing a politician in a compromising position; faked audio of a world leader discussing sensitive information.

The threat doesn’t seem too distant. The recent viral success of ChatGPT, an A.I. chatbot that can answer questions and write prose, is a reminder of how powerful this kind of technology can be.

But despite the warnings, we haven’t seen many notable instances, that we know of, where deepfakes have successfully been deployed in geopolitics.

But there is one group the technology has been weaponized against consistently and for several years: women.

Deepfakes have been used to put women’s faces, without their consent, into often aggressive pornographic videos. It’s a depraved AI spin on the humiliating practice of revenge porn, with deepfake videos appearing so real it can be hard for female victims to deny it isn’t really them.

The long-simmering issue exploded into public view last week when it emerged Atrioc, a high-profile male video game streamer on the hugely popular platform Twitch, had accessed deepfake videos of some of his female Twitch streaming colleagues. He later apologized.

Amid the fallout, the Twitch streamer “Sweet Anita” realized deepfake depictions of her in pornographic videos exist online.

“It’s very, very surreal to watch yourself do something you’ve never done,” Twitch streamer “Sweet Anita” told CNN after realizing last week her face had been inserted into pornographic videos without her consent.

“It’s kind of like if you watched anything shocking happening to yourself. Like, if you watched a video of yourself being murdered, or a video of yourself jumping off a cliff,” she said.

But the deeply disturbing use of the technology in this way is not novel.

Indeed, the very term “deepfake” is derived from the username of an anonymous Reddit contributor who began posting manipulated videos of female celebrities in pornographic scenes in 2017.

“From the very beginning, the person who created deepfakes was using it to make pornography of women without their consent,” Samantha Cole, a reporter with Vice’s Motherboard, who has been tracking deepfakes since their inception, told CNN.

Twitch streamer "Sweet Anita" during her interview with CNN. CNN

The online gaming community is a notoriously difficult place for women – the 2014 “Gamergate” harassment campaign a most prominent example.

But concerns over the use of nonconsensual pornographic images isn’t exclusive to this community, and threatens to become more commonplace as artificial intelligence technology develops at breakneck speed and the ease of creating deepfake videos continues to improve.

“I am baffled by how awful people are to each other on the Internet in a way that I don’t think they would be face to face,” Hany Farid, a professor at the University of California, Berkeley, and digital forensics expert, told CNN.

“I think we have to start sort of trying to understand, why is it that this technology, this medium, allows and brings out seemingly the worst in human nature? And if we’re going to have these technologies ingrained in our lives the way they seem to be, I think we’re going to have to start to think about how we can be better human beings with these types of devices,” he said.

It’s part of a much larger systemic problem.

“It’s all rape culture,” Cole said, “I don’t know what the actual solution is other than getting to that fundamental problem of disrespect and non-consent and being okay with violating women’s consent.”

There have been efforts from lawmakers to crack down on the creation of nonconsensual imagery, whether it is AI-generated or not. In California, laws have been brought in to try to counter the potential for deepfakes to be used in an election campaign and in nonconsensual pornography.

But there’s skepticism. “We haven’t even solved the problems of the technology sector from 10, 20 years ago,” Farid said, pointing out that the development of artificial intelligence “is moving much, much faster than the original technology revolution.”

“Move fast and break things,” was Facebook founder Mark Zuckerberg’s motto back in the company’s early days. As the power, and indeed the danger, of his platform came into focus he later changed the motto to, “Move fast with stable infrastructure.”

Whether it was willful negligence or ignorance, Silicon Valley was not prepared for the onslaught of hate and disinformation that has festered on its platforms. The same tools it had built to bring people together have also been weaponized to divide.

And while there has been a good deal of discussion about “ethical AI,” as Google and Microsoft look set for an AI arms race, there’s concern things could be moving too rapidly.

“The people who are developing these technologies – the academics, the people in the research labs at Google and Facebook – you have to start asking yourself, ‘why are you developing this technology?,’” Farid suggested.

“If the harms outweigh the benefits, should you carpet bomb the Internet with your technology and put it out there and then sit back and say, ‘well, let’s see what happens next?’”. Jenna Ryu

USA TODAY

Her inbox was flooded with explicit screenshots, seemingly from a pornographic video. She recognized her face, but not her body. For just a second, she questioned whether the footage could be real. It certainly looked that way, though she knew she had never filmed herself nude.

It was a deepfake – so realistic that even the woman featured in it was momentarily fooled.

QTCinderella is a fan-favorite Twitch streamer who is known for wholesome gaming and baking content. But late last month, on Jan. 30, a fellow streamer briefly showed a browser window that featured a website that creates AI-generated explicit content of women, including female streamers. On the site, deepfake porn of the 28-year-old could be found, and since then, she says her name, her face and her brand have become associated with pornography.

Overwhelmed with shock, confusion, panic and pain, she decided to share her feelings in an impromptu livestream to her 800,000 followers.

This, she says, is what sexual trauma looks like.

"I wanted to show this is a big deal," says QT, who asked we refer to her using her username for privacy reasons. "That every single woman on that website, this is how they feel. Stare at me sobbing, and tell me you still think this is OK."

Deepfake porn lives on in screenshots

Deepfake, or videos that use artificial intelligence to combine images or videos onto a source material, is not new. The process can be used to make it look as if people said, or did, things they did not. Experts worry that the technology can do more harm than good – especially for women in the public eye.

What is a deepfake? This video technology is spooking some politicians

Platforms like Reddit have banned deepfake porn, but smaller sites, like the one that shared fake images of QT, still exist. And even if a person succeeds in getting an explicit video removed, as QT was able do, screenshots continue to circulate.

Along with the harassment, stalking and misogyny that female streamers often grapple with online, this modern form of sexualization, QT says, made her feel exploited and "purely like an object."

The porn may be fake. But the trauma isn't.

Though fans and other women in the public eye are voicing their support, QT says she has also received an influx of hate and victim-blaming messages, most of them from men who don't understand how fake images can cause real harm. Others believe this is the price women pay for internet fame.

"This is nothing I've done. I haven't done anything wrong. That's what's crazy about all this: We (as women) have done nothing wrong. We just existed," QT says.

She has also struggled with the pain of having these pictures sent to her family – the discomfort of having to explain the photos, over and over. It's a humiliating conversation she never thought she'd have to have.

Contrary to popular belief, licensed clinical social worker Jessica Klein says, an image, altered or not, is enough to create real, tangible trauma, and for some, diagnosable PTSD. Research has supported that the mental health effects of sexual assault and image-based, nonconsensual abuse (like revenge porn) are similar.

"Something doesn't need to physically happen to your body to be traumatizing," says Klein, who works with victims of revenge porn. "It's a violation. A sense of helplessness, fear and shame. Your sense of safety is completely annihilated when your body is being portrayed in this nonconsensual way for millions to view."

For QT, the objectification of her body – against her will – was all too familiar. It triggered memories of her sexual assault experience.

"Minutes after I saw that photo, I felt the same way," she says. "The same feeling of guilt, with the same feeling of being used. And it's because it's another thing I didn't agree to. Another thing I didn't want to do. Another version of me I didn't want seen or touched or looked at."

Maya Higa, a fellow streamer who creates conservationist content and was also a victim of deepfake pornography, shared the same sentiment.

“Today, I have been used by hundreds of men for sexual gratification without my consent. The world calls my 2018 experience rape. The world is debating over the validity of my experience today," Higa wrote in a Twitter statement on Jan. 31.

'I'm a normal girl'

QTCinderella had a lot to lose when deciding whether she wanted to publicly speak about the incident. She wants to move past the controversy.

But the reason she is doing this is "for the women that really can't afford to have this on there."

"We need federal laws," QT says. Experts like Klein agree: While many U.S. states have laws against "revenge porn" and nonconsensual nude images, only three states (California, Virginia and Texas) specifically include deepfakes.

"We need something to happen to people that take advantage of others," QT says. "That's fundamentally a change that hopefully we can all agree with, and if you can't see it that way, I beg every person to imagine seeing these types of photos or videos someone they care about, against their will."

Beyond this controversy and their bodies, women like QT deserve to be known for their humanity. For instance, her contributions in the Twitch space: She spearheaded the annual Streamer Awards, an opportunity to bring gamers together and celebrate one another as a community. In her free time, she raises money for Alveus Sanctuary for animals.

"I'm a normal girl," she says. "I like Taylor Swift. I like baking cookies. I like going to Disneyland."

If you are a survivor of sexual assault, RAINN offers support through the National Sexual Assault Hotline (800.656.HOPE & online.rainn.org).. Artificial intelligence-generated pornography featuring the faces of nonconsenting women is becoming more pervasive online, and the issue is spilling into the world of popular influencers and streamers.

In January, the British livestreamer “Sweet Anita,” who has 1.9 million followers on Twitch, where she posts videos of her gaming and interacting with followers, was notified that a trove of fake sexually explicit videos featuring the faces of Twitch streamers was circulating online.

Her first thought was: “Wait, am I on this?”

She quickly Googled her name alongside the term “deepfake,” a word used to describe a highly realistic but fake, digitally manipulated video or image, and a technique that is increasingly being used — typically without consent — for pornography purposes. Anita’s initial search brought up several videos that showed her face edited onto another person’s body.

“This has obviously been going on for quite a while without my knowledge, I had no idea — it could have been years for all I know,” said Anita, 32, who did not want to share her full name with NBC News out of concerns for her safety and privacy offline.

Hany Farid, a professor of computer science at the University of California, Berkeley, said deepfakes are a phenomenon that is “absolutely getting worse” as it’s become easier to produce sophisticated and realistic video through automated apps and websites.

The number of deepfake pornographic videos available online has seen a sharp increase, nearly doubling each year since 2018, according to research conducted by Genevieve Oh, a livestreaming analyst. In 2018, just 1,897 videos had been uploaded to a well-known deepfake streaming site, but by 2022 this number increased to over 13,000 with over 16 million monthly views.

Now suddenly the people who are vulnerable are people who have very small footprints online. -Hany Farid, a professor of computer science at the University of California, Berkeley

Previously, celebrities were primarily the targets of deepfakes.

“Now suddenly the people who are vulnerable are people who have very small footprints online,” said Farid. “The technology is getting so good that it can generate images from relatively small training stats, not these hours and hours of video that we used to need.”

Anyone interested in creating deepfakes can quickly access a plethora of free and paid-for face-swapping apps available in the Google Play and Apple App stores, making it easy for anyone to upload a photo and edit it onto a photo or video within seconds.

Some major platforms like Reddit, Facebook, TikTok and Twitter have attempted to address the spread of deepfake porn with policy changes. While each of the platforms specifically prohibits the material, some have struggled to moderate it. A search of Twitter, for instance, found deepfake pornographic videos claiming to feature Twitch stars, along with hashtags promoting deepfakes.

In January, the proliferation of deepfake pornography made waves online, when a popular Twitch streamer with more than 300,000 followers admitted to paying for explicit material featuring AI-generated versions of his peers.

On Jan. 30 in a tearful apology video that was reshared on Twitter and gained millions of views, Brandon Ewing — who uses the screen name “Atrioc” on Twitch — said he clicked on an ad for deepfake pornography while browsing a popular porn website. He said he then went on to subscribe and pay for content on a different website that showed other female streamers after becoming “morbidly curious.”

In a longer statement posted on Twitter on Feb. 1, Ewing directly addressed Twitch livestreamers Maya Higa and Pokimane, whose likeness briefly appeared in a tab for a website that hosts deepfake pornography during one of his livestreams.

“Your names were dragged into it and you were sexualized against your will,” he said. “I’m sorry my actions have lead to further exploitation of you and your body, and I’m sorry your experience is not uncommon.”

Ewing did not respond to request for comment.

Pokimane also did not respond to request for comment, but in a Jan. 31 tweet she wrote, "stop sexualizing people without their consent. that’s it, that’s the tweet."

Higa said she had no further comments to make beyond her Twitter statement on Jan. 31, in which she wrote, in part, the “situation makes me feel disgusting, vulnerable, nauseous, and violated -- and all of these feelings are far too familiar to me.”

The incident highlighted the growing prevalence of nonconsensual AI-generated pornography and the ethical problems it creates.

There has been an “uptick” in websites that are “willing, eager and monetizing the hosting of this material,” Farid said.

QTCinderella, another Twitch streamer who found out she had been featured on the deepfake website, said she found it particularly hurtful because Ewing is a close friend.

“I think that’s what was most unfortunate: I didn’t find out from Atrioc. I found out from the internet talking about it,” said QTCinderella, 28, who also did not share her full name with NBC News in order to protect her privacy and safety offline.

She said she quickly tracked down the video content to an account on a subscription-based website and issued a takedown notice, but the videos continue to spread like “wildfire.”

In the United States, while the majority of states have laws that ban revenge porn, only New York, Virginia, Georgia and California have laws that specifically address deepfake media, according to the Cyber Civil Rights Initiative. Meanwhile, the United Kingdom announced in November last year that it was planning to criminalize explicit nonconsensual deepfake media.

QTCinderella said the current legal framework is “disheartening.”

“Every single lawyer I’ve talked to essentially have come to the conclusion that we don’t have a case; there’s no way to sue the guy.”

While a lot of deepfake pornography can look amateur and low-quality, Farid said he’s now also seeing accounts offering to create sophisticated custom deepfakes of any woman for a small fee.

After seeing the deepfake videos that were being sold of her online, Anita said she felt numb, tired and disassociated.

“I’m being sold against my will,” she said. “I didn’t consent to being sexualized.”

QTCinderella said she experienced “body dysmorphia.”

“When you see a porn star’s body so perfectly grafted onto where yours should be, it’s the most obvious game of comparisons that you could ever have in your life,” she said. “I cried and was like ‘my body will never be like that.’”

Sophie Compton who campaigns against intimate image abuse with the organization My Image, My Choice said women who are targeted are “shamed or silenced” and feel their experience is minimized because there are few legal options available to those affected by deepfakes.

“We need to find a way to make these sites and their business model impossible,” Compton said.

Specific platforms that host nonconsensual sexual imagery need to be held accountable, rather than individual accounts and creators, Farid said. “If you really want to tackle this problem, go upstream,” he said. “That’s where all the power is.”

Anita said she wants there to be “very visible consequences.”

What unsettles her the most going forward is that it’s impossible to know who bought the fake videos.

“When I go to a meet-and-greet I could end up hugging and signing something for somebody who’s watched me be deepfaked … and I’d have no way to know that they’re consuming that,” she said. “That they’d buy my body against my will is just all really, really horrible.”. 1. How relevant is this ad to you?

Video player was slow to load content Video content never loaded Ad froze or did not finish loading Video content did not start after ad Audio on ad was too loud Other issues. . Now, this is really faked up.

Sweet Anita, a 32-year-old British social media star with more than 1.9 million Twitch followers, is deeply freaked out after discovering that her face has been digitally superimposed into deepfake pornography clips.

“I have never made a single drop of sexual content in my life, but now they just assume that I have and [that] I must want this,” Sweet Anita, who reportedly chose to withhold her real name, lamented to the Sun. “You could deepfake anyone. Anyone from any walk of life could be targeted by this, and it feels like people don’t give a s – – t.”

For deepfakes, creators use artificial intelligence and machine learning software to replace the likeness of one person with another in videos and other digital media.

5 British Twitch star Sweet Anita, 32, was “horrified” to discover that her image has been used in deepfake pornographic films online. NYPost Composite

High-powered women such as Emma Watson, Gal Gadot, Scarlett Johansson and Michelle Obama have all been targeted in X-rated deepfake films. And while the nonconsensual use of one’s image in pornography could result in damaging outcomes, in the US, laws prohibiting deepfakes have only been established in Texas, Virginia and California.

Previous 1 of 3 Next Advertisement Sweet Anita fears deepfake pornographic films have the potential to ruin her life. Twitch / Sweet_Anita After receiving a virtual apology from a fellow Twitch star who shared the link to her deepfake porn, Anita has uncovered additional sites featuring her in falsified X-rated clips. YouTube/SweetAnita Advertisement

Anita, from East Anglia, England — where a forthcoming amendment to the UK’s Online Safety Bill will reportedly criminalize deepfakes — learned that her likeness was being featured in internet porn sans her consent in January. The troubling discovery came when fellow Twitch personality Brandon Ewing, 31, known virtually as Atrioc, shared a link to the deepfake video site during a livestreaming event.

Ewing later issued an apology for sharing the link to Anita, tearfully atoning for his “gross” and “embarrassing” conduct.

But after watching his cyber mea culpa and Googling her own name, Anita was “horrified” to find that multiple pornographic deepfakes using her image had been published online.

5 Ewing, more popularly known as Atrioc, cried as he apologized for sharing the link to deepfake porn sites that featured X-rated images of Sweet Anita and QTCinderella. YouTube/Atrioc

“It’s not easy to differentiate [a deepfake] from reality,” Anita said. “If people see this video in 10 or 20 years’ time, no one will know whether I was a sex worker or [if] this was a deepfake.”

The victimized Twitch gamer continued, “It could potentially get you fired from jobs in the future if people think you’ve done sex work. It affects your security [and] how people treat you. You are stigmatized.”

And as for Ewing’s weepy “I’m sorry” post, Anita said: “I don’t really accept the apology because it’s too little, too late.”

5 Sweet Anita says she and other female content creators targeted in deepfake porn clips will forever be subjected to unjust scrutiny. YouTube/SweetAnita

“It looks like damage control from someone facing a pending lawsuit rather than actual remorse,” she continued.

Last month, Google searches for “deepfake porn” skyrocketed 1,000% in the UK, per Scams.info. And in the weeks since, Sweet Anita — as well as other social media personalities, including Los Angeles-based Twitch star QTCinderella, 28, née Blaire — discovered she’d been targeted in deepfake porn, and searches for “how to make a deepfake” rose 120%.

Searches for “are deepfakes illegal” were also said to have soared a staggering 5,000%.

5 Anita claims digital wrongdoers have been using her image in X-rated material for years. Getty Images

And Anita fears the mass circulation of her misused image will have lasting ramifications.

“This was nonconsensual and the impacts are permanent,” she said. “This will impact my life in a similar way to revenge porn, so I’m just frustrated, tired and numb.”

Unfortunately, she’s no stranger to being wrongfully highlighted in raunchy digital content.

“Before deepfakes, people had been photoshopping still images to have my face on porn,” said Anita.

“People have amassed huge collections of clips of me getting up and walking away from the camera during livestreams to look at my body, even though it’s clothed,” she continued.

“There are people who make whole Reddit forums just to roleplay as me,” the brunette mourned, “where they are like, ‘I’m going to pretend to be Sweet Anita, do you want to do spicy DMs with me?’ ”

5 Anita worries that she and other female content creators will have to spend “thousands” on therapy in order to cope with being violated online. YouTube/SweetAnita

She went on to claim that there are online communities comprised of “thousands” of members who are joined by their shared desire to unjustly sexualize her and other women streamers.

“It’s one extra thing in an endless and exhaustive list that female content creators have to take on that [is] traumatizing,” Anita said. In addition to the cost of therapy to help cope with the trauma, it costs “thousands” to take legal action, confront the perpetrators and get the material taken down.

She also noted a perceived “lack of empathy” from online gamers, who she says often “blame women” for the distasteful deepfake content.

“They don’t seem to think through what’s happened to us is a reality. They just see it as women complaining about being objects of sexual fantasies,” said Anita.

“It’s really, really difficult,” she added, “but the greatest challenge so far has been trying to explain to people the real genuine impact on people’s lives and how permanent it is, too.”

Previous 1 of 3 Next Advertisement QTCinderella, like Sweet Anita, is one of the many female content creators who were targeted by deepfake porn creators. Twitch/QTCinderella In an emotional address, QTCinderella said “F – – k the internet” for its circulation of falsified sex content that features her image. Twitch/QTCinderella Advertisement

In early February, a despondent and “violated” QTCinderella blasted deepfake creeps for using her image in their NSFW production.

“I’m so exhausted and I think you guys need to know what pain looks like because this is it,” she sobbed. “This is what it looks like to feel violated. This is what it feels like to be taken advantage of, this is what it looks like to see yourself naked against your will being spread all over the internet. This is what it looks like.”

She then scolded the entire online community, including Atrioc, for the embarrassment and harassment.

“F – – k the f – – king internet. F – – k Atrioc for showing it to thousands of people,” she barked. “F – – k the people DMing me pictures of myself from that website. F – – k you all!“. Twitch, which is largely dedicated to gaming streams, “is a male-dominated space,” Anita said. According to StreamScheme , 78.4% of Twitch users are male, and 19.6% are female. Women also are a small minority of Twitch’s top-earning content creators. According to a 2021 leak that contained the earnings of the platform’s top performers, only 3% of the top 100 were women.



Anita said she has been the subject of “cum tributes” — videos in which men ejaculate onto images of her — and compilations of her streaming footage edited in a lascivious manner. “If I stopped myself from doing things that people can masturbate to, I wouldn't leave my house,” Anita said. “I gave up trying to deal with it, and I just tried to focus on what I do love about my job.”

Still, Anita said she is “looking into” legal action over the deepfakes of her. Meanwhile, other streamers featured in the deepfakes have hired lawyer Ryan Morrison, whose Los Angeles–based firm sent takedown notices to sites where the content was being hosted. (It’s unclear whether the original creator took down their content before or after these takedowns were issued.)



"There's no appropriate way to use someone's content or likeness without their consent," Morrison told BuzzFeed News. “Deepfakes themselves are newer to the ecosystem of porn. The deepfake technology is getting to a point where it looks real, it can come across as legitimate.” Morrison declined to reveal the women he’s representing so as not to bring more harassment their way but said they also were considering legal action.. Twitch streamer QTCinderella has vented her frustration with the inadequacies of the US legal system after she couldn’t find a lawyer to file a suit against the owner of a website that published deepfake porn videos depicting her.

She had previously vowed to sue the porn site hosting the deepfakes in an emotional livestream, saying: "And to the person that made that website, I'm going to fucking sue you. I promise you, with every part of my soul. I'm going to fucking sue you."

QTCinderella and other female Twitch stars discovered that deepfake porn using their faces was available online in a viral clip from fellow streamer Atrioc, who afterwards apologized for the incident. He stepped down from content creation in the aftermath of this scandal, stating that he’d help cover legal fees for those who want to try and combat such porn websites.

In an interview with NBC News, QTCinderella commented that the response she got from the lawyers she spoke to, who are familiar with the issue, was “disheartening”.

“Every single lawyer I’ve talked to essentially has come to the conclusion that we don’t have a case,” she told NBC. “There’s no way to sue the guy.”

While regulation of deepfakes is under way in the UK, where the use of one’s image for deepfakes without consent and permission will be punishable, the US has no such legal tools available to victims right now. That makes it very difficult to legally combat the creators of this sort of content.

The incident had another effect on QTCinderella, which she opened up about towards NBC: body dysmorphia. “When you see a porn star’s body so perfectly grafted onto where yours should be,” the streamer said, “it’s the most obvious game of comparisons that you could ever have in your life. I cried and was like ‘my body will never be like that.’”

Another one of the victims, Sweet Anita, recently said that while “there isn’t any moving on” from the incident for her, she is skeptical about the effect of legal measures. “Frankly, you can change the law, but the law won’t change anything until the culture does”, she commented.. Yesterday, Twitch streamer Brandon “Atrioc” Ewing issued a tearful apology during his livestream after he accidentally revealed that he had deepfake pornography of popular female Twitch streamers open on his computer. The video of his apology—in which he claims to have clicked an ad on PornHub because he was curious—quickly gained traction online. Twitter users, in a misguided attempt to “draw attention” to the controversy, shared screenshots of the original stream, which signal boosted the porn site and its contents. This made matters worse for the affected women, who learned they were on the deepfake site only because of Ewing’s slip-up.



What’s even worse is that the site Ewing was visiting is a lot like OnlyFans: It requires people to pay a subscription to view its content, and the particular content creator’s page he was on was centered entirely around making deepfakes of famous Twitch streamers—several of whom Ewing is real-life friends with.



Advertisement

Vice has reported that the deepfake creator in question has removed all content from their page and issued an apology. Kotaku will not link to the original site (neither did Vice), but can confirm that the apology is there, in place of any content. In it, the creator claims they stopped making videos and deleted them if a particular streamer ever DMed them. They also wrote that seeing the impact of Ewing’s stream was “eye-opening” for them, which makes me want to put my foot through a wall because, of course, you’d only find issue with grafting famous streamers’ faces onto lewd bodies after Twitter came for you. It’s unclear if this person’s content exists anywhere else on the internet, but it’s not unlikely.



Advertisement

Streamers like QTCinderella and Sweet Anita spoke out against the existence of this deepfake pornography on their respective Twitter accounts yesterday, with QTCinderella expressing the pain she’s facing in the wake of such horrors on a Twitch stream. “It should not be part of my job to have to pay money to get this stuff taken down,” she said, before promising to sue the people behind the deepfakes.



Advertisement

She’s right, she shouldn’t have to pay money to get fake, explicit images of herself taken down, and what’s even more disconcerting is how difficult it is to get deepfake pornography removed from the internet. If the creator in question didn’t pull all of their content down, it’s unclear if any of the victims would be able to use the legal system’s current laws to effectively get it removed themselves. Several years ago, Reddit and PornHub banned deepfake porn, and Discord banned servers selling apps that aided in the creation of it, but smaller sites like the one Ewing was caught on still frequently post such violative pornography with little to no repercussions. That’s because, like so many things surrounding the internet, women’s bodies, and non-consensual porn, the government doesn’t know what the fuck to do about it.



Only California and Virginia have explicit laws surrounding deepfake pornography: AB 602 and HB 2678, respectively. AB 602 allows California residents to sue if their image is used for sexually explicit content, and “seek injunctive relief and recover reasonable attorney’s fees and costs.” But according to Andy I. Chen, a California-based lawyer I spoke with over the phone, it could be difficult for the victim to sue any defendant outside of the state.

Advertisement

Back in 2019 Virginia added an amendment to its revenge porn law that covers “falsely created videographic or still images.” Violation of that law is a class 1 misdemeanor, which could result in “confinement in jail for not more than 12 months and/or a possible fine of not more than $2,500.”



QTCinderella is based in California (as are several other streamers who were reportedly targeted by the deepfake creator), so if she wants to sue the fuck out of them she is within her rights based on state laws. But Sweet Anita lives in the United Kingdom. An Online Safety Bill, which would include protections against both revenge and deepfake porn, is currently making its way through the House of Lords in the UK Parliament. I reached out to QTCinderella, Sweet Anita, and Twitch for comment, but did not receive a response back before publication.



Advertisement

Deepfake pornography is not a new problem, but it’s incredibly disheartening, saddening, and infuriating that it has now bled into the streaming world. Victims can visit C.A. Goldberg Law’s website for information regarding revenge porn laws throughout the United States, or get support by reaching out to the Cyber Civil Rights Initiative online or via their 24/7 hotline at 1-844-878-2274.

. As one of many streamers who have been objectified and exploited by the deepfaked lewd photo plague gaining traction after Atrioc inadvertently exposed it to thousands of viewers, QTCinderella vowed to sue the person responsible for creating that particular website.

Recommended Videos

The Twitch star said being exposed to something like that should not be part of her job, nor should she need to pay money to make sure it’s taken down, but the fact she has to is exhausting. “To the person that made that website, I’m going to fucking sue you,” she said during her Jan. 30 stream. “I promise you. With every part of my soul, I’m going to fucking sue you. That’s all I have to say.”

QTCinderella admitted she was hesitant to make a statement on stream but ultimately decided to because she wanted people—especially those involved in some way—to see the toll a situation like this can take on any person.

QTCinderella also said she doesn’t condone the behavior of anyone involved, including Atrioc and all the people who don’t seem to see the issue with viewing deepfake content like that.

“If you are able to look at women who are not selling themselves or benefiting off of being seen sexually. They’re not benefitting. They’re not selling it. They’re not platforming it themselves. If you are able to look at that, then you are the problem,” she said. “You see women as an object. You should not be okay doing that.”

In a separate post, she also called out journalists and social media reporters for covering the situation insensitively by advertising the website and, in turn, spreading the images—intentionally or otherwise.

Some U.S. states, including California, New York, Texas, and Virginia, already have laws in place for issues like this. The U.K. is also reportedly planning to make sharing non-consensual explicit deepfake content a crime in England and Wales. It seems like only a matter of time before others follow suit.

Dot Esports is supported by our audience. When you purchase through links on our site, we may earn a small affiliate commission. Learn more. In a live stream on Monday, Ewing inadvertently showed browser windows open to a website that hosts non-consensual, AI-generated images. The window showed that he was viewing images on the account of someone who specialized in making deepfakes of popular streamers. Viewers of the stream caught the leak and screenshotted the site, then shared the site, images from it, and names of the women who were deepfaked.

On Monday, Twitch streamer Brandon Ewing, who goes by Atrioc online, admitted to buying and watching deepfakes from an account that makes non-consensual, sexually explicit AI-generated videos of his colleagues in the streaming world.

As first reported by As first reported by Dextero , Ewing said during his Monday stream, in a tearful, now-viral apology, that he clicked an ad for deepfake porn while browsing Pornhub. That ad took him to another subscriber-only website, he said, where he paid to view the images of popular female streamers. He said he was driven by “morbid curiosity” and that his watching non-consensual porn is not a “pattern of behavior.” His wife, cosplayer Arianna Ewing, sat in the background of the stream and cried.

At the time of writing, the deepfake creator’s page on the site Ewing used has been scrubbed of deepfakes and now hosts a long apology to streamers that describes the non-consensual fake sex tapes as “immoral.” At the time of writing, the deepfake creator’s page on the site Ewing used has been scrubbed of deepfakes and now hosts a long apology to streamers that describes the non-consensual fake sex tapes as “immoral.”

“To be quite honest if I wanted to continue this, what I got was the best advertisement I could ever ask for but after seeing the situation of that couple apologizing and a few streamers’ reactions who thought [I] ‘did not care’, I feel like the total piece of shit I am,” they wrote. “The best course of action I have understood is to just wipe my part off the internet and help decrease the number of future videos of those involved. You will not see me pop up again.” “To be quite honest if I wanted to continue this, what I got was the best advertisement I could ever ask for but after seeing the situation of that couple apologizing and a few streamers’ reactions who thought [I] ‘did not care’, I feel like the total piece of shit I am,” they wrote. “The best course of action I have understood is to just wipe my part off the internet and help decrease the number of future videos of those involved. You will not see me pop up again.”. A popular gamer on Twitch has tearfully revealed she is the latest high-profile victim of deepfake porn, with predators pasting her face into a pre-existing adult video to make it seem she legitimately appeared in the kinky clip.

QTCinderella, a 28-year-old American whose real name is Blaire, went live on the streaming site last week to lash out at the cyber sickos who made the video, as well as a prominent male Twitch star who had admitted to buying deepfake porn.

“I’m so exhausted and I think you guys need to know what pain looks like because this is it,” the gamer wept. “This is what it looks like to feel violated. This is what it feels like to be taken advantage of, this is what it looks like to see yourself naked against your will being spread all over the internet. This is what it looks like.”

She then took aim at gamer Atrioc who had earlier told fans that he purchased two doctored clips featuring other famous female Twitch stars, prompting a spike in traffic to the deepfake porn site.

“F- -k the f- -king internet. F- -k Atroic for showing it to thousands of people. F- -k the people DMing me pictures of myself from that website. F- -k you all! This is what it looks like, this is what the pain looks like,” QTCinderella continued during her emotional livestream.

4 GTCinderella has tearfully revealed she’s a victim of deepfake porn, with predators pasting her face into a pre-existing adult video to make it seem that she legitimately appeared in the kinky clip. Twitch/QTCinderella

“To the person that made that website, I’m going to f- -king sue you,” she vowed. “I promise you, with every part of my soul I’m going to f- -king sue you.”

The Post has reached out to QTCinderella for further comment.

Given the rapid advancement of technology, it’s difficult to distinguish deepfake porn from legitimately filmed videos, adding to the distress experienced by victims who insist they weren’t party to the production. Laws have also not kept up with current online activity, meaning it may prove difficult for QTCinderella to sue the person who created the disturbing, doctored video.

Tech writer River Page first reported on the Twitch star’s deepfake horror. In his essay, republished by the Free Press, Page explained that “there is a federal revenge porn law that allows victims of nonconsensual porn to file lawsuits against perpetrators, but the law doesn’t address deepfakes specifically.”

4 QTCinderella — whose 28-year-old American whose real name is Blaire — is one of the most popular woman gamers on Twitch. Instagram/QTCinderella

4 “To the person that made that website, I’m going to f- -king sue you. I promise you, with every part of my soul I’m going to f- -king sue you,” QTCinderella said. Twitch/QTCinderella

“A federal law should be in place,” Page further wrote. “Will it stop deepfake porn? Not completely. Federal law hasn’t eliminated the production and distribution of child pornography either, but the enforcement of those laws has driven the practice to the extreme margins, and has attached a heavy cost to participating in the trade.”

At present, cyber pervs use software involving machine learning or artificial intelligence to create deepfakes with relative ease — and with little fear of prosecution.

Celebs such as Scarlett Johansson and Emma Watson have been the victims of deepfake porn videos, and some sleazebags are charging just $20 to create fabricated videos of exes, co-workers, friends, enemies and classmates.

Robert Chesney from the University of Texas and Danielle Citron from the University of Maryland have said the damage from being a victim of deepfake porn can be “profound.”

“Victims may feel humiliated and scared,” they wrote in a 2019 research paper. “When victims discover that they have been used in fake sex videos, the psychological damage may be profound — whether or not this was the aim of the creator of the video.”

4 At present, cyber pervs use software involving machine learning or artificial intelligence to create deepfakes with relative ease — with little fear of prosecution. Shutterstock

The damage appears to be obvious QTCinderella, but some on social media have expressed little sympathy.

“I don’t get the big deal at all. Like, there could be terabytes of photoshopped porn of me, and I wouldn’t care… because I’m not actually experiencing those scenarios being depicted. It’s literally not real,” one Twitter user wrote.

“I get being upset But if you’re crying over people putting your face on a pornstar then maybe you’re too soft for the internet It’s wild out here,” another declared.. A month after Twitch streamer Brandon “Atrioc” Ewing accidentally revealed that he was watching sexually explicit deepfakes of his fellow streamers while on stream, Twitch has issued a lengthy statement regarding its stance on the controversy and what the company is calling “synthetic non-consensual exploitative images” or NCEI in general.



Rooster Teeth's RWBY x Justice League Collab Puts Batman's New Anime Abilities To Good Use CC Share Subtitles Off

English view video Rooster Teeth's RWBY x Justice League Collab Puts Batman's New Anime Abilities To Good Use

Read More: The Aftermath of Twitch’s Deepfake Porn Scandal

The lengthy community update, titled “Addressing Explicit Deepfake Content,” was shared today, March 7 alongside the announcement that Twitch is hosting a Creator Camp on March 14 to “help protect women streamers.” The update references the Atrioc incident, which directly affected several prominent streamers like Pokimane, Sweet Anita, and QTCinderella, who were victims of the deepfake content in question.

Advertisement

“In January, a brief ‘deepfake porn’ incident was live streamed on Twitch,” the post reads. “This moment caused immense distress to those whose images were used without their consent, and set off a wave of conversations about the dangers that AI-generated explicit imagery can pose.”

Advertisement

The post goes on to state that “deepfake porn isn’t a problem on Twitch, but it’s a terrible issue that some streamers (almost exclusively women) may face on the internet at large.” And even though this kind of content isn’t prominent on Twitch (aside from Atrioc briefly revealing a site that contained deepfakes of popular streamers), the company states that it wants “to help streamers protect themselves or respond quickly to this kind of situation anywhere it arises.”

“In the weeks since the event, we’ve been listening to the community, talking with streamers, and consulting with experts in the field about how to keep streamers protected—on and off our service.”

Advertisement

Twitch also states that, while “deepfake porn” is the commonly used term, their consultation with “experts” suggests that the term synthetic NCEI is more appropriate, as “porn (while prohibited on Twitch) should be consensual and should feature people who know they’re taking part in activities that others are going to see. That’s not what’s happening here...”



In response to the scandal, Twitch is updating two of its policies that will be implemented in updates over the next month:

1. We’re updating our Adult Sexual Violence and Exploitation policy to make it more clear that intentionally promoting, creating, or sharing synthetic NCEI can result in an indefinite suspension on the first offense. 2. We’re updating our Adult Nudity policy to include synthetic NCEI. Even if that NCEI is shown only briefly, or, for example, shown to express your outrage or disapproval of the content, it will be removed and will result in an enforcement.

Advertisement

The company states that it’s working with law professor and vice president of the Cyber Civil Rights Initiative Daniel Keats Citron along with employees of the UK Revenge Porn Helpline to “make sure we’re addressing this topic from multiple, valuable perspectives.”

Finally, the statement announces a Creator Camp led by Revenge Porn helpline manager and Twitch streamer Zara Ward taking place on March 14 before listing a few resources we’ve linked above.



Advertisement

While it’s great to see Twitch making a big, bold statement about the scandal, it’s certainly a bit frustrating that it comes over a month after the inciting incident took place. Did the company want to wait until Women’s History Month kicked off before it started loudly supporting its women-identifying creators or...?

Kotaku has reached out to Twitch for co mment and will update the story accordingly.. Human read | Listen 10 min

QTCinderella built a name for herself by gaming, baking and discussing her life on the video-streaming platform Twitch, drawing hundreds of thousands of viewers at once. She pioneered “The Streamer Awards” to honor other high-performing content creators and recently appeared in a coveted guest spot in an esports champion series.

Nude photos aren’t part of the content she shares, she says. But someone on the internet made some, using QTCinderella’s likeness in computer-generated porn. This month, prominent streamer Brandon Ewing admitted to viewing those images on a website containing thousands of other deepfakes, drawing attention to a growing threat in the AI era: The technology creates a new tool to target women.

“For every person saying it’s not a big deal, you don’t know how it feels to see a picture of yourself doing things you’ve never done being sent to your family,” QTCinderella said in a live-streamed video.

Advertisement

Streamers typically don’t reveal their real names and go by their handles. QTCinderella did not respond to a separate request for comment. She noted in her live stream that addressing the incident has been “exhausting” and shouldn’t be part of her job.

Until recently, making realistic AI porn took computer expertise. Now, thanks in part to new, easy-to-use AI tools, anyone with access to images of a victim’s face can create realistic-looking explicit content with an AI-generated body. Incidents of harassment and extortion are likely to rise, abuse experts say, as bad actors use AI models to humiliate targets ranging from celebrities to ex-girlfriends — even children.

Women have few ways to protect themselves, they say, and victims have little recourse.

As of 2019, 96 percent of deepfakes on the internet were pornography, according to an analysis by AI firm DeepTrace Technologies, and virtually all pornographic deepfakes depicted women. The presence of deepfakes has ballooned since then, while the response from law enforcement and educators lags behind, said law professor and online abuse expert Danielle Citron. Only three U.S. states have laws addressing deepfake porn.

Advertisement

“This has been a pervasive problem,” Citron said. “We nonetheless have released new and different [AI] tools without any recognition of the social practices and how it’s going to be used.”

The research lab OpenAI made waves in 2022 by opening its flagship image-generation model, Dall-E, to the public, sparking delight and concerns about misinformation, copyrights and bias. Competitors Midjourney and Stable Diffusion followed close behind, with the latter making its code available for anyone to download and modify.

Abusers didn’t need powerful machine learning to make deepfakes: “Face swap” apps available in the Apple and Google app stores already made it easy to create them. But the latest wave of AI makes deepfakes more accessible, and the models can be hostile to women in novel ways.

Since these models learn what to do by ingesting billions of images from the internet, they can reflect societal biases, sexualizing images of women by default, said Hany Farid, a professor at the University of California at Berkeley who specializes in analyzing digital images. As AI-generated images improve, Twitter users have asked if the images pose a financial threat to consensually made adult content, such as the service OnlyFans where performers willingly show their bodies or perform sex acts.

Advertisement

Meanwhile, AI companies continue to follow the Silicon Valley “move fast and break things” ethos, opting to deal with problems as they arise.

“The people developing these technologies are not thinking about it from a woman’s perspective, who’s been the victim of nonconsensual porn or experienced harassment online,” Farid said. “You’ve got a bunch of White dudes sitting around like ‘Hey, watch this.’”

Deepfakes’ harm is amplified by the public response

People viewing explicit images of you without your consent — whether those images are real or fake — is a form of sexual violence, said Kristen Zaleski, director of forensic mental health at Keck Human Rights Clinic at the University of Southern California. Victims are often met with judgment and confusion from their employers and communities, she said. For example, Zaleski said she’s already worked with a small-town schoolteacher who lost her job after parents learned about AI porn made in the teacher’s likeness without her consent.

Advertisement

“The parents at the school didn’t understand how that could be possible,” Zaleski said. “They insisted they didn’t want their kids taught by her anymore.”

Share this article Share

The growing supply of deepfakes is driven by demand: Following Ewing’s apology, a flood of traffic to the website hosting the deepfakes caused the site to crash repeatedly, said independent researcher Genevieve Oh. The number of new videos on the site almost doubled from 2021 to 2022 as AI imaging tools proliferated, she said. Deepfake creators and app developers alike make money from the content by charging for subscriptions or soliciting donations, Oh found, and Reddit has repeatedly hosted threads dedicated to finding new deepfake tools and repositories.

Asked why it hasn’t always promptly removed these threads, a Reddit spokeswoman said the platform is working to improve its detection system. “Reddit was one of the earliest sites to establish sitewide policies that prohibit this content, and we continue to evolve our policies to ensure the safety of the platform,” she said.

Advertisement

Machine learning models can also spit out images depicting child abuse or rape and, because no one was harmed in the making, such content wouldn’t violate any laws, Citron said. But the availability of those images may fuel real-life victimization, Zaleski said.

Some generative image models, including Dall-E, come with boundaries that make it difficult to create explicit images. OpenAI minimizes the nude images in Dall-E’s training data, blocks people from entering certain requests and scans output before showing it to the user, lead Dall-E researcher Aditya Ramesh told The Washington Post.

Another model, Midjourney, uses a combination of blocked words and human moderation, said founder David Holz. The company plans to roll out more advanced filtering in coming weeks that will better account for the context of words, he said.

Advertisement

Stability AI, maker of the model Stable Diffusion, stopped including porn in the training data for its most recent releases, significantly reducing bias and sexual content, said founder and CEO Emad Mostaque.

But users have been quick to find workarounds by downloading modified versions of the publicly available code for Stable Diffusion or finding sites that offer similar capabilities.

No guardrail will be 100 percent effective in controlling a model’s output, said Berkeley’s Farid. AI models depict women with sexualized poses and expressions because of pervasive bias on the internet, the source of their training data, regardless of whether nudes and other explicit images have been filtered out.

Social media has been flooded by AI generated images produced by an app called Lensa. Tech reporter Tatum Hunter addresses both the craze and the controversy. (Video: Monica Rodman/The Washington Post)

For example, the app Lensa, which shot to the top of app charts in November, creates AI-generated self portraits. Many women said the app sexualized their images, giving them larger breasts or portraying them shirtless.

Advertisement

Lauren Gutierrez, a 29-year-old from Los Angeles who tried Lensa in December, said she fed it publicly available photos of herself, such as her LinkedIn profile picture. In turn, Lensa rendered multiple naked images.

Gutierrez said she felt surprised at first. Then she felt nervous.

“It almost felt creepy,” she said. “Like if a guy were to take a woman’s photos that he just found online and put them into this app and was able to imagine what she looks like naked.”

For most people, removing their presence from the internet to avoid the risks of AI abuse isn’t realistic. Instead, experts urge you to avoid consuming nonconsensual sexual content and to familiarize yourself with the ways it affects the mental health, careers and relationships of its victims.