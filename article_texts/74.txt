michael barbaro

From The New York Times, I’m Michael Barbaro. This is “The Daily.”

[music]

Today: Facial recognition is becoming an increasingly popular tool for solving crimes. The Daily’s Annie Brown speaks to Kashmir Hill about how that software is not treating everybody equally. It’s Monday, August 3.

kashmir hill I’m just going the tape record with an app that I use. Do you guys have any questions or concerns before we start talking about what happened? robert williams No. melissa williams No.

annie brown

OK. So where do you think we should start the story of this case, Kashmir?

kashmir hill

The story started, for the Williams family, in January of 2020.

robert williams Melissa got the call first. I got the call from her.

kashmir hill

It’s a Thursday afternoon in Farmington Hills, Michigan, which is just outside of Detroit.

melissa williams So I picked up Julia from school. Regular Thursday.

kashmir hill

And Melissa Williams, a realtor, is driving home from work. She’s picking up her daughter.

melissa williams And so it was right around, like, 4 o’clock. And I got a call.

kashmir hill

And she gets a call from somebody who says they’re a police officer.

melissa williams They immediately said, we’re calling about Robert from an incident in 2018. He needs to turn himself in. So I was confused off the bat.

kashmir hill

She is white. And her husband, Robert Williams, is Black.

melissa williams And they said, we assume you’re his baby mama or that you’re not together anymore. And — kashmir hill What? melissa williams Yeah. I said, that’s my husband. And what is this regarding? And they said, we can’t tell you. But he needs to come turn himself in. And I said, well, why didn’t you call him? And they said, can’t you just give him a message?

annie brown

Wait. So why is this officer calling her?

kashmir hill

She doesn’t know why the officer is calling her. All she knows is that the police want to be in touch with her husband. So she gives the officer her husband’s number. And then she calls Robert.

melissa williams And I said, I just got a really weird call. I was like, what did you do? Like, what is this about?

kashmir hill

And while they’re talking, Robert Williams gets a call from the police department.

robert williams Of course, I answered the other line. And he said he was a detective from Detroit and that I need to come turn myself in. So of course I’m like, for what? And he’s like, well, I can’t tell you over the phone. So I’m like, well, I can’t turn myself in then.

kashmir hill

It was a couple of days before his birthday. So he thought maybe it was a prank call. But it became pretty clear that the person was serious.

robert williams About, uh, probably ten minutes later, I pull in the driveway.

kashmir hill

And when he pulls into his driveway, a police car pulls in behind him, blocking him in. And two officers get out.

robert williams Yeah. So I get out of the car. And the driver, like, runs up. And he’s like, are you Robert Williams? I’m like, yeah. He’s like, you’re under arrest. I’m like, no I’m not. And the guy comes up with, like, a white sheet of paper. And it’s said “felony warrant” on the top, “larceny.” And I’m confused, like, isn’t larceny stealing?

kashmir hill

His wife comes out with his two young daughters. And his oldest daughter, who’s 5, is watching this happen.

robert williams I said, Juju (ph), go back in the house. I’ll be back in a minute. They’re just making a mistake. The guy, the other cop, is behind me with his handcuffs out already. So he’s like, come on, man. You already — you know the drill. And I’m like, what?

kashmir hill

The officers arrest him. They have to use two pairs of handcuffs to get his hands behind his back, because he’s a really big guy.

robert williams We started moving seats around, trying to get me in the back of this little bitty Impala. And off we go.

kashmir hill

And then they drive to the detention center.

[music]

robert williams I took fingerprints. I took — kashmir hill Your mug shot. robert williams Mug shot pictures.

kashmir hill

Then he’s put in a cell to sleep overnight.

robert williams At this point, I’m in a holding cell with two other guys. And they’re like, what you in here for? And I’m like, I don’t know. kashmir hill So when do you actually find out why you’ve been arrested, beyond this kind of vague larceny? robert williams Um, so — well, maybe like noon the next day.

kashmir hill

Around noon the next day, he is taken to an interrogation room. And there’s two detectives there. And they have three pieces of paper face down in front of them. And they turn over the first sheet of paper. And it’s a picture from a surveillance video of a large Black man standing in a store, wearing a red Cardinals cap and a black jacket. And the detectives ask, is this you?

robert williams I laugh a little bit, and I say, no, that’s not me. So then he turns over another paper.

kashmir hill

And they turn over a second piece of paper, which is just a close up of that same guy’s face.

robert williams And he says I guess that’s not you either. And I said, no. This is not me.

kashmir hill

So Robert picks the piece of paper up, holds it next to his own face —

robert williams I was like, what you think, all Black men look alike?

kashmir hill

— and says, do all Black men look the same to you?

annie brown

So what’s your understanding, Kashmir, of what happened to bring Robert Williams into that police department?

kashmir hill

So Robert Williams had no idea what was happening. But two years earlier, in October 2018, a man who was not him walked into a Shinola store in downtown Detroit. And Shinola is kind of like a high-end store that sells expensive watches and bikes. So this man came in. He was there for a few minutes. He stole five watches worth $3,800 and walked out. None of the employees there actually saw the theft occur. And so they had to review the surveillance footage. And they found the moment it happened. So they sent that surveillance footage picture that Robert Williams had been shown to the Detroit police. And the police turned to what a lot of police turn to these days when they have a suspect that they don’t recognize — a facial recognition system. So they ran a search on this, what they call a probe image, this picture for the surveillance video, which is really grainy. It’s not a very good photo. And the way these systems work is that they have access not just to mug shots but also to driver’s license photos. You get a bunch of different results. And there’s a human involved who decides which of the results looks the most like the person who committed the crime.

annie brown

Mm. So you’re saying the facial recognition algorithm basically created a lineup of potential suspects. And then from that lineup, someone picks the person that they think looks the most like the man in the surveillance video.

kashmir hill

Right. And so that is how they wound up arresting Robert Williams.

[music]

So back in this room, the two detectives now have the real Robert Williams in front of them. And he doesn’t look like this guy.

robert williams You know, they sat back and looked at each other and was like, with the oops face, right? Says, so I guess the computer got it wrong too.

kashmir hill

And so they kind of leaned back and said, I guess the computer got it wrong.

robert williams Well, the computer got it wrong is what threw me off. And I’m like, computer got it wrong?

annie brown

And what is the significance of that statement, “that the computer got it wrong“?

kashmir hill

So this was an admission by the detectives that it was a computer that had pointed the finger at Robert Williams. And that’s significant, because this is the first documented case of an innocent person being arrested because of a flawed facial recognition match.

[music]

annie brown

And just to put all of this into context for a second, the last time that you and I talked, Kashmir, we were talking about a different development in facial recognition — this new algorithm being used by some police departments that drew from pictures all over social media and all over the internet to make a kind of super algorithm. But the fear wasn’t that it wasn’t accurate. It was almost that it was too accurate, that it knew too much. But what you’re describing is something altogether different. Right?

kashmir hill

So when we talk about facial recognition, we often think of it as a monolith, that there’s kind of one facial recognition. But in fact, there’s a bunch of different companies that all have their own algorithms. And some work well. And some don’t work well. And some work well sometimes. Like, identifying a really clear photo is a lot easier than identifying surveillance footage.

annie brown

And why wouldn’t police departments be using the most sophisticated, the most kind of up-to-date version of this software?

kashmir hill

I mean, this is where you run into just bureaucracy. Right? You have contracts with companies that go back years and just a lot of different vendors. And so in this case, I tried to figure out exactly whose algorithms were responsible for Robert Williams getting arrested. And I had to really dig down. And I discovered the police had no idea. You know, they contract out to a company called DataWorks Plus. And DataWorks Plus contracts out to two other companies called N.E.C. and Rank One that actually supply the algorithm. It’s this whole chain of companies that are involved. And there is no standardized testing. There’s no one really regulating this. There’s just nobody saying which algorithms, you know, pass the test to be used by law enforcement. It’s just up to police officers, who, for the most part, seem to be just testing it in the field to see if it works, if it’s identifying the right people. But the really big problem is that these systems have been proven to be biased.

[music]

michael barbaro

We’ll be right back.

annie brown

So, Kashmir, help me understand how an algorithm can become biased.

kashmir hill

Well, the bias tends to come from how the algorithm is trained. And these algorithms tend to be trained by basically feeding them with a bunch of images of people. But the problem with the algorithms is that they tended to be trained with non-diverse data sets.

annie brown

Mm.

kashmir hill

So one good example is that many of the algorithms used by law enforcement in the U.S., by government in the U.S., are very good at recognizing white men and not as good at recognizing Black people or Asian-Americans. But if you go to an algorithm from a company in China, where they fed it with a lot of images of Asian people, they’re really good at recognizing Asian people and not as good at recognizing white men. So you can just, you can see the biases that come in from the kind of data that we feed into these systems.

annie brown

And is this a widely agreed upon reality — that because of these methods, the algorithms used in the U.S. are just worse at identifying faces that aren’t white men?

kashmir hill

Yeah. A few years ago, an M.I.T. researcher did this study and found that facial recognition algorithms were biased to be able to recognize white men better. And shortly after that, NIST, the National Institute of Standards and Technology, decide to run its own study on this. And it found the same thing. It looked at over 100 different algorithms. And it found that they were biased. And actually, the two algorithms that were at the heart of this case — the Robert Williams’s case — were in that study.

annie brown

So the algorithm that was used by this police department was actually studied by the federal government and was proven to be biased against faces like Robert Williams.

kashmir hill

Exactly.

annie brown

So given these widely-understood problems with these algorithms, how can police departments justify continuing to use them?

kashmir hill

So police departments are aware of the bias problem. But they feel that face recognition is just too valuable a tool in their tool set to solve crimes. And their defense is that they never arrest somebody based on facial recognition alone, that facial recognition is only what they call an investigative lead. It doesn’t supply probable cause for arrest. So what police are supposed to do is they get a facial recognition match, and you’re supposed to do more investigating. So you could go to the person’s social media account and see if there are other photos of them wearing the same clothes that they were wearing on the day they committed this crime. Or, you know, you can try to get proof that they were in that part of town on the day that the theft occurred. You know, try to get location data. Basically, find other evidence that this person is the person that committed the crime. The detectives just went to the woman who had spotted the theft on the video and showed her a photo of six people — they call it a six pack. And she said Robert Williams looked the most like the person that was in the video.

annie brown

Mm. So they’re supposed to use the facial recognition match as a kind of clue. And then the protocol calls for them to do more police work to verify it. But in this case, they basically just had someone watch the video and then identify Robert Williams as the one who looks most like the guy in the video.

kashmir hill

Yeah, they just did facial recognition a second time, but with a human who’s not actually trained. And they didn’t do any other investigating. Based on that, they went out and they arrested Mr. Williams.

annie brown

But if the police had done their job correctly — if they had looked into his social media accounts, if they had tried to get his location data from his phone records, essentially surveilling him more closely — wouldn’t that be its own sort of violation? Just because their technology wrongfully identified this man, he gets more closely watched by the police without his knowledge.

kashmir hill

Right. And this is actually what police asked the facial recognition vendors to do. They want to have more, what you call false positives, because they want to have the greatest pool of possible suspects that they can, because they want to find the bad guy.

annie brown

Huh.

kashmir hill

But there’s a real toll from that.

annie brown

Hmm.

kashmir hill

I just, you know, as a person who’s been reporting on technology for a decade, I just think people trust computers. And even when we know something is flawed, if it’s a computer telling us to do it, we just think it’s right. And this is why we always used to see, for a long time, when mapping technology was first being developed and it wasn’t that great, you know, people would drive into lakes. They would drive over cliffs, because a mapping app said, you’re supposed to go straight here.

annie brown

Right.

kashmir hill

And even though they could look and see that their life is going to be in danger, they would think, well, this app must know what it’s talking about. That’s facial recognition now. And when I was reporting this story, all the experts I talked to said this is surely not the first case where somebody has been mistakenly — an innocent person has been mistakenly arrested because of a bad face recognition match. But usually people don’t find out about it. Police don’t tell people that they’re there because of face recognition.

annie brown

Hmm.

kashmir hill

Usually, when they charge them, they’ll just say they were identified through investigative means. It’s kind of a vague, “There were clues that pointed at you.” In that way, Robert’s case was unusual, because there was so little evidence against him. They basically had to tell him that they used facial recognition, you know, to put him there.

annie brown

Right. They showed him what most people don’t get to see, which is this false match between his photo and the photo of the crime.

kashmir hill

Right.

annie brown

And what’s happened since Robert was arrested?

kashmir hill

So Robert had to hire a lawyer to defend himself. But when he went to the hearing, the prosecutor decided to drop the case. But they dropped it without prejudice, which meant that they could charge him again.

annie brown

For the same crime?

kashmir hill

With the same crime. So as I was reporting out the story, you know, I went to the prosecutor’s office. I went to the Detroit Police Department. And I said, you know, what happened here? Did you have any other evidence? This just seems like a clear misfire and misuse of facial recognition. And everyone involved was pretty defensive and said, well, you know, there might be more evidence that proves that Robert Williams did it. But after the story came out, everybody’s tune changed dramatically. Prosecutors office apologized, said that Robert Williams shouldn’t have spent any time in jail. The Detroit Police Department said this was a horrible investigation. The police officers involved just did this all wrong. This isn’t how it’s supposed to work. And they said that Robert Williams would have his information expunged from the system — his mug shot, his DNA. And they personally apologized to the Williams family, though the Williams family told me that no one ever actually called them to personally apologize.

annie brown

But he can no longer be charged in the future for this crime?

kashmir hill

That’s exactly right.

annie brown

And what about their use of facial recognition software? Has there been any change there?

kashmir hill

So one thing the Detroit Police Department said was, well, this was a case that predates this new policy we have that says, you know, we’re only supposed to be using facial recognition for violent crimes.

annie brown

Hmm. And what do you make of that? Why only use this tool for that?

kashmir hill

Well, their justification is that when it comes to violent crimes, when it comes to murder, you know, rape, they need to solve these cases. And they’ll use any clue they can to do it, including facial recognition. But I think about something that Robert’s wife said.

melissa williams When they pulled up to our house, they were already combative on the phone. They were aggressive in the doorway to me. What if he had been argumentative? If he’d been defensive, if he hadn’t complied, you know, what could that have turned into in our yard? Like, it could have went a different way. And the recent news has shown us that it definitely could have went a different way.

[music]

kashmir hill Do you feel like there’s a shame to this, that the police arrested you even though you did nothing? robert williams It’s a little humiliating. You know, it’s not something that easily rolls off the tongue, like, oh yeah, and guess what? I got arrested.

[music]

annie brown

And what about for Robert himself? What has life been like for him after the arrest?

kashmir hill

So this was very embarrassing for him and kind of painful in some ways. So he had a perfect attendance at work until that day that he was arrested. And his wife had to email his boss and say that they had a family emergency and that he couldn’t show up that day. Once he did tell his boss what happened, his boss said, you know, you don’t want to tell other people at work. You know, it could be bad for you. The night he got home, his daughter — his 5-year-old was still awake.

robert williams Julia was still up. And I was like, what are you doing up? And she was like, I’m waiting for you. And I was like, I told you I’ll be right back. And she was like, you didn’t come right back though. So I just kept telling her that they made a mistake. And it just took longer than we expected. But —

kashmir hill

She started wanting to play cops and robbers. And she would always pretend like he was the robber who stole something, and she would need to lock him up in the living room.

annie brown

Hmm.

melissa williams Oh yeah. She told us that she told one of her — Jackson, her friend at school. And we weren’t sure, did she tell her teacher? Did she tell her friends? We were not sure. And we didn’t know what to say to people. Like, just bring it up out of nowhere, like, oh yeah, in case anyone mentioned it, he was arrested, but he didn’t do anything. kashmir hill Has this made you look back to see where you — like, where you were October 2018? robert williams Yeah. I pulled it up. At the time, I was on my Facebook or on my Instagram Live.

kashmir hill

He has since looked back and realized that he had posted to Instagram at basically the same time as the shoplifting was occurring. He was driving home from work, and a song came on the radio that his mother loved: the song “We Are One” by Maze and Frankie Beverly.

robert williams I was singing songs on my way home in the car.

annie brown

So if the cops had looked in to his social media, if they had tried to verify that it was possible that he could have committed this crime, they could have found this video.

kashmir hill

Right. If the police had done a real investigation, they would have found out he had an alibi that day.

archived recording [“WE ARE ONE” PLAYING]

annie brown

Kashmir, thank you so much.

kashmir hill

Thank you.

[music]

michael barbaro

We’ll be right back. Here’s what else you need to know today. Federal unemployment benefits have expired for tens of millions of Americans after Congress failed to reach a deal to renew them last week.

archived recording So what do you say to those 30 million Americans who are now without federal unemployment help? archived recording (nancy pelosi) I say to them, talk to President Trump. He’s the one who is standing in the way of that. We have been for the $600. They have a $200 proposal, which does not meet the needs of America’s working families. And —

michael barbaro

In interviews on Sunday with ABC’s “This Week,” House Speaker Nancy Pelosi blamed Republicans for demanding a drastic cut in the weekly benefit, while Treasury Secretary Steve Mnuchin claimed that the $600 payments risked overpaying unemployed workers.

archived recording So you do think it is a disincentive to find a job if you have that extra $600? archived recording (steven mnuchin) There’s no question. In certain cases where we’re paying people more stay home than to work, that’s created issues in the entire economy.

michael barbaro

And The Times reports that July was a devastating month for the pandemic in the U.S. The country recorded nearly 2 million new infections, twice as many as any previous month.

archived recording (deborah birx) I want to be very clear. What we’re seeing today is different from March and April. It is extraordinarily widespread. It’s into the rural as equal urban areas.

michael barbaro

In an interview on Sunday with CNN, Dr. Deborah Birx, a top White House adviser on the pandemic, acknowledged that the United States has failed to contain the virus.

archived recording (deborah birx) And to everybody who lives in a rural area, you are not immune or protected from this virus. And that’s why we keep saying, no matter where you live in America, you need to wear a mask and socially distance. Do the personal hygiene —

michael barbaro

That’s it for “The Daily.” I’m Michael Barbaro. See you tomorrow.

[music]. 'The Computer Got It Wrong': How Facial Recognition Led To False Arrest Of Black Man

Updated 9:05 p.m. ET Wednesday

Police in Detroit were trying to figure out who stole five watches from a Shinola retail store. Authorities say the thief took off with an estimated $3,800 worth of merchandise.

Investigators pulled a security video that had recorded the incident. Detectives zoomed in on the grainy footage and ran the person who appeared to be the suspect through facial recognition software.

A hit came back: Robert Julian-Borchak Williams, 42, of Farmington Hills, Mich., about 25 miles northwest of Detroit.

In January, police pulled up to Williams' home and arrested him while he stood on his front lawn in front of his wife and two daughters, ages 2 and 5, who cried as they watched their father being placed in the patrol car.

His wife, Melissa Williams, wanted to know where police were taking her husband.

" 'Google it,' " she recalls an officer telling her.

Robert Williams was led to an interrogation room, and police put three photos in front of him: Two photos taken from the surveillance camera in the store and a photo of Williams' state-issued driver's license.

"When I look at the picture of the guy, I just see a big Black guy. I don't see a resemblance. I don't think he looks like me at all," Williams said in an interview with NPR.

"[The detective] flips the third page over and says, 'So I guess the computer got it wrong, too.' And I said, 'Well, that's me,' pointing at a picture of my previous driver's license," Williams said of the interrogation with detectives. " 'But that guy's not me,' " he said, referring to the other photographs.

"I picked it up and held it to my face and told him, 'I hope you don't think all Black people look alike,' " Williams said.

Williams was detained for 30 hours and then released on bail until a court hearing on the case, his lawyers say.

At the hearing, a Wayne County prosecutor announced that the charges against Williams were being dropped due to insufficient evidence.

Civil rights experts say Williams is the first documented example in the U.S. of someone being wrongfully arrested based on a false hit produced by facial recognition technology.

Lawyer: Artificial intelligence 'framed and informed everything'

What makes Williams' case extraordinary is that police admitted that facial recognition technology, conducted by Michigan State Police in a crime lab at the request of the Detroit Police Department, prompted the arrest, according to charging documents reviewed by NPR.

The pursuit of Williams as a possible suspect came despite repeated claims by him and his lawyers that the match generated by artificial intelligence was faulty.

The alleged suspect in the security camera image was wearing a red St. Louis Cardinals hat. Williams, a Detroit native, said he would under no circumstances be wearing that hat.

"They never even asked him any questions before arresting him. They never asked him if he had an alibi. They never asked if he had a red Cardinals hat. They never asked him where he was that day," said lawyer Phil Mayor with the ACLU of Michigan.

On Wednesday, the ACLU of Michigan filed a complaint against the Detroit Police Department asking that police stop using the software in investigations.

In a statement to NPR, the Detroit Police Department said after the Williams case, the department enacted new rules. Now, only still photos, not security footage, can be used for facial recognition. And it is now used only in the case of violent crimes.

"Facial recognition software is an investigative tool that is used to generate leads only. Additional investigative work, corroborating evidence and probable cause are required before an arrest can be made," Detroit Police Department Sgt. Nicole Kirkwood said in a statement.

In Williams' case, police had asked the store security guard, who had not witnessed the robbery, to pick the suspect out of a photo lineup based on the footage, and the security guard selected Williams.

Victoria Burton-Harris, Williams' lawyer, said in an interview that she is skeptical that investigators used the facial recognition software as only one of several possible leads.

"When that technology picked my client's face out, from there, it framed and informed everything that officers did subsequently," Burton-Harris said.

Academic and government studies have demonstrated that facial recognition systems misidentify people of color more often than white people.

One of the leading studies on bias in face recognition was conducted by Joy Buolamwini, an MIT researcher and founder of the Algorithmic Justice League.

"This egregious mismatch shows just one of the dangers of facial recognition technology which has already been shown in study after study to fail people of color, people with dark skin more than white counterparts generally speaking," Buolamwini said.

"The threats to civil liberties posed by mass surveillance are too high a price," she said. "You cannot erase the experience of 30 hours detained, the memories of children seeing their father arrested, or the stigma of being labeled criminal."

Maria Miller, a spokeswoman for the prosecutor's office, said the case was dismissed over insufficient evidence, including that the charges were filed without the support of any live witnesses.

Wayne County Prosecutor Kym Worthy said any case sent to her office that uses facial recognition technology cannot move forward without other supporting evidence.

"This case should not have been issued based on the DPD investigation, and for that we apologize," Worthy said in a statement to NPR. "Thankfully, it was dismissed on our office's own motion. This does not in any way make up for the hours that Mr. Williams spent in jail."

Worthy said Williams is able to have the case expunged from his record.

Williams: "Let's say that this case wasn't retail fraud. What if it's rape or murder?"

According to Georgetown Law's Center on Privacy and Technology, at least a quarter of the nation's law enforcement agencies have access to face recognition tools.

"Most of the time, people who are arrested using face recognition are not told face recognition was used to arrest them," said Jameson Spivack, a researcher at the center.

While Amazon, Microsoft and IBM have announced a halt to sales of face recognition technology to law enforcement, Spivack said that will have little effect, since most major facial recognition software contracts with police are with smaller, more specialized companies, like South Carolina-based DataWorks Plus, which is the company that supplied the Detroit Police Department with its face-scanning software.

The company did not respond to an interview request.

DataWorks Plus has supplied the technology to government agencies in Santa Barbara, Calif., Chicago and Philadelphia.

Facial recognition technology is used by consumers every day to unlock their smartphones or to tag friends on social media. Some airports use the technology to scan passengers before they board flights.

Its deployment by governments, though, has drawn concern from privacy advocates and experts who study the machine learning tool and have highlighted its flaws.

"Some departments of motor vehicles will use facial recognition to detect license fraud, identity theft, but the most common use is law enforcement, whether it's state, local or federal law enforcement," Spivack said.

The government use of facial recognition technology has been banned in half a dozen cities.

In Michigan, Williams said he hopes his case is a wake-up call to lawmakers.

"Let's say that this case wasn't retail fraud. What if it's rape or murder? Would I have gotten out of jail on a personal bond, or would I have ever come home?" Williams said.

Williams and his wife, Melissa, worry about the long-term effects the arrest will have on their two young daughters.

"Seeing their dad get arrested, that was their first interaction with the police. So it's definitely going to shape how they perceive law enforcement," Melissa Williams said.

In his complaint, Williams and his lawyers say if the police department won't ban the technology outright, then at least his photo should be removed from the database, so this doesn't happen again.

"If someone wants to pull my name and look me up," Williams said, "who wants to be seen as a thief?". A police officer holding a video camera at a protest. Credit: Mobilus In Mobili. CC BY-SA 4.0.

Since a Minneapolis police officer killed George Floyd in March 2020 and re-ignited massive Black Lives Matter protests, communities across the country have been re-thinking law enforcement, from granular scrutiny of the ways that police use force to overarching criticism of racial bias in policing. Minneapolis, where Floyd was killed, even held a vote on whether to do away with the police department and replace it with a social-service oriented agency. Amid the push for reform, one trend in policing is in urgent need of overhaul: police departments’ expanding use of artificial intelligence, namely facial recognition, to aid crime fighting.

Police agencies are increasingly deploying advanced artificial intelligence-driven identification to fight crime. AI algorithms are now employed to identify individuals by face, fingerprint, and DNA, with varying degrees of success. Among these AI technologies, facial recognition technology is arguably the most troubling. Studies have documented the racial and gender biases of these systems, and unlike with fingerprint or DNA-analysis algorithms, police are using facial recognition in the field to make on-the-spot decisions. It’s already having a corrosive impact on society.

Take Robert Williams, a Black man living in Detroit, Michigan, who was called by the Detroit police and told to turn himself in on a shoplifting charge. “I assumed it was a prank call,” he told a congressional subcommittee in July, but police later showed up at his house and arrested him in front of his wife and children. They held him for 30 hours. The evidence? A surveillance photo depicting someone else. “I held that piece of paper up to my face and said, ‘I hope you don’t think all Black people look alike,'” Williams said, according to The Detroit News. Around the country, 18,000 police departments are using this generally unregulated technology, the committee chair, Rep. Sheila Jackson Lee, said. “To add untested and unvetted facial recognition technology to our policing would only serve to exacerbate the systemic issues still plaguing our criminal justice system,” she said.

Williams is free; the charges against him were dropped—so were the charges against Michael Oliver and Nijeer Parks, two other Black men arrested on the basis of faulty facial recognition matches. But Williams’s tense encounter with the police could have ended badly, as such moments have for others. “As any other Black man would be, I had to consider what could happen if I asked too many questions or displayed my anger openly—even though I knew I had done nothing wrong,” Williams wrote in The Washington Post. In an era of racially biased law enforcement—police killed more than 1,000 people in the year following Floyd’s murder, a disproportionate number of them Black—police continue to turn to largely unregulated facial recognition technology—software known to be significantly less accurate when it comes to identifying Black people and other minorities—to make decisions with potentially lethal consequences.

How facial recognition works. To understand the risks of police use of facial recognition, it’s helpful to understand how the technology works. Conceptually, these systems can be broken down into three main parts: the known-faces database, the algorithm, and the query image.

Known-face images can come from drivers’ license pictures, passport photos, mugshots, stills from CCTV cameras, social media images, and many other places.

Facial recognition algorithms are packaged into software by vendors, but the algorithms themselves can come from anywhere. Most often the underlying algorithms are created by researchers within universities, governmental organizations, and companies. But just about any entity can become a vendor by licensing, buying, copying, or developing a facial recognition algorithm and packaging it for easy use.

Query images are often images captured on camera systems built into police cruisers, security cameras, and in police body-worn cameras. Image quality depends heavily on the image capture system used, lighting conditions, distance, and even the pose of the face being captured. These images can be matched with a face in the known-faces dataset.

Under the right set of circumstances, these elements can conspire to produce a false match, underscoring the risk that facial recognition technology poses to civil liberties.

The danger of false positives. Facial recognition errors come in two types: false negatives and false positives. False negatives are instances where a query image, for example, one captured on a police cruiser’s camera system, is an image of a person contained in the known-faces database, maybe a suspect in a crime, but the facial recognition system’s algorithm doesn’t detect the match. False positives, on the other hand, are instances where the algorithm matches a query image with a face from the known-faces database erroneously—potentially matching a person’s face with that of a criminal. These two types of error both could result in negative outcomes for the public, but false negative errors do not introduce bad outcomes that wouldn’t have already happened in the absence of facial recognition systems. False positives, conversely, introduce new dangers to both police and everyday civilians.

So, let’s look at just how bad these false positive rates are and what variables influence these rates.

A 2019 report by the National Institute of Standards and Technology (NIST) on the accuracy of facial recognition technology found that the rate of false positives varied heavily based on factors like the query image quality, the underlying dataset, and the race of the faces being queried. The range of false positive error rates was between 3 errors out of 100,000 queries (0.003 percent) in optimal conditions to 3 errors out of every 1,000 queries (0.3 percent). Moreover, the query image sources that were tested by the federal institute were of better quality than what police in the field are likely to process.

In the same report, researchers tested the accuracy of facial recognition systems given different demographic variables such as sex, age, and race and found that varying the race of the face matching pairs created false positive rates that were often two orders of magnitude greater for darker skinned individuals. Images of women produced higher false positive rates than those of men. Images for East African women produced roughly 100 times more false positives (3/1,000) than images for white men (3/100,000).

These false positive error rates are especially dangerous when combined with the practical differences related to when and where AI systems are deployed. For fingerprint and DNA analysis, AI systems operate on collected samples and are generally performed in a lab setting where these systems’ findings can be reanalyzed by human experts; facial recognition systems, on the other hand, can be be deployed by officers in the field. The final judgement on how accurate a match is can depend simply on an officer’s discernment.

Varying degrees of accuracy, little oversight. Generally, software vendors combine existing or proprietary facial detection algorithms with user interfaces in software packages for use by law enforcement. Altogether, there are more than 80 software vendors with more than 600 facial detection algorithms that have been tested by the NIST, many of which are available for local law enforcement to purchase. The performance of these algorithms varies widely, and the underlying algorithms are often not made available for code and training data inspection (as with most proprietary software) to protect the intellectual property of software vendors. What’s more, vendors such as Clearview AI advertise directly to local law enforcement organizations, which then are responsible for vetting and procuring the facial recognition systems they deploy. Further, without an overarching framework for the deployment of these systems, many law enforcement organizations are not required to seek guidance before deploying a system locally. Given the variability in vendors and their software and the lack of oversight for departments selecting vendors, it’s little surprise that the accuracy of their systems varies a lot. And it is worth reiterating, varying accuracy of these systems can, and will likely, result in real world harm.

As organizations like the American Civil Liberties Union, academic researchers, and activists raise the increasingly urgent issue of police facial recognition technologies in Congress, state houses, and town halls, a handful of cities, states, and even some countries have moved to ban the use of facial recognition by public agencies. Most recently, the state of Maine restricted the use of facial recognition to a specific set of instances. Elsewhere, however, efforts to reign in the technology have fallen short. In West Lafayette, Ind., Mayor John Dennis recently vetoed a bill written to ban the use of facial recognition by public agencies. Meanwhile, the technology is becoming more powerful. Clearview AI sells a system that expanded the universe of known faces to some 10 billion photos on the public internet.

In written testimony at the congressional hearing with Williams, law professor Barry Friedman at New York University compared the regulatory environment surrounding facial recognition software to the “wild west.” It’s clear that facial recognition technology, however accurate it might be under ideal circumstances, can fall woefully short in practice. This is a failure of policy and technology and it’s likely that Williams won’t be the last person to bear the costs of it.

Editor’s note: This article incorrectly stated that police had killed more than 1,000 Black people in the year following George Floyd’s murder. In fact, according to data from the group Mapping Police Violence analyzed by Al Jazeera, police killed 1,068 people, not all of whom were Black. According to the tracking group, Black people are three times more likely to be killed by police than white people.. Racial bias and facial recognition. Black man in New Jersey arrested by police and spends ten days in jail after false face recognition match

Accuracy and racial bias concerns about facial recognition technology continue with the news of a lawsuit filed by a New Jersey man, Nijeer Parks, against local police, the prosecutor and the City of Woodbridge in New Jersey.

According to the New York Times, Nijeer Parks is the third person known to be falsely arrested for a crime he did not commit based on a bad face recognition match. The other two were Robert Williams and Michael Oliver. All three falsely arrested men are black it is reported.

This particular case began on a Saturday in January 2019, when two police officers showed up at the Hampton Inn in Woodbridge (New Jersey) after receiving a report about a man stealing snacks from the gift shop.

The crime

The alleged shoplifter was a black man, nearly 6 feet tall, wearing a black jacket, who was reportedly visiting a Hertz office in the hotel lobby, trying to get the rental agreement for a gray Dodge Challenger extended.

The officers confronted him, and he apologised, according to the police report. According to the New York Times, the suspect said he would pay for the snacks and gave the officers a Tennessee driver’s license.

When the officers checked the license, they discovered it was a fake drivers licence. This, coupled a bag of suspected marijuana in the man’s jacket, resulted in the officers trying to arrest the suspect. But the suspect ran away and drove off in his rental car, hitting a parked police car in the process, as well as a column in front of the hotel.

One of the police officers had to reportedly jump out of the way of the vehicle to avoid being hit.

The rental car was later found abandoned in a parking lot a mile away.

What happened next is that a detective in the Woodbridge Police Department sent the photo from the fake driver’s license to state agencies that had access to face recognition technology.

The next day, state investigators said they had a facial recognition match: which happened to be Nijeer Parks, who lived in Paterson, N.J., 30 miles away, and worked at a grocery store.

The detective compared Parks’s New Jersey state ID with the fake Tennessee driver’s license and agreed it was the same person. After a Hertz employee confirmed that the license photo was of the shoplifter, the police issued a warrant for Parks’s arrest.

“I don’t think he looks like me,” Parks was quoted as saying. “The only thing we have in common is the beard.”

Parks it should be noted has previous criminal convictions for selling drugs.

The arrest

The only problem for the police was that Parks was 30 miles away at the time of the incident, but that did not stop Parks being arrested by local police and spending ten days in jail, coupled paying around $5,000 to defend himself.

Parks was able to get proof from Western Union that he had been sending money at a pharmacy in Haledon (New Jersey), when the incident happened.

Parks told the judge he was willing to go to trial to defend himself. But a few months later in November 2019, his case was dismissed for lack of evidence.

Parks is now reportedly suing the police, the prosecutor and the City of Woodbridge for false arrest, false imprisonment and violation of his civil rights.

“I was locked up for no reason,” Parks reportedly said. “I’ve seen it happen to other people. I’ve seen it on the news. I just never thought it would happen to me. It was a very scary ordeal.”

The case drew the attention of the American Civil Liberties Union (ACLU).

“Multiple people have now come forward about being wrongfully arrested because of this flawed and privacy-invading surveillance technology,” Nathan Freed Wessler, senior staff attorney for the ACLU’s Speech, Privacy, and Technology Project told Silicon UK.

“There are likely many more wrongful interrogations, arrests, and possibly even convictions because of this technology that we still do not know about,” said Freed Wessler. “Unsurprisingly, all three false arrests that we know about have been of Black men, further demonstrating how this technology disproportionately harms the Black community. Law enforcement use of face recognition technology must be stopped immediately.”

Controversial tech

On this side of the pond, the use of facial recognition, especially by authorities, has also proven to be controversial.

In August the Court of the Appeal ruled that the use of automatic facial recognition (APR) by South Wales Police had breached privacy rights, data protection laws and equality legislation.

And in 2019 the Information Commissioner’s Office (ICO) warned that any organisation using facial recognition technology, and which then scans large databases of people to check for a match, is processing personal data, and that “the potential threat to privacy should concern us all.”

Indeed in 2019 an academic study found that 81 percent of ‘suspects’ flagged by Met’s police facial recognition technology were innocent, and that the overwhelming majority of people identified are not on police wanted lists.

And in August 2019, the ACLU civil rights campaign group in the United States ran a demonstration to show how inaccurate facial recognition systems can be.

It ran a picture of every California state legislator through a facial-recognition program that matches facial pictures to a database of 25,000 criminal mugshots.

That test saw the facial recognition program falsely flag 26 legislators as criminals.

Despite that, in July 2019 then Home Secretary Sajid Javid gave his backing to police forces using facial recognition systems, despite growing concern about the technology.

Tech boycott

Facial recognition systems have also been previously criticised in the US after research by the Government Accountability Office found that FBI algorithms were inaccurate 14 percent of the time, as well as being more likely to misidentify black people.

And tech firms have begun boycotting the supplying of the tech to police forces.

Microsoft first refused to install facial recognition technology for a US police force a year or so ago, due to concerns about artificial intelligence (AI) bias.

This boycott was subsequently been joined by Amazon and IBM, among others.

Microsoft has also deleted a large facial recognition database, that was said to have contained 10 million images that were used to train facial recognition systems.

San Francisco banned the use of facial recognition technology, meaning that local agencies, such as the local police force and other city agencies such as transportation would not be able to utilise the technology in any of their systems.

But the police remain in favour of its use.

In February this year, the UK’s most senior police officer, Metropolitan Police Commissioner Cressida Dick, said criticism of the tech was “highly inaccurate or highly ill informed.”

She also said facial recognition was less concerning to many than a knife in the chest.