July 27, 2020

To Facebook:

The undersigned – activists, journalists, human rights organizations – call on you to stop the spread of violence and hate-inciting speech on your services in Ethiopia. On June 29th, 2020, Haacaaluu Hundeessa, an Oromo musician and social activist, was shot and killed in Addis Ababa. Following his death, parts of Ethiopia were engulfed in protests, unrest, and violence. Since then, more than 160 people have been killed. The state-ordered internet shutdown has obscured any human rights violations perpetrated by government authorities and others, and prevented reporting and documentation.



The offline troubles that rocked the country are fully visible on the online space. The actors that are instigating violence offline also incite violence and propagate hate in the country online. Content shared online in text, livestream, and other formats called for violence, discrimination, and destruction of property of different ethnic groups. This is not the first time this has happened; previous incidents have led to similar situations.

We understand that incitement to violence is a complex issue where government action – or lack thereof – plays a key role in its materialization. Companies, including those that provide and curate a platform for communication, have a responsibility under human rights law “to prevent or mitigate adverse human rights impacts that are directly linked to their […]…services” and have the obligation to “[remedy] any adverse human rights impacts they cause or to which they contribute.” So far, Facebook has failed to prevent the escalation of incitement to violence on its services, and particularly in Ethiopia.

This is not the first time Facebook has neglected its responsibility to respect human rights, or to offer remedy for abuses to the extent it has contributed to them, in Ethiopia or other parts of the world. For instance, according to the Independent International Fact-Finding Mission on Myanmar, Facebook was used to campaign against and spread anti-Rohingya Muslim sentiment, and the company has admitted that “[it] weren’t doing enough to help prevent [the] platform from being used to foment division and incite offline violence” specifically against the Rohingya Muslims. This lack of action and its effects are still evident in Myanmar and where Rohingya refugees reside.

As human rights organizations, journalists, and activists, we are seeing the negative impact that content on Facebook that incites violence has on the communities we serve. This content can lead to physical violence and other acts of hostility and discrimination against minority groups. Based on international human rights law, content that meets the threshold of incitement to violence, hostility and discrimination does not belong to the protective scope of the right to freedom of expression. Despite the real risk it carries for minority groups and others in Ethiopia, such content remains online and visible on the platform.

Concerned individuals, organizations, and communities have warned Facebook and other social media platforms repeatedly on numerous occasions in private and in public about the imminence of the escalation we are seeing now. David Kaye, the former U.N Special Rapporteur on Freedom of Expression, during his last visit to Ethiopia, called on social media platforms, including Facebook, to regularly engage with Ethiopian authorities and civil society. Specifically, he requested that “at a minimum, [social media platforms] establish regular and rapid-reaction mechanisms to enable civil society to report the most concerning kinds of content on their platforms.”

We call on Facebook to address incitement to violence in Ethiopia as a priority and take these immediate and long-term mitigation measures:

Immediate actions:

Make content reporting on Facebook services in Afaan Oromo and Tigrinya languages fully available (it is already available in Ahmaric and English).

Do not allow amplification of content that incites violence and discrimination , whether by content recommendation systems that are in use or as a result of targeted advertising practices.

, Consider transparent and temporary changes to limit massive sharing functionalities in specific cases where there is an imminent risk of human rights abuse . Additionally, add specific, transparent, and temporary modifications to user interfaces to add tags to content that help users contextualize information.

Preserve restricted content that Facebook makes unavailable that incites violence and discrimination, as it could serve as evidence for victims and organizations seeking to hold perpetrators accountable. Ensure said content is available to victims, organizations, and international and national judicial authorities without undue delay.

Inform Ethiopian users about reporting mechanisms and relevant platform rules or guidelines. Reporting incitement to violence should be easy and intuitive. Facebook can achieve this, among other ways, by making adjustments to its user interfaces, pinned news feed announcements, etc. Facebook should invest in advertising, even in traditional media, to make sure that users know about these mechanisms and are able to use them.

Establish early warnings systems for emergency escalation that will help detect imminent harm to the physical security of individuals. Facebook should develop early warning systems in close cooperation with all relevant stakeholders operating at the grassroots level in Ethiopia, including civil society organizations and human rights experts. Facebook should ensure these systems enable trusted national partners to evaluate their performance regularly.

Essential long-term mitigation efforts:

Add significant resources to right-respecting content moderation efforts. Facebook should ensure that it recruits a sufficient number of content reviewers that have the required skills in all languages spoken in Ethiopia. The local-language content reviewers should also demonstrate sufficient understanding of the national political, social, historical, and cultural context in the country. The ratio of Facebook moderators has to correspond adequately with the number of Facebook users in Ethiopia. Sources estimate that Facebook ads can reach up to 6 – 7 million users in Ethiopia.

Content-moderation activities concerning Ethiopia should be coordinated by individuals that understand the dynamic local context , and they should be able to coordinate locally. These individuals should have the capacity to appropriately identify and assess calls for violence and must be able to take the necessary measures to mitigate harm.

Enforce meaningful and robust transparency initiatives about policies, standards, and practices for identification, removal, or other restrictions of online content that incites violence, propagates hates, and discrimination in accordance with human rights laws. Such data should be publicly released on a regular basis and, at minimum, should contain the following information: number and type of content violation, number of received complaints and average response time, and number of content removals as well as pages and disabled accounts.

Facilitate access to the internet for trusted partners during any internet shutdowns. Trusted partners can’t collaborate with Facebook to report content if they are deliberately cut off from the platform or the internet. Even though the majority of Ethiopians do not have access to the internet, the diaspora can still propagate violence-inciting content.

Actively support and help to develop initiatives that promote human rights, tolerance, diversity, and equality for all people in Ethiopia. Engage in efforts promoting media literacy and help users distinguish credible news sources from propaganda and disinformation. In doing so, develop strong and continuous cooperation with trusted partners, independent media organizations, individuals, and flaggers on the ground, especially when matters of the highest public interest are at stake or if activities are likely to escalate violence.

Coordinate the efforts of global, regional, and local offices and staff to provide timely and coherent decision-making, led by human rights officers that are well versed in the dynamic context in Ethiopia. There are clear gaps between the Africa Policy Program managers who engage with trusted partners, civil society in Ethiopia, and the Facebook team that handles public policy issues. But the involvement of experts specializing in human rights issues should be a priority, with cross-functional reach to product, engineering, marketing, emergencies, elections, and other teams as necessary.

Finally, we call on Facebook and other dominant social media platforms, including private messaging services, to conduct in-depth human rights impact assessments for their products, policies, and operations, based on the national context, before they enter any new market . These assessments should be publicly available and translated to relevant local languages. This is particularly important for regions of the world that suffer from volatile ethnic, religious, political, or other social tensions. Private actors should employ measures to reduce risks as much as possible.

These requests do not preclude other efforts Facebook should make to help end incitement to violence in Ethiopia. We propose an initial set of long-overdue measures that must be implemented urgently.. On July 18, two Ethiopian protest groups

clashed

in front of the Ethiopian embassy in Washington, DC, in the United States, as Ethiopia’s tense national politics spread to its diasporas.

Protesters demonstrated their opposition to the current government, waving flags missing the characteristic yellow pentagram, a symbol of Ethiopia’s collective national identity.

Pro-government supporters, mostly of Oromo ethnicity, marched to confront the anti-government protesters. They wore red, yellow and green, Ethiopia’s national colors, and carried Oromo Liberation Front flags.

Demonstrators got close enough to hurl insults at each other.

But this real-life confrontation pales in comparison to the intense, bitter political agitation taking place in every corner of Ethiopian social media, and especially on Facebook. Ethiopians use Facebook more than any other social media platform and many citizens consider Facebook to be the “internet.”

Since last year, Ethiopia has undergone a much-lauded transition from dictatorship to democracy. Many Ethiopians embraced the transition spearheaded by Prime Minister Abiy Ahmed. But the euphoria accompanying Ethiopia’s embrace of civil liberties has deflated as political rifts resurfaced between non-ruling elites of Ethiopia's two main ethnic groups: the Amharas and the Oromos.

Tensions between the two groups manifest online as Facebook Live and YouTube videos featuring Ethiopian personalities with strong opinions and political viewpoints. Hundreds of Facebook pages and YouTube channels create content for Ethiopia's diverse cultural and language groups. In Ethiopia, there are about 83 native languages spoken but Amharic, Afan Oromo, Tigrinya, and Somali are Ethiopia's major internet languages.

These widely shared videos shape and inform the current discourse on Ethiopian politics, both at home and among its large diaspora, often exchanging and amplifying mis- and disinformation originating with these videos.

Online conspiracy theories, political rants and rumors laced with communal hatred are now common genres in Ethiopian social media.

Almaz, an Amhara anti-government protester who is using a pseudonym to protect herself from abusive attacks online, was one among the incensed crowds at the Washington D.C. demonstration. Her experience of social media organizing and agitation is typical of many of the other protesters. She first heard about the anti-government demonstration via Facebook and follows a range of anti-government activists on the platform.

Since she moved to the US in September 2018, she has been using Facebook to follow the news from home — especially Facebook’s live streaming services.

Almaz and her friends use the newsfeed to both signal interest in certain stories and to coordinate watch parties, in which like-minded viewers gather online to watch popular live streams. She often joins parties to watch Yoseph Yitna, a conspiracy theorist who broadcasts daily from abroad about Ethiopian politics.

As part of its democratic reforms, the new government stopped blocking websites from Ethiopian diaspora opposition groups and ended politically-motivated content filtering. It has decreased surveillance and harassment of journalists and opened up the telecom market. In the absence of developed local media institutions, Facebook has become the primary portal for news and information for Ethiopian internet users.

For every political development, a fresh set of broadcasts appears, mainly on Facebook and increasingly on YouTube in Amharic and Afan Oromo.

Almaz also tunes into political ranter Yoni Magna for news analysis on Facebook.

Manga launched his rage brand of social media in 2015 and regularly offers inflammatory comments ranging from current political events to religion to pop culture in Amharic language. His tagline reads “Thanks to the Mother (Mary) and Her Son” and he describes himself as a truth-advocate for Ethiopians. His Facebook page has a significant and engaged audience. At the moment, he has about 161,000 followers and his YouTube channel has been viewed more than 29 million times. On a typical show, he talks for nearly an hour and bounces from topic to topic. He shouts, feigns menace, and insults his critics with ethnic slurs that target various ethnic and religious groups.

Almaz also watches dozens of Facebook live videos by Tolosa Ibsa, a Facebook conspiracy theorist whose YouTube channel has been viewed more than 11 million times. Gigi Kiya, a commentator who also trolls her opponents, garners about 10 million views on her YouTube channel.

These formerly fringe Ethiopian social media figures have benefited from Ethiopia’s new opening. They are mostly diaspora-based monologuists broadcasting from their living rooms, complaining vigorously about the Ethiopian government, and attacking each other. Facebook is their headquarters, although some have migrated to YouTube, to monetize their work through ads.

Many of these content creators share inaccurate and blatantly false reporting. They run Facebook pages populated by people who share similar political views, hardening political differences by creating echo chambers, information-cascade and filter bubble effects among Ethiopia’s diverse ethnolinguistic groups.

The comment sections of Facebook live broadcasts from these figures fill with people throwing insults and memes at each other.

Amhara regional ‘coup’ attempt: A disinformation case

Over the past few weeks, Almaz incessantly shared Facebook Live video links and posts about an incident that happened in June 2019 in Amhara, Ethiopia's second-largest region. On June 22, an armed group murdered the region’s president along with his adviser and the region's attorney general in an alleged failed regional coup attempt. General Asaminew Tsige, chief of security forces in the Amhara region, commanded the armed group. This incident was triggered by a simmering intra-party dispute in the region’s governing party, Amhara Democratic Party (ADP).

On Facebook, rumors swirled that the assassinations were part of a plot created by Prime Minister Abiy Ahmed, who is an Oromo, to wipe out the leadership of the Amhara people.

The misinformation epidemic via Facebook live video transcends ideological and ethnic divides.

Another pro-government protester at the DC rally, who uses the name Yoseph, just returned to the US following a visit to Ethiopia after 15 years of exile.

Like Almaz, Yoseph heard about the demonstration on Facebook, but from a completely different universe of Facebook live video broadcasters. For his daily media diet, Yoseph usually visits the Facebook pages of Ambo Urge and Hangaasa Ahmed Ibraahim, two fervent supporters of Ethiopia’s prime minister who broadcast unsubstantiated claims about people who do not support him. They regularly bash citizens who oppose Abiy as settlers who are nostalgic for Ethiopia’s pre-1974 imperial era, where Ethiopians were merely considered as ”a collection of imperial subjects” rather than citizens. Yoseph shares videos that exaggerate Abiy's heroism and spreads rumors about his rivals.

Campaign urges Facebook to act

Earlier in July, activists and concerned Ethiopians began a social media campaign to limit misinformation and hateful content on Facebook. They flagged violent content to Facebook and encouraged others to report them using Facebook’s on-site reporting tool. One activist who asked Global Voices to remain anonymous says he flagged content that ranges from bigoted memes to inflammatory videos, yet most of the content he reported remained on Facebook. For example, he reported a YouTube video of an ethnic extremist who threatened to kill Amharas and their children who live in Oromia. The truncated version of the video was still circulating on Facebook at the time of publication of this article.

A Twitter user flagged another ethnic extremist who called on Facebook for Amhara women to poison their Oromo husbands to wipe out the Oromos.

In this video, this man speaking in Amharic calls for a genocide against ethnic #Oromo. He is appealing to all Amhara wives who are married to Oromo men to poison their husbands. I’m only translating his message for the world to see it. @BBCAfrica @USEmbassyAddis @hrw #Ethiopia pic.twitter.com/YxotYfXjty — Alex A (@lakimlakim) July 21, 2019

These are just a few examples of the estimated hundreds of thousands of hours of videos streamed in at least five major Ethiopian languages. Activists are frustrated that most malicious content flagged is not removed, leaving them to wonder if Facebook cares about the damage it might be causing in Ethiopia.

Facebook has recently employed an Ethiopian as one of its market specialists for sub-Saharan Africa. According to Facebook, markets specialists ”play a key role within the global community operations team by keeping the platform safe, vibrant and diverse.”

However, it is not clear if the employee's responsibilities include content moderation of Amharic videos. It also remains unclear whether Facebook and other platforms are investing enough resources to address problematic content in Ethiopia such as disinformation and hate speech.. London CNN —

Facebook employees repeatedly sounded the alarm on the company’s failure to curb the spread of posts inciting violence in “at risk” countries like Ethiopia, where a civil war has raged for the past year, internal documents seen by CNN show.

The social media giant ranks Ethiopia in its highest priority tier for countries at risk of conflict, but the documents reveal that Facebook’s moderation efforts were no match for the flood of inflammatory content on its platform.

The documents are among dozens of disclosures made to the US Securities and Exchange Commission (SEC) and provided to Congress in redacted form by Facebook whistleblower Frances Haugen’s legal counsel. A consortium of 17 US news organizations, including CNN, has reviewed the redacted versions received by Congress.

They show employees warning managers about how Facebook was being used by “problematic actors,” including states and foreign organizations, to spread hate speech and content inciting violence in Ethiopia and other developing countries, where its user base is large and growing. Facebook estimates it has 1.84 billion daily active users – 72% of which are outside North America and Europe, according to its annual SEC filing for 2020.

The documents also indicate that the company has, in many cases, failed to adequately scale up staff or add local language resources to protect people in these places.

Facebook used by militias ‘to seed calls for violence’

The reports CNN has obtained provide further insights into the scale of the problem in Ethiopia, elements of which were reported by The Wall Street Journal last month.

CNN’s publication of these warnings from within Facebook comes seven months after a Facebook team initially shared an internal report entitled “Coordinated Social Harm.”

The report, distributed in March, said that armed groups in Ethiopia were using the platform to incite violence against ethnic minorities in the “context of civil war.” At that time, a conflict in the country’s northern Tigray region between its former ruling party, the Tigray People’s Liberation Front (TPLF), and the Ethiopian government had been rumbling on for five months. Intermittent internet blackouts and media restrictions had obscured much of the fighting.

A destroyed tank on a roadside in western Tigray in May. Ben Curtis/AP

Ethiopia is an ethnically and religiously diverse nation of about 110 million people who speak scores of languages. Its two largest ethnic groups, the Oromo and Amhara, make up more than 60% of the population. The Tigrayans, the third largest, are around 7%.

One of the groups flagged in the March report was the “Fano,” an ethnic Amhara militia group with a reputation for brutality that has been drawn into the war in Tigray, sometimes fighting alongside Ethiopian government forces. Facebook said it had observed a cluster of accounts affiliated with the militia group, including some based in Sudan, using its platform to “seed calls for violence,” promote armed conflict, recruit and fundraise.

Since the war started last November, the Fano militia have been linked by displaced Tigrayans to human rights abuses, including the killings of civilians, looting and rape, according to the United Nations rights office, Amnesty International and other human rights groups.

Though the Facebook team said it had recommended the Fano-affiliated network be taken down, it suggested that other bad actors promoting violence on its platform were simultaneously slipping through the cracks. In a headline in bold, the team warned: “Current mitigation strategies are not enough.”

The Facebook documents also detail the platform’s removal of a cluster of accounts linked to the Oromo diaspora, mostly based in Egypt, which was targeting Ethiopian audiences with highly inflammatory content, including “explicit calls to violence against government officials and other ethnic groups.” One inciteful post highlighted in a report shared a photo of what appears to be a Molotov cocktail being lit and the statement: “Burn the whole country down.”

The whistleblower, Haugen, said one of her core motivations for gathering the internal documents was bringing to light “how badly Facebook is handling places like Ethiopia,” where she suggested engagement-based ranking was fanning ethnic violence.

“I genuinely fear that a huge number of people are going to die in the next five to ten years, or twenty years, because of choices and underfunding” by Facebook, Haugen said.

In comments made to the consortium, Haugen emphasized the vast difference between the integrity and security systems rolled out by Facebook in the United States versus the rest of the world, adding that the company was not adequately policing its platform in most non-English languages.

“The raw version [of Facebook] roaming wild in most of the world doesn’t have any of the things that make it kind of palatable in the United States, and I genuinely think there’s a lot of lives on the line – that Myanmar and Ethiopia are like the opening chapter,” she said.

A Facebook spokesperson told CNN that the company has been “actively focused on Ethiopia.”

“Over the past two years we have actively invested to add more staff with local expertise, operational resources and additional review capacity to expand the number of local languages we support to include Amharic, Oromo, Somali and Tigrinya. We have worked to improve our proactive detection so that we can remove more harmful content at scale. We have also partnered extensively with international and local experts to better understand and mitigate the biggest risks on the platform,” the spokesperson said.

Current mitigation strategies are not enough

None of the revelations from the Facebook documents are news to activists and human rights groups, who have warned for years that the social media giant has made insufficient efforts to protect human rights in Ethiopia, Africa’s second most populous country.

A busy street in the Ethiopian capital Addis Ababa last December. Minasse Wondimu Hailu/Anadolu Agency/Getty Images

Some politicians and civil society groups said that if no action was taken, the platform risked repeating the same mistakes it made in Myanmar – now a case study in the deadly impact that hate speech shared on Facebook can have.

In 2018, the UN slammed Facebook’s role in the Myanmar crisis, which the global body said, “bore the hallmarks of genocide.” By promoting violence and hatred against the minority Rohingya population, the UN said Facebook had “turned into a beast.” The social media company later acknowledged that it didn’t do enough to prevent its platform being used to fuel bloodshed, and Chief Executive Mark Zuckerberg wrote an open letter apologizing to activists and promising to increase its moderation efforts.

Much like in Myanmar, Facebook’s rise in popularity in Ethiopia came at a moment of rapid political and societal change, which helped to boost the platform’s growth. In 2018, Abiy Ahmed was appointed Prime Minister and launched a series of reforms, including freeing thousands of political prisoners and lifting restrictions on the press.

But as Ethiopians began to use Facebook to engage in public debate, observers saw that the platform was being abused by a variety of actors, including politicians, to incite discrimination and violence.

Former UN special rapporteur for freedom of expression David Kaye told CNN that this problem came up repeatedly in conversations with civil society groups during his trip to Ethiopia in December 2019: “It was on everybody’s radar that there could be real spill over from the platform to offline harm.”

“Given the experience in Myanmar, it was really incumbent on Facebook to do a human rights impact assessment and evaluate what they needed to do so that Facebook in Ethiopia didn’t become a place for incitement to violence,” Kaye said, adding that he didn’t know what that assessment looked like or if it was done.

In June 2020, a Facebook employee posted a report to an internal group with about 1,500 members called “At Risk Countries FYI” recapping an ongoing audit into how well its Artificial Intelligence and other signals, like third party fact checkers, worked in the most at-risk countries where the platform operates.

“We found significant gaps in our coverage (especially in Myanmar & Ethiopia), showcasing that our current signals may be inadequate,” the employee wrote, sharing a spreadsheet with a list of at-risk countries and the languages supported by the platform in each.

The spreadsheet showed that Facebook had failed to build automated systems, called classifiers, to detect misinformation or hate speech in Oromo or Amharic – two of the most widely spoken languages spoken in Ethiopia.

A Tigrayan man looks for cellular service on a mountain overlooking Um Rakuba refugee camp in eastern Sudan in January. Abdulmonam Eassa/Getty Images

Even as the conflict in Tigray escalated, Haugen said she had only found evidence that Facebook had allocated “even slight language support” in two of the country’s many native languages.

Facebook says it does not believe it should be the “arbiters of truth,” so the firm relies on third-party fact-checking organizations to identify, review and rate potential misinformation on its platforms.

Facebook has partnered with two such organizations in Ethiopia: AFP Fact Check and PesaCheck, an East Africa-based non-profit initiative run by Code for Africa.

PesaCheck has five full-time Ethiopian fact-checkers working in four languages – Oromo, Amharic, Tigrinya and English – but says it recently had to relocate one staff member from Ethiopia due to intimidation. AFP Fact Check employs one fact-checker in Ethiopia, Amanuel Neguede, who reviews content in Amharic and English.

Each day, Neguede told CNN that he reviews thousands of posts on an internal Facebook tool, which surfaces content flagged as false or misleading through a combination of AI and human moderators. Originally, Neguede said that AFP only had access to English-language content in Ethiopia through the tool, which would surface only limited content each day.

The tool began serving AFP Amharic-language content in May, and now the number of claims he says he sees on a daily basis has drastically risen. The tool does not always accurately identify mis- and dis-information, but Neguede says it helps with his work.

“I’ve seen a lot of a lot of hate speech, that definitely does fuel ethnic violence in Ethiopia,” Neguede said.

“Whenever there’s a major offensive, for example that’s happening in the north, we can see a lot of images of conflict that’s happened in different countries used in a misleading context. I would say that most of the time we’ll see posts surface – especially posts that are widely shared – after real news events.”

But researchers like Berhan Taye say Facebook in Ethiopia is in desperate need of more content moderators, pointing to how the platform was used to stoke a wave of deadly violence after the murder of Oromo musician Hachalu Hundessa last year. Taye, then a policy manager at digital rights group Access Now, recalls watching livestreams of lynchings and posts calling for the targeted extermination of certain ethnic groups, following Hundessa’s death. She penned an open letter calling on Facebook to take action to protect Ethiopians. She says little has changed since.

Buldings burned by a mob in Ethiopia's Oromo region during a wave of violence following Hachalu Hundessa's murder last July. AFP/Getty Images

For that reason, Taye, now an independent Nairobi-based analyst, works with grassroots volunteers to collate misinformation and hate speech they spot on the platform into Excel spreadsheets, which they then send on to Facebook for removal. But she says that much of what they flag – including posts calling for the extermination of certain ethnic groups – does not get taken down and, occasionally, the company has responded to activists asking for them to translate posts.

Facebook says it has improved its reporting tools for people in Ethiopia to make it faster and easier for them to report content they believe violates its community standards, which are now available in Amharic and recently launched in Oromo. It says it has also established reporting channels for international and local human rights groups and civil society organizations to flag potentially harmful content in Ethiopia.

Content moderation is extremely dangerous work in Ethiopia, Taye says, adding that she has personally been accused of siding with a person or group because a post was removed. The task is also mentally exhausting; volunteers spend hours reviewing graphic content, often doing it alongside full-time jobs and raising their kids. Taye said it was unacceptable that poor people have effectively been left to the “dirty work” of one of the world’s richest companies.

Facebook will not reveal exactly how many local language speakers are evaluating content in Ethiopia that has been flagged as possibly violating its standards, or how much it has invested in resources to better police its platform in the country. A Facebook spokesperson said the company had invested “$13 billion and have 40,000 people working on the safety and security on our platform, including 15,000 people who review content in more than 70 languages working in more than 20 locations all across the world to support our community. Our third party fact-checking program includes over 80 partners who review content in over 60 languages, and 70 of those fact checkers are outside of the US.”

“Our track record shows that we crack down on abuse outside the US with the same intensity that we apply in the US,” the spokesperson added.

A war on social media

Since war broke out in Tigray, supporters of both the government and TPLF have waged a parallel fight on social media, creating a virtual battleground of toxic ethnic and religious hatred. But it’s difficult to determine when atrocities unfolding on the ground are the direct result of hateful content shared online.

The head of the US Agency for International Development Samantha Power expressed concern in August about the “inflamed” and “dehumanizing rhetoric” that she said Ethiopia’s leaders were invoking amid the conflict in Tigray, whose forces were described by Prime Minister Abiy Ahmed as “weeds” and “cancer” in a post shared on Facebook. Power emphasized that “increasingly virulent speech” used by the prime minister and other officials, also shared on social media, “often accompanies ethnically-motivated atrocities.”

Abiy’s language was also condemned by the UN special adviser on the prevention of genocide Alice Wairimu Nderitu, who warned that “hate speech, together with its propagation through social media is part of a worrisome trend that contributes to further fuel ethnic tensions in the country.”

Activists say that divisive language has been echoed by Abiy’s supporters online.

CNN has reached out to the Ethiopian prime minister’s office for comment.

In June, days before Ethiopia’s national elections, which Abiy won in a landslide, Facebook said it removed a network of fake accounts targeting domestic users primarily in Amharic. Facebook linked the accounts to individuals associated with Ethiopia’s Information Network Security Agency (INSA), the cybersecurity agency that Abiy established and ran before becoming Prime Minister.

People watch Prime Minister Abiy Ahmed's swearing-in ceremony at an Addis Ababa coffee shop in October. Amanuel Sileshi/AFP/Getty Images

A Facebook page with an INSA-affiliated administrator was also flagged in internal Facebook documents in March, but was not recommended for removal.

As the conflict escalates online and off, the Ethiopian government has accused Facebook of blocking user and removing posts “disseminating the true reality about Ethiopia.”

And in the latest sign of its efforts to consolidate control over Ethiopia’s information landscape, the government announced in August it had begun developing its own social media platform to rival and replace Facebook, WhatsApp and Twitter.. . On November 3, 2021, Meareg Amare, a professor of chemistry at Bahir Dar University in Ethiopia, was gunned down outside his home. Amare, who was ethnically Tigrayan, had been targeted in a series of Facebook posts the month before, alleging that he had stolen equipment from the university, sold it, and used the proceeds to buy property. In the comments, people called for his death. Amare’s son, researcher Abrham Amare, appealed to Facebook to have the posts removed but heard nothing back for weeks. Eight days after his father’s murder, Abrham received a response from Facebook: One of the posts targeting his father, shared by a page with more than 50,000 followers, had been removed.

"I hold Facebook personally responsible for my father's murder," he says.

Today, Abrham, as well as fellow researcher and Amnesty International legal adviser Fisseha Tekle, filed a lawsuit against Meta in Kenya, alleging that the company has allowed hate speech to run rampant on the platform, causing widespread violence. The suit calls for the company to deprioritize hateful content in the platform’s algorithm and to add to its content moderation staff.

“Facebook can no longer be allowed to prioritize profit at the expense of our communities. Like the radio in Rwanda, Facebook has fanned the flames of war in Ethiopia,” says Rosa Curling, director of Foxglove, a UK-based nonprofit that tackles human rights abuses by global technology giants. The organization is supporting the petition. “The company has clear tools available—adjust their algorithms to demote viral hate, hire more local staff and ensure they are well-paid, and that their work is safe and fair—to prevent that from continuing.”

Since 2020, Ethiopia has been embroiled in civil war. Prime Minister Abiy Ahmed responded to attacks on federal military bases by sending troops into Tigray, a region in the country’s north that borders neighboring Eritrea. An April report released by Amnesty International and Human Rights Watch found substantial evidence of crimes against humanity and a campaign of ethnic cleansing against ethnic Tigrayans by Ethiopian government forces.

Fisseha Tekle, Amnesty International's lead Ethiopia researcher, has further implicated Facebook in propagating abusive content, which, according to the petition, endangered the lives of his family. Since 2021, Amnesty and Tekle have drawn widespread rebuke from supporters of Ethiopia’s Tigray campaign—seemingly for not placing the blame for wartime atrocities squarely at the feet of Tigrayan separatists. In fact, Tekle’s research into the countless crimes against humanity amid the conflict fingered belligerents on all sides, finding the separatists and federal Ethiopian government mutually culpable for systematic murders and rapes of civilians. Tekle told reporters during an October press conference: “There’s no innocent party which has not committed human rights violations in this conflict.”

In a statement Foxglove shared with WIRED, Tekle spoke of witnessing “firsthand” Facebook’s alleged role in tarnishing research aimed at shining a light on government-sponsored massacres, describing social media platforms perpetuating hate and disinformation as corrosive to the work of human rights defenders.. . In late October, Dejene Assefa, an Addis Ababa–based activist known for his appearances on state television in Ethiopia, posted a message to his more than 120,000 followers on Facebook. The post exhorted his compatriots to rise up across the country and murder members of the Tigrayan ethnic group. “The war is with those you grew up with, your neighbor,” he wrote in Amharic, one of the main languages of Ethiopia. “If you can rid your forest of these thorns … victory will be yours.” The message was shared nearly 900 times and attracted over 2,000 reactions. Many of the replies echoed the call to violence and promised to heed Dejene’s advice.

Ethiopia’s federal army is currently engaged in a brutal civil war with rebel groups, mostly from the Tigray region. In Addis Ababa, police have reportedly conducted citywide raids — dragging people identified as having Tigrayan ancestry out of homes, businesses, even churches. On Facebook, calls for the murder and mass internment of ethnic Tigrayans have proliferated.

“The content is some of the most terrifying I’ve ever seen anywhere,” Timnit Gebru, a former Google data scientist and leading expert on bias in AI, who is fluent in Amharic, told Rest of World. “It was literally a clear and urgent call to genocide. This is reminiscent of what was seen on Radio Mille Collines in Rwanda.” Radio Television Libre des Mille Collines, a station set up by Hutu extremists in Rwanda, broadcast calls to violence that helped spark the genocide in the country in 1994.

Facebook knows the risks of misinformation and hate speech in Ethiopia, and it knows it doesn’t have a handle on dangerous content. In internal messages and documents, released as part of the so-called Facebook Papers leaks and seen by Rest of World, the company acknowledged in 2020 that it has insufficient moderation capabilities in Amharic, and that language barriers have prevented users from reporting problematic content. To try to fill the gap in its understanding of the context in Ethiopia, the company proposed using “network-based models,” an opaque, data-driven and experimental mechanism.

The recent surge in hate speech on Facebook in Ethiopia, which has been linked to violence, demonstrates that the company has still not fixed its problems in the country, a year into a civil war that has divided the country along ethnic lines.

“What I saw was shocking. It was not one random person with 10 or 100 or 1,000 followers. It was a group of leaders, with hundreds of thousands of followers, clearly instructing people what to do,” Gebru said. “The most shocking part was the urgency, and the horrifying way in which the words were designed to make people act now.”

Ethiopia’s civil war broke out in late 2020, when the country’s prime minister, Abiy Ahmed, sent troops to the country’s northern Tigray region to oust fighters who had attacked federal military bases. At first, federal government forces, which were eventually backed by soldiers from neighboring Eritrea, seemed to have the upper hand, but they have since been pushed back by a sustained counterattack. Human rights groups have documented atrocities on both sides, but a joint report by the United Nations High Commissioner for Refugees has found abuses committed by government forces, including massacres of ethnic Tigrayans and the weaponized rape of what could amount to thousands of women. The U.S. government is considering declaring the campaign a genocide.

In Ethiopia, where journalists have been jailed and state media outlets censor all news of abuses by state and allied forces, the government’s response has been buttressed by an army of social media activists and personalities, who manufacture consent for the conduct of its forces. Several have large followings on Facebook, which has more than 6 million users in the country. These accounts have often singled out journalists, human rights activists, and anyone critical of the Ethiopian military, labeling them “traitors.”

Dehumanizing language targeting ethnic minorities has become normalized. In July, with the tide of war shifting, a frustrated Abiy Ahmed launched a Facebook tirade, vowing to crush the “cancerous” rebels he also described as “weeds.” Ethiopian government–backed Facebook accounts began using the terms to loosely refer to the entire ethnic Tigrayan population.

The rhetoric online was not just a reflection of the political environment in the country but is likely to have actively contributed to worsening violence.

Facebook’s internal documents show that this year it identified at least two campaigns by diaspora groups, one mainly based in Egypt, the other partly based in Sudan, which were allegedly trying to stoke ethnic divisions. One was affiliated to the Fano militia, which has been accused of human rights abuses, the other to an Oromo group which was calling for violence against the state. Oromo militants are also waging war against the federal government.

“Content on Facebook has had real-life impacts on civilians,” Yohannes Ayalew, a former lecturer in law at Bahir Dar University in Ethiopia, now a PhD candidate at Monash University in Australia, told Rest of World. He pointed to a surge in hate speech and calls for “revenge” on Facebook, after the murder of the Ethiopian singer and activist Hachalu Hundessa, in June 2020. That led to a surge of brutal mob violence, in which hundreds of ethnic Amhara civilians and members of other minorities in Ethiopia’s Oromia region were murdered.

“Content on Facebook has had real life impacts on civilians.”

During testimony before a U.S. senate subcommittee in October, Facebook whistleblower Frances Haugen said the company’s failures in Ethiopia could match those in Myanmar, where U.N. officials alleged that the company had played a prominent role in facilitating genocidal violence.

Internal documents show the reasons the company failed. Facebook knew it didn’t have sufficient coverage in local languages to actively identify hate speech or calls to violence. It also collected a low number of reports from users to help it identify problematic content, which it attributed to digital literacy, its reporting interfaces being confusing to Ethiopian users, and a lack of local language support.

In June 2020, employees reviewing the platform’s “signals” — the data, collected from users and partners, that it uses to understand problematic content — said that they found “significant gaps” in the most at-risk countries, especially in Myanmar and Ethiopia, “showcasing that our current signals may be inadequate.” A report this fall found that even among the “tier 1” at-risk countries, Ethiopia was an outlier, with the “lowest completion rate” for user reports.

The papers contain several references to how weak Facebook’s signals are in “ARCs” — at-risk countries.

Aware of its poor coverage in Ethiopia, Facebook proposed using a different approach to tackling its problem: network-based moderation. The company began to invest in earnest in this approach after the 2016 U.S. presidential election, in response to allegations of Russian interference using its platform, Evelyn Douek, a lecturer at the Harvard Law School and an expert in social media moderation, told Rest of World. Rather than using specific words or phrases to directly identify hate speech or misinformation, network-based moderation is based on identifying patterns of behavior that are consistent with malicious activity.

“You can understand why they might do it in countries where they don’t have the language capacity, because it doesn’t rely on understanding the content of individual posts,” Douek said.

However, Douek said that this is “an especially opaque form of content moderation,” and one that the company has released few details about. This form of moderation relies on the platform’s own research and data, which it rarely shares with external researchers, and on its own models.

Documents released as part of the Facebook Papers show that this approach is still experimental, and that it isn’t clear whether its models work in the context of hate speech — even in the U.S., where it is based and where it has the greatest volume of data. “The networks surrounding organized hate are another clear example of harmful networks that our current policies and procedures can’t handle,” one internal document, looking at the networks that share and boost certain content, including that of white supremacists content in the U.S., reads. Other documents refer to the complexity of mapping network effects, and how far the company has to go to understand them. Even so, staff seem keen to try these models out in Ethiopia and Myanmar, “where our classifier and language signals are weaker, and where network-based models may be able to help the most.”

A spokesperson for Meta, the recently renamed holding company that owns Facebook, said in a statement: “As of now, we haven’t used this new protocol to disrupt networks in Ethiopia.”

Douek echoed other researchers, saying that even if network-based moderation does what it’s supposed to, it’s very unlikely to be enough on its own. “You cannot enter a market without the language understanding or the contextual understanding or political expertise and expect this kind of moderation to be sufficient or prevent harm,” she said. “It’s just not adequate.”

Tigray Defense Force fighters rest south of the Ethiopian city of Mekelle in the country’s Tigray region, June 29, 2021. Finbarr O’Reilly/The New York Times/Redux

In one leaked document, Facebook staff said that once the company identifies a vulnerable country, either because of a spike in reported problematic speech or because of an active conflict there, it can take up to a year to then implement enforcement.

The reality may be longer. The company recognized its issues in Ethiopia as early as 2019, but in May 2021, in one conversation about measuring the prevalence of hate speech in the run-up to elections in Ethiopia, an employee wrote, “We don’t have coverage for Ethiopia due to lack of human review capacity there.”

Analysts said that the company clearly wasn’t ready for the latest upsurge in hate and calls for violence. Accusations of “treason” against ethnic Tigrayans have become commonplace over the last few months. In August, Ethiopian state media commentator Muktar Ousman, who has over 210,000 followers on Facebook, claimed exactly this; two months later, two ethnically Tigrayan university lecturers were murdered, acts that Muktar celebrated in postings on Facebook and Twitter, where he has 168,000 followers.

The violence has spread beyond Tigrayan communities. Another ethnic minority, the Qemant, has come under attack by government forces and allied militias. Thousands of Qemant civilians have fled their homes for the safety of neighboring Sudan this year.

Later in September, a Facebook post alleged, without evidence, that terrorists from a Qemant village hijacked a bus, a purported incident in which two people were killed. The post got hundreds of reactions. The next day, the village was pillaged and burnt down, an attack that lasted several days.

Between late October and early November, Tigrayan fighters made their most significant gains of the war, capturing the cities of Dessie and Kombolcha, about 400 kilometers away from the Ethiopian capital Addis Ababa. The loss of two strategic cities lying on the highways connecting the country to neighboring Djibouti led to another escalation in violent and sectarian rhetoric on Facebook. Ethiopian government–affiliated Facebook accounts accused local Tigrayan residents of the two cities of acting as spies for the rebels.

“While safety work in Ethiopia has been going on for a long time, we know that the risks on the ground right now are higher,” Mercy Ndegwa, public policy director for East and Horn of Africa at Meta, said in a statement sent to Rest of World. “We … stand ready to take additional action to meet the demands of this ongoing human rights situation.”

Facebook said it has recently designated Ethiopia a “temporary high-risk location,” and promised to remove posts promoting violence and misinformation. It said it had added the Oromo Liberation Army, an armed rebel group, to a list of blacklisted entities.

In October, researcher Gebru took to Twitter to express alarm about Assefa’s tirade and reported it to Facebook. It was nearly 24 hours before the post was taken down. It isn’t clear on what grounds it was removed, and Gebru said she was originally told the post didn’t violate the site’s community standards. The content was widely shared and can still be found, word-for-word, on pages of other government supporters.

In early November, Facebook removed a post by Prime Minister Abiy Ahmed, in which he called on citizens to rise and “bury” the rebels, for violating Facebook rules on inciting violence. It was an important intervention, analysts said, but a day after that Abiy’s post was taken down, Addis Ababa mayor Adanech Abiebie took to Facebook to applaud volunteers conducting citywide neighborhood-watch searches for rebel sympathizers and added, “Without any doubt the junta [a term used to refer to Tigrayan rebels] will be buried wherever they roam!” That post is yet to be removed from the platform.

“It seems to me that what Facebook is doing is a lip service for its bombshell criticisms in all corners,” Ayalew, from Monash University, said.. The son of an Ethiopian academic who was murdered during the country’s ongoing violence has filed a lawsuit against Facebook parent company Meta in Kenya’s High Court, alleging the social media giant is fueling violence and hate across eastern and southern Africa.

Abrham Meareg Amare claims in the lawsuit that his father, Meareg Amare Abrha, a Tigrayan professor, was gunned down in November 2021 after he was targeted on Facebook with hateful and inaccurate posts. He said he tried to get Facebook to remove some of the problematic content — including a post with a photo of his father — but he didn’t receive a response until after his father was killed.