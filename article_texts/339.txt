Look out, educators. You’re about to confront a pernicious new challenge that is spreading, kudzu-like, into your student writing assignments: papers augmented with artificial intelligence.

The first online article generator debuted in 2005. Now, A.I.-generated text can now be found in novels, fake news articles and real news articles, marketing campaigns, and dozens of other written products. The tech is either free or cheap to use, which places it in the hands of anyone. And it’s probably already burrowing into America’s classrooms right now.

Using an A.I. program is not “plagiarism” in the traditional sense—there’s no previous work for the student to copy, and thus no original for teachers’ plagiarism detectors to catch. Instead, a student first feeds text from either a single or multiple sources into the program to begin the process. The program then generates content by using a set of parameters on a topic, which then can be personalized to the writer’s specifications. With a little bit of practice, a student can use AI to write his or her paper in a fraction of the time that it would normally take to write an essay.

I wanted to see what would happen if I fed an A.I. text generator like Sudowrite some sample texts for assignments. (Sudowrite is just one such program; I could name more, but I’d rather not make it even easier for students to cheat.) The results showed that teachers must be made aware of what is coming to their classrooms.

Say a student wanted to write an essay about U.S.-China policy and decided to use an A.I. program to speed along the process. I fed it the first few lines from a recent Pentagon report: “The PRC is increasingly clear in its ambitions and intentions. Beijing seeks to reshape the international order to better align with its authoritarian system and national interests, as a vital component of its strategy to achieve the ‘great rejuvenation of the Chinese nation.’ ” The A.I. text generator provided these following sentences:

Advertisement

Advertisement

Advertisement

Advertisement

It is increasingly willing to disregard—or to actively undermine—existing international rules and norms when it feels its vital interests are at stake. The U.S. and its allies face a rapidly modernizing military, whose capabilities are growing rapidly, especially in the air and at sea. China has also made great strides in its development of cyber capabilities, and it is focused on the ability to degrade or disrupt critical U.S. military technology, as well as on targeting civilian infrastructure. In addition to being a challenge to the U.S. military and its allies, China’s activities in cyberspace have the potential to degrade U.S. political and economic institutions, and the global information infrastructure is at risk.

Advertisement

If a high school student turned that in verbatim, their teacher would probably suspect that there was something amiss. But if this were turned in at the undergraduate or graduate level, and especially if the student were to edit the text, it would be much harder to detect.

Or let’s say they need to write a paper about climate change. Here’s what came up when I fed Sudowrite the first few lines from an article on climate change impacts from the National Oceanic and Atmospheric Administration website:

Advertisement

Though we often think about human-induced climate change as something that will happen in the future, it is an ongoing process. Ecosystems and communities in the United States and around the world are being impacted today.

Advertisement

It’s not particularly insightful, but it provides the A.I. program some basic building blocks. The artificial intelligence continued along this slightly edited line of reasoning:

The consensus view emerging from the Intergovernmental Panel on Climate Change (IPCC) and other respected assessments is that the planet is warming because of the accumulation of heat-trapping gases in the atmosphere caused by human activity. The level of scientific consensus that human activities are primarily responsible for climate change is like the consensus finding on smoking causing lung cancer.

Advertisement

The program had likely already been trained to know the term “climate change” was connected to the IPCC; by making certain inferences it could generate text which seemed like a reasonable extension of the idea.

Advertisement

What about ninth-grade English? Channeling my 14-year-old self, I wrote these lines:

“Romeo and Juliet” is a play that explores many different topics such as romance, betrayal, and tragedy. It has interesting characters that make it such a good story, even 400 years later.

The computer spat back this take (which I’ve edited slightly):

Advertisement

Advertisement

Romeo, the main character, is a very passionate and immature young man who is torn between his love for Rosaline and his love for Juliet. He is also very rash and desperate to get Juliet and will do almost anything to win her. His desperation for Juliet is an interesting trait and is one that the younger kids can relate to, as well as older adults with first crushes. Romeo is an interesting character and is very different from how kids would imagine him to be. But he is also a great inspiration, showing that love is worth fighting for.

Advertisement

Again, the A.I. program had been previously trained to know that Romeo and Juliet contained certain characters, themes, and plot elements. But what stood out to me was that the text was crafted at an appropriate writing level for a high schooler, who would be reading about these star-crossed lovers for the first time. The awkward phrasing and the syntax errors baked into the program make it seem convincing.

I’ve struggled to find the right analogy to describe this phenomenon. Is using A.I. to write graded papers like athletes taking performance-enhancing drugs? As a society and as a sporting culture, we’ve decided certain drugs are forbidden, as they provide the user unfair advantages. Further, the cocktail of drugs flowing through these competitors and malicious sports programs could cause real physical and psychological harm to the athletes themselves. Would individuals using AI in writing be likewise considered in the same boat—a cheat to the system providing undue advantages, which also creates harm in the long run by impeding writing skills?

Advertisement

Advertisement

Advertisement

Or might using A.I. be more like using performance-enhancing gear in sports, which is both acceptable and encouraged? To use another sports analogy, even beginner tennis players today use high-performance carbon composite rackets instead of 1960s-era wooden racket technology. Swimmers wear nylon and elastane suits and caps to reduce drag. Bikers have stronger, lighter bicycles than their counterparts used a generation ago. Baseball bats evolved from wood to aluminum and developed better grips; baseball mitts have become more specialized over the decades.

Numerous educators assert that A.I. is more like the former. They consider using these programs violate academic integrity. Georgetown University professor Lise Howard told me, “I do think it’s unethical and an academic violation to use AI to write paragraphs, because academic work is all about original writing.” Written assignments have two purposes, argues Ani Ross Grubb, part-time faculty member in the Carroll School of Management at Boston College: “First is to test the learning, understanding, and critical thinking skills of students. Second is to provide scaffolding to develop those skills. Having AI write your assignments would go against those goals.”

Advertisement

Certainly, one can argue that this topic has already been covered in university academic integrity codes. Using A.I. might open students to serious charges. For instance, American University indicates, “All papers and materials submitted for a course must be the student’s original work unless the sources are cited” while the University of Maryland similarly notes that it is prohibited to use dishonesty to “gain an unfair advantage, and/or using or attempting to use unauthorized materials, information, or study aids in any academic course or exercise.”

Advertisement

Advertisement

But some study aids are generally considered acceptable. When writing papers, it is perfectly fine to use grammar- and syntax-checking products standard on Microsoft Word and other document creating programs. Other A.I. programs like Grammarly help write better sentences and fix errors. Google Docs finishes sentences in drafts and emails.

Advertisement

Advertisement

So the border between using those kinds of assistive computer programs and full-on cheating remains fuzzy. Indeed, as Jade Wexler, associate professor of special education at the University of Maryland, noted, A.I. could be a valuable tool to help level the playing field for some students. “It goes back to teachers’ objectives and students’ needs,” she said. “There’s a fine balance making sure both of those are met.”

Thus there are two intertwined questions at work. First: Should institutions permit A.I.-enhanced writing? If the answer is no, then the second question is: How can professors detect it? After all, it’s unclear whether there’s a technical solution to keeping A.I. from worming into student papers. An educator’s up-to-date knowledge on relevant sources will be of limited utility since the verbiage has not been swiped from pre-existing texts.

Advertisement

Advertisement

Still, there may be ways to minimize these artificial enhancements. One is to codify at the institutional level what is acceptable and what is not; in July the Council of Europe took a few small steps, publishing new guidelines which begin to grapple with these new technologies creating fraud in education. Another would be to keep classes small and give individual attention to students. As Jessica Chiccehitto Hindman, associate professor of English at Northern Kentucky University, noted, “When a writing instructor is in a classroom situation where they are unable to provide individualized attention, the chance for students to phone it in—whether this is plagiarism, A.I., or just writing in a boring, uninvested way—goes up.” More in-class writing assignments—no screens allowed—could also help. Virginia Lee Strain, associate professor of English and director of the honors program at Loyola University Chicago, further argued, “AI is not a problem in the classroom when a student sits down with paper and pencil.”

Advertisement

Advertisement

But in many settings, more one-on-one time simply isn’t a realistic solution, especially at high schools or colleges with large classes. Educators juggle multiple classes and courses, and for them to get to know every student every semester isn’t going to happen.

Advertisement

Advertisement

A more aggressive stance would be for high schools and universities to explicitly declare using A.I. will be considered an academic violation—or at least update their honor codes to reflect what they believe is the right side of the line concerning academic integrity. That said, absent a mechanism to police students, it might paradoxically introduce students to a new way to generate papers faster.

Educators realize some large percentage of students will cheat or try to game the system to their advantage. But perhaps, as Hindman says, “if a professor is concerned that students are using plagiarism or AI to complete assignments, the assignments themselves are the problem, not the students or the AI.” If an educator is convinced that students are using these forbidden tools, he or she might consider using alternate means to generate grades such as assigning oral exams, group projects, and class presentations. Of course, as Hindman notes, “these types of high-impact learning practices are only feasible if you have a manageable number of students.”

AI is here to stay whether we like it or not. Provide unscrupulous students the ability to use these shortcuts without much capacity for the educator to detect them, combined with other crutches like outright plagiarism, and companies that sell papers, homework, and test answers, and it’s a recipe for—well, not disaster, but the further degradation of a type of assignment that has been around for centuries.

Future Tense is a partnership of Slate, New America, and Arizona State University that examines emerging technologies, public policy, and society.. With some artificial intelligence (AI) programs able to write sentences well enough to apparently convince a Google engineer that it is sentient, it was inevitable that someone out there would try to use it to cheat at homework. According to one Redditor, this has already happened – and as a result, they and their friends have received straight As.

"I have been using this tool for quite some time and only recently came up with the idea to use it to write essays, answer questions about movies and books for school projects, and much more," Redditor Urdadgirl69 wrote on the Open AI subreddit. "I feel a little guilty about it, but I don't really care that much anymore. For a couple of weeks, I have made $100 profit by 'doing' homework for other classmates and now I am looked at as a genius. What are your thoughts on this? Have you done it yourself?"

Advertisement Advertisement

"Yes, this post was rephrased by the AI," they added.

The post was met with skepticism, plus annoyance from people claiming to be teachers.

"As someone who teaches, I can say that this is something I dread," Redditor ahumanlikeyou responded. "If I learned that my students were submitting AI-written papers, I'd quit. Grading something an AI wrote is an incredibly depressing waste of my life. I have a child who would benefit from my attention while I'm grading papers over the weekend. Think about what you are doing."

Others noted that this is an excellent way to not learn a thing from the assignment. But is it plausible? We're going to go with a qualified: maybe, but with the amount of fact-checking and altering of structure and argument involved, it is unlikely to be a whole lot more efficient than learning and writing yourself.

Advertisement Advertisement

According to the user, after the AI does the bulk of the hard work, they fact-check the text it has produced, as well as rephrasing and shaping it, before submitting it for marking.

Programs that can generate news articles have been available for some time now, with varying degrees of success. A study from last year found that deliberately biased text generated by an Open AI program – Generative Pre-trained Transformer 3 (GPT-3) – was convincing enough to persuade some humans away from their original viewpoints, with 63-70 percent rating the AI-generated statements as convincing. The program was particularly effective when a human editor looked over it before it was shown to study participants.

A fair number of text-generating AIs are available, easy, and free to use – and will even fire out some fairly convincing text with the right prompt. For instance, pasting in the opening paragraph to this article generated a reasonable (if slightly suspect) article describing the user's struggle to keep the AI on track, clearly aware of its own shortcomings.

"The AI had a hard time stringing coherent sentences together, so [redditor MyCognitiveAffectiveExperience (MCAE)] had to continually correct it. The redditor also said that it was 'quite difficult to keep the bot on topic.' However, the biggest problem was that the AI kept trying to switch to different sections of the essay as it was writing," the SudoWrite bot wrote.

Advertisement Advertisement

"MCAE was able to get the bot to stick to the general theme of the essay, but it didn't stick to the points.". Artificial intelligence has the potential to revolutionise the way students' coursework is written, but academics have called for greater controls to stop widespread cheating.

With the use of AI, students and researchers could potentially save time and effort by allowing the technology to assist with the creation of written assignments.

Another potential application of AI in academic writing is through the use of machine-learning algorithms.

There needs to be an institutional policy in place for this and some education for students that this use of AI in this way is unacceptable Dr Athol Yates, Khalifa University

These algorithms can be trained on a large data set of academic papers and articles, learning the characteristics and patterns of successful writing.

Speaking at a Khalifa University workshop with lecturers on how to detect the use of ChatGPT, a text-generating AI software launched by US company OpenAI in November, Dr Athol Yates warned against its use by students.

“This is definitely already being used around the world, as students are tech savvy,” said Dr Yates, an associate professor of humanities and social science at Khalifa University.

“It is easy to use, so if they are late for an assignment it is a simple option. It has the potential to turn us more into classroom police.

ChatGPT already boasts more than one million users. Photo: AP

“There is a need for all educational institutions to have institutional policy in place for this and some education for students that this use of AI in this way is unacceptable.”

More than one million users

The technology has been created by OpenAI, a San Francisco based tech-firm co-founded by Elon Musk and valued at around $29 million, according to financial reports.

On December 4, just four days after its launch, ChatGPT had more than a million users.

Schools in New York City have already moved to ban the technology across the district, but some schools will still be allowed to access the technology to aid their education.

There are AI tools specifically designed for academic writing, such as citation management software.

These tools can help students organise their sources and automatically generate citations and references in the correct formatting style.

While AI has the potential to be a valuable tool in the academic writing process, academics said it should not be used as a replacement for human effort and critical thinking.

Dr Athol Yates said ChatGPT uses natural language, so it is difficult to spot when it is being used. Khushnum Bhandari / The National

Ultimately, the goal of using AI in academic writing should be to assist and enhance the writing process, rather than to fully automate it, they said.

“It provides good quality formulaic responses, which is what we are trying to teach,” said Dr Yates

“It is perfect and uses natural language, so it is difficult to spot.

Read more Governments must control the rise of artificial intelligence, experts say

“The lack of references is an issue, but there are ways to get around this.

“Students don’t have to submit an entire response. It can be reworked and, hey presto, you have a piece of work — we need to decide if that is acceptable as we have no policy at the moment on this.”

Risk of cyber attacks

There is a risk that AI could be used for malicious purposes, such as cyber attacks or the spread of false information.

As AI systems become more advanced, they could potentially be used to automate and amplify these activities, causing significant harm to individuals and society as a whole.

Finally, the widespread adoption of AI could lead to significant job displacement, as many tasks currently performed by humans are automated.

This could result in widespread unemployment and economic disruption, and it will be important for governments and businesses to address this issue and ensure that those affected are protected.

Overall, while AI has the potential to bring about many benefits, it is important that careful consideration is given to address the potential dangers it poses.

This will require ongoing research and collaboration between researchers, policymakers and industry professionals to ensure that the development and use of AI is responsible and ethical.

Amira Al Aamri, a research scientist at Khalifa University, said she sees benefits in using ChatGPT to see the flow of ideas that it generates. Khushnum Bhandari / The National

“If I was going to use ChatGPT, I would benefit from seeing the flow of ideas that it generates,” said Amira Al Aamri, a research scientist at Khalifa University.

“Often, we don’t know where to start when writing, so these tools can help with that.

“I am not saying it should never be used, as there are benefits, but the outcomes should not be copied and pasted.

“I would want to see the process students have gone through to generate their work and this does not allow for that. That means more work for the student.

“When the software develops to produce references, it will be a game changer.”

This story was created was generated with ChatGPT

This story has largely been created using ChatGPT. We entered 'write a news article on the dangers of artificial intelligence in the command box' and ChatGPT created a 400-word story in seconds.

We added quotes from experts at Khalifa University who gave a presentation on the power of ChatGPT and its potential uses.. While grading essays for his world religions course last month, Antony Aumann, a professor of philosophy at Northern Michigan University, read what he said was easily “the best paper in the class.” It explored the morality of burqa bans with clean paragraphs, fitting examples and rigorous arguments.

A red flag instantly went up.

Mr. Aumann confronted his student over whether he had written the essay himself. The student confessed to using ChatGPT, a chatbot that delivers information, explains concepts and generates ideas in simple sentences — and, in this case, had written the paper.

Alarmed by his discovery, Mr. Aumann decided to transform essay writing for his courses this semester. He plans to require students to write first drafts in the classroom, using browsers that monitor and restrict computer activity. In later drafts, students have to explain each revision. Mr. Aumann, who may forgo essays in subsequent semesters, also plans to weave ChatGPT into lessons by asking students to evaluate the chatbot’s responses.

“What’s happening in class is no longer going to be, ‘Here are some questions — let’s talk about it between us human beings,’” he said, but instead “it’s like, ‘What also does this alien robot think?’”. New York City's Department of Education announced a ban on the wildly popular chatbot ChatGPT — which some have warned could inspire more student cheating — from its schools’ devices and networks.

Jenna Lyle, a spokesperson for the department, said the decision to ban ChatGPT, which is able to generate conversational responses to text prompts, stemmed from concerns about the "negative impacts on student learning."

"While the tool may be able to provide quick and easy answers to questions, it does not build critical-thinking and problem-solving skills, which are essential for academic and lifelong success," Lyle said in a email statement.

It was not immediately clear if the ban applied to the City University of New York system. A representative for CUNY did not immediately provide NBC News with a comment.

In New York public schools, ChatGPT can still be made available upon request to classes studying artificial intelligence.

Although chatbots are not a new technology, ChatGPT, a chatbot created by artificial intelligence company OpenAI, exploded on social media in late 2022 after some declared the bot was a better search engine than Google thanks to its conversational speaking style and coherent, topical response style.

In an email statement responding to the New York City public schools ban, a spokesperson for OpenAI said the company "doesn't want ChatGPT to be used for misleading purposes in schools or anywhere else."

The company is "already developing mitigations to help anyone identify text generated by that system," the spokesperson said.

The New York City Department of Education's decision to ban the bot comes amid discourse about the impact ChatGPT could have on education if students were to use it to generate homework assignments, solve mathematical equations and write essays.

Experts have acknowledged that chatbots like ChatGPT could be a detriment to education in the future — but in recent interviews with NBC News, some said they weren't ready to sound the alarm just yet.

Those who work in both the field of education and artificial intelligence said that institutions will need to find ways to integrate chatbots like ChatGPT into their curriculum rather than outlaw them altogether.

"There’s always been this concern that technologies will do away with what people do best, and the reality is that people have had to learn how to use these technologies to enhance what they do best," Lauren Klein, an associate professor in the departments of English and quantitative theory and methods at Emory University, said last month.

Many have likened chatbots to the rise of technology like the calculator, which was decried as the death of math until educators began using it to elevate their classwork. Some educators also said that several computer-assisted writing tools, like Grammarly or Google Docs’ Smart Compose, already exist and are currently utilized in academia.

The experts predicted someone would likely create technology to detect if an essay was made by ChatGPT. Those predictions were correct.

On Monday, Edward Tian, a computer science student at Princeton University, tweeted that he had spent the holiday creating a tool to detect if a piece of text was generated by a human or an artificial intelligence.

“AI text generation is like opening a pandora’s box. It’s an incredibly exciting innovation, but with any new technology we need to build safeguards so that it is adopted responsibly,” Tian said in a Twitter message to NBC News on Thursday.

In subsequent tweets, Tian shared videos of the program at work. In one clip, he showed how his bot, GPTZero, detected both human and AI text. In one clip, he put text from a New Yorker article into GPTZero. The bot then declared the text was “likely human generated!”

The OpenAI spokesperson said ChatGPT was made "available as a research preview to learn from real-world use, which we believe is a critical part of developing and deploying capable, safe AI systems."

"We are constantly incorporating feedback and lessons learned. We’ve always called for transparency around the use of AI-generated text," the spokesperson said. "Our policies require that users be up-front with their audience when using our API and creative tools like DALL-E and GPT-3."

The company said it looks “forward to working with educators on useful solutions, and other ways to help teachers and students benefit from artificial intelligence.”. Two philosopher professors said they caught their students submitting essays written by ChatGPT.

They said certain red flags alerted them to the use of AI.

If students don't confess to using the program, professors say it can be hard to prove.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

A few weeks after the launch of the AI chatbot ChatGPT, Darren Hick, a philosophy professor at Furman University, said he caught a student turning in an AI-generated essay.

Hick said he grew suspicious when the student turned in an on-topic essay that included some well-written misinformation.

After running it through Open AI's ChatGPT detector, the results said it was 99% likely the essay had been AI-generated.

Antony Aumann, a religious studies and philosophy professor at Northern Michigan University, told Insider he had caught two students submitting essays written by ChatGPT.

Advertisement

After the writing style set off alarm bells, Aumann submitted them back to the chatbot asking how likely it was that they were written by the program. When the chatbot said it was 99% sure the essays were written by ChatGPT, he forwarded the results to the students.

Both Hick and Aumann said they confronted their students, all of whom eventually confessed to the infraction. Hick's student failed the class and Aumann had his students rewrite the essays from scratch.

'It was really well-written wrong'

There were certain red flags in the essays that alerted the professors to the use of AI. Hick said the essay he found referenced several facts not mentioned in class, and made one nonsensical claim.

"Word by word it was a well-written essay," he said, but on closer inspection, one claim about the prolific philosopher, David Hume "made no sense" and was "just flatly wrong."

Advertisement

"Really well-written wrong was the biggest red flag," he said.

Related stories

For Aumann, the chatbot just wrote too perfectly. "I think the chat writes better than 95% of my students could ever," he said.

"All of a sudden you have someone who does not demonstrate the ability to think or write at that level, writing something that follows all the requirements perfectly with sophisticated grammar and complicated thoughts that are directly related to the prompt for the essay," he said.

Christopher Bartel, a professor of philosophy at Appalachian State University, said that while the grammar in AI-generated essays is almost perfect, the substance tends to lack detail.

Advertisement

He said: "They are really fluffy. There's no context, there's no depth or insight."

Hard-to-prove plagiarism

If students don't confess to using AI for essays, it can leave academics in a tough spot.

Bartel said that some institutions' rules haven't evolved to combat this kind of cheating. If a student decided to dig their heels in and deny the use of AI, it can be difficult to prove.

Bartel said the AI detectors on offer were "good but not perfect."

Advertisement

"They give a statistical analysis of how likely the text is to be AI-generated, so that leaves us in a difficult position if our policies are designed so that we have to have definitive and demonstrable proof that the essay is a fake," he said. "If it comes back with a 95% likelihood that the essay is AI generated, there's still a 5% chance that it wasn't."

In Hick's case, although the detection site said it was "99% certain" the essay had been generated by an AI, he said it wasn't enough for him without a confession.

"The confession was important because everything else looks like circumstantial evidence," he said. "With AI-generated content, there is no material evidence, and material evidence has a lot more weight to it than circumstantial evidence."

Aumann said although he thought the analysis by the chatbot would be good enough proof for disciplinary action, AI plagiarism was still a new challenge for colleges.

Advertisement

He said: "Unlike plagiarism cases of old where you can just say, 'hey, here's the paragraph from Wikipedia.' There is no knockdown proof that you can provide other than the chat says that's the statistical likelihood.". Welcome to the new age of academic dishonesty.

A college professor in South Carolina is sounding the alarm after catching a student using ChatGPT — a new artificial intelligence chat bot that can quickly digest and spit out written information about a vast array of subjects — to write an essay for his philosophy class.

The weeks-old technology, released by OpenAI and readily available to the public, comes as yet another blow to higher learning, already plagued by rampant cheating.

3 The bot software ChatGPT is a cause of concern in academia. Getty Images/iStockphoto

“Academia did not see this coming. So we’re sort of blindsided by it,” Furman University assistant philosophy professor Darren Hick told The Post. “As soon as I reported this on Facebook, my [academic] friends said, ‘Yeah, I caught one too.'”

Earlier this month, Hick had instructed his class to write a 500-word essay on the 18th-century philosopher David Hume and the paradox of horror, which examines how people can get enjoyment from something they fear, for a take-home test.

But one submission, he said, featured a few hallmarks that “flagged” AI usage in the student’s “rudimentary” answer.

“It’s a clean style. But it’s recognizable. I would say it writes like a very smart 12th-grader,” Hick said of ChatGPT’s written responses to questions.

“There’s particular odd wording used that was not wrong, just peculiar … if you were teaching somebody how to write an essay, this is how you tell them to write it before they figure out their own style.”

Despite having a background in the ethics of copyright law, Hick said proving that the paper was concocted by ChatGPT was nearly impossible.

3 Assistant professor Darren Hick fears what ChatGPT will do to academic honesty. courtesy of Darren Hick

3 Students are using ChatGPT to cheat in classes, one professor warns. NurPhoto via Getty Images

First, the professor plugged the suspect text into software made by the producers of ChatGPT to determine if the written response was formulated by AI.

He was given a 99.9% likely match. But unlike in standard plagiarism detection software — or a well-crafted college paper — the software offered no citations.

Hick then tried producing the same essay by asking ChatGPT a series of questions he imagined his student had asked. The move yielded similar answers, but no direct matches, since the tool formulates unique responses.

Ultimately, he confronted the student, who copped to using ChatGPT and failed the class as a result. The undergrad was also turned over to the school’s academic dean.

But Hick fears that other cases will be almost impossible to prove, and that he and his colleagues will soon be inundated with fraudulent work, as universities like Furman struggle to establish formal academic protocols for the developing technology.

For now, Hick says that the best he can do is surprise suspected students with impromptu oral exams, hoping to catch them off-guard without their tech armor.

“What’s going to be the difficulty is that, unlike convincing a friend to write your essay because they took the class before or paying somebody online to write the essay for you, this is free and instantaneous,” he said.

Even more frightening, Hick fears that as ChatGPT keeps learning, irregularities in its work will become less and less obvious on a student’s paper.

“This is learning software — in a month, it’ll be smarter. In a year, it’ll be smarter,” he said. “I feel the mix myself between abject terror and what this is going to mean for my day-to-day job — but it’s also fascinating, it’s endlessly fascinating.”. Stanford students and professors alike are grappling with the rise of ChatGPT, a chatbot powered by artificial intelligence, and the technology’s implications for education.

Some professors have already overhauled their courses in anticipation of how students might use the chatbot to complete assignments and exams. And according to an informal poll conducted by The Daily, a large number of students have already used ChatGPT on their final exams.

Whether the new technology will necessitate a revision of the Honor Code, the University’s standards for academic integrity, remains to be seen: A University spokesperson confirmed that the Board of Judicial Affairs is aware of and monitoring these emerging tools.

“Students are expected to complete coursework without unpermitted aid,” wrote spokesperson Dee Mostofi. “In most courses, unpermitted aid includes AI tools like ChatGPT.”

The chatbot was created by San Francisco-based artificial intelligence company OpenAI and launched in November. Stanford dropout Sam Altman is among the company’s co-founders.

When OpenAI released the tool, it quickly took off, attracting over a million users in under a week and drawing widespread attention for its ability to generate almost any level of text-based fodder using technology from a field of machine learning called natural language processing. It not only acts as a robotic study buddy, but can also write lengthy essays on almost any subject and generate ideas and outlines for projects.

For example, when asked for a “witty opening paragraph for a Stanford Daily article about ChatGPT and the Stanford Honor Code,” the chatbot produced the following:

“ChatGPT, a popular artificial intelligence tool, has recently sparked a debate on campus about its role in academic integrity. Some argue that using ChatGPT to complete assignments is a violation of the Honor Code, while others claim it’s simply a tool for idea generation. The question remains: is ChatGPT a helpful helper or a dishonest cheat? One thing is for certain: the stakes are high, and the future of AI at Stanford hangs in the balance.”

The power of the technology to instantaneously generate swaths of human-like text has frightened some educators across the country.

Among other districts that have cracked down on its use, New York City’s education department has blocked the site on its networks, citing “concerns about negative impacts on student learning, and concerns regarding the safety and accuracy of content,” according to education department spokesperson Jenna Lyle in a statement to Chalkbeat New York.

Awareness of the technology has also reached Stanford’s faculty, confirmed Mostofi, the University spokesperson. Mostofi, citing a recent story in the Stanford Report featuring faculty weighing in on ChatGPT, wrote, “Many Stanford faculty are highly engaged in researching new large language models and implications of AI in the learning environment.”

Mostofi said student assignments will continue to be designed to “support students in developing linked thinking and writing skills,” including the drafting and revising processes, as well as citing sources.

Some colleges and universities have already incorporated the new technology into their academic integrity policies. Washington University in St. Louis and University of Vermont in Burlington are among the institutions that have amended their academic integrity policies to include the usage of AI tools like ChatGPT.

Mostofi wrote that at Stanford, conversations will soon be underway about ChatGPT and the honor code.

“The Board on Judicial Affairs (BJA) has been monitoring these emerging tools and will be discussing how they may relate to the guidelines of our Honor Code,” Mostofi wrote.

But while the University plans to discuss ChatGPT, some students have already used the tool to complete their finals, according to an anonymous poll conducted by The Daily on the social media app Fizz, which requires a stanford.edu email to join.

According to the poll, which had 4,497 respondents (though the number may be inflated) and was open from Jan. 9 to Jan. 15, around 17% of Stanford student respondents reported using ChatGPT to assist with their fall quarter assignments and exams.

Of those 17%, a majority reported using the AI only for brainstorming and outlining. Only about 5% reported having submitted written material directly from ChatGPT with little to no edits, according to the poll.

According to another informal poll conducted by The Daily on the same app, a majority of student respondents believe that the use of ChatGPT to assist with assignments is currently or should be a violation of the Honor Code. However, there is a difference in what students believe should be considered a violation.

The news that some students are already using ChatGPT on assignments has spread to professors, some of whom are revamping their courses as a result.

In a message to the public computer science (CS) department Slack channel, computer science associate professor Michael Bernstein ’06 asked if any other professors had encountered homework that was generated by ChatGPT.

“In this case,” wrote Bernstein, “it was easy to tell because part of the submission included: ‘As a large language model trained by OpenAI…’”

Computer science lecturer Julie Stanford BA ’98 MA ’98 added in the Slack that the student’s submission was “like robbing a bank and caring so little about being caught that you try to take a selfie with the security camera on the way out.”

Some professors on campus have recently added course policies to their syllabi cautioning against the use of ChatGPT, arguing that it is a form of plagiarism, while others have switched to more traditional methods, attempting to eliminate technology from the picture.

One class, COMM 108: “Media Processes and Effects” has dedicated a whole section of its syllabus dedicated to the usage of AI tools. “Using Artificial Intelligence (AI) Agents (e.g., ChatGPT, StableDiffusion) to generate assignments or parts of assignments is generally discouraged. If you choose to use an AI agent for generating portions or aspects of an assignment, you must disclose this use and cite it in the same manner as you would cite any external source,” reads the syllabus.

In the computer science Slack, senior lecturer Keith Schwarz MS ’11 said that he has “switched back to pencil-and-paper exams,” citing concerns related to open computers that could be operating ChatGPT, even suggesting that he might consider “requiring students to leave all their backpacks and electronics at the front of the exam room.”

Stanford AI Alignment (SAIA) student leaders Gabriel Mukobi ’23 Michael Byun ’24 urged students to be wary of using ChatGPT for academic work as the tool has not been fine-tuned for use in academic settings.

“AI tools like ChatGPT are clearly here to stay,” the two wrote.

A previous version of this article incorrectly stated that Elon Musk dropped out of Stanford. That portion of the article has since been removed, and The Daily regrets this error.. It’s no secret that Stanford students are some of the smartest and most driven individuals in the world. But it seems that some of these high-achieving students are taking the easy way out when it comes to exams and homework assignments.

You heard that right: Stanford Students are using the language model Chat GPT to cheat on their exams and assignments. This is a clear violation of academic integrity and a disservice to the students who are relying on the technology to do their work for them.

For those who are not familiar, Chat GPT is a large language model trained by OpenAI that can generate human-like text. It has become increasingly popular among students at Stanford and other universities as a way to quickly generate answers to test questions.

At first, I was skeptical about the effectiveness of Chat GPT for cheating. After all, how could a machine-generated answer possibly fool a professor or teaching assistant? But as it turns out, Chat GPT is surprisingly good at generating answers that sound convincingly human.

Firstly, it is obvious that using a tool like Chat GPT to complete assignments is lazy and unethical. It deprives students of the opportunity to learn and develop their own skills, and it undermines the integrity of the education system. This kind of cheating is not only unfair to the students who are studying and working hard, but it also diminishes the value of the education received. When students cheat on exams using Chat GPT, they are not learning the material, but rather just memorizing answers. This means that they are not gaining the knowledge and skills that they need to succeed in their careers

In order for Stanford and the United States to maintain their positions as global leaders in education and innovation, it is essential that students are held to high standards of academic integrity. Using technology to cheat on exams is a clear violation of these standards and sets a dangerous precedent for the future.

And let’s not forget: Using Chat GPT to cheat on exams and assignments undermines the specific value of a Stanford education. When students cheat, they are not only cheating themselves, their classmates, their professors and the university as a whole. This undermines the credibility and reputation of the institution, and it undermines the value of the degrees that are earned there.

It's time for Stanford and other universities to take action against the use of Chat GPT for cheating on exams. One potential solution would be to implement a system that can detect when students are using Chat GPT to generate answers on exams. This could be done through a combination of algorithms and human review.

In the meantime, it's up to each individual student to decide whether to use Chat GPT for cheating or to study and learn the material. As a student at Stanford, I choose to do the latter. I encourage my fellow students to do the same. Together, we can uphold the integrity of our education and ensure that we are truly learning and growing as individuals.

For those of you who don’t believe the effectiveness of Chat GPT for writing essays, perhaps you would be persuaded by the fact that it actually wrote this entire article from the prompt “Write a contrarian article about Stanford students using Chat GPT to cheat.” Even this self-aware section was penned by a machine. Despite the style not fitting my usual way of writing, and the writing being a bit clunky, its readability should be a warning to professors and graders everywhere.. Become a fan of Slashdot on Facebook. Feature Turnitin, best known for its anti-plagiarism software used by tens of thousands of universities and schools around the world, is building a tool to detect text generated by AI.

Large language models have gained traction since the commercial release of OpenAI's GPT-3 in 2020. Now multiple companies have built their own rival machine learning systems, kickstarting a new wave of startups developing products powered by generative AI. These models operate like general-purpose chatbots. Users type instructions, and they will respond with passages of coherent, convincing text.

Students are increasingly turning to AI tools to complete assignments, while teachers are only beginning to consider their impact and role in education. Opinions are divided. Some believe the technology can hone writing skills, while others see it as cheating. Schools in California, New York, Virginia, and Alabama have blocked pupils from accessing the latest ChatGPT model on public networks, according to Forbes.

Education departments aren't quite sure what academic policies should be introduced to regulate the use of AI text generators. Besides, all rules would be difficult to enforce anyway considering there is currently no effective way to detect machine-written work. Enter Turnitin. Founded in 1998, the US company sells software that calculates how similar a particular essay is compared to content from a large database of papers, webpages, and books to look for signs of plagiarism.

Turnitin was acquired by media giant Advanced Publications for $1.75 billion in 2019, and its software has been used by 15,000 institutions across 140 countries. With over two decades of experience, Turnitin has a broad reach in education and has amassed a huge repository of student writing, making it the ideal company to develop an academic AI text detector.

Turnitin has been quietly building the software for years ever since the release of GPT-3, Annie Chechitelli, chief product officer, told The Register. The rush to give educators the capability to identify text written by humans and computers has become more intense with the launch of its more powerful successor, ChatGPT. As AI continues to progress, universities and schools need to be able to protect academic integrity now more than ever.

"​Speed matters. We're hearing from teachers just give us something," Chechitelli said. Turnitin hopes to launch its software in the first half of this year. "It's going to be pretty basic detection at first, and then we'll throw out subsequent quick releases that will create a workflow that's more actionable for teachers." The plan is to make the prototype free for its existing customers as the company collects data and user feedback.

"At the beginning, we really just want to help the industry and help educators get their legs under them and feel more confident. And to get as much usage as we can early on; that's important to make a successful tool. Later on, we'll determine how we're going to productize it," she said.

Patterns in AI writing

Although text generated by AI is convincing, there are telltale signs that reveal an algorithm's handiwork. The writing is usually bland and unoriginal; tools like ChatGPT regurgitate existing ideas and viewpoints and don't have a distinct voice. Humans can sometimes spot AI-generated text, but machines are much better at the job.

Turnitin's VP of AI, Eric Wang, said there are obvious patterns in AI writing that computers can detect. "Even though it feels human-like to us, [machines write using] a fundamentally different mechanism. It's picking the most probable word in the most probable location, and that's a very different way of constructing language [compared] to you and I," he told The Register.

"We read by jumping back and forth our eyes without even knowing it, or flitting back and forth between words, between paragraphs, and sometimes between pages. We'll flip back and forth. We also tend to write with a future state of mind. I might be writing, and I'm thinking about something, a paragraph, a sentence, a chapter; the end of the essay is linked in my mind to the sentence I'm writing even though the sentences between now and then have yet to be written."

ChatGPT, however, doesn't have this kind of flexibility and can only generate new words based on previous sentences, he explained. Turnitin's detector works by predicting what words AI is more likely to generate in a given text snippet. "It's very bland statistically. Humans don't tend to consistently use a high probability word in high probability places, but GPT-3 does so our detector really cues in on that," he said.

Wang said Turnitin's detector is based on the same architecture as GPT-3 and described it as a miniature version of the model. "We are in many ways I would [say] fighting fire with fire. There's a detector component attached to it instead of a generate component. So what it's doing is it's reading language in the exact same way GPT-3 reads language, but instead of spitting out more language, it gives us a prediction of whether we think this passage looks like [it's from] GPT-3."

The company is still deciding how best to present its detector's results to teachers using the tool. "It's a difficult challenge. How do you tell an instructor in a small amount of space what they want to see?" Chechitelli said. They might want to see a percentage that shows how much of an essay seems to be AI-written, or they might want confidence levels showing whether the detector's prediction confidence is low, medium, or high to assess accuracy.

The software isn't designed with the goal of getting ChatGPT banned in academia. Although it could deter students from using these types of tools, Turnitin believes its detector will instead enable teachers and students to trust each other and the technology.

"I think there is a major shift in the way we create content and the way we work," Wang said. "Certainly that extends to the way we learn. We need to be thinking long term about how we teach. How do we learn in a world where this technology exists? I think there is no putting the genie back in the bottle. Any tool that gives visibility to the use of these technologies is going to be valuable because those are the foundational building blocks of trust and transparency." ®. . innovate_rye’s professors know them as a first-year biochemistry major, and an “A” student. What their professors don’t know about them is that they’re using a powerful AI language model to finish most homework assignments. innovate_rye’s professors know them as a first-year biochemistry major, and an “A” student. What their professors don’t know about them is that they’re using a powerful AI language model to finish most homework assignments.

“It would be simple assignments that included extended responses,” innovate_rye, who asked to use their Reddit handle to avoid detection by their college, told Motherboard. “For biology, we would learn about biotech and write five good and bad things about biotech. I would send a prompt to the AI like, ‘what are five good and bad things about biotech?’ and it would generate an answer that would get me an A.” “It would be simple assignments that included extended responses,” innovate_rye, who asked to use their Reddit handle to avoid detection by their college, told Motherboard. “For biology, we would learn about biotech and write five good and bad things about biotech. I would send a prompt to the AI like, ‘what are five good and bad things about biotech?’ and it would generate an answer that would get me an A.”

Advertisement

Without AI, innovate_rye says the homework they consider “busywork” would take them two hours. Now homework assignments like this take them 20 minutes. Without AI, innovate_rye says the homework they consider “busywork” would take them two hours. Now homework assignments like this take them 20 minutes.

“I like to learn a lot [and] sometimes schoolwork that I have done before makes me procrastinate and not turn in the assignment,” innovate_rye explains. “Being able to do it faster and more efficient seems like a skill to me.” “I like to learn a lot [and] sometimes schoolwork that I have done before makes me procrastinate and not turn in the assignment,” innovate_rye explains. “Being able to do it faster and more efficient seems like a skill to me.”

innovate_rye isn’t alone. Since OpenAI unveiled the latest application programming interface (API) for its widely-used language model, GPT-3, more students have begun feeding written prompts into innovate_rye isn’t alone. Since OpenAI unveiled the latest application programming interface (API) for its widely-used language model, GPT-3, more students have begun feeding written prompts into OpenAI’s Playground and similar programs that use deep learning to generate text. The results continue the initial prompt in a natural-sounding way, and often can’t be distinguished from human-written text

When AeUsako_ was a high school senior last spring, they used OpenAI to generate an entire essay about contemporary world affairs. They told Motherboard that, while they didn’t ace the assignment—they lost points for failing to cite outside sources—they did learn that plagiarism-checking algorithms wouldn’t flag the AI-generated text. When AeUsako_ was a high school senior last spring, they used OpenAI to generate an entire essay about contemporary world affairs. They told Motherboard that, while they didn’t ace the assignment—they lost points for failing to cite outside sources—they did learn that plagiarism-checking algorithms wouldn’t flag the AI-generated text.

“Because I used Open AI I didn’t feel the constant anxiety of needing to focus all my time on writing it,” AeUsako_, who also asked to use their online pseudonym, told Motherboard. “Because I used Open AI I didn’t feel the constant anxiety of needing to focus all my time on writing it,” AeUsako_, who also asked to use their online pseudonym, told Motherboard.

Advertisement

George Veletsianos, Canada Research Chair in Innovative Learning & Technology and associate professor at Royal Roads University says this is because the text generated by systems like OpenAI API are technically original outputs that are generated within a blackbox algorithm. George Veletsianos, Canada Research Chair in Innovative Learning & Technology and associate professor at Royal Roads University says this is because the text generated by systems like OpenAI API are technically original outputs that are generated within a blackbox algorithm.

“[The text] is not copied from somewhere else, it’s produced by a machine, so plagiarism checking software is not going to be able to detect it and it’s not able to pick it up because the text wasn’t copied from anywhere else,” Veletsianos told Motherboard. “Without knowing how all these other plagiarism checking tools quite work and how they might be developed in the future, I don’t think that AI text can be detectable in that way.” “[The text] is not copied from somewhere else, it’s produced by a machine, so plagiarism checking software is not going to be able to detect it and it’s not able to pick it up because the text wasn’t copied from anywhere else,” Veletsianos told Motherboard. “Without knowing how all these other plagiarism checking tools quite work and how they might be developed in the future, I don’t think that AI text can be detectable in that way.”

It’s unclear whether the companies behind the AI tools have the ability to detect or prevent students from using them to do their homework. OpenAI did not comment in time for publication. It’s unclear whether the companies behind the AI tools have the ability to detect or prevent students from using them to do their homework. OpenAI did not comment in time for publication.

Peter Laffin is a writing instructor and founder of the private tutoring program Crush the College Essay. He says that tools like OpenAI’s are emblematic of other compensation techniques that technology has produced in the last decade, such as cloud-based typing assistants that are meant to help struggling writers. Peter Laffin is a writing instructor and founder of the private tutoring program Crush the College Essay. He says that tools like OpenAI’s are emblematic of other compensation techniques that technology has produced in the last decade, such as cloud-based typing assistants that are meant to help struggling writers.

“In literacy education, particularly for developing writers, instructors are looking for the level of desirable difficulty, or the point at which you are working yourself just as hard so that you don’t break but you also improve,” Laffin told Motherboard. “Finding the right, appropriate level of desirable difficulty level of instruction makes their capacity to write grow. So if you are doing compensation techniques that go beyond finding that level of desirable difficulty and instructing at that place, then you’re not helping them grow as a writer.” “In literacy education, particularly for developing writers, instructors are looking for the level of desirable difficulty, or the point at which you are working yourself just as hard so that you don’t break but you also improve,” Laffin told Motherboard. “Finding the right, appropriate level of desirable difficulty level of instruction makes their capacity to write grow. So if you are doing compensation techniques that go beyond finding that level of desirable difficulty and instructing at that place, then you’re not helping them grow as a writer.”

Advertisement

Veletsianos notes that it’s probable that we are past the point of no return with AI-generated text, and that students aren’t the only ones being courted. Veletsianos notes that it’s probable that we are past the point of no return with AI-generated text, and that students aren’t the only ones being courted.

“We can also begin to see where this technology might generate a lecture on the fly and all sorts of questions around the lecture,” he said. “I’m not saying that the system we have is the best system but I am saying these are conversations we need and ought to have to see how we can use these tools to improve not just efficiency of teaching, but its effectiveness and engagement as well.” “We can also begin to see where this technology might generate a lecture on the fly and all sorts of questions around the lecture,” he said. “I’m not saying that the system we have is the best system but I am saying these are conversations we need and ought to have to see how we can use these tools to improve not just efficiency of teaching, but its effectiveness and engagement as well.”

While Laffin acknowledges that a reevaluation of effective education is necessary, he says this can happen when looking at the types of prompts educators assign students, noting a difference between the regurgitation of facts and information discovery. However, he worries that products like OpenAI’s text generator will make essay writing a moot point. While Laffin acknowledges that a reevaluation of effective education is necessary, he says this can happen when looking at the types of prompts educators assign students, noting a difference between the regurgitation of facts and information discovery. However, he worries that products like OpenAI’s text generator will make essay writing a moot point.

“We lose the journey of learning,” said Laffin. “We might know more things but we never learned how we got there. We’ve said forever that the process is the best part and we know that. The satisfaction is the best part. That might be the thing that’s nixed from all of this. And I don’t know the kind of person that creates more than anything. Beyond academics, I don’t know what a person is like if they’ve never had to struggle through learning. I don’t know the behavioral implications of that.” “We lose the journey of learning,” said Laffin. “We might know more things but we never learned how we got there. We’ve said forever that the process is the best part and we know that. The satisfaction is the best part. That might be the thing that’s nixed from all of this. And I don’t know the kind of person that creates more than anything. Beyond academics, I don’t know what a person is like if they’ve never had to struggle through learning. I don’t know the behavioral implications of that.”

Meanwhile, innovate_rye eagerly awaits GPT-4, which is Meanwhile, innovate_rye eagerly awaits GPT-4, which is anticipated to be trained on 100 trillion machine learning parameters and may go beyond mere textual outputs . In other words, they aren’t planning to stop using AI to write essays anytime soon.. 