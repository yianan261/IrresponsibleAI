"It's perfectly legitimate for a parent to believe that something called Peppa Pig is going to be Peppa Pig," she says. "And I think many of them have come to trust YouTube... as a way of entertaining your child for ten minutes while the parent makes a phone call. I think if it wants to be a trusted brand then parents should know that protection is in place.". Earlier this week, a report in The New York Times and a blog post on Medium drew a lot of attention to a world of strange and sometimes disturbing videos on YouTube aimed at young children. The genre, which we reported on in February of this year, makes use of popular characters from family-friendly entertainment, but it’s often created with little care, and can quickly stray from innocent themes to scenes of violence or sexuality.

In August of this year, YouTube announced that it would no longer allow creators to monetize videos which “made inappropriate use of family friendly characters.” Today it’s taking another step to try and police this genre.

“We’re in the process of implementing a new policy that age restricts this content in the YouTube main app when flagged,” said Juniper Downs, YouTube’s director of policy. “Age-restricted content is automatically not allowed in YouTube Kids.” YouTube says that it’s been formulating this new policy for a while, and that it’s not rolling it out in direct response to the recent coverage.

The first line of defense for YouTube Kids are algorithmic filters. After that, there is a team of humans that review videos which have been flagged. If a video with recognizable children’s characters gets flagged in YouTube’s main app, which is much larger than the Kids app, it will be sent to the policy review team. YouTube says it has thousands of people working around the clock in different time zones to review flagged content. If the review finds the video is in violation of the new policy, it will be age restrictied, automatically blocking it from showing up in the Kids app.

YouTube says it typically takes at least a few days for content to make its way from YouTube proper to YouTube Kids, and the hope is that within that window, users will flag anything potentially disturbing to children. YouTube also has a team of volunteer moderators, which it calls Contributors, looking for inappropriate content. YouTube says it will start training its review team on the new policy and it should be live within a few weeks.

Along with filtering content out of the Kids app, the new policy will also tweak who can see these videos on YouTube’s main service. Flagged content will be age restricted, and users won’t be able to see those videos if they’re not logged in on accounts registered to users 18 years or older. All age-gated content is also automatically exempt from advertising. That means this new policy could put a squeeze on the booming business of crafting strange kid’s content.

YouTube is trying to walk a fine line between owning up to this problem and arguing that the issue is relatively minor. It says that the fraction of videos on YouTube Kids that were missed by its algorithmic filters and then flagged by users during the last 30 days amounted to just 0.005 percent of videos on the service. The company also says the reports that inappropriate videos racked up millions of views on YouTube Kids without being vetted are false, because those views came from activity on YouTube proper, which makes clear in its terms of service that it’s aimed at user 13 years and older.. Update, Nov. 7, 2017: TODAY Parents is resharing this story from 2016 because a new series of inappropriate videos have cropped up on YouTube Kids and are making headlines. While the channel names and characters used may be different, the problem remains the same — disturbing videos hiding behind familiar cartoon characters are making their way on to YouTube and are finding young audiences.

Here's TODAY's video about the latest developments. Helpful tips to avoid such content include following only trusted YouTube channels, carefully setting and updating parental controls for video programs and apps, listening to and watching content with your children, and keeping electronic devices in an open area while they're being used.

Parents of tablet-using kids are no stranger to YouTube videos with the catchy tune of "Finger Family" songs — videos in which cartoon characters dance on the ends of illustrated fingers while singing lyrics like, "Daddy finger, daddy finger, where are you? Here I am. Here I am. How do you do?"

Several videos on the Superkidz Finger Family YouTube channel begin with familiar cartoon characters singing, then turn to images of graphic violence. YouTube/Superkidz Finger Family

While moms and dads have joked about these videos being annoying, they were presumably safe on the YouTube Kids app, a version of the video site that YouTube describes as "a delightfully simple and free app, where kids can discover videos, channels and playlists they love."

However, one channel available on YouTube Kids, Superkidz Finger Family, has offered a dark take on the "Finger Family" song. Recently, some moms have shared on social media their shock at finding graphic images of Mickey Mouse and his family, shooting one another — and themselves — in the head with guns.

As this video progresses, Mickey Mouse is shown holding a gun, shooting others and himself. YouTube/Superkidz Finger Family

YouTube removed the channel after speaking to TODAY Parents about it — days after moms started to spread the alarm about the totally inappropriate content. Beth Brister-Kaster, an Ohio mom, posted a video to Facebook, sharing one of the videos — which begins with innocent cartoons before switching to the violent scenes.

"I just deleted YouTube Kids' app forever. Never again," Brister-Kaster says as she shows the questionable video. "This is absolutely insane. This is what our children are watching...it's just Peppa Pig and then, all of a sudden, it goes to Mickey Mouse shooting people."

Brister-Kaster, whose daughter is 4, says she posted the video because she was horrified to see such content on a site she trusted.

RELATED: Child advocacy groups say YouTube Kids rife with 'inappropriate' videos

"I need to protect (my daughter) and I needed other parents to be aware of this kind of garbage that is out on the Internet," Brister-Kaster told TODAY Parents. "I definitely don't want (my daughter) to see that kind of stuff. Who would have thought to do something like that three minutes into a little kids video?"

"This is absolutely insane," Brister-Kaster says in her video. "This is what our children are watching...it's just Peppa Pig and then, all of a sudden, it goes to Mickey Mouse shooting people." YouTube/Superkidz Finger Family

Maryland mom Chaye Benjamin also posted a response to the content, filming a Facebook video while she hid in her bathroom, so that her 3-year-old daughter would not see the video a second time.

"This is a kids' YouTube channel...this isn't even a regular YouTube Channel, it's a kids' channel," Benjamin says through tears. "Please be careful. Please watch the videos with your children, don't just let them watch the videos by themselves."

"Please be careful," Benjamin said in her video. "Please watch the videos with your children, don't just let them watch the videos by themselves." YouTube/Superkidz Finger Family

Benjamin's plea to other parents inspired a Change.org petition, asking YouTube to ban the Superkidz Finger Family channel completely. Other parents left supportive comments saying they had reported the inappropriate content to YouTube.

A spokesperson for YouTube told TODAY Parents that the company works hard to ensure content found on YouTube Kids is family-friendly, adding that they take viewer feedback very seriously. The company also has plans for future updates to the YouTube Kids app, which will allow parents to further customize the types of content they want their kids to watch through the parental control area of the app.

"We appreciate people drawing problematic content to our attention, and make it possible for anyone to flag a video," said the spokesperson. "Flagged videos are manually reviewed 24/7 and any videos that don't belong in the app are removed."

When asked about the Superkidz Finger Family channel specifically, YouTube was unable to comment.

While it appears that YouTube Kids has removed the Superkidz Finger Family channel from the app, the channel is still up on YouTube, with videos clearly aimed at children containing images of Mickey Mouse and graphic scenes of murder and violence.

While YouTube guidelines state that users of the site should be thirteen or older, community guidelines forbid "violent or gory content that's primarily intended to be shocking, sensational, or disrespectful."

Still, detailed guidelines on the site make allowances for "videos that contain dramatized depictions of violence," stating that such content may be age-restricted by the site, but not banned.

After the graphic content is displayed, the video returns to normal, non-violent cartoon images. YouTube/Superkidz Finger Family

Moms like Brister-Kaster still believe the channel should be removed from YouTube entirely. And, until YouTube finds a more thorough way of filtering their children's content, Brister-Kaster says she will not allow her daughter to use the app.

Never miss a parenting story with TODAY’s newsletters! Sign up here

"We'll stick to Disney Jr. and the PBS app," said Brister-Kaster. "And, I have gotten a lot of messages from other moms that have deleted the app, too, and say the same thing.". Google-owned YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app.

Investigators found several unsuitable videos including one of a burning aeroplane from the cartoon Paw Patrol and footage explaining how to sharpen a knife.

YouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate.

There have been hundreds of disturbing videos found on YouTube Kids in recent months that are easily accessed by children.

These videos have featured horrible things happening to various characters, including ones from the Disney movie Frozen, the Minions franchise, Doc McStuffins and Thomas the Tank Engine.

Parents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service.

Scroll down for video

YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app. Investigators found several unsuitable videos including one from the cartoon Paw Patrol on a burning aeroplane and footage showing how to sharpen a knife

A YouTube spokesperson has admitted the company needs to 'do more' to tackle inappropriate videos on their kids platform.

This investigation is the latest to expose inappropriate content on the video-sharing site which has been subject to a slew of controversies since its creation in 2005.

As part of an in-depth investigation by BBC Newsround, Google's Public Policy Manager Katie O'Donovan met five children who told her about the distressing videos they had seen on the site.

They included videos showing clowns covered in blood and messages warning them there was someone at the door.

Ms O'Donovan said she was 'very, very sorry for any hurt or discomfort'.

'We've actually built a whole new platform for kids, called YouTube Kids, where we take the best content, stuff that children are most interested in and put it on there in a packaged up place just for kids,' she said.

It normally takes five days for supposedly child-friendly content like cartoons to get from YouTube to YouTube Kids.

Within that window it is hoped users and a specially-trained team will flag disturbing content.

Once it has been flagged and reviewed, it won't appear on the YouTube Kids app and only people who are signed in and older than 18 years old will be able to view it.

The company say thousands of people will be working around the clock to flag content.

However, as part of the investigation Newsround revealed there are still lots of inappropriate videos on the Kids section.

'We have seen significant investment in building the right tools so people can flag that [content], and those flags are reviewed very, very quickly', Ms O'Donovan said.

'We're also beginning to use machine learning to identify the most harmful content, which is then automatically reviewed.'

The problem was managing an open platform where content is uploaded straight onto the site, she added.

'It is a difficult environment because things are moving so, so quickly', said Ms O'Donovan.

'We have a responsibility to make sure the platform can survive and can thrive so that we have a collection that comes from around the world on there'.

By the end of last year YouTube said it had removed more than 50 user channels and had stopped running ads on more than 3.5 million videos since June.

'Content that endangers children is unacceptable to us and we have clear policies against such videos on YouTube and YouTube Kids', a YouTube spokesperson told MailOnline.

'When we discover any inappropriate content, we quickly take action to remove it from our platform.

'Over the past few months, we've taken a series of steps to tackle many of the emerging challenges around family content on YouTube, including: tightening enforcement of our Community Guidelines, age-gating content that inappropriately targets families, and removing it from the YouTube Kids app.'

YouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate (stock image)

In March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress.

Mrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her.

'Peppa does a lot of screaming and crying and the dentist is just a bit sadistic and it's just way, way off what a three-year-old should watch,' she said.

'But the animation is close enough to looking like Peppa - it's crude but it's close enough that my daughter was like 'This is Peppa Pig.''

Another video depicted Peppa Pig and a friend deliberately burning down a house with someone in it.

All of these videos are easily accessed by children through YouTube's search results or recommended videos.

In March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress. This image shows a Peppa Pig fake that depict the character being attacked by zombies

In addition to Peppa Pig, similar videos were found featuring characters from the Disney movie Frozen, the Minions franchise, Doc McStuffins, Thomas the Tank Engine, and more.

Some of the channels that run these cartoons generate millions of views from the disturbing videos.

One channel 'Toys and Funny Kids Surprise Eggs' is one of the 100 most popular videos on YouTube with over 5 billion video views total.

The channel's homepage includes a picture of a toddler next to pictures of Peppa Pig, Thomas the Tank Engine, the Cookie Monster, Mickey and Minnie Mouse and Elsa from Frozen that look official.

But the channel's videos include titled such as 'FROZEN ELSA HUGE SNOT', 'NAKED HULK LOSES HIS PANTS' and 'BLOODY ELSA: Frozen Elsa's Arm is Broken by Spiderman'.

Many of the videos feature graphic violence and toiler humour not appropriate for children.

WHAT'S THE CONTROVERSY OVER YOUTUBE'S CONTENT? YouTube has been subject to various controversies since its creation in 2005. It has become one of Google's fastest-growing operations in terms of sales by simplifying the process of distributing video online but putting in place few limits on content. However, parents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service. They have contended that Google must do more to banish and restrict access to inappropriate videos, whether it be propaganda from religious extremists and Russia or comedy skits that appear to show children being forcibly drowned. Child exploitation and inappropriate content By the end of last year YouTube said it had removed more than 50 user channels and has stopped running ads on more than 3.5 million videos since June. In March last year, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress. Mrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her. Hundreds of these disturbing videos were found on YouTube by BBC Trending back in March. By the end of last year YouTube said it had removed more than 50 user channels and has stopped running ads on more than 3.5 million videos since June. One of the deleted videos was the wildly popular Toy Freaks YouTube channel featuring a single dad and his two daughters All of these videos are easily accessed by children through YouTube's search results or recommended videos. YouTube has been getting more stringent about deleting videos. One example is the wildly popular Toy Freaks YouTube channel featuring a single dad and his two daughters that was deleted last year. Although it's unclear what exact policy the channel violated, the videos showed the girls in unusual situations that often involved gross-out food play and simulated vomiting. The channel invented the 'bad baby' genre, and some videos showed the girls pretending to urinate on each other or fishing pacifiers out of the toilet. Adverts being shown next to inappropriate videos There has been widespread criticism that adverts are being shown on some clips depicting child exploitation. YouTube has now tightened its rules on who qualifies for posting money-making ads. Previously, channels with 10,000 total views qualified for the YouTube Partner Program which allows creators to collect some income from the adverts placed before their videos. But YouTube's parent company Google has announced that from February 20, channels will need 1,000 subscribers and to have racked up 4,000 hours of watch time over the last 12 months regardless of total views, to qualify. This is the biggest change to advertising rules on the site since its inception - and is another attempt to prevent the platform being 'co-opted by bad actors' after persistent complaints from advertisers over the past twelve months. In November last year Lidl, Mars, Adidas, Cadbury maker Mondelez, Diageo and other big companies all pulled advertising from YouTube. An investigation found the video sharing site was showing clips of scantily clad children alongside the ads of major brands. One video of a pre-teenage girl in a nightie drew 6.5 million views. Issues with system for flagging inappropriate videos Another investigation in November found YouTube's system for reporting sexual comments had serious faults. As a result, volunteer moderators have revealed there could be as many as 100,000 predatory accounts leaving inappropriate comments on videos. Users use an online form to report accounts they find inappropriate. Part of this process involves sending links to the specific videos or comments they are referring to. Investigators identified 28 comments that obviously violated YouTube's guidelines. According to the BBC, some include the phone numbers of adults, or requests for videos to satisfy sexual fetishes. The children in the videos appeared to be younger than 13, the minimum age for registering an account on YouTube. Advertisement

Yesterday it was revealed YouTube has started labelling news broadcasts that get government money as it vows to be stricter about content.

A feature currently being rolled out in the US displays notices below 'propaganda' videos uploaded by news broadcasters that receive government or public money.

The move is likely to affect videos from services such as Russia-backed RT, which critics call a propaganda outlet for Moscow.

The flagging may also apply to state-chartered news organisations such as the BBC and AFP, and US-based public broadcasters.

'Our goal is to equip users with additional information to help them better understand the sources of news content that they choose to watch on YouTube', according to a blog post by YouTube News senior product manager Geoff Samek.

'News is an important vertical for us and we want to be sure to get it right.'

The blog post included a screen shot with a disclaimer about the US government-funded Radio Free Asia.