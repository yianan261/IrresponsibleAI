The driver behind the wheel of an autonomous Uber vehicle that fatally struck a woman in Tempe in March was watching "The Voice" via a streaming service in the minutes leading up to the crash, a police report says.

The detailed report of more than 300 pages was released by Tempe police Thursday night, along with video and photos from the scene of the March 18 collision. Also released was the 911 call made by the driver, Rafaela Vasquez, 44, after the crash.

The documents indicate police are seeking manslaughter charges against Vasquez.

The Mill Avenue collision, which killed 49-year-old Elaine Herzberg as she walked across the street midblock, was the first fatal crash with a pedestrian and a self-driving car.

UBER IN ARIZONA: A timeline of events leading up to shutdown of self-driving cars

The material includes blurred video from officers' body cameras. One video captures an officer's conversation with Vasquez still seated behind the wheel.

"The car was in auto-drive," Vasquez says to the officer.

"All of a sudden ... the car didn't see it, I couldn't see it," she says. "I know I hit her.''

Uber said Friday that interacting with any mobile device, including smart watches, while operating one of its vehicles on a public road is a fireable offense, and that policy is made clear to employees in training and through workplace posters.

In light of the accident, the company plans to bolster its safety training, the company said.

“We have a strict policy prohibiting mobile device usage for anyone operating our self-driving vehicles," a company spokesperson said in a written statement. "We plan to share more on the changes we’ll make to our program soon.”

Vasquez was let go from the company along with all the other autonomous drivers in Arizona when Uber decided to end its tests here in May, the company official said.

Vasquez was trained and expected to remain attentive to take control of the vehicle and avoid a collision, according to the company.

"Our system is a developmental self-driving system, which requires the close attention of a human operator at all times," Uber's spokesperson said. "Our operators are expected to maintain attentiveness to the road ahead and react when the system fails to do so, and are trained to uphold this responsibility."

Vasquez was given a field test and police initially determined she was not impaired. A few days after the crash, police obtained a search warrant for Vasquez's two cellphones and served warrants on three companies that provide streaming services — Hulu, Netflix and Google, which owns YouTube — in an effort to determine if the driver had been watching shows on her phones while driving.

One of those providers, Hulu, later provided a record of usage on one of Vasquez's phones that showed she was watching "The Voice,'' a talent competition show on NBC, right before the collision. The Hulu record showed her streaming ended at 21:59 hours — or 9:59 p.m.

The crash occurred at 10 p.m., according to records.

Tempe police, in the report, reviewed video from inside the Volvo XC90 — some of which previously was made public — that showed Vasquez looking down moments before the crash.

“She appears to be looking down at the area near her right knee at various points in the video,'' the report says. "Sometimes, her face appears to react and show a smirk or laugh at various points during the times that she is looking down. Her hands are not visible in the frame of the video during these times.’’

The report details an exhaustive analysis of data from the vehicle and re-enacting the crash at the site.

The analysis showed that nine video segments from dashboard cameras in the vehicle covered 11.8 miles prior to the crash. During that distance, Vasquez looked down 204 times toward her right knee, the report says. Of the nearly 22 minutes that elapsed during that distance, Vasquez was looking down for 6 minutes and 47 seconds.

"This crash would not have occurred if Vasquez would have been monitoring the vehicle and roadway conditions and was not distracted,'' the report says.

Tempe police referred the case to the Maricopa County Attorney's Office for possible charges. Police initially said the county attorney would determine when the report could be released.

The Maricopa County Attorney's Office referred the case to the Yavapai County Attorney's Office after citing a possible conflict of interest.

READ MORE: County attorney cites conflict in Uber case

An official with the Yavapai County Attorney's Office told The Arizona Republic Friday that a review of the case is underway. They declined to provide an estimated completion date.

A crash report indicated that the self-driving vehicle was traveling too fast for the road conditions.

It also noted that the roads were dry and level, and that there was no apparent medical condition that would have affected the driver at the time of the collision.

The report says Vasquez initially told police she had her hands "hovering'' in front of the steering wheel preparing to take over control of the vehicle. However, police noted that videos from the car show her hands were not visible.

OPINION: Roberts: Why was Arizona clueless about Uber?

The report concludes that, while Herzberg was not in a crosswalk when hit, Vasquez was "inattentive,'' failed to take control of the vehicle to avoid the crash and that her "disregard for assigned job function to intervene in a hazardous situation'' all contributed to the crash.

A report released by the National Transportation Safety Board on the crash last month showed that the emergency braking system on the car had been disabled, in part to reduce the jerkiness of the ride. That report also said toxicology reports found Herzberg had methamphetamine and marijuana in her system.

The police report released late Thursday indicated Uber company officials were consulted early on in the investigation. One video released from a police body camera shows a police sergeant talking to company representatives at the scene.

"You guys know as well as I do that this is going to be, like, an international story,'' he says. "We want to make sure we not only do what we normally do, not do anything different, but also make sure that everything is aboveboard.''

The officer added that "we hope you guys do the same,'' noting the two parties were "going to be working together.''

Staff writers Ryan Randazzo and Bree Burkitt contributed to this article.

READ MORE:. Police in Tempe, Arizona, have released a new report regarding Uber’s fatal self-driving car crash last March, which reveals that the safety driver Rafaela Vasquez was streaming The Voice on Hulu on her phone at the time of the accident, via Reuters.

The crash killed 49-year old Elaine Herzberg, who was crossing the street with a bicycle when Uber’s self-driving test car struck her at 39 mph. Why the car failed to avoid her has been the subject of scrutiny from both officials and the wider industry. The Tempe Police Department’s 318-page report of its investigation of the incident found that Vasquez’s Hulu account was watching the show for 42 minutes prior to the crash, right up until the time the accident occurred. According to the National Transportation Safety Board’s (NTSB) timeline of the crash, that means that Vasquez would have started watching her show within minutes of starting the drive.

The report claims that the crash was “entirely avoidable”

The report claims that the crash was “entirely avoidable” had Vasquez been paying more attention. Vasquez could face charges of vehicular manslaughter, although police haven’t said whether or not she will be charged yet.

In a video of the crash released by police officers, Vasquez can be seen looking down in front of her for several seconds before the crash. Vasquez had previously told investigators from the NTSB that she had been “monitoring the self-driving system interface,” which is displayed on an iPad mounted on the vehicle’s center console, at the time of the crash. She also said that both her personal and business phones were with her in the vehicle, but neither was in use until after the crash.

Uber says that in the past it didn’t monitor its safety drivers in real time, although it did perform spot checks to try and make sure they were following the rules. It also says a handful of drivers were previously fired for using a cell phone while operating test cars, though it didn’t say exactly how many. In a statement released to The Verge, an Uber spokesperson commented that “We continue to cooperate fully with ongoing investigations while conducting our own internal safety review. We have a strict policy prohibiting mobile device usage for anyone operating our self-driving vehicles. We plan to share more on the changes we’ll make to our program soon.”. . Police in Tempe, Arizona said evidence showed the "safety" driver behind the wheel of a self-driving Uber was distracted and streaming a television show on her phone right up until about the time of a fatal accident in March, deeming the crash that rocked the nascent industry "entirely avoidable."

A 318-page report from the Tempe Police Department, released late on Thursday in response to a public records request, said the driver, Rafaela Vasquez, repeatedly looked down and not at the road, glancing up just a half second before the car hit 49-year-old Elaine Herzberg, who was crossing the street at night.

According to the report, Vasquez could face charges of vehicle manslaughter. Police said that, based on testing, the crash was "deemed entirely avoidable" if Vasquez had been paying attention.

Police obtained records from Hulu, an online service for streaming television shows and movies, which showed Vasquez's account was playing the television talent show "The Voice" the night of the crash for about 42 minutes, ending at 9:59 p.m., which "coincides with the approximate time of the collision," the report says.

It is not clear if Vasquez will be charged, and police submitted their findings to county prosecutors, who will make the determination. The Maricopa County Attorney's Office referred the case to the Yavapai County Attorney's office because of a conflict and that office could not be reached late Thursday.

Vasquez could not immediately be reached for comment and Reuters could not locate her attorney.

The Uber car was in autonomous mode at the time of the crash, but Uber, like other self-driving car developers, requires a back-up driver in the car to intervene when the autonomous system fails or a tricky driving situation occurs.

Vasquez looked up just 0.5 seconds before the crash, after keeping her head down for 5.3 seconds, the Tempe Police report said. Uber's self-driving Volvo SUV was traveling at just under 44 miles-per-hour.

Uber declined to comment.

Last month, an Uber spokeswoman said the company was undergoing a "top-to-bottom safety review," and had brought on a former federal transportation official to help improve the company's safety culture. The company prohibits the use of any mobile device by safety drivers while the self-driving cars are on a public road, and drivers are told they can be fired for violating this rule.

Police said a review of video from inside the car showed Vasquez was looking down during the trip, and her face "appears to react and show a smirk or laugh at various points during the times that she is looking down." The report found that Vasquez "was distracted and looking down" for close to seven of the nearly 22 minutes prior to the collision.

Tempe Police Detective Michael McCormick asked Hulu for help in the investigation, writing in a May 10 email to the company that "this is a very serious case where the charges of vehicle manslaughter may be charged, so correctly interpreting the information provided to us is crucial." Hulu turned over the records on May 31.

According to a report last month by the National Transportation Safety Board, which is also investigating the crash, Vasquez told federal investigators she had been monitoring the self-driving interface in the car and that neither her personal nor business phones were in use until after the crash. That report showed Uber had disabled the emergency braking system in the Volvo, and Vasquez began braking less than a second after hitting Herzberg.

Herzberg, who was homeless, was walking her bicycle across the street, outside of a crosswalk on a four-lane road, the night of March 18 when she was struck by the front right side of the Volvo.

The police report faulted Herzberg for "unlawfully crossing the road at a location other than a marked crosswalk."

In addition to the report, police released on Thursday a slew of audio files of 911 calls made by Vasquez, who waited at the scene for police, and bystanders the night of the crash; photographs of Herzberg's damaged bicycle and the Uber car; and videos from police officers' body cameras that capture the minutes after the crash, including harrowing screams in the background.

The crash dealt Uber a major setback in its efforts to develop self-driving cars, and the company shuttered its autonomous car testing program in Arizona after the incident. It says it plans to begin testing elsewhere this summer, although in some cities it will have to first win over increasingly wary regulators.. . The fatal crash that killed pedestrian Elaine Herzberg in Tempe, Arizona, in March occurred because of a software bug in Uber's self-driving car technology, The Information's Amir Efrati reported on Monday. According to two anonymous sources who talked to Efrati, Uber's sensors did, in fact, detect Herzberg as she crossed the street with her bicycle. Unfortunately, the software classified her as a "false positive" and decided it didn't need to stop for her.

Distinguishing between real objects and illusory ones is one of the most basic challenges of developing self-driving car software. Software needs to detect objects like cars, pedestrians, and large rocks in its path and stop or swerve to avoid them. However, there may be other objects—like a plastic bag in the road or a trash can on the sidewalk—that a car can safely ignore. Sensor anomalies may also cause software to detect apparent objects where no objects actually exist.

Software designers face a basic tradeoff here. If the software is programmed to be too cautious, the ride will be slow and jerky, as the car constantly slows down for objects that pose no threat to the car or aren't there at all. Tuning the software in the opposite direction will produce a smooth ride most of the time—but at the risk that the software will occasionally ignore a real object. According to Efrati, that's what happened in Tempe in March—and unfortunately the "real object" was a human being.

Advertisement

"There's a reason Uber would tune its system to be less cautious about objects around the car," Efrati wrote. "It is trying to develop a self-driving car that is comfortable to ride in."

"Uber had been racing to meet an end-of-year internal goal of allowing customers in the Phoenix area to ride in Uber’s autonomous Volvo vehicles with no safety driver sitting behind the wheel," Efrati added.

The more cautiously a car's software is programmed, the more often it will slam on its brakes unnecessarily. That will produce a safer ride but also one that's not as comfortable for passengers.

This provides some useful context for Efrati's March report that cars from Cruise, GM's self-driving car subsidiary, "frequently swerve and hesitate." He wrote that Cruise cars "sometimes slow down or stop if they see a bush on the side of a street or a lane-dividing pole, mistaking it for an object in their path."

You could read that as a sign that Cruise's software isn't very good. But you could also view it as a sign that Cruise's engineers are being appropriately cautious. It's obviously much better for software to produce a jerky, erratic ride than to provide a smooth ride that occasionally runs over a pedestrian. And such caution is especially warranted when you're testing in a busy urban environment like San Francisco.

Of course, the long-term goal is for self-driving cars to become so good at recognizing objects that false positives and false negatives both become rare. But Herzberg's death provides a tragic reminder that companies shouldn't get too far ahead of themselves. Getting fully self-driving cars on the road is a worthwhile goal. But making sure that's done safely is more important.

Uber declined to comment to The Information, citing confidentiality requirements related to an ongoing investigation by the National Transportation Safety Board. We've asked Uber for comment and will update if the company responds.. ARIZONA (BLOOMBERG) - The backup "safety driver" in an Uber Technologies self-driving vehicle that killed a pedestrian in March was streaming the popular television show The Voice on her mobile phone in the moments before the crash, according to police in Arizona.

A 318-page report filed by the Tempe Police Department refutes driver Rafaela Vasquez's previous statement to federal safety investigators that she wasn't using her mobile devices when the car struck and killed a woman who was crossing the street at night.

Police were able to obtain records of Vasquez's account from the television streaming service Hulu LLC, which showed she'd streamed the talent show for 42 minutes on the night of the March 18 crash. Her stream ended at 9.59pm, around the same time Elaine Herzberg, 49, was hit by the Uber, which was in self-driving mode, the report said.

Police concluded in the report that the crash was "entirely avoidable" and said Vasquez could face vehicular manslaughter charges.

Emergency Braking Sensors on the Uber Volvo SUV's system had detected Herzberg six seconds before the impact and recognized that it was going to crash 1.3 seconds prior, according to a preliminary report by the National Transportation Safety Board. But the system couldn't activate the brakes because, according to Uber, emergency braking isn't enabled while the vehicle is under computer control, the report said. The responsibility for braking was left up to Vasquez - who didn't look up until 0.5 seconds before the accident.

Vasquez was looking away from the road for long stretches in the time before the crash, according to an internal video showing her that was released by police. She told NTSB investigators she was monitoring the self-driving system's interface. Vasquez couldn't immediately be reached for comment.

A spokesperson for Uber said the company continues to cooperate fully with ongoing investigations while conducting our own internal safety review, adding that Uber policy prohibits mobile device usage for anyone operating its self-driving vehicles.. Uber has reportedly found that a software problem likely caused a fatal accident involving one of its self-driving cars in Tempe, Arizona in March. That software is meant to determine how the car should react to detected objects, two people familiar with the matter told The Information.

Although the car's sensors reportedly detected the pedestrian, Uber's software determined that it didn't need to immediately react because of how it was tuned.

The software is supposed to ignore what are known as "false positives," or objects that wouldn't be an issue for the vehicle, like a plastic bag or piece of paper. Company executives told The Information that they believe the system was tuned in a way that made it react less to these objects -- meaning it reportedly didn't react fast enough when the pedestrian crossed the street.

During the collision, an operator was behind the wheel but the car was in autonomous mode. The operator was not looking at the road the moments before the car hit 49-year-old Elaine Herzberg at around 40 mph. Uber settled with the victim's family later that month. This was the first known fatality from an autonomous vehicle accident on a public road.

For now, Uber has temporarily halted its self-driving operations in all cities where it's been testing its vehicles, including Tempe, Phoenix, Pittsburgh, San Francisco and Toronto.

An Uber spokeswoman said the company has initiated a top-to-bottom safety review of its autonomous vehicle program and hired the former chair of the US National Transportation Safety Board, Christopher Hart, to advise the company on its overall safety culture.

"Our review is looking at everything from the safety of our system to our training processes for vehicle operators," the spokeswoman said.

Meanwhile, the Tempe police are working with Uber representatives, the NTSB and the US Department of Transportation's National Highway Traffic Safety Administration in their investigation to determine who, or what, was at fault for the accident. Uber declined to say whether the tuned-down software was responsible for the crash.

"We're actively cooperating with the NTSB in their investigation," the Uber spokeswoman said. "Out of respect for that process and the trust we've built with NTSB, we can't comment on the specifics of the incident."

First published May 7 at 11:29 a.m. PT.

Update, 11:40 a.m. PT: Adds background on the crash.. Uber has reached a settlement agreement with the family of the woman killed by an autonomous vehicle being tested by the ride-hailing company, according to Reuters.

Cristina Perez Hesano, an attorney with Bellah Perez, told the news agency that "the matter has been resolved." The terms of the agreement weren't revealed.

An Uber spokesperson declined to comment. Bellah Perez didn't immediately respond to a request for comment,

Uber's self-driving vehicle testing program has come under intense scrutiny since an accident in Tempe, Arizona, on March 18 resulted in the first fatality of a pedestrian from a car in full autonomous mode. A dashcam video of the incident released by Tempe police shows 49-year-old Elaine Herzberg walking her bike loaded with bags across a dark road.

The video, which stops at the moment of impact, also shows the vehicle operator, Rafaela Vasquez, sitting at the wheel constantly glancing down at her lap. She looks up just as the car collides with Herzberg, who was pronounced dead by the time she reached the hospital.

Arizona Gov. Doug Duccey said earlier this week he's suspending Uber's ability to test self-driving cars on the state's roads, saying he found video of the deadly collision "disturbing and alarming."

The Tempe police are working with Uber representatives, the National Transportation Safety Board and the US Department of Transportation's National Highway Traffic Safety Administration in their investigation to determine who, or what, was at fault for the accident.

iHate: CNET looks at how intolerance is taking over the internet.

Special Reports: CNET's in-depth features in one place.. Sunday marked a turning point for self-driving cars. For the first time, a car in full autonomous mode struck and killed a pedestrian.

It happened at 10 p.m. in Tempe, Arizona, where ride-hailing company Uber had been picking up passengers in autonomous vehicles for more than a year.

Elaine Herzberg, 49, was walking her bicycle down a four-lane road and was starting to cross when the gray Volvo, operated by Uber, hit her at about 40 mph, according to local police. It's believed Herzberg was homeless. She was pronounced dead by the time she reached the hospital.

Although the car was driving itself, Uber vehicle operator Rafaela Vasquez, 44, was behind the wheel.

"Our investigation did not show at this time that there were significant signs of the vehicle slowing down," Tempe Police Sgt. Roland Elcock said in a press conference Monday. The police are "going to attempt to find who was at fault and how we can better be safe," he added.

The Tempe police on Wednesday released a video of the collision. It shows footage of Herzberg crossing the road outside the vehicle and of Vasquez at the moment of impact. It's graphic and difficult to watch.

"The video is disturbing and heartbreaking to watch, and our thoughts continue to be with Elaine's loved ones," an Uber spokeswoman said in an emailed statement Wednesday. "Our cars remain grounded, and we're assisting local, state and federal authorities in any way we can."

Almost every automaker in the world -- including General Motors, BMW, Ford, Tesla Motors and Toyota -- has plans to offer self-driving cars in the next few years. Several Silicon Valley giants such as Intel and Google have invested years in developing the technologies as well. The promise: greater safety, since such vehicles use software and sensors that let them "see" and react to their surroundings supposedly faster than humans can.

Read: Levels of self-driving cars

National Transportation Safety Board investigators in Tempe, Arizona, examine the Uber vehicle involved in Sunday's fatal accident. National Transportation Safety Board

Earlier this month, Uber also began regular operation of self-driving trucks hauling cargo in Arizona, after it wrapped up its testing period. The trucks always have a safety operator who's a licensed truck driver.

For the most part, testing of the technology has shown the cars to be safe. But autonomous technology is still a work in progress. The vast majority of car tests haven't been done on public roads, and the cars are still learning how to drive. Sunday's fatality has some people questioning if the cars belong on public roads at all.

"This is their vehicle, they put it on the road, and they're responsible for making sure it's safe," said consumer attorney Neama Rahmani, who isn't involved with any of the legal proceedings regarding the accident. "Uber needs to not implement any of this until it's fully vetted and tested and 100 percent safe."

Safety first

This wasn't the first time a self-driving car has been involved in a collision.

Google reported an accident with one of its cars in March 2016, and there have been at least three crashes involving Teslas in autopilot mode, one of which was fatal -- though Tesla warns owners that autopilot isn't fully autonomous. One of Uber's vehicles was involved in another crash in Tempe last year, but no serious injuries were reported. And just last week, an Uber self-driving car in Pittsburgh was in a collision with another car.

Uber's cars have also had some other near misses in Pittsburgh, including reports of fender-benders going the wrong way down one-way streets and ignoring traffic signals, according to Quartz. No injuries have been reported.

Within hours of Uber launching its autonomous vehicles in San Francisco in December 2016, one ran a red light. A dashboard camera video of the incident, captured by a Luxor Cab taxi, shows a self-driving Volvo SUV zooming through the light long after it turned red and coming uncomfortably close to a pedestrian.

"Sadly, this is a reminder of the risk that comes with rushing to get more and more self-driving cars on public roads before we know they are safe," said David Friedman, director of cars for Consumers Union, which is the advocacy division of Consumer Reports. "The promise of self-driving cars is that they can avoid fatalities, but this tragedy makes clear they have a long way to go."

What now?

After Sunday's fatality, some regulators are starting to question whether driverless cars are ready for public roads. Both the National Transportation Safety Board and the US Department of Transportation's National Highway Traffic Safety Administration confirmed they've sent teams to Tempe to investigate the collision alongside the local police department.

"The investigation will address the vehicle's interaction with the environment, other vehicles and vulnerable road users such as pedestrians and bicyclists," the National Transportation Safety Board said in a statement.

The Tempe Police Department would like to reaffirm that fault has not been determined in this case. Ronald Elcock, Tempe Police Sergeant

The NHTSA said that, along with dispatching its Special Crash Investigation team, it's also in contact with Uber and Volvo, and with federal, state and local authorities, about the incident.

Democratic Sen. Edward Markey of Massachusetts has also issued a statement saying he's committed to working with the Senate on creating autonomous vehicle legislation that includes safety protections.

"This tragic accident underscores why we need to be exceptionally cautious when testing and deploying autonomous vehicle technologies on public roads," said Markey. "If these technologies are to reap their purported safety, efficiency, and environmental benefits, we must have robust safety, cybersecurity, and privacy rules in place."

Tempe police said that both Uber and its driver are cooperating fully with the investigation, which is still in its beginning stages. Once they're done they'll submit the investigation to the County Attorney's Office to determine if criminal charges are warranted.

Initially, Tempe Police Chief Sylvia Moir said it would've been difficult to avoid Herzberg because she was emerging from the shadows, according to the San Francisco Chronicle.

From viewing the videos, Moir said, "It's very clear it would have been difficult to avoid this collision in any kind of mode (autonomous or human-driven) based on how she came from the shadows right into the roadway."

However, Tempe police released a statement Tuesday evening saying it has yet to pinpoint responsibility for the collision.

"Chief Moir and the Tempe Police Department would like to reaffirm that fault has not been determined in this case," Sergeant Elcock said in the statement. They added that the investigation will look into driver interaction with the vehicle and opportunities for the vehicle or driver to detect Herzberg before she was struck.

Uber, best known for its ride-hailing service, has temporarily halted its self-driving operations in all cities where it's been testing its vehicles, including Tempe, Phoenix, Pittsburgh, San Francisco and Toronto. Toyota has followed suit and also put a pause on its driverless car program on public roads because of the fatality, said a company spokesman.

In a tweet Monday, Uber's CEO Dara Khosrowshahi said the company is thinking of the victim's family and it will do what it can to help the police figure out what happened. Tempe's mayor, Mark Mitchell, echoed that sentiment and said it's of utmost importance to keep the city's public roads safe.

"I support the step that Uber has taken to temporarily suspend testing in Tempe until this event is fully examined and understood," Mitchell said in a statement. "That is a responsible step to take at this time."

First published March 20, 12:05 p.m. PT.

Update, 3:25 p.m.: Adds comment from Sen. Edward Markey and information on Toyota temporarily halting its driverless car program on public roads.

Update, 6:41 p.m.: Adds statement from Tempe Police Department.

Update, March 21, 5:49 p.m.: Adds information on Tempe Police Department releasing the video of the collision and a statement from an Uber spokeswoman.

The Smartest Stuff: Innovators are thinking up new ways to make you, and the things around you, smarter.

Blockchain Decoded: CNET looks at the tech powering bitcoin -- and soon, too, a myriad of services that will change your life.. A woman was struck and killed by one of Uber's self-driving cars in Tempe, Arizona, late Sunday night. This is the first known fatality from an autonomous vehicle accident on a public road.

At the time of the collision, 10 p.m., a vehicle operator was behind the wheel but the car was in autonomous mode, according to Tempe police. No passengers were in the car.

"The vehicle was traveling northbound just south of Curry Road when a female walking outside of the crosswalk crossed the road from west to east when she was struck by the Uber vehicle," the Tempe police said in a statement. "She was transported to a local area hospital where she passed away from her injuries."

The police said Uber is assisting in the still-active investigation. The ride-hailing company has also confirmed that it's temporarily halted its self-driving car operations in all other cities where it's been testing its vehicles, including Phoenix, Pittsburgh, San Francisco and Toronto.

"Our hearts go out to the victim's family," an Uber spokeswoman said in a statement. "We are fully cooperating with local authorities in their investigation of this incident."

Uber CEO Dara Khosrowshahi also tweeted his condolences on Monday morning,

"Some incredibly sad news out of Arizona," he said. "We're thinking of the victim's family as we work with local law enforcement to understand what happened."

Watch this: Self-driving Uber kills pedestrian 01:01

Given how the woman suddenly emerged from the shadows and walked in front of the car, Tempe Police Chief Sylvia Moir said Uber may not have been at fault. Moir was able to look at the video feed from the cameras mounted in the car.

From viewing the videos, "it's very clear it would have been difficult to avoid this collision in any kind of mode (autonomous or human-driven) based on how she came from the shadows right into the roadway," Moir said in an interview with the San Francisco Chronicle.

Initially, the crash was thought to have been between a bicyclist and the Uber car, according to a report by TV station ABC15, which first reported the accident. But the woman was actually walking the bicycle, according to Tempe police.

Most companies working on self-driving cars tout the vehicles as a potentially safer alternative to human drivers. And, for the most part, testing of the technology has shown the cars to be safe. However, this isn't the first time an autonomous vehicle has been involved in a collision.

Google reported an accident with one of its self-driving cars in March 2016 and there have been at least three crashes involving Teslas in autopilot mode, one of which was fatal. Just last week, an Uber self-driving car in Pittsburgh was involved in a collision with another car. In that accident, no injuries were reported but both vehicles had serious damage.

First published March 19 at 9:07 a.m. PT.

Update, 10:36 a.m. PT: Adds that the victim was a pedestrian, not a bicyclist as initially reported.

Update, 11:07 a.m. PT: Adds comment from Tempe police, Uber spokeswoman and additional background information.

Update, 12:10 p.m. PT: Adds additional background information.

Update, 3:51 p.m. PT: Adds that the crash took place at 10 p.m. Sunday, not early Monday as was initially reported.

Update, March 20 at 7:45 a.m. PT: Adds comments from the Tempe, Arizona, police chief.

The Smartest Stuff: Innovators are thinking up new ways to make you, and the things around you, smarter.

Blockchain Decoded: CNET looks at the tech powering bitcoin -- and soon, too, a myriad of services that will change your life.. Tempe Police have released new body camera video and the police report from the night of March 19, when a self-driving Uber hit and killed a pedestrian mid-block on Mill Avenue. The backup driver was streaming the TV show "The Voice" before fatally striking the pedestrian, the police report said.

The case has now been handed over to the Yavapai County Attorney for review of possible charges against the backup driver, Rafaela Vasquez. The Maricopa County Attorney's Office was originally given the case, but handed it over to Yavapai because of a "conflict of interest."

Vasquez was still in the driver's seat of the Uber when she was questioned by officers.

"Are you alright?" an officer asked.

"Yeah," Vasquez said. "I'm just shaken up."

Vasquez is seen giving the officers her license and insurance and asked about the victim.

"Is the person OK?" Vasquez asked.

The pedestrian, Elaine Herzberg, was killed in the collision. New pictures show her bike left behind and a big dent on the right front end of the Uber.

"So what exactly happened?" an officer asked.

"The car was in auto-drive and all of a sudden, I didn't see it," Vasquez said. "The car didn't see it. And all of a sudden it was just there. Shot out in front and I know I hit them."

In a separate video released by Uber in March, Vasquez is seen looking down for a few seconds at a time. But it's unclear if it has something to do with the program.

According to a police report, investigators noted Vazquez looking down and to her right side multiple times. Before the collision occurred, Vasquez's eyes were averted from the roadway for approximately 5.3 seconds.

"Sometimes, her face appears to react and show a smirk or laugh at various points during the times that she is looking down," the report said. "Her hands are not visible in the frame of the video during these times."

Officers calculated that had Vasquez been paying attention, she could have reacted 143 feet before impact and brought the SUV to a stop about 42.6 feet before hitting Herzberg.

"This crash would not have occurred if Vasquez would have been monitoring the vehicle and roadway conditions and was not distracted," the report stated.

After the crash, Vasquez was given a field sobriety test.

"Your internal clock is a little slow, but not anything alarming," the officer is heard saying.

"I'm sick over what happened," Vasquez told the officer.

The Tempe police report says Vasquez had a “disregard for assigned job function to intervene in a hazardous situation” and the crash may have been avoided if she was watching the road.

She told officers she "slammed" on the brakes immediately after hitting the woman but Herzberg "came out of nowhere and she didn't see her," the police report said.

In the police report, the investigator explained there were no skid marks or antilock brake marks from the autonomous vehicle that would have indicated "dynamic braking" prior to the crash.

The report also says Herzberg unlawfully crossing the road at an unmarked location was a factor in the crash.

After the crash occurred, officers attempted to locate Vasquez at her Phoenix home but were unsuccessful. She was later found in Tucson and her personal and work cell phones were seized as evidence.

As part of the investigation, Tempe police reached out to YouTube, Netflix and Hulu for the video viewing history of both cell phones. Netflix was the first to respond, saying there was no activity during the specified time period on either phones.

On May 31, Hulu responded saying the most accurate way to measure playback times is to look at player "heartbeats."

It was discovered based on Hulu's heartbeat data that a user on Vasquez's account was watching an episode of "The Voice" during the time of the collision. However, the heartbeats were not consistent, meaning the user may have paused the player, switched apps, experienced buffering issues or lost service.

Uber has since shut down their self-driving program in the Valley. The victim's family reached a settlement with Uber.

The detective seeking the warrant, identified as J. Barutha, wrote that based on information from the vehicular homicide unit, "it is believed that the crime of vehicular manslaughter has occurred and that evidence of this offense is currently located in a 2017 Grey Volvo XC-90."

The National Transportation Safety Board, in a preliminary report issued last month, said the autonomous driving system on Uber's Volvo XC-90 SUV spotted Herzberg about six seconds before hitting her, but did not stop because the system used to automatically apply brakes in potentially dangerous situations had been disabled.

RELATED:Uber announces more security for riders, including abilityto dial 911 from the app

The system is disabled while Uber's cars are under computer control, "to reduce the potential for erratic vehicle behavior," the NTSB report said. Instead of the system, Uber relies on the human backup driver to intervene, the report stated. But the system is not designed to alert the driver.

An Uber spokeswoman said in a prepared statement Friday morning that any use of a mobile device by a backup driver while a vehicle is moving is a fireable offense. "This is emphasized during training and on an ongoing basis," the statement said.

The Yavapai County Attorney does not have a projected time line for a decision in the case.

RELATED: Uber and Lyft drivers willing to drive children without car seats, investigation shows

After the crash, the ride-hailing company said it did a top-to-bottom safety evaluation, reviewing internal processes and safety culture. Uber also said it brought in former transportation safety board chairman Christopher Hart to advise the company on safety.. The crash of an Uber self-driving car that killed an Arizona woman in March was “entirely avoidable,” according to police reports released by the Tempe Police Department. Cellphone data obtained by police suggests that the Uber operator was also streaming an episode of reality show The Voice at the time of the fatal incident.

The documents, released to Gizmodo in response to a public records request, show that Tempe police found that the operator of the Uber autonomous vehicle could likely have avoided the fatal crash, had she been paying attention—but instead she was likely watching a video on her phone. Police also noted that Uber’s vehicles apparently did not alert operators to take over the vehicle during incidents.

Advertisement

After the crash, Uber laid off 300 test drivers from its autonomous vehicle unit and halted testing in Arizona. An Uber spokesperson noted that using any device behind the wheel, even a smartwatch, would be considered a fireable offense.

Advertisement

The crash killed Elaine Herzberg, a 49-year-old woman who was walking across the street with a bicycle when she was struck by the Uber vehicle. Uber’s autonomous technology failed to detect Herzberg, and the human driver who was supposed to act as a safeguard appeared distracted in previously released footage. Now, several documents in the 318-page report suggest the driver, Rafaela Vasquez, may have been streaming an episode of The Voice at the time of the crash.

Advertisement

“The driver in this case could have reacted and brought the vehicle to a stop 42.61 feet prior to the pedestrian,” one of the documents concluded.

However, Tempe police also noted that Uber’s vehicles did not alert autonomous vehicle operators about when to take control of their cars.

Advertisement

“During the current development phase, vehicle operators are relied upon to perform evasive maneuvers,” one Tempe detective wrote. “I was not able to find anywhere in the literature that the self-driving systems alerts the vehicle operator to potential hazards or when they should take manual control of the vehicle to perform an evasive maneuver.” Uber’s spokesperson said that the company is reviewing safety practices and procedures and has hired former National Transportation Safety Board chair Christopher Hart as a safety advisor.

In a statement, an Uber spokesperson said the company’s policy prohibits drivers of its autonomous vehicles from using mobile devices while behind the wheel.

Advertisement

“We continue to cooperate fully with ongoing investigations while conducting our own internal safety review. We have a strict policy prohibiting mobile device usage for anyone operating our self-driving vehicles,” the spokesperson told Gizmodo. “We plan to share more on the changes we’ll make to our program soon.”

Following the crash, police obtained warrants for work and personal cellphones belonging to Vasquez. Police also sent search warrants to YouTube, Netflix, and Hulu to recover Vasquez’s viewing history on her devices around the time of the accident.

Advertisement

“The driver in this case could have reacted and brought the vehicle to a stop 42.61 feet prior to the pedestrian.”

The crash occurred around 10pm—more specifically, the report states that video from inside the car “ceases recording with a UTC time of 4:58:50 [9:58pm local time], just moments after striking the pedestrian.”

Advertisement

The data Hulu provided to authorities shows Vasquez was streaming an episode of The Voice called “The Blind Auditions, Part 5" between 9:16pm and 9:59pm local time, according to account history turned over to the police by Hulu’s legal team. In response to the warrants, YouTube and Netflix said that Vasquez was not actively viewing video on either platform at the time of the crash.

Advertisement

Hulu’s legal team initially released viewing data for another user’s account, according to the police reports, then corrected the error in response to follow-up questions from the Tempe police and released Vasquez’s data.



In reviewing footage collected from Uber’s own cameras, which filmed the car’s view of the road and Vasquez as she was driving, police determined that Vasquez was frequently distracted.

Advertisement

“She appears to be looking down at the area near her right knee at various points in the video,” the report reads. “During the 9 video clips, I found that the driver looked down 204 times with nearly all of them having the same eye placement at the lower center console near her right knee. One hundred sixty-six of these instances of looking down occurred while the vehicle was in motion.” Vasquez was appeared to laugh or smirk during moments when she was looking towards her knee, the report added.

Police didn’t just note the number of instances her eyes were off the road, they also focused on the aggregate amount of time that Vasquez was apparently distracted. “The vehicle was in motion for 21 minutes, 48 seconds. Of that time, the total amount of time the driver’s eyes were averted from the roadway was 6 minutes, 47.2 seconds, or approximately 32% of the time,” police found.

Advertisement

However, Vasquez did call 911 after the crash occurred. She also passed a field sobriety test. Approximately half an hour into the test, the officer administering it reassured Vasquez that she was unlikely to face criminal charges, according to the officer’s bodycam video.

Advertisement

Vasquez was alone in the vehicle the night of the collision—a standard practice for Uber’s autonomous vehicle operators. Several other autonomous vehicle companies have opted to have two operators in their cars while testing. This was Uber’s practice when it started testing, CityLab reported, but the company switched to single operators in late 2017. An Uber spokesperson told CityLab at the time that the second operator had been present strictly to take notes and was not expected to maintain safety.

However, a co-driver might have noticed Herzberg walking into the road. In bodycam footage following the crash, an officer who spoke to Vasquez asked about the two-person policy. “Did you have a passenger? Because I know sometimes you guys ride two people together,” the officer asks.

Advertisement

In a preliminary report released late last month, the National Transportation Safety Board found that Uber’s vehicle detected Herzberg a mere six seconds before the crash, and initiated emergency braking 1.3 seconds before impact. Uber’s system is not designed to alert the operator of a potential hazard, the NTSB report found. The NTSB’s investigation into the crash is ongoing and its final report has yet to be released.

Update 10:30am, June 22: Added statement on the police report from an Uber spokesperson.

Advertisement

Correction: A previous version of this story mistakenly stated that Uber did not train operators about vehicle takeover; that section of the police report refers to instructions provided by the vehicle itself rather than a training manual.. The person behind the wheel of the Uber self-driving Volvo SUV that struck and killed a woman in Arizona was likely streaming "The Voice" on a cellphone at the time of the incident.

The Tempe Police Department released a 318-page document late Thursday that sheds new light on what likely happened when the vehicle hit pedestrian Elaine Herzberg, 49, in March as she walked a bicycle across a road in Tempe. According to an analysis of the incident, police said "the crash was deemed entirely avoidable."

The test driver, Rafaela Vasquez, who was hired by Uber to sit behind the wheel and take over in case of emergencies, had both personal and business phones in the car at the time of the crash. Tempe Police Department's investigation requested data from Hulu, YouTube and Netflix as part of its investigation. The apps were found on one or both of Vasquez's phones.

According to data provided by Hulu, the driver was streaming NBC's popular show "The Voice" on the Hulu app for about 40 minutes. The end time coincides with the crash. Netflix and YouTube reportedly weren't in use.

Related: Uber shuts down self-driving operations in Arizona

During nine video segments obtained from the vehicle's dashcam, Vasquez looked down 204 times. The report states each of those times had "the same eye placement at the lower center console near the area of her right knee."

Vasquez's eyes were averted for a total of more than six minutes, or 3.67 miles out of 11.8 miles traveled during the trip. Vasquez had previously told National Transportation Safety Board investigators in a post-crash interview that the phones weren't in use.

Vasquez could face charges for vehicular manslaughter, according to Tempe Police Department.

Vasquez could not immediately be reached for comment.

Uber said it is cooperating with ongoing investigations and conducting its own internal safety review.

"We have a strict policy prohibiting mobile device usage for anyone operating our self-driving vehicles. We plan to share more on the changes we'll make to our program soon," according to an Uber spokesperson

The spokesperson added that any physical mobile device usage while the vehicle is on the road is a fireable offense.

The crash was a major setback for Uber's self-driving car operation. In May, Uber said it was ending self-driving car testing in Arizona and laying off 300 Uber workers. The company said it would focus on its autonomous vehicle efforts in San Francisco and Pittsburgh.

This is the latest development in the ongoing investigation into the fatal crash. In May, the National Transportation Safety Board determined Uber's self-driving car accurately identified Herzberg, but Uber had turned off the vehicle's automatic emergency braking, so the SUV did not attempt to brake. The feature was disabled to reduce the potential for unwanted braking, such as for a plastic bag in the road.

Earlier this year, Uber said it employs about 400 human safety drivers like Vasquez across various cities.

CNNMoney's Matt McFarland contributed to this report. THEY are one of the most talked-about topics in technology—but lately they have been for all the wrong reasons. A series of accidents involving self-driving cars has raised questions about the safety of these futuristic new vehicles, which are being tested on public roads in several American states. In March 2018 an experimental Uber vehicle, operating in autonomous mode, struck and killed a pedestrian in Tempe, Arizona—the first fatal accident of its kind. On May 24th America’s National Transportation Safety Board (NTSB) issued its preliminary report into the crash. What caused the accident, and what does it say about the safety of autonomous vehicles (AVs) more broadly?

The computer systems that drive cars consist of three modules. The first is the perception module, which takes information from the car’s sensors and identifies relevant objects nearby. The Uber car, a modified Volvo XC90, was equipped with cameras, radar and LIDAR (a variant of radar that uses invisible pulses of light). Cameras can spot features such as lane markings, road signs and traffic lights. Radar measures the velocity of nearby objects. LIDAR determines the shape of the car’s surroundings in fine detail, even in the dark. The readings from these sensors are combined to build a model of the world, and machine-learning systems then identify nearby cars, bicycles, pedestrians and so on. The second module is the prediction module, which forecasts how each of those objects will behave in the next few seconds. Will that car change lane? Will that pedestrian step into the road? Finally, the third module uses these predictions to determine how the vehicle should respond (the so-called “driving policy”): speed up, slow down, or steer left or right.

Of these three modules, the most difficult to build is the perception module, says Sebastian Thrun, a Stanford professor who used to lead Google’s autonomous-vehicle effort. The hardest things to identify, he says, are rarely-seen items such as debris on the road, or plastic bags blowing across a highway. In the early days of Google’s AV project, he recalls, “our perception module could not distinguish a plastic bag from a flying child.” According to the NTSB report, the Uber vehicle struggled to identify Elaine Herzberg as she wheeled her bicycle across a four-lane road. Although it was dark, the car’s radar and LIDAR detected her six seconds before the crash. But the perception system got confused: it classified her as an unknown object, then as a vehicle and finally as a bicycle, whose path it could not predict. Just 1.3 seconds before impact, the self-driving system realised that emergency braking was needed. But the car’s built-in emergency braking system had been disabled, to prevent conflict with the self-driving system; instead a human safety operator in the vehicle is expected to brake when needed. But the safety operator, who had been looking down at the self-driving system’s display screen, failed to brake in time. Ms Herzberg was hit by the vehicle and subsequently died of her injuries.

The cause of the accident therefore has many elements, but is ultimately a system-design failure. When its perception module gets confused, an AV should slow down. But unexpected braking can cause problems of its own: confused AVs have in the past been rear-ended (by human drivers) after slowing suddenly. Hence the delegation of responsibility for braking to human safety drivers, who are there to catch the system when an accident seems imminent. In theory adding a safety driver to supervise an imperfect system ensures that the system is safe overall. But that only works if they are paying attention to the road at all times. Uber is now revisiting its procedures and has suspended all testing of its AVs; it is unclear when, or even if, it will be allowed to resume testing. Other AV-makers, having analysed video from the Tempe accident, say their systems would have braked to avoid a collision. In the long term, AVs promise to be much safer than ordinary cars, given that 94% of accidents are caused by driver error. But right now the onus is on Uber and AV-makers to reassure the public that they are doing everything they can to avoid accidents on the road to a safer future.

Dig deeper

What it’s like to ride in a self-driving Uber (Mar 2018)

Why self-driving cars will be mostly shared, not owned (Mar 2018)

Reinventing wheels: a special report on self-driving cars (Mar 2018). The first rule of safe flying: Pay attention, even when you think you don’t need to. According to a 1994 review by the National Transportation Safety Board, 31 of the 37 serious accidents that occurred on U.S. air carriers between 1978 and 1990 involved “inadequate monitoring.” Pilots, officers, and other crew members neglected to crosscheck instruments, confirm inputs, or speak up when they caught an error.

Over the period of that study, aviation had moved into the automation era, as Maria Konnikova reported for The New Yorker in 2014. Cockpit controls that once required constant vigilance now maintained themselves, only asking for human intervention on an as-needed basis. The idea was reduce the margin of error via the precision of machines, and that was the effect, in some respects. But as planes increasingly flew themselves, pilots became more complacent. The computers had introduced a new problem: the hazardous expectation that a human operator can take control of an automated machine in the moments before disaster when their attention isn’t otherwise much required.. An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.

Tempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. There was a vehicle operator inside the car at the time of the crash.

Uber said in a statement on Twitter: “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.” A spokesman declined to comment further on the crash.

The company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Dara Khosrowshahi, Uber’s CEO, tweeted: “Some incredibly sad news out of Arizona. We’re thinking of the victim’s family as we work with local law enforcement to understand what happened.”

Uber has been testing its self-driving cars in numerous states and temporarily suspended its vehicles in Arizona last year after a crash involving one of its vehicles, a Volvo SUV. When the company first began testing its self-driving cars in California in 2016, the vehicles were caught running red lights, leading to a high-profile dispute between state regulators and the San Francisco-based corporation.



Police identified the victim as 49-year-old Elaine Herzberg and said she was walking outside of the crosswalk with a bicycle when she was hit at around 10pm on Sunday. Images from the scene showed a damaged bike. The 2017 Volvo SUV was traveling at roughly 40 miles an hour, and it did not appear that the car slowed down as it approached the woman, said Tempe sergeant Ronald Elcock.

Elcock said he had watched footage of the collision, which has not been released to the public. Police identified the operator of the car as Rafaela Vasquez, 44, and said she was cooperative and there were no signs of impairment.

View image in fullscreen A still image taken from video provided by ABC-15 at the scene, where a pedestrian with a bicycle was hit. Photograph: AP

The self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.

John M Simpson, privacy and technology project director with Consumer Watchdog, said the collision highlighted the need for tighter regulations of the nascent technology.

“The robot cars cannot accurately predict human behavior, and the real problem comes in the interaction between humans and the robot vehicles,” said Simpson, whose advocacy group called for a national moratorium on autonomous car testing in the wake of the deadly collision.

Simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.

Tesla Motors was the first to disclose a death involving a self-driving car in 2016 when the sensors of a Model S driving in autopilot mode failed to detect a large white 18-wheel truck and trailer crossing the highway. The car drove full speed under the trailer, causing the collision that killed the 40-year-old behind the wheel in the Tesla.

Earlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.

“The technology is not ready for it yet, and this just sadly proves it,” said Simpson.

In one recent incident, California police officers found a Tesla that was stopped in the middle of a five-lane highway and found a driver asleep behind the wheel. The man said the vehicle was in “autopilot”, which is Tesla’s semi-autonomous driver assist system, and he was arrested on suspicion of drunk driving.

View image in fullscreen An Uber self-driving car travels in Pittsburgh, Pennsylvania. Photograph: Angelo Merendino/AFP/Getty Images

In another recent case, a Tesla car rear-ended a fire truck on a freeway, with the driver again telling the authorities the car was in autopilot mode at the time of the collision.

Michael G Bennett, an Arizona State University associate research professor who studies autonomous cars, said the self-driving vehicles have become ubiquitous around campus and on the streets in Tempe. Often they have operators behind the wheels, but sometimes they are fully autonomous with no human inside.

The fatal collision could spark significant calls for reform and reflections within the industry, he said.

“It may be problematic for the industry, because one of their central arguments for the value of the technology is that it is superior to human drivers,” said Bennett, adding that autonomous cars should be able to detect pedestrians and avoid hitting them, even if they aren’t in crosswalks: “Every day, pedestrians in cities around the world step outside of the crosswalk.”

The governor of Arizona, Doug Ducey, has been a strong proponent of allowing corporations to test the technology in his state, publicly slamming other governments for “over regulation” and in 2016 urging Uber to “ditch California” and launch in his region. In March, he issued new rules and said that more than 600 automated vehicles have driven on public roads in the state.

“Our prayers are with the victim, and our hearts go out to her family,” Patrick Ptak, Ducey’s spokesman, said in an email to the Guardian, adding, “Public safety is our top priority.”

Linda Bailey, the executive director of the National Association of City Transportation Officials (Nacto), said in an interview that there has not been enough regulatory oversight of testing and that some governments are overwhelmed trying to understand autonomous technology and its limitations.

“There’s an essential role for the public sector in regulating the safety of these vehicles, which has been largely left to private companies,” she said, adding that Nacto supports third-party testing of the vehicles.

Tempe’s mayor, Mark Mitchell, defended the city’s ongoing support of autonomous vehicles in a statement Monday, saying: “All indications we had in the past show that traffic laws are being obeyed by the companies testing here.”. SAN FRANCISCO -- Uber is pulling its self-driving cars out of Arizona. The ride-sharing company's reversal was triggered by the recent death of woman who was run over by one of its robotic vehicles while crossing a darkened street in a Phoenix suburb.

The decision announced Wednesday means Uber won't be bringing back its self-driving cars to the streets to Arizona, eliminating the jobs of about 300 people who served as backup drivers and performed other jobs connected to the vehicles.

Uber had suspended testing of its self-driving vehicles in Arizona, Pittsburgh, San Francisco and Toronto while regulators investigated the cause of a March 18 crash that killed 49-year-old Elaine Herzberg in Tempe, Arizona. It marked the first death involving a fully autonomous vehicle, raising questions about the safety of computer-controlled cars being built by Uber and dozens of other companies, including Google spin-off Waymo.

Tempe police said, meanwhile, that they have completed a report into the fatal March crash. They said the in-depth traffic collision investigation was submitted Wednesday to the Maricopa County Attorney's Office for review. It's still considered an active investigation and police aren't releasing the report or any details of their investigation.

Uber still plans to build and test self-driving cars, which the San Francisco company considers to be critical to maintaining its early lead in the ride-hailing market. This as Waymo and other rivals prepare to enter the field with robotic vehicles that may be able to offer cheaper fares.

In a Wednesday statement, Uber said its self-driving cars will return to Pittsburgh this summer. The company said it is focusing its efforts to build self-driving cars in that city as well as in San Francisco, although it didn't make a commitment to bring its robotic vehicles back to the streets of California, where it no longer has a permit to operate them after allowing its license in that state to expire earlier this year.

About 550 Uber employees will remain in Arizona working on its other operations in the state, including its traditional ride-hailing service with cars driven by humans responding to requests made through a mobile app.

Uber brought a fleet of self-driving cars to Arizona at the end of 2016, just days after the vehicles were banned from California for not having the proper permits at that time.

California's action prompted Arizona Gov. Doug Ducey to send out a derisive tweet in an effort to persuade Uber to bring its self-driving cars to his state. "This is what OVER-regulation looks like!" Ducey wrote.

Ducey prohibited Uber from continuing its tests of self-driving cars after Herzberg was run over, a ban that a spokesman said Wednesday remains in effect.

"The governor's focus has always been on what's best for Arizonans and for public safety, not for any one company," said Ducey spokesman Daniel Scarpinato.

The fatal collision involving Uber's self-driving car added to the headaches vexing CEO Dara Khosrowshahi as he tries to repair the damage done by a regime led by his predecessor, Uber co-founder Travis Kalanick. The company is trying to recover from a wave of revelations and allegations about rampant sexual harassment in Uber's workforce, a cover-up of a massive data breach , dirty tricks and stolen trade secrets .

Khosrowshahi has promised he won't allow Uber's self-driving cars back on public roads again until he is convinced the vehicles are safe. That won't happen until Uber completes "a top-to-bottom safety review," according to a statement the company issued Wednesday. As part of that process, Uber hired Christopher Hart, a former chairman of the National Transportation Safety Board, to review its self-driving car program.

Meanwhile, Waymo is preparing to launch a ride-hailing service in Arizona that will pick up passengers in robotic cars that won't have humans to take control if the vehicle malfunctions. The service is supposed to begin before the end of this year.. A self-driving Uber in Tempe, Arizona, struck and killed a woman at a crosswalk yesterday (March 18), the New York Times reports.

The car was in autonomous mode, but had an Uber safety driver in the driver’s seat. The woman had walked into the street outside of a crosswalk and was hit by the car. She later died from her injuries. It’s believed that this is the first time an autonomous car has killed a pedestrian.

Advertisement

In the wake of the crash, Uber has now suspended all its self-driving car tests in the city, as well as in the Bay Area, Pittsburgh, and Toronto, according to The Wall Street Journal (paywall).

Uber is cooperating with the Tempe police investigation, the company told Quartz.

Advertisement

Uber’s self-driving cars have been involved in multiple fender-benders and traffic errors since they were first introduced to the streets of Pittsburgh in late 2016. Almost exactly a year ago, one of its cars flipped over in Tempe while in autonomous mode.

Arizona has experienced a surge of pedestrian fatalities recently, with more than 10 in a single week of March in Phoenix alone. The state has the highest rate of pedestrian fatalities in the United States.. The 18th of March 2018, was the day tech insiders had been dreading. That night, a new moon added almost no light to a poorly lit four-lane road in Tempe, Arizona, as a specially adapted Uber Volvo XC90 detected an object ahead. Part of the modern gold rush to develop self-driving vehicles, the SUV had been driving autonomously, with no input from its human backup driver, for 19 minutes. An array of radar and light-emitting lidar sensors allowed onboard algorithms to calculate that, given their host vehicle’s steady speed of 43mph, the object was six seconds away – assuming it remained stationary. But objects in roads seldom remain stationary, so more algorithms crawled a database of recognizable mechanical and biological entities, searching for a fit from which this one’s likely behavior could be inferred.

At first the computer drew a blank; seconds later, it decided it was dealing with another car, expecting it to drive away and require no special action. Only at the last second was a clear identification found – a woman with a bike, shopping bags hanging confusingly from handlebars, doubtless assuming the Volvo would route around her as any ordinary vehicle would. Barred from taking evasive action on its own, the computer abruptly handed control back to its human master, but the master wasn’t paying attention. Elaine Herzberg, aged 49, was struck and killed, leaving more reflective members of the tech community with two uncomfortable questions: was this algorithmic tragedy inevitable? And how used to such incidents would we, should we, be prepared to get?

“In some ways we’ve lost agency. When programs pass into code and code passes into algorithms and then algorithms start to create new algorithms, it gets farther and farther from human agency. Software is released into a code universe which no one can fully understand.”

View image in fullscreen ‘When algorithms start to create new algorithms, it gets farther and farther from human agency,’ says Ellen Ullman. Photograph: Sean Smith/The Guardian

If these words sound shocking, they should, not least because Ellen Ullman, in addition to having been a distinguished professional programmer since the 1970s, is one of the few people to write revealingly about the process of coding. There’s not much she doesn’t know about software in the wild.

“People say, ‘Well, what about Facebook – they create and use algorithms and they can change them.’ But that’s not how it works. They set the algorithms off and they learn and change and run themselves. Facebook intervene in their running periodically, but they really don’t control them. And particular programs don’t just run on their own, they call on libraries, deep operating systems and so on ...”

What is an algorithm?

Few subjects are more constantly or fervidly discussed right now than algorithms. But what is an algorithm? In fact, the usage has changed in interesting ways since the rise of the internet – and search engines in particular – in the mid-1990s. At root, an algorithm is a small, simple thing; a rule used to automate the treatment of a piece of data. If a happens, then do b; if not, then do c. This is the “if/then/else” logic of classical computing. If a user claims to be 18, allow them into the website; if not, print “Sorry, you must be 18 to enter”. At core, computer programs are bundles of such algorithms. Recipes for treating data. On the micro level, nothing could be simpler. If computers appear to be performing magic, it’s because they are fast, not intelligent.

Recent years have seen a more portentous and ambiguous meaning emerge, with the word “algorithm” taken to mean any large, complex decision-making software system; any means of taking an array of input – of data – and assessing it quickly, according to a given set of criteria (or “rules”). This has revolutionized areas of medicine, science, transport, communication, making it easy to understand the utopian view of computing that held sway for many years. Algorithms have made our lives better in myriad ways.

Only since 2016 has a more nuanced consideration of our new algorithmic reality begun to take shape. If we tend to discuss algorithms in almost biblical terms, as independent entities with lives of their own, it’s because we have been encouraged to think of them in this way. Corporations like Facebook and Google have sold and defended their algorithms on the promise of objectivity, an ability to weigh a set of conditions with mathematical detachment and absence of fuzzy emotion. No wonder such algorithmic decision-making has spread to the granting of loans/ bail/benefits/college places/job interviews and almost anything requiring choice.

We no longer accept the sales pitch for this type of algorithm so meekly. In her 2016 book Weapons of Math Destruction, Cathy O’Neil, a former math prodigy who left Wall Street to teach and write and run the excellent mathbabe blog, demonstrated beyond question that, far from eradicating human biases, algorithms could magnify and entrench them. After all, software is written by overwhelmingly affluent white and Asian men – and it will inevitably reflect their assumptions (Google “racist soap dispenser” to see how this plays out in even mundane real-world situations). Bias doesn’t require malice to become harm, and unlike a human being, we can’t easily ask an algorithmic gatekeeper to explain its decision. O’Neil called for “algorithmic audits” of any systems directly affecting the public, a sensible idea that the tech industry will fight tooth and nail, because algorithms are what the companies sell; the last thing they will volunteer is transparency.

The good news is that this battle is under way. The bad news is that it’s already looking quaint in relation to what comes next. So much attention has been focused on the distant promises and threats of artificial intelligence, AI, that almost no one has noticed us moving into a new phase of the algorithmic revolution that could be just as fraught and disorienting – with barely a question asked.

View image in fullscreen Cathy O’Neil has shown that algorithms can magnify human biases. Photograph: Adam Morganstern

The algorithms flagged by O’Neil and others are opaque but predictable: they do what they’ve been programmed to do. A skilled coder can in principle examine and challenge their underpinnings. Some of us dream of a citizen army to do this work, similar to the network of amateur astronomers who support professionals in that field. Legislation to enable this seems inevitable.

We might call these algorithms “dumb”, in the sense that they’re doing their jobs according to parameters defined by humans. The quality of result depends on the thought and skill with which they were programmed. At the other end of the spectrum is the more or less distant dream of human-like artificial general intelligence, or AGI. A properly intelligent machine would be able to question the quality of its own calculations, based on something like our own intuition (which we might think of as a broad accumulation of experience and knowledge). To put this into perspective, Google’s DeepMind division has been justly lauded for creating a program capable of mastering arcade games, starting with nothing more than an instruction to aim for the highest possible score. This technique is called “reinforcement learning” and works because a computer can play millions of games quickly in order to learn what generates points. Some call this form of ability “artificial narrow intelligence”, but here the word “intelligent” is being used much as Facebook uses “friend” – to imply something safer and better understood than it is. Why? Because the machine has no context for what it’s doing and can’t do anything else. Neither, crucially, can it transfer knowledge from one game to the next (so-called “transfer learning”), which makes it less generally intelligent than a toddler, or even a cuttlefish. We might as well call an oil derrick or an aphid “intelligent”. Computers are already vastly superior to us at certain specialized tasks, but the day they rival our general ability is probably some way off – if it ever happens. Human beings may not be best at much, but we’re second-best at an impressive range of things.

Here’s the problem. Between the “dumb” fixed algorithms and true AI lies the problematic halfway house we’ve already entered with scarcely a thought and almost no debate, much less agreement as to aims, ethics, safety, best practice. If the algorithms around us are not yet intelligent, meaning able to independently say “that calculation/course of action doesn’t look right: I’ll do it again”, they are nonetheless starting to learn from their environments. And once an algorithm is learning, we no longer know to any degree of certainty what its rules and parameters are. At which point we can’t be certain of how it will interact with other algorithms, the physical world, or us. Where the “dumb” fixed algorithms – complex, opaque and inured to real time monitoring as they can be – are in principle predictable and interrogable, these ones are not. After a time in the wild, we no longer know what they are: they have the potential to become erratic. We might be tempted to call these “frankenalgos” – though Mary Shelley couldn’t have made this up.

Clashing codes

View image in fullscreen Algorithms are beginning to learn from their environments. Illustration: Marco Goran Romano

These algorithms are not new in themselves. I first encountered them almost five years ago while researching a piece for the Guardian about high frequency trading (HFT) on the stock market. What I found was extraordinary: a human-made digital ecosystem, distributed among racks of black boxes crouched like ninjas in billion-dollar data farms – which is what stock markets had become. Where once there had been a physical trading floor, all action had devolved to a central server, in which nimble, predatory algorithms fed off lumbering institutional ones, tempting them to sell lower and buy higher by fooling them as to the state of the market. Human HFT traders (although no human actively traded any more) called these large, slow participants “whales”, and they mostly belonged to mutual and pension funds – ie the public. For most HFT shops, whales were now the main profit source. In essence, these algorithms were trying to outwit each other; they were doing invisible battle at the speed of light, placing and cancelling the same order 10,000 times per second or slamming so many into the system that the whole market shook – all beyond the oversight or control of humans.

No one could be surprised that this situation was unstable. A “flash crash” had occurred in 2010, during which the market went into freefall for five traumatic minutes, then righted itself over another five – for no apparent reason. I travelled to Chicago to see a man named Eric Hunsader, whose prodigious programming skills allowed him to see market data in far more detail than regulators, and he showed me that by 2014, “mini flash crashes” were happening every week. Even he couldn’t prove exactly why, but he and his staff had begun to name some of the “algos” they saw, much as crop circle hunters named the formations found in English summer fields, dubbing them “Wild Thing”, “Zuma”, “The Click” or “Disruptor”.

Neil Johnson, a physicist specializing in complexity at George Washington University, made a study of stock market volatility. “It’s fascinating,” he told me. “I mean, people have talked about the ecology of computer systems for years in a vague sense, in terms of worm viruses and so on. But here’s a real working system that we can study. The bigger issue is that we don’t know how it’s working or what it could give rise to. And the attitude seems to be ‘out of sight, out of mind’.”

Facebook would claim they know what’s going on at the micro level … But what happens at the level of the population​? Neil Johnson

Significantly, Johnson’s paper on the subject was published in the journal Nature and described the stock market in terms of “an abrupt system-wide transition from a mixed human-machine phase to a new all-machine phase characterized by frequent black swan [ie highly unusual] events with ultrafast durations”. The scenario was complicated, according to the science historian George Dyson, by the fact that some HFT firms were allowing the algos to learn – “just letting the black box try different things, with small amounts of money, and if it works, reinforce those rules. We know that’s been done. Then you actually have rules where nobody knows what the rules are: the algorithms create their own rules – you let them evolve the same way nature evolves organisms.” Non-finance industry observers began to postulate a catastrophic global “splash crash”, while the fastest-growing area of the market became (and remains) instruments that profit from volatility. In his 2011 novel The Fear Index, Robert Harris imagines the emergence of AGI – of the Singularity, no less – from precisely this digital ooze. To my surprise, no scientist I spoke to would categorically rule out such a possibility.

All of which could be dismissed as high finance arcana, were it not for a simple fact. Wisdom used to hold that technology was adopted first by the porn industry, then by everyone else. But the 21st century’s porn is finance, so when I thought I saw signs of HFT-like algorithms causing problems elsewhere, I called Neil Johnson again.

“You’re right on point,” he told me: a new form of algorithm is moving into the world, which has “the capability to rewrite bits of its own code”, at which point it becomes like “a genetic algorithm”. He thinks he saw evidence of them on fact-finding forays into Facebook (“I’ve had my accounts attacked four times,” he adds). If so, algorithms are jousting there, and adapting, as on the stock market. “After all, Facebook is just one big algorithm,” Johnson says.

View image in fullscreen ‘Facebook is just one big algorithm,’ says the physicist Neil Johnson. Photograph: Christophe Morin/IP3/Getty Images

“And I think that’s exactly the issue Facebook has. They can have simple algorithms to recognize my face in a photo on someone else’s page, take the data from my profile and link us together. That’s a very simple concrete algorithm. But the question is what is the effect of billions of such algorithms working together at the macro level? You can’t predict the learned behavior at the level of the population from microscopic rules. So Facebook would claim that they know exactly what’s going on at the micro level, and they’d probably be right. But what happens at the level of the population? That’s the issue.”

To underscore this point, Johnson and a team of colleagues from the University of Miami and Notre Dame produced a paper, Emergence of Extreme Subpopulations from Common Information and Likely Enhancement from Future Bonding Algorithms, purporting to mathematically prove that attempts to connect people on social media inevitably polarize society as a whole. He thinks Facebook and others should model (or be made to model) the effects of their algorithms in the way climate scientists model climate change or weather patterns.

O’Neil says she consciously excluded this adaptive form of algorithm from Weapons of Math Destruction. In a convoluted algorithmic environment where nothing is clear, apportioning responsibility to particular segments of code becomes extremely difficult. This makes them easier to ignore or dismiss, because they and their precise effects are harder to identify, she explains, before advising that if I want to see them in the wild, I should ask what a flash crash on Amazon might look like.

“I’ve been looking out for these algorithms, too,” she says, “and I’d been thinking: ‘Oh, big data hasn’t gotten there yet.’ But more recently a friend who’s a bookseller on Amazon has been telling me how crazy the pricing situation there has become for people like him. Every so often you will see somebody tweet ‘Hey, you can buy a luxury yarn on Amazon for $40,000.’ And whenever I hear that kind of thing, I think: ‘Ah! That must be the equivalent of a flash crash!’”

Anecdotal evidence of anomalous events on Amazon is plentiful, in the form of threads from bemused sellers, and at least one academic paper from 2016, which claims: “Examples have emerged of cases where competing pieces of algorithmic pricing software interacted in unexpected ways and produced unpredictable prices, as well as cases where algorithms were intentionally designed to implement price fixing.” The problem, again, is how to apportion responsibility in a chaotic algorithmic environment where simple cause and effect either doesn’t apply or is nearly impossible to trace. As in finance, deniability is baked into the system.

Real-life dangers

Where safety is at stake, this really matters. When a driver ran off the road and was killed in a Toyota Camry after appearing to accelerate wildly for no obvious reason, Nasa experts spent six months examining the millions of lines of code in its operating system, without finding evidence for what the driver’s family believed had occurred, but the manufacturer steadfastly denied – that the car had accelerated of its own accord. Only when a pair of embedded software experts spent 20 months digging into the code were they able to prove the family’s case, revealing a twisted mass of what programmers call “spaghetti code”, full of algorithms that jostled and fought, generating anomalous, unpredictable output. The autonomous cars currently being tested may contain 100m lines of code and, given that no programmer can anticipate all possible circumstances on a real-world road, they have to learn and receive constant updates. How do we avoid clashes in such a fluid code milieu, not least when the algorithms may also have to defend themselves from hackers?

You have all these pieces of code running on people’s iPhones, and collectively it acts like one multicellular organism George Dyson

Twenty years ago, George Dyson anticipated much of what is happening today in his classic book Darwin Among the Machines. The problem, he tells me, is that we’re building systems that are beyond our intellectual means to control. We believe that if a system is deterministic (acting according to fixed rules, this being the definition of an algorithm) it is predictable – and that what is predictable can be controlled. Both assumptions turn out to be wrong.

“It’s proceeding on its own, in little bits and pieces,” he says. “What I was obsessed with 20 years ago that has completely taken over the world today are multicellular, metazoan digital organisms, the same way we see in biology, where you have all these pieces of code running on people’s iPhones, and collectively it acts like one multicellular organism.

“There’s this old law called Ashby’s law that says a control system has to be as complex as the system it’s controlling, and we’re running into that at full speed now, with this huge push to build self-driving cars where the software has to have a complete model of everything, and almost by definition we’re not going to understand it. Because any model that we understand is gonna do the thing like run into a fire truck ’cause we forgot to put in the fire truck.”

Unlike our old electro-mechanical systems, these new algorithms are also impossible to test exhaustively. Unless and until we have super-intelligent machines to do this for us, we’re going to be walking a tightrope.

View image in fullscreen Federal investigators examine the self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona. Photograph: Handout/Reuters

Dyson questions whether we will ever have self-driving cars roaming freely through city streets, while Toby Walsh, a professor of artificial intelligence at the University of New South Wales who wrote his first program at age 13 and ran a tyro computing business by his late teens, explains from a technical perspective why this is.

“No one knows how to write a piece of code to recognize a stop sign. We spent years trying to do that kind of thing in AI – and failed! It was rather stalled by our stupidity, because we weren’t smart enough to learn how to break the problem down. You discover when you program that you have to learn how to break the problem down into simple enough parts that each can correspond to a computer instruction [to the machine]. We just don’t know how to do that for a very complex problem like identifying a stop sign or translating a sentence from English to Russian – it’s beyond our capability. All we know is how to write a more general purpose algorithm that can learn how to do that given enough examples.”

Hence the current emphasis on machine learning. We now know that Herzberg, the pedestrian killed by an automated Uber car in Arizona, died because the algorithms wavered in correctly categorizing her. Was this a result of poor programming, insufficient algorithmic training or a hubristic refusal to appreciate the limits of our technology? The real problem is that we may never know.

“And we will eventually give up writing algorithms altogether,” Walsh continues, “because the machines will be able to do it far better than we ever could. Software engineering is in that sense perhaps a dying profession. It’s going to be taken over by machines that will be far better at doing it than we are.”

Walsh believes this makes it more, not less, important that the public learn about programming, because the more alienated we become from it, the more it seems like magic beyond our ability to affect. When shown the definition of “algorithm” given earlier in this piece, he found it incomplete, commenting: “I would suggest the problem is that algorithm now means any large, complex decision making software system and the larger environment in which it is embedded, which makes them even more unpredictable.” A chilling thought indeed. Accordingly, he believes ethics to be the new frontier in tech, foreseeing “a golden age for philosophy” – a view with which Eugene Spafford of Purdue University, a cybersecurity expert, concurs.

“Where there are choices to be made, that’s where ethics comes in. And we tend to want to have an agency that we can interrogate or blame, which is very difficult to do with an algorithm. This is one of the criticisms of these systems so far, in that it’s not possible to go back and analyze exactly why some decisions are made, because the internal number of choices is so large that how we got to that point may not be something we can ever recreateto prove culpability beyond doubt.”

The counter-argument is that, once a program has slipped up, the entire population of programs can be rewritten or updated so it doesn’t happen again – unlike humans, whose propensity to repeat mistakes will doubtless fascinate intelligent machines of the future. Nonetheless, while automation should be safer in the long run, our existing system of tort law, which requires proof of intention or negligence, will need to be rethought. A dog is not held legally responsible for biting you; its owner might be, but only if the dog’s action is thought foreseeable. In an algorithmic environment, many unexpected outcomes may not have been foreseeable to humans – a feature with the potential to become a scoundrel’s charter, in which deliberate obfuscation becomes at once easier and more rewarding. Pharmaceutical companies have benefited from the cover of complexity for years (see the case of Thalidomide), but here the consequences could be both greater and harder to reverse.

The military stakes

Commerce, social media, finance and transport may come to look like small beer in future, however. If the military no longer drives innovation as it once did, it remains tech’s most consequential adopter. No surprise, then, that an outpouring of concern among scientists and tech workers has accompanied revelations that autonomous weapons are ghosting toward the battlefield in what amounts to an algorithmic arms race. A robotic sharpshooter currently polices the demilitarized zone between North and South Korea, and while its manufacturer, Samsung, denies it to be capable of autonomy, this claim is widely disbelieved. Russia, China and the US all claim to be at various stages of developing swarms of coordinated, weaponized drones , while the latter plans missiles able to hover over a battlefield for days, observing, before selecting their own targets. A group of Google employees resigned over and thousands more questioned the tech monolith’s provision of machine learning software to the Pentagon’s Project Maven “algorithmic warfare” program – concerns to which management eventually responded, agreeing not to renew the Maven contract and to publish a code of ethics for the use of its algorithms. At time of writing, competitors including Amazon and Microsoft have resisted following suit.

View image in fullscreen Google employees resigned over the company’s provision of machine learning software to the Pentagon’s ‘algorithmic warfare’ program. Photograph: Mike Blake/Reuters

In common with other tech firms, Google had claimed moral virtue for its Maven software: that it would help choose targets more efficiently and thereby save lives. The question is how tech managers can presume to know what their algorithms will do or be directed to do in situ – especially given the certainty that all sides will develop adaptive algorithmic counter-systems designed to confuse enemy weapons. As in the stock market, unpredictability is likely to be seen as an asset rather than handicap, giving weapons a better chance of resisting attempts to subvert them. In this and other ways we risk in effect turning our machines inside out, wrapping our everyday corporeal world in spaghetti code.

Lucy Suchman of Lancaster University in the UK co-authored an open letter from technology researchers to Google, asking them to reflect on the rush to militarize their work. Tech firms’ motivations are easy to fathom, she says: military contracts have always been lucrative. For the Pentagon’s part, a vast network of sensors and surveillance systems has run ahead of any ability to use the screeds of data so acquired.

“They are overwhelmed by data, because they have new means to collect and store it, but they can’t process it. So it’s basically useless – unless something magical happens. And I think their recruitment of big data companies is a form of magical thinking in the sense of: ‘Here is some magic technology that will make sense of all this.’”

Suchman also offers statistics that shed chilling light on Maven. According to analysis carried out on drone attacks in Pakistan from 2003-13, fewer than 2% of people killed in this way are confirmable as “high value” targets presenting a clear threat to the United States. In the region of 20% are held to be non-combatants, leaving more than 75% unknown. Even if these figures were out by a factor of two – or three, or four – they would give any reasonable person pause.

“So here we have this very crude technology of identification and what Project Maven proposes to do is automate that. At which point it becomes even less accountable and open to questioning. It’s a really bad idea.”

Suchman’s colleague Lilly Irani, at the University of California, San Diego, reminds us that information travels around an algorithmic system at the speed of light, free of human oversight. Technical discussions are often used as a smokescreen to avoid responsibility, she suggests.

“When we talk about algorithms, sometimes what we’re talking about is bureaucracy. The choices algorithm designers and policy experts make are presented as objective, where in the past someone would have had to take responsibility for them. Tech companies say they’re only improving accuracy with Maven – ie the right people will be killed rather than the wrong ones – and in saying that, the political assumption that those people on the other side of the world are more killable, and that the US military gets to define what suspicion looks like, go unchallenged. So technology questions are being used to close off some things that are actually political questions. The choice to use algorithms to automate certain kinds of decisions is political too.”

The legal conventions of modern warfare, imperfect as they might be, assume human accountability for decisions taken. At the very least, algorithmic warfare muddies the water in ways we may grow to regret. A group of government experts is debating the issue at the UN convention on certain conventional weapons (CCW) meeting in Geneva this week.

Searching for a solution

View image in fullscreen ‘Basically, we need a new science,’ says Neil Johnson. Illustration: Marco Goran Romano

Solutions exist or can be found for most of the problems described here, but not without incentivizing big tech to place the health of society on a par with their bottom lines. More serious in the long term is growing conjecture that current programming methods are no longer fit for purpose given the size, complexity and interdependency of the algorithmic systems we increasingly rely on. One solution, employed by the Federal Aviation Authority in relation to commercial aviation, is to log and assess the content of all programs and subsequent updates to such a level of detail that algorithmic interactions are well understood in advance – but this is impractical on a large scale. Portions of the aerospace industry employ a relatively new approach called model-based programming, in which machines do most of the coding work and are able to test as they go.

Model-based programming may not be the panacea some hope for, however. Not only does it push humans yet further from the process, but Johnson, the physicist, conducted a study for the Department of Defense that found “extreme behaviors that couldn’t be deduced from the code itself” even in large, complex systems built using this technique. Much energy is being directed at finding ways to trace unexpected algorithmic behavior back to the specific lines of code that caused it. No one knows if a solution (or solutions) will be found, but none are likely to work where aggressive algos are designed to clash and/or adapt.

As we wait for a technological answer to the problem of soaring algorithmic entanglement, there are precautions we can take. Paul Wilmott, a British expert in quantitative analysis and vocal critic of high frequency trading on the stock market, wryly suggests “learning to shoot, make jam and knit”. More practically, Spafford, the software security expert, advises making tech companies responsible for the actions of their products, whether specific lines of rogue code – or proof of negligence in relation to them – can be identified or not. He notes that the venerable Association for Computing Machinery has updated its code of ethics along the lines of medicine’s Hippocratic oath, to instruct computing professionals to do no harm and consider the wider impacts of their work. Johnson, for his part, considers our algorithmic discomfort to be at least partly conceptual; growing pains in a new realm of human experience. He laughs in noting that when he and I last spoke about this stuff a few short years ago, my questions were niche concerns, restricted to a few people who pored over the stock market in unseemly detail.

“And now, here we are – it’s even affecting elections. I mean, what the heck is going on? I think the deep scientific thing is that software engineers are trained to write programs to do things that optimize – and with good reason, because you’re often optimizing in relation to things like the weight distribution in a plane, or a most fuel-efficient speed: in the usual, anticipated circumstances optimizing makes sense. But in unusual circumstances it doesn’t, and we need to ask: ‘What’s the worst thing that could happen in this algorithm once it starts interacting with others?’ The problem is we don’t even have a word for this concept, much less a science to study it.”

He pauses for moment, trying to wrap his brain around the problem.

“The thing is, optimizing is all about either maximizing or minimizing something, which in computer terms are the same. So what is the opposite of an optimization, ie the least optimal case, and how do we identify and measure it? The question we need to ask, which we never do, is: ‘What’s the most extreme possible behavior in a system I thought I was optimizing?’”

Another brief silence ends with a hint of surprise in his voice.

“Basically, we need a new science,” he says.

Andrew Smith’s Totally Wired: The Rise and Fall of Joshua Harris and the Great Dotcom Swindle will be published by Grove Atlantic next February. The self-driving Uber car that hit and killed a woman walking her bike across a street wasn’t designed to detect “jaywalking pedestrians.”

That's according to an official dossier published by the US National Safety Transportation Board (NTSB) on Tuesday.

The March 2018 accident was the first recorded death by a fully autonomous vehicle. On-board video footage showed the victim, 49-year-old Elaine Herzberg, pushing her bike at night across a road in Tempe, Arizona, moments before she was struck by the AI-powered SUV at 39 MPH.

Now, an investigation by the NTSB into the crash has pinpointed a likely major contributing factor: the code couldn't recognize her as a pedestrian, because she was not at an obvious designated crossing. Rather than correctly anticipating her movements as a person moving across the road, it ended up running right into her.

“The system design did not include a consideration for jaywalking pedestrians,” the watchdog stated [PDF] in its write-up. “Instead, the system had initially classified her as an 'other' object which are not assigned goals.”

The computer-vision systems in self-driving cars are trained to identify things, such as other vehicles, trees, sign posts, bicycles, and so on, and make decisions on what to do next using that information. It appears Uber’s software wasn’t able to identify Herzberg since there was no classification label for a person not using a proper crossing point, and it wasn't able to make the right decisions.

Countdown to impact

Some 5.6 seconds before hitting her, the car's radar detected Herzberg, and at 5.2 seconds, she was picked out by the Lidar. However, the machine-learning system more or less ignored her, figuring her to be a non-moving object not in the vehicle's way.

As the robo-vehicle drew nearer, it categorized her variously as a vehicle, a bike, or some other thing that was not, or was only partially, in its way.

Just 1.2 seconds before hitting her, it identified her not only as a bicycle but also clearly in the path of its travel, by which point it was far too late to change course.

Crucially, the software was "unable to correctly predict the path" of Herzberg, the report noted. Perhaps if it had correctly identified her early on as a person crossing the street, it could have dramatically slowed down or otherwise tried to avoid her. But it did not anticipate the collision because, for the most part, it considered her to be some unknown object not in the way.

Also, don't forget: the SUV's emergency braking system was deliberately disabled because when it was switched on, the vehicle would act erratically, according to Uber. The software biz previously said “the vehicle operator is relied on to intervene and take action," in an emergency.

The self-driving car was fully autonomous at the time at the accident, though it had a human driver at the wheel. An internal camera caught the Uber worker looking down and away from the road moments before the accident, unaware of Herzberg’s presence before it was too late.

Below is a timetable, produced by the NTSB, detailing the car's decision-making and speed in the seconds before the accident:

The report comes just two weeks before the board is due to hold a public meeting to “determine the probable cause of a crash involving a pedestrian and an Uber test vehicle,” on 19 November.

The safety board also released more than 40 documents totaling at least 430 pages with various bits of supporting evidence and comprehensive accounts into things like the vehicle’s properties and the internal safety culture at Uber.

The massive data dump also revealed that the ride-hailing biz's self-driving cars were involved in 37 smashes between September 2016 and March 2018, prior to the deadly accident. In these 37 incidents, all of the robo-vehicles were driving in autonomous mode, and in 33 of these cases, other vehicles crashed into the self-driving cars.

'Tragedy'

On Tuesday evening, a spokesperson for Uber told us: “We regret the March 2018 crash involving one of our self-driving vehicles that took Elaine Herzberg’s life. In the wake of this tragedy, the team at Uber Advanced Technologies Group has adopted critical program improvements to further prioritize safety.

"We deeply value the thoroughness of the NTSB’s investigation into the crash and look forward to reviewing their recommendations once issued after the NTSB’s board meeting later this month.”

El Reg asked Uber how it planned to train its autonomous vehicles to detect jaywalkers in future, and the biz told us: "We conduct hundreds of tests in simulation and on our test track to help us improve the overall safety of our system. These tests include taking our software and having it go through scenarios that involve complex situations like jaywalking, hard braking, and unprotected turns." ®. Uber, Waymo and a long list of tech companies and automakers have begun to expand testing of their self-driving vehicles in cities around the country. The companies say the cars will be safer than regular cars simply because they take easily distracted humans out of the driving equation. But the technology is still only about a decade old, and just now starting to experience the unpredictable situations that drivers can face.

It was not yet clear if the crash in Arizona will lead other companies or state regulators to slow the rollout of self-driving vehicles on public roads.. Rafaela Vasquez liked to work nights, alone, buffered from a world she had her reasons to distrust. One Sunday night in March 2018, Uber assigned her the Scottsdale loop. She drove a gray Volvo SUV, rigged up with cameras and lidar sensors, through the company’s garage, past the rows of identical cars, past a poster depicting a driver staring down at a cell phone that warned, “It Can Wait.” The clock ticked past 9:15, and Vasquez reached the route’s entry point. She flipped the Volvo into autonomous mode, and the car navigated itself through a blur of suburban Arizona, past auto dealers and Zorba’s Adult Shop and the check-cashing place and McDonald’s. Then it jagged a short stint through Tempe to start the circuit again. It was a route Vasquez had cruised in autonomy some 70 times before.

This article appears in the April 2022 issue. Subscribe to WIRED Illustration: Jules Julien

As she was finishing her second loop, the Volvo blazed across a bridge strung with bistro lights above Tempe Town Lake. Neon signs on glass office buildings were reflected in the water, displaying the area’s tech hub ambitions—Zenefits, NortonLifeLock, Silicon Valley Bank. Beyond the bridge, the car navigated a soft bend into the shadows under a freeway overpass. At 9:58 pm, it glided to a forlorn stretch of road between a landscaped median and a patch of desert scruff. Four signs in the median warned people not to jaywalk there, directing them to a crosswalk 380 feet away.

The Uber driving system—which had been in full control of the car for 19 minutes at that point—registered a vehicle ahead that was 5.6 seconds away, but it delivered no alert to Vasquez. Then the computer nixed its initial assessment; it didn’t know what the object was. Then it switched the classification back to a vehicle, then waffled between vehicle and “other.” At 2.6 seconds from the object, the system identified it as “bicycle.” At 1.5 seconds, it switched back to considering it “other.” Then back to “bicycle” again. The system generated a plan to try to steer around whatever it was, but decided it couldn’t. Then, at 0.2 seconds to impact, the car let out a sound to alert Vasquez that the vehicle was going to slow down. At two-hundredths of a second before impact, traveling at 39 mph, Vasquez grabbed the steering wheel, which wrested the car out of autonomy and into manual mode.

It was too late. The smashed bike scraped a 25-foot wake on the pavement. A person lay crumpled in the road.

Vasquez did what Uber had taught its employees in the test program to do in case of emergencies: She pulled the vehicle over and called 911. “A bicyclist, um, I, um, hit a bicyclist that was in the road,” she told the dispatcher, her voice tense. “They shot out in the street … They are injured, they need help, paramedics.”

“I know it’s pretty scary,” the dispatcher said in soothing tones. She told Vasquez to breathe. Within six minutes of the crash, cops started to arrive. Paramedics too. One cop scanned a flashlight over the person on the ground. A paramedic kneeled down and pumped the victim’s chest.

Dashcam footage from Rafaela Vasquez's self-driving Uber, pulling out of the garage on the night of the crash. Video: Uber via Tempe Police Department

A couple of minutes later, an officer walked up to the Volvo, where Vasquez sat behind the wheel. He asked if she was OK. “Yeah, I’m just shaken up,” Vasquez said. “Is the person OK? Are they badly hurt?” Back by the figure who lay on the ground, a woman began wailing. Vasquez asked the officer, “Is that the person screaming?” He answered: “No no, that’s probably some people that they know.”

For the next two hours, Vasquez waited, doing what the police asked. Uber reps arrived. In the early minutes after the crash, one jogged up to Vasquez’s car, and an officer asked him to let the cops talk to her first. Eventually Vasquez moved to sit in a supervisor’s car. She asked for updates about the victim. And she learned that the person with the bicycle had died.. Advanced driver assist systems (ADAS) have been widely adopted by motor carriers for collision mitigation, automatic cruise control, lane keeping and driver fatigue monitoring, among other functions. Some fleets are now taking the next step forward and testing fully autonomous vehicle technology

[Related: J.B. Hunt partnering with Waymo on self-driving deliveries in Texas]

The shifting of primary responsibility for vehicle operations from a human to a machine – often referred to as SAE Level 4 and Level 5 autonomy – raises unsettled questions for the court system and the insurance industry. Who — or what — will be liable if something goes wrong?

In crash claims, the legal system has traditionally used tort law or negligence, explains J.W. Taylor, owner of Taylor and Associates, a West Haven, Fla.-based law firm specializing in transportation. Up to this point, any claims and associated damages regarding auto crashes have been tied to a breach of the duty of safe vehicle operations that a “reasonable person” would typically observe.

Emerging ADAS and autonomous vehicle technology muddy that tort system in ways that will have far-reaching impacts on the future of automobiles and commercial transportation.

Tort law, as it relates to motor vehicle accidents, has remained largely unchanged since the first automobile accident in 1869 when scientist Mary Ward fell from, and was fatally thrown under, a steam-powered auto. In 1896, Bridget Driscoll was the first pedestrian killed by an automobile. She stepped off a curb in London into the path of an auto driven by Arthur Edsall, who was arrested and charged with an accidental death.

Driscoll’s coroner was quoted as saying he hoped that “such a thing would never happen again.”

Fast forward to March 18, 2018. Elaine Herzberg was pushing a bicycle across a four-lane road in Tempe, Arizona, and was fatally struck by a semi-autonomous Uber vehicle that was equipped with a human driver as a backup for the autonomous driving system. This arrangement, wherein a semi-autonomous vehicle is operated with a human driver who may override the vehicle's controls, is referred to as SAE Level 3 autonomy.

Uber settled this accidental death suit prior to trial and suspended its autonomous driving program indefinitely. There have been no reported accidents among vehicles that meet the SAE Level 4 or Level 5 standard to-date, but that day could come soon.

“We don’t have case law or a lot of information pointing to how this is going to be handled in the courts,” Taylor said, since a human driver has always been named as the negligent party. In a circumstance with no human driver to blame, the manufacturer or software provider could be found to be liable. “How do we sort all that out?” Taylor asked.

An SAE Edge report, co-authored by Taylor and Dr. Rahul Razdan, a leading researcher on autonomous vehicles at Florida Polytechnic University, examined the impact of autonomous vehicles and artificial intelligence on the insurance industry and legal system.

In the report, Taylor compares autonomous vehicles to equine law.

“The concept is that a horse has intelligence, and it has horsepower, much like a vehicle. But what happens when you are relying on the intelligence of that horse and that horse spooks or reacts in way you did not anticipate?” he asked.

Just as there is a reasonable expectation for horse behavior, there is a reasonable expectation for how an autonomous vehicle should behave. Proving negligence in horse law depends on the jurisdiction and facts surrounding those cases, Taylor said. “The same type of scenario has to be prepared for with autonomous vehicles.”

Insurance impact

The main regulatory agency for laws that would accelerate or impede the rollout of full AVs is the National Highway Traffic Safety Administration. Yet if an accident were to occur with a Level 4 or Level 5 vehicle, the courts, and not regulatory bodies, would most likely assign liability, noted Razdan last fall during a virtual conference held by the Motor Carrier Insurance Education Foundation (MCIEF).

“Even with this emerging technology, by default, we are leaving it to the court system to set standardization of this technology," Taylor said at the conference. "I wish that we had someone else leading this charge.”

Insurance companies will have to figure out the liability question in order to underwrite policies for self-driving trucks. To do this, insurers will either have to get into the technology assessment business, which is not something they have typically done, or get other parties to help them do it, Razdan said.

To avoid the issue of human liability, insurance companies might have to offer single-policies that do away with separate coverage such as general auto, cargo liability, property damage and products liability. Offering a "wrap-around" policy is a real possibility, Taylor said, where you would purchase insurance with a vehicle that covers everything.

“The industry is working on it, because it recognizes there is going to be a different way to look at who’s at fault — the driver, OEM or software,” he said.

Convergence of technology

With so many unsettled questions, how will Level 4 and Level 5 autonomy begin to emerge?

As an expert in emerging AV technology, Razdan believes that AVs will first be deployed by fleets on a limited basis on highways where the vehicles may avoid the presence of pedestrians and where traffic flows in a fairly standard and predictable pattern.

Razdan is bullish on the idea of convoying where a lead truck has a human driver, and the trailing vehicle is fully autonomous. Such a model is being used by Locomation and “has enormous consequences” for fleet efficiencies, he said.

[Related: Technology startups benefitting from early successes, failures in trucking]

A driver in the trailing vehicle could be sleeping while the lead driver is working, and the drivers could swap shifts to maximize drive time.

Locomation has a three-year deal with Wilson Logistics to pilot test its autonomous technology on the road across several critical Wilson Logistics shipping routes. As part of the test, drivers in the trailing vehicle can go off duty. In June, PGT Trucking Inc. inked a deal with Locomation to test its platooning technology. During the eight-year agreement, PGT Trucking (CCJ Top 250, No. 103) will deploy 1,000 Autonomous Relay Convoy (ARC) systems, which enables one driver to pilot a lead truck equipped with technology augmentation while a follower truck operates in tandem through Locomation’s fully autonomous system, allowing the follower driver to log off and rest while the truck is in motion.

Once the dust settles on liability and insurance questions, a future possibility is to leverage a connected system, most likely blockchain, to orchestrate movements of vehicles in order to prevent accidents, and to improve traffic flow and business efficiency.

Taylor describes this scenario as a “robot derby.” Such a system is being developed called Noblis Pieces of Eight, he said. In this system, when a group of autonomous vehicles come to an intersection, they will have communicated with each other beforehand to determine who has the right-of-way based on economic and other factors.

Through a blockchain, a tractor-trailer could have pre-paid for fast access on a route for a time-critical pickup or delivery. Other motorists who yield take a share of the payment using a digital currency like Bitcoin.

“Maybe our streets and intersections will look nothing like today,” remarked Taylor.

Razdan sees a similar possibility of economically connected transportation models that will make it possible for everyone “to get what we want,” he said.. Video of the first self-driving car crash that killed a pedestrian suggests a “catastrophic failure” by Uber’s technology, according to experts in the field, who said the footage showed the autonomous system erring on one of its most basic functions.

Days after a self-driving Uber SUV struck a 49-year-old pedestrian while she was crossing the street with her bicycle in Tempe, Arizona, footage released by police revealed that the vehicle was moving in autonomous mode and did not appear to slow down or detect the woman even though she was visible in front of the car prior to the collision. Multiple experts have raised questions about Uber’s Lidar technology, which is the system of lasers that the autonomous cars uses to “see” the world around them.

“This is exactly the type of situation that Lidar and radar are supposed to pick up,” said David King, an Arizona State University professor and transportation planning expert. “This is a catastrophic failure that happened with Uber’s technology.”

The videos of the car hitting Elaine Herzberg also demonstrated that the “safety driver” inside the car did not seem to be monitoring the road, raising concerns about the testing systems Uber and other self-driving car companies have deployed in cities across the US.

“This safety driver was not doing any safety monitoring,” said Missy Cummings, a Duke University engineering professor who has testified about the dangers of self-driving technology. Research has shown that humans monitoring an automated system are likely to become bored and disengaged, she said, which makes this current phase of semi-autonomous testing particularly dangerous.

“The problem of complacent safety drivers is going to be a problem for every company.”

The footage “strongly suggests a failure by Uber’s automated driving system and a lack of due care by Uber’s driver”, Bryant Walker Smith, a University of South Carolina law school professor and autonomous vehicle expert, said in an email. He noted that the victim is visible about two seconds before the collision, saying: “This is similar to the average reaction time for a driver. That means an alert driver may have at least attempted to swerve or brake.”

The car was traveling at 38 miles per hour at 10pm on Sunday, according to the Tempe police chief, Sylvia Moir, who told a reporter that she thought the video showed Uber was not at fault. Experts who reviewed the footage, however, said the opposite appeared to be true.

“I really don’t understand why Lidar didn’t pick this up,” said Ryan Calo, a University of Washington law professor and self-driving expert. “This video does not absolve Uber.”

View image in fullscreen An Uber self-driving Volvo fitted with ‘Lidar’ technology. Photograph: Uber Handout/EPA

Even though the video appeared dark, King said there was likely more visibility than the footage suggested and noted that the darkness should not affect the car’s detection abilities.

“Shadows don’t matter to Lidar,” added Cummings. “There is no question it should have been able to see her.”

Police have emphasized that the victim was not in a crosswalk at the time of the crash, but experts said the technology still should have stopped the vehicle, a Volvo, and King noted that the exact section where Herzberg entered the street is a common area for pedestrians to cross near a local park.

John Simpson, the privacy and technology project director with Consumer Watchdog, said the video revealed a “complete failure” of Uber’s technology and its safety protocols, and said all testing programs on public roads should be suspended while the case is under investigation.

“Uber appears to be a company that has been rushing and taking shortcuts to get these things on the road,” said Simpson, noting that Arizona leaders lured the corporation to its state with promises of fewer regulations, after Uber fought with California over its vehicles running red lights. “It’s inexcusable.”

Uber, which temporarily suspended testing, declined to comment on the causes of the crash. A spokesperson said in a statement that the video was “disturbing and heartbreaking”, adding: “Our cars remain grounded, and we’re assisting local, state and federal authorities in any way we can.”