Last month, I wrote a blog post warning about how, if you follow popular trends in NLP, you can easily accidentally make a classifier that is pretty racist. To demonstrate this, I included the very simple code, as a ‚Äúcautionary tutorial‚Äù.

The post got a fair amount of reaction. Much of it positive and taking it seriously, so thanks for that. But eventually I heard from some detractors. Of course there were the fully expected ‚ÄúI‚Äôm not racist but what if racism is correct‚Äù retorts that I knew I‚Äôd have to face. But there were also people who couldn‚Äôt believe that anyone does NLP this way. They said I was talking about a non-problem that doesn‚Äôt show up in serious machine learning, or projecting my own bad NLP ideas, or something.

Well. Here‚Äôs Perspective API, made by an offshoot of Google. They believe they are going to use it to fight ‚Äútoxicity‚Äù online. And by ‚Äútoxicity‚Äù they mean ‚Äúsaying anything with negative sentiment‚Äù. And by ‚Äúnegative sentiment‚Äù they mean ‚Äúwhatever word2vec thinks is bad‚Äù. It works exactly like the hypothetical system that I cautioned against.

On this blog, we‚Äôve just looked at what word2vec (or GloVe) thinks is bad. It includes black people, Mexicans, Islam, and given names that don‚Äôt usually belong to white Americans. You can actually type my examples into Perspective API and it will actually respond that the ones that are less white-sounding are more ‚Äúlikely to be perceived as toxic‚Äù.

‚Äú Hello, my name is Emily‚Äù is supposedly 4% likely to be ‚Äútoxic‚Äù. Similar results for ‚ÄúSusan‚Äù, ‚ÄúPaul‚Äù, etc.

Hello, my name is Emily‚Äù is supposedly likely to be ‚Äútoxic‚Äù. Similar results for ‚ÄúSusan‚Äù, ‚ÄúPaul‚Äù, etc. ‚Äú Hello, my name is Shaniqua‚Äù (‚ÄúJamel‚Äù, ‚ÄúDeShawn‚Äù, etc.): 21% likely to be toxic.

Hello, my name is Shaniqua‚Äù (‚ÄúJamel‚Äù, ‚ÄúDeShawn‚Äù, etc.): likely to be toxic. ‚Äú Let‚Äôs go get Italian food‚Äù: 9% .

Let‚Äôs go get Italian food‚Äù: . ‚Äú Let‚Äôs go get Mexican food‚Äù: 29%.

Here are two more examples I didn‚Äôt mention before:

‚Äú Christianity is a major world religion‚Äù: 37% . Okay, maybe things can get heated when religion comes up at all, but compare:

Christianity is a major world religion‚Äù: . Okay, maybe things can get heated when religion comes up at all, but compare: ‚Äú Islam is a major world religion‚Äù: 66% toxic.

I‚Äôve heard about Perspective API from many directions, but my proximate source is this Twitter thread by Dan Luu, who has his own examples:

It‚Äôs ü§£ to poke around and see what biases the system picked up from the training data. üò∞ to think about actual applications, though. pic.twitter.com/VJ9y9yxz2D ‚Äî Dan Luu (@danluu) August 12, 2017

I have previously written positive things about researchers at Google who are looking at approaches to de-biasing AI, such as their blog post on Equality of Opportunity in Machine Learning.

But Google is a big place. It contains multitudes. And it seems it contains a subdivision that will do the wrong thing, which other Googlers know is the wrong thing, because it‚Äôs easy.

Google, you made a very bad investment. (That sentence is 61% toxic, by the way.)

As I update this post in April 2018, I‚Äôve had some communication with the Perspective API team and learned some more details about it.

Some details of this post were incorrect, based on things I assumed when looking at Perspective API from outside. For example, Perspective API does not literally build on word2vec. But the end result is the same: it learns the same biases that word2vec learns anyway.

In September 2017, Violet Blue wrote an expos√© of Perspective API for Engadget. Despite the details that I had wrong, the Engadget article confirms that the system really is that bad, and provides even more examples.

Perspective API has changed their online demo to lower toxicity scores across the board, without fundamentally changing the model. Text with a score under a certain threshold is now labeled as ‚Äúnot toxic‚Äù. I believe this remedy could be described technically as ‚Äúweak sauce‚Äù.

The Perspective API team claims that their system has no inherent bias against non-white names, and that the higher toxicity scores that appear for names such as ‚ÄúDeShawn‚Äù is an artifact of how they handle out-of-vocabulary words. All the names that are typical for white Americans are in-vocabulary. Make of that what you will.

The Perspective API team continues to promote their product, such as via hackathons and TED talks. Users of the API are not warned of its biases, except for a generic warning that could apply to any AI system, saying that users should manually review its results. It is still sometimes held up as a positive example of fighting toxicity with NLP, misleading lay audiences into thinking that present NLP has a solution to toxicity.. In the examples below on hot-button topics of climate change, Brexit and the recent US election -- which were taken directly from the Perspective API website -- the UW team simply misspelled or added extraneous punctuation or spaces to the offending words, which yielded much lower toxicity scores. For example, simply changing "idiot" to "idiiot" reduced the toxicity rate of an otherwise identical comment from 84% to 20%. Credit: University of Washington

University of Washington researchers have shown that Google's new machine learning-based system to identify toxic comments in online discussion forums can be bypassed by simply misspelling or adding unnecessary punctuation to abusive words, such as "idiot" or "moron."

Perspective is a project by Google's technology incubator Jigsaw, which uses artificial intelligence to combat internet trolls and promote more civil online discussion by automatically detecting online insults, harassment and abusive speech. The company launched a demonstration website on Feb. 23 that allows anyone to type in a phrase and see its "toxicity score"‚Äîa measure of how rude, disrespectful or unreasonable a particular comment is.

In a paper posted Feb. 27 on the e-print repository arXiv, the UW electrical engineers and security experts demonstrated that the early stage technology system can be deceived by using common adversarial tactics. They showed one can subtly modify a phrase that receives a high toxicity score so that it contains the same abusive language but receives a low toxicity score.

Given that news platforms such as The New York Times and other media companies are exploring how the system could help curb harassment and abuse in online comment areas or social media, the UW researchers evaluated Perspective in adversarial settings. They showed that the system is vulnerable to both missing incendiary language and falsely blocking non-abusive phrases.

In the examples in Graphic 2, the researchers also showed that the system does not assign a low toxicity score to a negated version of an abusive phrase. Credit: University of Washington

"Machine learning systems are generally designed to yield the best performance in benign settings. But in real-world applications, these systems are susceptible to intelligent subversion or attacks," said senior author Radha Poovendran, chair of the UW electrical engineering department and director of the Network Security Lab. "We wanted to demonstrate the importance of designing these machine learning tools in adversarial environments. Designing a system with a benign operating environment in mind and deploying it in adversarial environments can have devastating consequences."

To solicit feedback and invite other researchers to explore the strengths and weaknesses of using machine learning as a tool to improve online discussions, Perspective developers made their experiments, models and data publicly available along with the tool itself.

In the examples in Graphic 1 on hot-button topics of climate change, Brexit and the recent U.S. election‚Äîwhich were taken directly from the Perspective API website‚Äîthe UW team simply misspelled or added extraneous punctuation or spaces to the offending words, which yielded much lower toxicity scores. For example, simply changing "idiot" to "idiiot" reduced the toxicity rate of an otherwise identical phrase from 84 percent to 20 percent.

In the examples in Graphic 2, the researchers also showed that the system does not assign a low toxicity score to a negated version of an abusive phrase.

The UW electrical engineering research team includes (left to right) Radha Poovendran, Hossein Hosseini, Baosen Zhang and Sreeram Kannan (not pictured). Credit: University of Washington

The researchers also observed that the duplicitous changes often transfer among different phrases‚Äîonce an intentionally misspelled word was given a low toxicity score in one phrase, it was also given a low score in another phrase. That means an adversary could create a "dictionary" of changes for every word and significantly simplify the attack process.

"There are two metrics for evaluating the performance of a filtering system like a spam blocker or toxic speech detector; one is the missed detection rate, and the other is the false alarm rate," said lead author and UW electrical engineering doctoral student Hossein Hosseini. "Of course scoring the semantic toxicity of a phrase is challenging, but deploying defensive mechanisms both in algorithmic and system levels can help the usability of the system in real-world settings."

The research team suggests several techniques to improve the robustness of toxic speech detectors, including applying a spellchecking filter prior to the detection system, training the machine learning algorithm with adversarial examples and blocking suspicious users for a period of time.

"Our Network Security Lab research is typically focused on the foundations and science of cybersecurity," said Poovendran, the lead principal investigator of a recently awarded MURI grant, of which adversarial machine learning is a significant component. "But our expanded focus includes developing robust and resilient systems for machine learning and reasoning systems that need to operate in adversarial environments for a wide range of applications."

More information: Deceiving Google's Perspective API Built for Detecting Toxic Comments, arXiv:1702.08138 [cs.LG] arxiv.org/abs/1702.08138