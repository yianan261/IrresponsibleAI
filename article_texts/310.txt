. A British police agency is defending (this link is inoperable for the moment) its use of facial recognition technology at the June 2017 Champions League soccer final in Cardiff, Wales—among several other instances—saying that despite the system having a 92-percent false positive rate, "no one" has ever been arrested due to such an error.

New data about the South Wales Police's use of the technology obtained by Wired UK and The Guardian through a public records request shows that of the 2,470 alerts from the facial recognition system, 2,297 were false positives. In other words, nine out of 10 times, the system erroneously flagged someone as being suspicious or worthy of arrest.

In a public statement, the SWP said that it has arrested "over 450" people as a result of its facial recognition efforts over the last nine months.

"Of course, no facial recognition system is 100 percent accurate under all conditions. Technical issues are normal to all face recognition systems, which means false positives will continue to be a common problem for the foreseeable future," the police wrote. "However, since we introduced the facial recognition technology, no individual has been arrested where a false positive alert has led to an intervention and no members of the public have complained."

The agency added that it is "very cognizant of concerns about privacy, and we have built in checks and balances into our methodology to make sure our approach is justified and balanced."

However, Big Brother Watch, a London-based advocacy group, is unsatisfied with the police's response:. Something went wrong, please try again later.

Invalid email Something went wrong, please try again later.

Keep up to date with the latest stories with our WalesOnline newsletter

Something went wrong, please try again later.

Keep up to date with the latest stories with our WalesOnline newsletter

Facial recognition software wrongly identified more than 2,000 people as potential criminals as police patrolled the Champions League final in Cardiff .

The technology provided hundreds of “false positives” wrongly marking out innocent people as possible troublemakers when an estimated 170,000 people descended on the city for the showpiece match between Real Madrid and Juventus.

A South Wales Police spokesman admitted “no facial recognition system is 100% accurate under all conditions” but added that in the months since it was first deployed “no-one has been arrested where a ‘false positive alert’ has occurred and no members of the public have complained”.

Data published by the force showed police covering the Champions League final at the Principality Stadium on June 3 last year were alerted to 2,470 potential matches with custody pictures by the facial recognition programme.

But of these 92% – a total if 2,297 – were incorrect, with just 173 providing ‘true positive alerts’.

The force blamed the high number of false positives at the Champions League final on the “poor quality of images” on the watch list supplied by Uefa, Interpol and other partner agencies as well as the fact it was the first major use of the technology.

The statistics provided by the force, covering the period between June last year and March this year, also showed other occasions – including sporting events and gigs – when the technology had been used.

They included the Elvis Festival in Porthcawl , where there were 10 true positive alerts and seven false positives, as well as during Wales’ four autumn internationals when there were a total of 15 true positives and 71 false positives.

(Image: Richard Williams)

Officers patrolling boxer Anthony Joshua’s world heavyweight title fight with Carlos Takam in Cardiff recorded five true positives compared to 46 false positives while there were no recordings of either kind when Prince Harry and Meghan Markle visited the city in January .

The technology was also used at shows by Stereophonics, Kasabian and Liam Gallagher.

Chief Constable Matt Jukes told the BBC : “We know that our major sporting events and our crowded places are potential terrorist targets – that’s a reality.

“So we need to use technology when we’ve got tens of thousands of people in those crowds to protect everybody, and we are getting some great results from that.

“But we don’t take the use of it lightly and we are being really serious about making sure it is accurate.”

Civil liberties group Big Brother Watch criticised the use of the technology, saying in a tweet: “Not only is real-time facial recognition a threat to civil liberties, it is a dangerously inaccurate policing tool.”

South Wales Police became the first in the UK to introduce the technology last year with the first arrest as a result occurring on May 31.

The system allows real-time cameras to monitor people against a database of around half a million custody images as well as other information like CCTV and E-fit images.

When it was brought in ahead of the Champions League final a force spokesman said the event had “clearly provided a perfect testing ground” for the technology.

In light of the latest statistics a South Wales Police spokesman said since the facial recognition technology had been brought in last year more than 2,000 “positive matches” had been made resulting in more than 450 arrests.

“Successful convictions so far include six years in prison for robbery and four and a half years imprisonment for burglary.

“The technology has also helped identify vulnerable people in times of crisis,” the spokesman said.

(Image: South Wales Police)

But they added: “Of course, no facial recognition system is 100% accurate under all conditions resulting in what are termed ‘false positives’. This where the system incorrectly matches a person against a watch list.

“Technical issues are common to all face recognition systems, which means false positives will be an issue as the technology develops.

“Since initial deployments during the European Champions League Final in June 2017 the accuracy of the system used by South Wales Police has continued to improve.

“It is also important to stress that since we introduced facial recognition technology no-one has been arrested where a ‘false positive alert’ has occurred and no members of the public have complained. This is due to importance we place on human judgement.

“In all cases, an operator will consider an initial alert and will either disregard it, which happens in the majority of cases, or dispatch an intervention team where a match is considered to have been made.

“Where this happens, officers can quickly establish if the person has been correctly or incorrectly matched by traditional policing methods, either by looking at the person or through a brief conversation. If an incorrect match has been made officers will explain to the individual what has happened and invite them to see the equipment along with providing them with a Fair Processing Notice.”. Police in South Wales have been relying on facial recognition technology for 12 months.

An FOI request has revealed that the technology provides a "false positive" ID in more than 90% of cases.

The police have admitted that "of course no facial recognition system is 100% accurate".

Advertisement



British police have been forced to defend facial recognition technology which falsely identified 2297 out of 2470 football fans as "persons of interest."

South Wales police rolled out its "Automated Facial Recognition" (AFR) pilot program just before the Champion's League Final between Juventus and Real Madrid almost a year ago in Cardiff, Wales.

Late last week, Wired's Matt Burgess got an FOI request back in regard to how effective the AFR system has been for the South Wales Police since then.

It wasn't great.

Advertisement

The "false positive" rate – 92% unsuccessful – from AFR's work at the Champions League final was the most damning.

In other events, only six positive matches were confirmed out of 48 alerts at a Wales versus Australia rugby match, and no matches were made at all during a royal visit from Prince Harry and Meghan Markle to Cardiff in January 2018.

Almost immediately following the report, South Wales police put out an explainer on how its facial recognition system worked in two stages, AFR "Identity" and AFR "Locate".

Related stories

AFR "Identity" allows officers to load images of persons of interest and compare them to 500,000 images they already have on file and see if there's a match.

Advertisement

AFR "Locate" uses CCTV and roving police vehicle cameras to locate that person of interest.

The force made it clear no one had ever been arrested on the basis of a false positive result and claimed the "overall effectiveness of facial recognition has been high."

It also claimed 2000 positive matches have led to 450 arrests in the past nine months.

But Wired's FOI request has returned a total result of just 234 "true positives" from 2685 alerts across 15 events.

Advertisement

South Wales Police opened its release with:

"Of course no facial recognition system is 100% accurate under all conditions."

Its system for dealing with potential false positives is a) disregard the alert; or b) dispatch a team to "have an interaction with the potentially matched individual."

Officers would then use "traditional policing methods" (mostly, a chat) to decide whether more action was required.

Advertisement

That a new technology isn't exactly performing at 100% should come as no surprise to anyone. But for the general public, there's no doubt facial recognition and criminal profiling definitely fall in a preferred category of "get it right first."

Immediately, civil liberties groups have begun preparing campaigns for British parliament.

You can read the full story at Wired here.. Tokyo & London, July 11, 2017 - NEC Corporation (NEC; TSE: 6701) today announced that it has provided a facial recognition system for South Wales Police in the UK through NEC Europe Ltd.



The system utilizes NeoFace® Watch, NEC's flagship facial recognition software platform featuring the world's highest recognition precision (*). NeoFace Watch is used for real-time CCTV surveillance, as well as still image and recorded video face search, which helps to ensure security in crowded locations, such as airports and stadiums.



South Wales Police has deployed NeoFace Watch using CCTV cameras mounted on a number of police vehicles and is using its real-time surveillance capability to locate persons of interest on pre-determined watchlists, including criminals, suspects, vulnerable individuals and missing persons.

Police vehicle equipped with facial recognition system

Assistant Chief Constable Richard Lewis said: "Facial recognition technology will enable us to search, scan and monitor images and video against a range of offender databases leading to faster and more accurate identification of persons of interest. This has been borne out by the recent arrest of a 34-year old man from Cardiff who was wanted for a recall to prison. He had walked past several officers on a main street in Cardiff before he was identified by the cameras and it is probably an arrest we would not have made at any previous time."



The South Wales Police system was deployed for the final of the UEFA Champions League, held on June 3 at the National Stadium of Wales with up to 170,000 football fans in Cardiff on that day. "We deployed NEC's real-time solution, which enabled trained officers to monitor the movement of people at strategic locations in and around the city centre during this massive event. It was a great success," said ACC Lewis. South Wales Police is the first police force to use a real-time facial recognition system at a large-scale sports event in the UK.



The South Wales system will also be used to cross-check still images and recorded video taken at crime scenes against approximately 500,000 photos currently held in its custody image database.



"NEC is very proud to be working closely with South Wales Police. We now have deployments of NeoFace Watch in 47 countries, used by a wide range of government and commercial organizations," said Chris de Silva, Head of Global Face Recognition Solutions, NEC Europe Ltd. "NEC is committed to providing valuable solutions for society and we will continue proposing innovative new solutions using face recognition, both in the UK and around the world."

***. Neutral Citation Number: [2020] EWCA Civ 1058

Case No: C1/2019/2670

IN THE COURT OF APPEAL (CIVIL DIVISION)

ON APPEAL FROM THE HIGH COURT OF JUSTICE

QUEEN’S BENCH DIVISION (ADMINISTRATIVE COURT)

Date: 11/08/2020

Before :

THE MASTER OF THE ROLLS

THE PRESIDENT OF THE QUEEN’S BENCH DIVISION

and

LORD JUSTICE SINGH

Between :

R (on the application of Edward BRIDGES)

– and –

THE CHIEF CONSTABLE OF SOUTH WALES POLICE

– and –

THE SECRETARY OF STATE FOR THE HOME DEPARTMENT

-and-

THE INFORMATION COMMISSIONER (1)

THE SURVEILLANCE CAMERA COMMISSIONER (2)

THE POLICE AND CRIME COMMISSIONER FOR SOUTH WALES (3)

COVID-19 PROTOCOL

This judgment was handed down remotely by circulation to the parties’ representatives by email, and by release to the British and Irish Legal Information Institute (BAILII) for publication. Copies were also made available to the Judicial Office for dissemination. The date and time of handing down shall be deemed for all purposes to be 10:00am on Tuesday 11 August 2020.. On August 11, 2020, the Court of Appeal of England and Wales overturned the High Court’s dismissal of a challenge to South Wales Police’s use of Automated Facial Recognition technology (“AFR”), finding that its use was unlawful and violated human rights.

In September 2019, the UK’s High Court had dismissed the challenge to the use of AFR, determining that its use was necessary and proportionate to achieve South Wales Police’s statutory obligations. Mr. Bridges, the civil liberties campaigner who originally brought judicial review proceedings after South Wales Police launched a project involving the use of AFR (“AFR Locate”) appealed the High Court’s dismissal. With AFR Locate, South Wales Police deployed AFR technology at certain events and in certain public locations where crime was considered likely to occur, and images of up to 50 faces per second. The police subsequently matched the captured images with “watchlists” of wanted persons in police databases using biometric data analysis. Where a match was not made with any of these watchlists, the images were immediately and automatically deleted.

Mr. Bridges challenged AFR Locate on the basis that it was unlawfully intrusive, including under Article 8 of the European Convention on Human Rights (“ECHR”) (right to respect for private and family life) and data protection law in the UK. His appeal was based on the following five grounds:

The High Court had erred in its conclusion that South Wales Police’s use of AFR and interference with Mr. Bridges’ rights was in accordance with the law under Article 8(2) of the ECHR. The High Court had incorrectly concluded that the use of AFR and interference with Mr. Bridges’ rights was proportionate under Article 8(2) of the ECHR. The High Court was wrong to consider the DPIA carried out in relation to the processing sufficient for the purposes of Section 64 of the DPA 2018. The High Court should not have declined to reach a conclusion as to whether South Wales Police had an “appropriate policy document” in place regarding the use of AFR Locate that was within the meaning of Section 42 of the DPA 2018 for carrying out sensitive data processing. The High Court was wrong to hold that South Wales Police had complied with the Public Sector Equality Duty (“PSED”) under Section 149 of the Equality Act 2010, on the grounds that the Equality Impact Assessment carried out was “obviously inadequate” and failed to recognize the risk of indirect discrimination on the basis of sex or race.

The Court of Appeal granted the appeal on grounds 1, 3 and 5, but rejected grounds 2 and 4.

Ground 1

On the first ground, the Court of Appeal overturned the High Court’s determination, finding “fundamental deficiencies” in the legal framework around the use of AFR, specifically the policies that governed its use. The Court found that South Wales Police’s policies gave too much discretion to individual police officers to determine which individuals were placed on watchlists and where AFR Locate could be deployed. The Court commented that “the current policies do not sufficiently set out the terms on which discretionary powers can be exercised by the police and for that reason do not have the necessary quality of law.” The Court further described the discretion as “impermissibly wide”, for example because the deployment of the technology was not limited to areas in which it could be thought on reasonable grounds that individuals on a watchlist might be present. The Court implied that this should be a significant factor in determining where AFR Locate should be deployed, stating, “it will often, perhaps always, be the case that the location will be determined by whether the police have reason to believe that people on the watchlist are going to be at that location.”

Ground 2

Since the Court decided that AFR Locate’s use was not lawful, it was not necessary for the Court to decide the second ground of appeal on proportionality. Regardless, the Court chose to address this question and rejected it. Mr. Bridges argued that the balancing test between the rights of the individual and the interests of the community, which forms part of the proportionality analysis, should not only consider the impact on Mr. Bridges, but also the impact on all other individuals whose biometric data was processed by the technology on the relevant occasions. The Court of Appeal disagreed, commenting that Mr. Bridges had only detailed the impact on himself, not the wider public, in his original complaint and that the impact on each of the other relevant individuals was as negligible as the impact on Mr. Bridges and should not be considered cumulatively. The Court stated, “An impact that has very little weight cannot become weightier simply because other people were also affected. It is not a question of simple multiplication. The balancing exercise which the principle of proportionality requires is not a mathematical one; it is an exercise which calls for judgement.”

Ground 3

On the third ground of appeal relating to South Wales Police’s failure to carry out a sufficient DPIA, Mr. Bridges argued that the DPIA was defective in three specific ways. First, it failed to recognize that the personal data of individuals not present on a watchlist (whose data was therefore immediately and automatically deleted) was nonetheless “processed” within the meaning of data protection law. Second, the DPIA also did not acknowledge that the rights of individuals under Article 8 of the ECHR were engaged by the processing, and third, it was silent as to other risks that may have been raised by AFR Locate’s use, such as the right to freedom of expression or freedom of assembly.

The UK Information Commissioner’s Office (“ICO”), an intervener in the case, also criticized the DPIA undertaken by South Wales Police on the basis that it did not contain an assessment of “privacy, personal data and safeguards,” failed to acknowledge that AFR involves the collection of personal data on a “blanket and indiscriminate basis” and that the risk of false-positive results may in fact result in longer retention periods rather than data being immediately deleted. In addition, the DPIA failed to address potential gender and racial bias that could arise from AFR Locate’s use. As such, the ICO stated that the DPIA failed to appropriately assess the risks and mitigation of them as required under Section 64 of the DPA 2018.

The Court of Appeal did not accept all of these arguments. For example, it highlighted that the DPIA had specifically referred to the relevance of Article 8 of the ECHR. However, based on its conclusion that the deployment of the technology was not lawful, the Court found that South Wales Police was wrong to conclude in its DPIA that Article 8 of the ECHR was not infringed. The Court of Appeal stated, “The inevitable consequence of those deficiencies is that, notwithstanding the attempt of the DPIA to grapple with the Article 8 issues, the DPIA failed properly to assess the risks to the rights and freedoms of data subjects and failed to address the measures envisaged to address the risks arising from the deficiencies we have found, as required by section 64(3)(b) and (c) of the DPA 2018.”

Ground 4

With regards to the requirement to have an “appropriate policy document” in place under Section 42 of the DPA 2018, Mr. Bridges argued that the assessment of the document’s sufficiency should not have been referred back to South Wales Police for consideration in light of guidance from the ICO, but instead, the High Court should have found it to be insufficient. The Court of Appeal rejected this argument on the basis that, at the time of AFR Locate’s deployment, the DPA 2018 was not yet in force, and therefore, there could not have been a failure to comply with the law. In relation to AFR Locate’s future use and the requirement for an appropriate policy document, the Court of Appeal commented that, “[A] section 42 document is an evolving document, which, in accordance with section 42(3), must be kept under review and updated from time to time.” Since ICO guidance had not been issued on the drafting of this type of document at the time of the High Court hearing, and given that South Wales Police had updated the document in light of the ICO’s subsequently published guidance, the Court of Appeal found that the High Court’s approach in this respect had been appropriate. It also referred to the fact that the ICO had repeatedly expressed the view that the original version of the document met Section 42 requirements, though it would ideally contain more detail.

Ground 5

On the final ground of appeal concerning the PSED under Section 149 of the Equality Act 2010, the Court found that South Wales Police had not gathered sufficient evidence to establish whether or not AFR Locate was inherently biased prior to its use for two reasons: (1) because the data of individuals whose images did not match those on the watchlists were automatically deleted (and therefore could not be analyzed for the purpose of assessing bias), and (2) because South Wales Police was not aware of the dataset on which AFR Locate had been trained and could not establish whether there had been a demographic imbalance in the relevant training data. Although it was not alleged that AFR Locate produced biased results, the Court determined that South Wales Police, “never sought to satisfy themselves, either directly or by way of independent verification, that the software program in this case does not have an unacceptable bias on grounds of race or sex.” The Court added, “We would hope that, as AFR is a novel and controversial technology, all police forces that intend to use it in the future would wish to satisfy themselves that everything reasonable which could be done had been done in order to make sure that the software used does not have a racial or gender bias.”

South Wales Police has stated that it will not appeal the decision. The Court of Appeal’s full judgement may be viewed here.. Use of live facial recognition technology by UK police fails to meet “minimum ethical and legal standards” and should be banned from application in public spaces, say researchers from the University of Cambridge.

A team of researchers at the Minderoo Centre for Technology and Democracy analyzed three separate instances of facial recognition technology (FRT) used by two police forces—South Wales Police and the Metropolitan Police Service (MPS). In every case, FRT was found to potentially breach human rights.

The researchers created an audit tool to check FRT deployments against current legal guidelines—including the UK’s Data Protection and Equality acts—as well as outcomes from UK court cases.

They applied their ethical and legal standards to three uses of FRT by UK police. In two cases, the technology was used by the MPS and South Wales Police to scan crowds and compare faces to those on a criminal database and “watch list." In the third case, officers from South Wales Police used FRT smartphone apps to scan crowds and identify “wanted” individuals in real-time.

In all three cases, there was found to be a lack of transparency, accountability, and oversight in the use of FRT.

The study found that important information about police use of FRT is “kept from view” such as demographic data published on arrests or other outcomes, which the researchers say makes it difficult to evaluate whether the tools “perpetuate racial profiling." The report also found police had not published internal audits to establish if their technology was biased.

In addition to lack of transparency, the researchers found there was very little accountability for the police—with no clear recourse for people or communities negatively affected by police use, or misuse, of the tech. “Police forces are not necessarily answerable or held responsible for harms caused by facial recognition technology,” said Evani Radiya-Dixit, the report’s lead author.

“There is a lack of robust redress mechanisms for individuals and communities harmed by police deployments of the technology. To protect human rights and improve accountability in how technology is used, we must ask what values we want to embed in technology," Radiya-Dixit said.

Professor Gina Neff, Executive Director at the Minderoo Centre for Technology and Democracy, said: “Over the last few years, police forces around the world, including in England and Wales, have deployed facial recognition technologies. Our goal was to assess whether these deployments used known practices for the safe and ethical use of these technologies.

“Building a unique audit system enabled us to examine the issues of privacy, equality, accountability, and oversight that should accompany any use of such technologies by the police,” Neff said.

The researchers have joined experts from the EU and UN High Commissioner for Human Rights in calling for the prohibition of FRT in public spaces.

UK police have been testing FRT use for years in multiple situations to fight crime and terrorism. Its first documented use in the UK was in 2015 by Leicestershire Police on festival-goers. It has since been used prolifically by South Wales Police and Metropolitan Police to scan hundreds of thousands of faces at protests, sporting events, concerts, Notting Hill Carnival, train stations, and busy shopping streets.

There is global concern regarding the use of FRT by police forces. The same technology used by the Metropolitan Police was found to have wrongly identified Black men. In 2020, Amnesty International led a call to ban the use of FRT by police forces as it could “exacerbate human rights violations."