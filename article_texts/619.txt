Rite Aid used facial recognition systems to identify shoppers that were previously deemed “likely to engage” in shoplifting without customer consent and misidentified people – particularly women and Black, Latino or Asian people – on “numerous” occasions, according to a new settlement with the Federal Trade Commission. As part of the settlement, Rite Aid has been forbidden from deploying facial recognition technology in its stores for five years.

The FTC said in a federal court complaint that Rite Aid used facial recognition technology in hundreds of stores from October 2012 to July 2020 to identify shoppers “it had previously deemed likely to engage in shoplifting or other criminal behavior”. The technology sent alerts to Rite Aid employees either by email or phone when it identified people entering the store on its watchlist.

The FTC said in its complaint that store employees would then put those people under increased surveillance, ban them from making purchases or accuse them in front of friends, family and other customers of previously committing crimes. The facial recognition system was largely used in New York City; Los Angeles; San Francisco; Philadelphia; Baltimore; Detroit; Atlantic City; Seattle; Portland, Oregon; Wilmington, Delaware and Sacramento, California, according to the settlement.

The settlement addresses charges that the struggling drugstore chain did not do enough to prevent harm to its customers and implement “reasonable procedures”, the government agency said. Rite Aid said late on Tuesday that it disagreed with the allegations, but that it was glad it had reached an agreement to resolve the issue.

As part of its contract with two private, unnamed vendors, Rite Aid created or directed the companies to create a database of “persons of interest” that included images of the people and other personally identifying information. Those images were often low quality and were captured through Rite Aid’s CCTV cameras, the facial recognition cameras or on the mobile phones of employees, according to the settlement.

Security workers were trained to “push for as many enrollments as possible” and the company “enrolled at least tens of thousands of individuals in its database”, according to FTC documents.

The federal complaint also said there were “numerous instances” in which the technology incorrectly identified someone who entered the store and Rite Aid failed to test its accuracy before using it. For instance, Rite Aid did not ask one of the two private vendors it worked with whether its technology had been tested for accuracy, according to the settlement. In fact, the vendor explicitly states in its contract that it “makes no representations or warranties as to the accuracy and reliability” of its facial recognition system.

The FTC also said the company “failed to take reasonable steps to train and oversee the employees charged with operating the technology in Rite Aid stores”.

Civil liberty and digital rights group, the Electronic Privacy Information Center (Epic), said that facial recognition can be harmful in any context but that Rite Aid failed to take even the most basic precautions. “The result was sadly predictable: thousands of misidentifications that disproportionately affected Black Asian, and Latino customers, some of which led to humiliating searches and store ejections,” said John Davisson, Epic’s director of litigation.

Rite Aid says the allegations center on a pilot program it used in a limited number of stores and it stopped using this technology more than three years ago.

“We respect the FTC’s inquiry and are aligned with the agency’s mission to protect consumer privacy, the company said in a statement posted on its website. “However, we fundamentally disagree with the facial recognition allegations in the agency’s complaint.”

skip past newsletter promotion Sign up to First Thing Free daily newsletter Our US morning briefing breaks down the key stories of the day, telling you what’s happening and why it matters Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

Studies have shown facial recognition systems have been found to routinely misidentify Black and brown people. In the last few years in the US, there have been six known cases of Black people being falsely arrested due to facial recognition.

“This is a groundbreaking case, a major stride for privacy and civil rights, and hopefully just the beginning of a trend,” Davisson said. “But it’s important to note that Rite Aid isn’t alone. Businesses routinely use unproven algorithms and snake oil surveillance tools to screen consumers, often in secret. The FTC is right to crack down on these practices, and businesses would be wise to take note. Algorithmic lawlessness is not an option any more.”

Rite Aid also noted in a prepared statement that any agreement would have to be approved in US bankruptcy court. The company announced last fall that it was closing more than 150 stores as it makes its way through a voluntary chapter 11 bankruptcy process. The company has struggled financially for years and also faces financial risk from lawsuits over opioid prescriptions like its bigger rivals, CVS and Walgreens.. New York CNN —

Rite Aid has agreed to a five-year ban from using facial recognition technology after the Federal Trade Commission found that the chain falsely accused customers of crimes and unfairly targeted people of color.

The FTC and Rite Aid reached a settlement Tuesday after a complaint accused the chain of using artificial intelligence-based software in hundreds of stores to identify people Rite Aid “deemed likely to engage in shoplifting or other criminal behavior” and kick them out of stores – or prevent them from coming inside.

But the imperfect technology led employees to act on false-positive alerts, which wrongly identified customers as criminals. In some cases, the FTC accused Rite Aid employees of publicly accusing people of criminal activity in front of friends, family and strangers. Some customers were wrongly detained and subjected to searches, the FTC said.

Rite Aid said in a statement that it’s “pleased to reach an agreement” with the FTC but added that “we fundamentally disagree with the facial recognition allegations in the agency’s complaint.” The tech was a pilot program and was only used in a “limited number of stores. The test stopped more than three years ago before the FTC’s investigation began.

The FTC’s legal filing, which contains customer complaints spanning from 2012 to 2020, said that some customers were “erroneously accused by employees of wrongdoing” because Rite Aid’s technology “falsely flagged the consumers as matching someone who had previously been identified as a shoplifter or other troublemaker.” The facial recognition software was mostly deployed in neighborhoods with large Black, Latino and Asian communities, the FTC said.

A facial recognition camera is shown pointed at the entrance of a Rite Aid store in Los Angeles in 2019. Mike Blake/Reuters

“Rite Aid’s reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers’ sensitive information at risk,” said Samuel Levine, director of the FTC’s Bureau of Consumer Protection, in a release.

The proposed order means that Rite Aid will have to “implement comprehensive safeguards” to prevent harm of its customers when deploying the AI-based technology to its locations. The order also prevents Rite Aid from using the tech if it “cannot control potential risks to consumers.”

“The safety of our associates and customers is paramount,” Rite Aid said. “As part of the agreement with the FTC, we will continue to enhance and formalize the practices and policies of our comprehensive information security program.”

The pilot program involved creating a database of thousands of low-quality pictures from store cameras and employees’ phones of customer faces, which were labeled as “persons of interest” because Rite Aid thought they were engaged in criminal activity its stores. The FTC is requiring Rite Aid to delete those pictures and notify customers that they’re in a database.

Since Rite Aid is engaged in bankruptcy proceedings, the FTC said its orders would go into effect after approval from the courts.. Rite Aid isn’t allowed to use AI-powered facial recognition technology for another five years as part of a settlement it reached with the Federal Trade Commission. In a complaint filed on Tuesday, the FTC accuses Rite Aid of using facial surveillance systems in a “reckless” manner from 2012 to 2020.

During this period, the FTC says Rite Aid used facial recognition technology to “capture images of all consumers as they entered or moved through the stores.” It then allegedly created a database of customers identified as shoplifters or exhibiting some other kind of suspicious behavior. For some customers, the database would have “accompanying information,” such as names, birth dates, and the activity deemed suspicious by the store, according to the complaint.

Rite Aid employees allegedly followed flagged customers around stores and performed searches

When a flagged shopper entered a Rite Aid store with facial recognition technology, the FTC says employees would receive a “match alert” sent to their mobile phones. As a result, Rite Aid employees allegedly followed customers around stores, performed searches, publicly accused them of shoplifting, and even asked the authorities to remove certain shoppers, according to the complaint. The FTC says Rite Aid falsely identified people as shoppers who had been previously flagged by the system, with incidents “disproportionality” impacting people of color.

Additionally, the pharmacy chain didn’t inform customers that it used facial recognition technology, and employees were “instructed employees not to reveal” this information, the complaint states. Most Rite Aid stores equipped with facial recognition technology were located in New York City, Los Angeles, Sacramento, Philadelphia, Baltimore, Detroit, Atlantic City, and a handful of other cities.

“Rite Aid’s reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers’ sensitive information at risk,” Samuel Levine, the FTC’s director of the Bureau of Consumer Protection, says in a statement. “Today’s groundbreaking order makes clear that the Commission will be vigilant in protecting the public from unfair biometric surveillance and unfair data security practices.”

In addition to a five-year ban from using facial recognition technology, the FTC’s proposed order requires Rite Aid to establish “comprehensive safeguards” to protect customers. The company must delete “all photos and videos” of customers collected by its facial recognition system, implement a data security program, and provide a written notice to customers who will have their biometric data enrolled in a database in the future, among other provisions. Since Rite Aid is currently going through bankruptcy proceedings, the FTC says the order will go into effect once the bankruptcy court and federal district court approve the measures.. Rite Aid will be banned from using AI-powered facial recognition technology for five years under a proposed settlement of Federal Trade Commission charges, the FTC announced Tuesday. Why it matters: The FTC alleged in a complaint Tuesday that the pharmacy retail chain failed to implement reasonable procedures in hundreds of stores and prevent harm to consumers with what the agency called Rite Aid's "reckless" use of facial recognition technology that it said "disproportionately impacted people of color."

Face-recognition tech has proven to be a popular option for retail and other industries, and these findings could galvanize advocacy groups that campaign against such surveillance.

Of note: While Rite Aid welcomed the proposed settlement, it said in a statement Tuesday "we fundamentally disagree with the facial recognition allegations in the agency's complaint," adding that the company used the technology in "a limited number of stores."

Driving the news: The FTC accuses Rite Aid in the complaint, filed in federal court in Pennsylvania, of failing to take reasonable measures to prevent harm to customers when using AI-based facial recognition technology from 2012 to 2020 to identify those they suspected of shoplifting or other problematic behavior.

It alleges Rite Aid's actions subjected consumers to embarrassment and harassment.

The commission said Rite Aid's actions violated a 2010 data security order by failing to adequately oversee its service providers.

Zoom in: The complaint alleges that Rite Aid used the technology "to capture images of all consumers" in its drugstores and created a database of those identified as carrying out suspicious behavior. The database included "accompanying information," such as names, birth years and details "related to criminal or 'dishonest' behavior.'"

Rite Aid workers would receive "match alerts" to their phones. "In numerous instances, the match alerts that led to these actions were false positives," the complaint states.

The company didn't inform consumers that it used the technology and "Rite Aid specifically instructed employees not to reveal" its use to customers, the FTC alleges.

What's next: The FTC's proposed order would require Rite Aid to implement comprehensive safeguards to prevent any future harm to customers.

It would require Rite Aid to stop using such technology and delete, and direct third parties to remove, any images or photos that have been collected.

Given that Rite Aid is going through bankruptcy proceedings, the FTC said the order would go into effect after approval from the courts.

What they're saying: "Rite Aid's reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers' sensitive information at risk," said Samuel Levine, director of the FTC's Bureau of Consumer Protection, in a statement accompanying the announcement.

The other side: "We respect the FTC's inquiry and are aligned with the agency's mission to protect consumer privacy," Rite Aid said in its statement.

"Rite Aid stopped using the technology in this small group of stores more than three years ago, before the FTC's investigation regarding the Company's use of the technology began," the company said in the statement.

Between the lines: Joy Buolamwini, an AI researcher who has studied face-recognitio's racial biases, told the Washington Post the Rite Aid case was an "urgent reminder" that the U.S. has failed to enact sweeping privacy laws.

The face is the final frontier of privacy and it is crucial now more than ever that we fight for our biometric rights, from airports to drugstores to schools and hospitals," Buolamwini said.

Go deeper... Report: Feds need rules for using facial recognition tech

Editor's note: This article has been updated with comment from AI researcher Joy Buolamwini.. The Federal Trade Commission (FTC) on Tuesday banned Rite Aid from using facial recognition powered by artificial intelligence (AI) for surveillance purposes for five years following charges the retailer’s use of AI lacked appropriate safeguards and falsely tagged customers as shoplifters.

In a complaint filed in federal court, the FTC argued that Rite Aid used AI-based facial recognition tools to identify customers who may have engaged in shoplifting or other problematic behavior. The agency said that Rite Aid failed to put in place safeguards to protect employees who were falsely accused of wrongdoing because the facial recognition technology mistakenly flagged them as matching someone previously identified as a shoplifter or other troublemaker.

The FTC said the facial recognition system "generated thousands of false-positive matches" and that it "sometimes matched customers with people who had originally been enrolled in the database based on activity thousands of miles away, or flagged the same person at dozens of different stores" all across the country.

It added that Rite Aid’s technology was also more likely to generate false positives at stores located in plurality-Black and Asian communities, used low-quality images that made false positives more likely and failed to both adequately train employees on the technology and regularly monitor its accuracy.

RITE AID CLOSING 154 STORES IN 15 STATES: HERE’S THE LIST

"Rite Aid’s reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers’ sensitive information at risk," said Samuel Levine, director of the FTC’s Bureau of Consumer Protection. "Today’s groundbreaking order makes clear that the Commission will be vigilant in protecting the public from unfair biometric surveillance and unfair data security practices."

The FTC will require Rite Aid to implement consumer safeguards when deploying automated systems using biometric information to track them or flag them as security risks. It will also require the company to discontinue the use of technology if can’t control potential risks to consumers, and Rite Aid executives will have to implement a "robust information security program" that has to be "overseen by the company’s top executives."

WHAT IS ARTIFICIAL INTELLIGENCE (AI)?

Ticker Security Last Change Change % RADCQ RITE AID CORP. 0.13 +0.00 +0.00%



Rite Aid released a statement saying that it’s "pleased to reach an agreement with the FTC and put this matter behind us," though it noted that while it shares the goal of protecting consumers’ privacy, it took issue with some of the agency’s allegations.

"We respect the FTC’s inquiry and are aligned with the agency’s mission to protect consumer privacy," the company wrote. "However, we fundamentally disagree with the facial recognition allegations in the agency’s complaint. The allegations relate to a facial recognition pilot program the Company deployed in a limited number of stores. Rite Aid stopped using the technology in this small group of stores more than three years ago, before the FTC’s investigation regarding the Company’s use of the technology began."

DOJ SUES RITE AID FOR ALLEGED INVOLVEMENT IN OPIOID CRISIS

"Looking ahead, we are focused on the important actions underway to strengthen our financial position as we continue providing leading healthcare products and services to the nearly one million customers that we serve daily," it added.

Rite Aid – the third-largest pharmacy chain in the U.S. – filed for Chapter 11 bankruptcy in October and announced that it would close more than 154 stores in 15 states as it looks to restructure its business. The company closed approximately 210 stores in the fiscal year ending Sept. 30, which left it with a total of 2,100 operating stores.

GET FOX BUSINESS ON THE GO BY CLICKING HERE

The company also faces lawsuits alleging that it helped fuel the opioid epidemic by filling illegal or suspicious prescriptions.

FOX Business’s Daniella Genovese and Reuters contributed to this report.. Rite Aid has been banned from using facial recognition technology for five years over allegations that its surveillance system was used incorrectly to identify potential shoplifters, especially Black, Latino, Asian or female shoppers.

The settlement with the Federal Trade Commission addresses charges that the struggling drugstore chain didn’t do enough to prevent harm to its customers and implement “reasonable procedures,” the government agency said.

Rite Aid said late Tuesday that it disagrees with the allegations, but that it’s glad it reached an agreement to resolve the issue.

The FTC said in a federal court complaint that technology used by Rite Aid for several years led to thousands of incorrect matches, including an incident where Rite Aid store employees stopped and searched an 11-year-old girl.

Rite Aid used facial recognition technology in hundreds of stores from October 2012 to July 2020 to identify shoppers “it had previously deemed likely to engage in shoplifting or other criminal behavior,” the FTC said. The company didn’t tell customers that it was using the technology.

It was installed at store locations in New York City, Baltimore, Philadelphia, Los Angeles and San Francisco, among other cities. Cameras would target customers as they entered the store or moved through it, the complaint said.

The technology would then compare the live images with a database.

The complaint noted that many images it used for its database were low-quality, coming from security cameras, employee phone cameras and news stories in some cases.

The technology sent alerts to Rite Aid employees either by email or phone when it identified people entering the store on its watchlist.

The FTC said in its complaint that store employees would then follow those people, order them to leave or call police. Federal officials also said employees would accuse people in front of friends, family and other customers of previously committing crimes.

The federal complaint said Rite Aid failed to test the accuracy of its technology before using it.

Rite Aid says the allegations center on a pilot program it used in a limited number of stores, and it stopped using this technology more than three years ago.

“We respect the FTC’s inquiry and are aligned with the agency’s mission to protect consumer privacy, the company said in a statement posted on its website. “However, we fundamentally disagree with the facial recognition allegations in the agency’s complaint.”

Facial recognition technology has a checkered history. Supporters say it has been vital in helping to catch drug dealers or resolve missing persons cases when used by law enforcement. But critics say it leads to a higher rate of misidentification for people of color.

Rite Aid also noted in a prepared statement that any agreement will have to be approved in U.S. Bankruptcy Court.

Rite Aid announced last fall that it was closing more than 150 stores as it makes its way through a voluntary Chapter 11 bankruptcy process.

Rite Aid Corp., based in Philadelphia, has more than 2,000 locations. The company has struggled financially for years and also faces financial risk from lawsuits over opioid prescriptions like its bigger rivals, CVS and Walgreens.. Rite Aid has been banned from using facial recognition software for five years, after the Federal Trade Commission (FTC) found that the U.S. drugstore giant’s “reckless use of facial surveillance systems” left customers humiliated and put their “sensitive information at risk.”

The FTC’s Order, which is subject to approval from the U.S. Bankruptcy Court after Rite Aid filed for Chapter 11 bankruptcy protection in October, also instructs Rite Aid to delete any images it collected as part of its facial recognition system rollout, as well as any products that were built from those images. The company must also implement a robust data security program to safeguard any personal data it collects.

A Reuters report from 2020 detailed how the drugstore chain had secretly introduced facial recognition systems across some 200 U.S. stores over an eight-year period starting in 2012, with “largely lower-income, non-white neighborhoods” serving as the technology testbed.

With the FTC’s increasing focus on the misuse of biometric surveillance, Rite Aid fell firmly in the government agency’s crosshairs. Among its allegations are that Rite Aid — in partnership with two contracted companies — created a “watchlist database” containing images of customers that the company said had engaged in criminal activity at one of its stores. These images, which were often poor quality, were captured from CCTV or employees’ mobile phone cameras.

When a customer entered a store who supposedly matched an existing image on its database, employees would receive an automatic alert instructing them to take action — and the majority of the time this instruction was to “approach and identify,” meaning verifying the customer’s identity and asking them to leave. Often, these “matches” were false positives that led to employees incorrectly accusing customers of wrongdoing, creating “embarrassment, harassment, and other harm,” according to the FTC.

“Employees, acting on false positive alerts, followed consumers around its stores, searched them, ordered them to leave, called the police to confront or remove consumers, and publicly accused them, sometimes in front of friends or family, of shoplifting or other wrongdoing,” the complaint reads.

Additionally, the FTC said that Rite Aid failed to inform customers that facial recognition technology was in use, while also instructing employees to specifically not reveal this information to customers.

Face-off

Facial recognition software has emerged as one of the most controversial facets of the AI-powered surveillance era. In the past few years we’ve seen cities issue expansive bans on the technology, while politicians have fought to regulate how police utilize it. And companies such as Clearview AI, meanwhile, have been hit with lawsuits and fines around the world for major data privacy breaches around facial recognition technology.

The FTC’s latest findings regarding Rite Aid also shines a light on inherent biases in AI systems. For instance, the FTC says that Rite Aid failed to mitigate risks to certain consumers due to their race — its technology was “more likely to generate false positives in stores located in plurality-Black and Asian communities than in plurality-White communities,” the findings note.

Additionally, the FTC said that Rite Aid failed to test or measure the accuracy of their facial recognition system prior to, or after, deployment.

In a press release, Rite Aid said that it was “pleased to reach an agreement with the FTC,” but that it disagreed with the crux of the allegations.

“The allegations relate to a facial recognition technology pilot program the Company deployed in a limited number of stores,” Rite Aid said in its statement. “Rite Aid stopped using the technology in this small group of stores more than three years ago, before the FTC’s investigation regarding the Company’s use of the technology began.”. The ban will last five years, the Federal Trade Commission said.

Rite Aid banned from use of facial recognition in stores after thousands of false matches

Drugstore chain Rite Aid has accepted a ban of its use of facial recognition software for five years due to false accusations stemming from the technology that disproportionately affected people of color, the Federal Trade Commission said.

The retailer failed to impose reasonable precautions in its deployment of facial recognition, resulting in thousands of false-positive matches with customers accused of shoplifting and other inappropriate behavior, a legal complaint from the FTC said.

Acting on false-positive alerts, employees followed consumers around its stores, searched them, accused them of wrongdoing in front of friends and family, and called the police to remove them, the complaint said.

The company also chose not to inform customers of its use of facial recognition and discouraged employees from doing so, the FTC said.

"Rite Aid's reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers' sensitive information at risk," Samuel Levine, director of the FTC's Bureau of Consumer Protection, said in a statement.

"Today's groundbreaking order makes clear that the Commission will be vigilant in protecting the public from unfair biometric surveillance and unfair data security practices," Levine added.

In a statement, the company said it was "pleased to reach an agreement" with the FTC. However, Rite Aid added: "We fundamentally disagree with the facial recognition allegations in the agency's complaint."

"Rite Aid's mission has always been and will continue to be to safely and conveniently serve the communities in which we operate," the company said.

In this undated stock photo, a secutiry camera is seen in a store. STOCK PHOTO/Getty Images

In all, Rite Aid collected tens of thousands of images of individuals, many of which were low-quality, the complaint said, noting that the activity took place between 2012 to 2020.

Mistakes made by the company's technology included false matches with an image collected thousands of miles away, as well as an incorrect positive result flagged at dozens of stores nationwide, the complaint added.

Rite Aid violated the terms of a 2010 agreement reached with the FTC after a finding that the company had failed to protect sensitive financial and medical information, the agency said.

The FTC issued a warning earlier this year that the company would be closely monitoring issues around the collection of data tied to individuals' physical characteristics, the agency said.

In addition to complying with the five-year ban, Rite Aid said it will impose safeguards for future use of its surveillance systems.

Those safety measures include deleting photos previously collected by its facial recognition software, informing customers when their physical characteristics are entered into a store database and providing notice to customers about use of the technology at a given location.. Rite Aid, the pharmacy chain, used facial recognition technology to falsely and disproportionately identify people of color and women as likely shoplifters, the Federal Trade Commission said on Tuesday, describing a system that embarrassed customers and raised new concerns about the biases baked into such technologies.

Under the terms of a settlement, Rite Aid will be barred from using facial recognition technology in its stores for surveillance purposes for five years, the F.T.C. said. The agency, which enforces federal consumer protection laws, appeared to be signaling just how seriously it would respond to concerns about facial recognition technology.

The F.T.C.’s 54-page complaint also shed light on how a once-theoretical worry — that human bias would bleed into artificial intelligence algorithms and amplify discrimination — has become a cause for concern in the real world.

Samuel Levine, the director of the F.T.C.’s Bureau of Consumer Protection, said in a statement that “Rite Aid’s reckless use of facial surveillance systems left its customers facing humiliation and other harms.”. The Federal Trade Commission proposed to bar Rite Aid from using facial recognition software in its drugstores for five years to settle allegations it improperly used the technology to identify shoplifters, the agency said Tuesday.

The FTC alleged that from 2012 to 2020, Rite Aid deployed facial recognition technology in hundreds of its retail pharmacies across several states in order to identify customers it had previously deemed likely to be shoplifting or engaging in other criminal activity. But the system generated thousands of false-positive matches, according to the FTC, resulting in some shoppers being mistakenly flagged as persons of interest.

Those individuals were detained or searched by Rite Aid employees, subjected to increased surveillance, publicly accused of criminal activity, reported to police, and in some cases banned from entering or making purchases at Rite Aid stores, the FTC alleged.

Rite Aid's facial recognition technology was more likely to generate false positives in stores located in predominantly Black and Asian neighborhoods than in predominantly white communities, where 80% of Rite Aid stores are located, the FTC claims.

Rite Aid relied on facial technology from two undisclosed vendors, the agency said. It maintained a database of persons of interest that included images collected from security camera footage, driver's licenses or government IDs, along with data such as names, years of birth, and "information related to criminal or 'dishonest' behavior in which individuals had allegedly engaged," the FTC alleged in its complaint, which was filed in U.S. District Court for the Eastern District of Pennsylvania. There were "at least tens of thousands of individuals in its database," the agency alleged.

As part of the proposed settlement, the FTC said Rite Aid must order third parties to delete images or photos collected by its facial recognition system, notify shoppers when biometric data is collected or used in connection with its security or surveillance systems, among other requirements. It will also require Rite Aid to permanently discontinue using the technology if it can't control potential risks to consumers.

Rite Aid said in a press release that it's pleased to reach an agreement with the FTC but that it disagrees with the agency's allegations.

"The allegations relate to a facial recognition technology pilot program the Company deployed in a limited number of stores," the company said, adding that it stopped using the technology more than three years ago, before the FTC initiated its investigation.

The FTC action comes after a Reuters investigation in 2020 detailed Rite Aid's use of facial recognition technology in primarily lower-income, non-white neighborhoods. Reuters identified facial recognition software providers DeepCam and FaceFirst as RiteAid's vendors. FaceFirst's technology routinely misidentified Black individuals as shoplifters, the Reuters investigation found.

Privacy and civil liberties advocates continue to raise alarms around the use of facial recognition software and the need for further regulation. The technology has led to increased surveillance, and numerous studies have shown the artificial intelligence underpinning the technology is more likely to misidentify people of color, leading to wrongful arrests.

The proposed settlement is subject to approval by a court overseeing Rite Aid's bankruptcy proceedings. The drugstore chain filed for Chapter 11 bankruptcy protection in October amid slowing sales, rising debt and lawsuits alleging it contributed to the U.S. opioid epidemic.. Rite Aid has “used facial recognition technology in its retail stores without taking reasonable steps to address the risks that its deployment of such technology was likely to result in harm to consumers as a result of false-positive facial recognition match alerts.” That’s the lawyerly language of the FTC’s just-filed action against drug store chain Rite Aid and a subsidiary. Put in more common parlance, the FTC alleges that Rite Aid launched an inadequately tested and operationally deficient covert surveillance program against its customers without considering the impact that its inaccurate facial recognition technology would have on people wrongly identified as “matching” someone on the company’s watchlist database. Among other things, a proposed settlement in the case would ban Rite Aid from using any facial recognition system for security or surveillance purposes for five years.

From at least 2012 until 2020, Rite Aid has used facial recognition technology in hundreds of its retail locations to “drive and keep persons of interest out of [Rite Aid’s] stores.” Most of those stores were in large urban areas. What’s more, the complaint alleges that Rite Aid didn’t tell consumers that it used facial recognition technology and specifically instructed employees not to reveal that fact to consumers or the media.

How did Rite Aid’s facial recognition system operate? Rite Aid supervised the creation of a “watchlist database” of images of people the company claimed had engaged in actual or attempted criminal activity at one of its stores. Called “enrollments,” these entries included – to the extent known – first and last names, years of birth, and a description of the behavior Rite Aid claimed the person in the photo had engaged in. Uploaded by in-store Rite Aid employees, the images were often low-quality – sometimes screenshots from closed-circuit TV or photos taken on employees’ cell phones. According to the complaint, Rite Aid directed store security to “push for as many enrollments as possible,” resulting in a watchlist database that included tens of thousands of people.

If someone who entered the store supposedly “matched” an image in Rite Aid’s watchlist database, employees received an alert on their company cell phone. Based in whole or in part on that alert, Rite Aid staff were directed to swing into action based on categories in the database that informed staff’s response. According to the complaint, “A majority of Rite Aid’s facial recognition enrollments were assigned the match alert instruction ‘Approach and Identify,’ which meant employees should approach the person, ask the person to leave, and, if the person refused, call the police.” But according to the complaint, in numerous instances, the match alerts that led to those actions were false positives – in other words, the technology incorrectly identified Rite Aid customers as people in the watchlist database.

You’ll want to read the complaint for allegations about the considerable – and injurious – inaccuracies of the system, but here’s just one example. During one five-day period, Rite Aid generated over 900 separate alerts in more than 130 stores from New York to Seattle, all claiming to match one single image in the database. Put another way, Rite Aid’s facial recognition technology told employees that just one pictured person had entered more than 130 Rite Aid locations from coast to coast more than 900 times in less than a week. Giving a whole new meaning to the phrase “facially inaccurate,” Rite Aid allegedly used that information to expel consumers from its stores.

Companies that are considering using AI surveillance technologies or other biometric surveillance systems, take note. The FTC says that in deploying facial recognition technology in some of its locations, Rite Aid has failed to take reasonable measures to prevent harm to consumers. Here are just some of the allegations in the complaint

Rite Aid failed to consider the risks that false positives had on consumers, including risks of misidentification based on race or gender. For a host of reasons outlined in the complaint, the FTC alleges that Black, Asian, Latino, and women consumers were at increased risk of being incorrectly “matched” with an image in the company’s watchlist database – leading to humiliating and injurious consequences. As the complaint charges in detail, “As a result of Rite Aid’s failures, Black, Asian, Latino, and women consumers were especially likely to be harmed by Rite Aid’s use of facial recognition technology.”

Rite Aid failed to test the system for accuracy. According to the FTC, Rite Aid didn’t bother to ask its first facial recognition technology v endor whether the system had been tested for accuracy. In fact, Rite Aid deployed the technology despite the vendor’s express statement that it:

MAKES NO REPRESENTATIONS OR WARRANTIES AS TO THE ACCURACY AND RELIABILITY OF THE PRODUCT IN THE PERFORMANCE OF ITS FACIAL RECOGNITION CAPABILITIES. [VENDOR] DISCLAIMS ANY RESPONSIBILITY OR WARRANTY, EXPRESS OR IMPLIED, WITH RESPECT TO ANY FALSE IDENTIFICATION OR MISIDENTIFICATION ARISING FROM THE USE OF THE PRODUCT.

Before going to a second vendor, Rite Aid was allegedly aware of the problem of false positives and yet again didn’t ask for test results about the accuracy of that vendor’s system.

Rite Aid failed to enforce image quality controls . As one vendor explained to Rite Aid, “The quality of the photos used for [facial recognition technology] is extremely important . . . Without good quality photos, an enrollment is not useful.” Aware of that warning, Rite Aid claimed to establish image quality standards. But according to the FTC, Rite Aid flouted its own policies by regularly using blurry, low-quality images taken in low light, increasing the likelihood of false positives.

Rite Aid failed to train its staff . Rite Aid’s training focused on navigating the website to use the technology and uploading new enrollments. The complaint alleges that Rite Aid’s training materials either didn’t address the risk of false positives or covered the topic only briefly. Even when the company had evidence of the false-positives problem, the FTC says Rite Aid didn’t take reasonable steps to improve its training.

Rite Aid failed to monitor, test, or track the accuracy of results. Even after the problem with false-positive matches became apparent, the FTC says Rite Aid didn’t adequately address the issue. As the complaint alleges, “In part because of Rite Aid’s failures to track, monitor, assess, or test its facial recognition technology, Rite Aid did not have a reasonable basis to believe that any given match alert was likely to be accurate. Nevertheless, Rite Aid continued to instruct store-level employees to take action against consumers on the basis of facial recognition match alerts.” Furthermore, the FTC says Rite Aid didn’t keep accurate records of the outcomes of alerts and didn’t track false positives.

The complaint includes more examples of the impact Rite Aid’s failures had on people who shopped at its stores. To cite one instance, in May 2020 Rite Aid staff in The Bronx uploaded an image to the watchlist database. For the next several months, Rite Aid’s facial recognition technology generated over 1,000 match alerts for that one photo – nearly 5% of all match alerts generated by Rite Aid’s system during that period. What’s more, 99% of those match alerts came from the Los Angeles area. In fact, four of the match alerts told Rite Aid staff that the one person was spotted in both New York and California stores in the same 24-hour period.

How were consumers injured by false-positive match alerts generated by Rite Aid’s facial recognition technology? According to the FTC, numerous consumers were mistakenly identified as shoplifters or wrongdoers. As a result, the complaint charges that Rite Aid surveilled them and followed them around the store; told them to leave without making purchases, including for prescription or over-the-counter medications; searched them; publicly accused them of being shoplifters and humiliated them in front of employers, coworkers, and family members, including their children; and called the police to confront or remove them – all based on facial recognition technology known to produce false positives and especially likely to result in inaccurate matches for Black, Latino, Asian, and women consumers.

The FTC filed its lawsuit against Rite Aid in a Pennsylvania federal court. Count I of that complaint alleges that Rite Aid used facial recognition technology in its stores without taking reasonable steps to address the risks that its use would likely harm consumers due to false-positive match alerts, especially women consumers and consumers of color. Count II stems from a 2010 order that Rite Aid is already under that requires it to maintain a comprehensive information security program to protect consumers’ personal information. That count alleges that Rite Aid’s failure to maintain that required program is an unfair practice, in violation of the FTC Act.

The FTC says the failures in Rite Aid’s information security program were significant. The complaint cites a number of specific deficiencies and shortcomings, including:

Rite Aid failed to properly vet vendors that had access to consumers’ personal information. Conducting an accurate and verifiable assessment of the data security capabilities of vendors is an essential part of any comprehensive information security program. According to the FTC, Rite Aid entrusted sensitive consumer data to vendors, including those the company deemed to be “high risk,” based just on conversations, rather than on a thorough evaluation of written materials and other documentation. Sure, talking things over can be part of an information security program, but it shouldn’t constitute the entire assessment process.

Rite Aid failed to periodically reassess service providers’ data security practices . Procedures and capabilities can change, so when sensitive data is at stake, assessing service providers isn’t a one-and-done task. The FTC says Rite Aid failed to conduct periodic reassessment to ensure that consumers’ information was safe in service providers’ hands – a key component of any comprehensive information security program.

Rite Aid failed to include sufficient information security requirements in contracts with service providers. The FTC alleges that Rite Aid’s contracts with vendors lacked information security standards or had only minimal requirements. Enforceable contract clauses help protect consumers’ information when it’s in the hands of vendors or other third parties.

The proposed settlement would ban Rite Aid from using any facial recognition or analysis system for security or surveillance purposes at its retail stores or online for five years. In addition, the company would have to delete the photos or videos collected as part of the facial recognition system it operated between 2012 and 2020, as well as any data, models, or algorithms derived from those visuals.

The proposed settlement covers the company’s use of all automatic biometric security or surveillance systems, not just facial recognition and analysis systems. If the company uses any such automated system in the future, it must implement a monitoring program that requires sound technical and organizational controls. Among other things, the monitoring program must address the potential risks to consumers posed by any automatic biometric system the company may implement. You’ll want to read the proposed new order for specifics, but it would put broad provisions in place to ensure appropriate training, testing, and evaluation. Before deploying any automatic biometric security or surveillance system, Rite Aid will need solid proof that it’s accurate. And if Rite Aid has reason to believe at some point that the system’s inaccuracies contribute to a risk of harm to consumers, the company must shut the system down.

Furthermore, if Rite Aid has an automatic biometric security or surveillance system in place in the future, under the proposed order, it must give individualized, written notice to any consumer the company adds to its system and anyone that it takes action against as a result. Rite Aid also would have to implement a robust consumer complaint procedure. In addition, the company would have to clearly disclose to consumers at retail locations and online if it’s using automatic biometric security and surveillance and the notices must be placed where consumers can read them in time to avoid the collection of their biometric information.

Additionally, Rite Aid must implement a comprehensive information security program, obtain biennial assessments of that program from a third party assessor, and provide an annual certification to the FTC from its CEO stating that Rite Aid is in compliance with the proposed order. You’ll want to read the proposed order for more about specific requirements.

Because Rite Aid is currently in bankruptcy, the proposed settlement is subject to the Bankruptcy Court’s approval.

Does your company use AI or other automated biometric surveillance technologies? The FTC’s action against Rite Aid demonstrates the need to test, assess, and monitor the operation of those systems and to ensure that their performance in real-world settings complies with consumer protection standards.. Rite Aid will be prohibited from using facial recognition technology for surveillance purposes for five years to settle Federal Trade Commission charges that the retailer failed to implement reasonable procedures and prevent harm to consumers in its use of facial recognition technology in hundreds of stores.

“Rite Aid's reckless use of facial surveillance systems left its customers facing humiliation and other harms, and its order violations put consumers’ sensitive information at risk," said Samuel Levine, Director of the FTC’s Bureau of Consumer Protection. “Today’s groundbreaking order makes clear that the Commission will be vigilant in protecting the public from unfair biometric surveillance and unfair data security practices.”

The proposed order will require Rite Aid to implement comprehensive safeguards to prevent these types of harm to consumers when deploying automated systems that use biometric information to track them or flag them as security risks. It also will require Rite Aid to discontinue using any such technology if it cannot control potential risks to consumers. To settle charges it violated a 2010 Commission data security order by failing to adequately oversee its service providers, Rite Aid will also be required to implement a robust information security program, which must be overseen by the company’s top executives.

In a complaint filed in federal court, the FTC says that from 2012 to 2020, Rite Aid deployed artificial intelligence-based facial recognition technology in order to identify customers who may have been engaged in shoplifting or other problematic behavior. The complaint, however, charges that the company failed to take reasonable measures to prevent harm to consumers, who, as a result, were erroneously accused by employees of wrongdoing because facial recognition technology falsely flagged the consumers as matching someone who had previously been identified as a shoplifter or other troublemaker.

Preventing the misuse of biometric information is a high priority for the FTC, which issued a warning earlier this year that the agency would be closely monitoring this sector. Rite Aid’s actions subjected consumers to embarrassment, harassment, and other harm, according to the complaint. The company did not inform consumers that it was using the technology in its stores and employees were discouraged from revealing such information. Employees, acting on false positive alerts, followed consumers around its stores, searched them, ordered them to leave, called the police to confront or remove consumers, and publicly accused them, sometimes in front of friends or family, of shoplifting or other wrongdoing, according to the complaint. In addition, the FTC says Rite Aid’s actions disproportionately impacted people of color.

According to the complaint, Rite Aid contracted with two companies to help create a database of images of individuals—considered to be “persons of interest” because Rite Aid believed they engaged in or attempted to engage in criminal activity at one of its retail locations—along with their names and other information such as any criminal background data. The company collected tens of thousands of images of individuals, many of which were low-quality and came from Rite Aid’s security cameras, employee phone cameras and even news stories, according to the complaint.

The system generated thousands of false-positive matches, the FTC says. For example, the technology sometimes matched customers with people who had originally been enrolled in the database based on activity thousands of miles away, or flagged the same person at dozens of different stores all across the United States, according to the complaint. Specifically, the complaint says Rite Aid failed to:

Consider and mitigate potential risks to consumers from misidentifying them, including heightened risks to certain consumers because of their race or gender. For example, Rite Aid’s facial recognition technology was more likely to generate false positives in stores located in plurality-Black and Asian communities than in plurality-White communities;

Test, assess, measure, document, or inquire about the accuracy of its facial recognition technology before deploying it, including failing to seek any information from either vendor it used to provide the facial recognition technology about the extent to which the technology had been tested for accuracy;

Prevent the use of low-quality images in connection with its facial recognition technology, increasing the likelihood of false-positive match alerts;

Regularly monitor or test the accuracy of the technology after it was deployed, including by failing to implement or enforce any procedure for tracking the rate of false positive matches or actions that were taken based on those false positive matches; and

Adequately train employees tasked with operating facial recognition technology in its stores and flag that the technology could generate false positives. Even after Rite Aid switched to a technology that enabled employees to report a “bad match” and required employees to use it, the company did not take action to ensure employees followed this policy.

In its complaint, the FTC also says Rite Aid violated its 2010 data security order with the Commission by failing to adequately implement a comprehensive information security program. Among other things, the 2010 order required Rite Aid to ensure its third-party service providers had appropriate safeguards to protect consumers’ personal data. For example, the complaint alleges the company conducted many security assessments of service providers orally, and that it failed to obtain or possess backup documentation of such assessments, including for service providers Rite Aid deemed to be “high risk.”

In addition to the ban and required safeguards for automated biometric security or surveillance systems, other provisions of the proposed order prohibit Rite Aid from misrepresenting its data security and privacy practices and also require the company to:

Delete, and direct third parties to delete , any images or photos they collected because of Rite Aid’s facial recognition system as well as any algorithms or other products that were developed using those images and photos;

Notify consumers when their biometric information is enrolled in a database used in connection with a biometric security or surveillance system and when Rite Aid takes some kind of action against them based on an output generated by such a system;

Investigate and respond in writing to consumer complaints about actions taken against consumers related to an automated biometric security or surveillance system;

Provide clear and conspicuous notice to consumers about the use of facial recognition or other biometric surveillance technology in its stores;

Delete any biometric information it collects within five years;

Implement a data security program to protect and secure personal information it collects, stores, and shares with its vendors;

Obtain independent third-party assessments of its information security program; and

Provide the Commission with an annual certification from its CEO documenting Rite Aid’s adherence to the order’s provisions.

The Commission voted 3-0 to authorize staff to file the complaint and the proposed stipulated order against Rite Aid. Commissioner Alvaro Bedoya released a statement.

The complaint and order were filed in the Eastern District of Pennsylvania. Rite Aid is currently going through bankruptcy proceedings and the order will go into effect after approval from the bankruptcy court and the federal district court as well as modification of the 2010 order by the Commission.

The principal attorneys on these matters are Robin Wetherill, Leah Frazier, Diana Chang, Christopher Erickson, and Brian Welke in the FTC’s Bureau of Consumer Protection.. On December 19, 2023, the Federal Trade Commission (FTC) announced an enforcement action against the retail pharmacy Rite Aid for unfair practices associated with its use of a facial recognition technology (FRT) surveillance system to deter theft in its retail stores and for violations of a previous FTC Order. The FTC’s 54-page complaint alleges that Rite Aid (1) failed to take reasonable measures to prevent harm to consumers from its use of facial recognition technology and (2) violated provisions from the 2010 Order that required a comprehensive information security program and document retention for vendor management.

This action marks an important moment in AI regulatory history, as it is the first time the FTC has taken enforcement action against a company for using AI in an allegedly biased and unfair manner. It also continues a trend we are seeing in FTC enforcement actions in terms of the agency relying on data and algorithmic disgorgement as a remedy: Rite Aid (and any third parties with whom they shared covered information with) must delete all of Rite Aid biometric data that was processed unfairly, and any AI models or algorithms associated with such data.

This enforcement decision also serves as a warning and a guidepost for companies using and developing AI systems, especially if those AI systems use facial recognition technology in order to identify individuals and make automated decisions about them that could result in potential harm. The FTC’s enforcement action in this case aligns with the unfairness criteria outlined by the agency in its May 2023 policy statement warning about misuses of biometric information and harm to consumers. Moreover, this enforcement action highlights the importance of conducting risk assessments to understand potential consumer impacts, implementing bias mitigation strategies, overseeing vendors, employee training, and complying with company security standards at every stage of the procurement and deployment of an AI system (vendor selection, model development, model maintenance, and post-deployment monitoring).

In this post, we summarize the FTC’s complaint, including its formal counts against Rite Aid and supporting allegations. We also present a consolidated assessment of its stipulated order and some key takeaways that companies currently deploying or interested in deploying AI technology should note. To stay up-to-date on the latest developments involving the FTC and other privacy regulators, please subscribe to the WilmerHale Privacy and Cybersecurity Law Blog.

The Complaint

The complaint details how Rite Aid deployed an FRT surveillance system in hundreds of its retail pharmacy locations, primarily in urban, low-income, and racially diverse communities, from October 2012 until July 2020. The company contracted with two different third-party vendors to develop the AI system, which involved creating a database of images of “persons of interest” comprised of (1) individuals who Rite Aid employees enrolled in the system for suspected or actual criminal activity at a Rite Aid store, and (2) individuals who had been flagged by law enforcement in “Be On the Look Out” alerts. Rite Aid encouraged its store-level employees to enroll as many individuals as possible. The FRT database contained the personal details and images, often low-quality still shots from CCTV cameras or cell phone photos, of “tens of thousands” of individuals. Rite Aid had no data retention policy—the FRT database stored these images indefinitely.

Rite Aid’s FRT surveillance system captured images of all customers entering the retail store and compared the images to the database to identify potential matches. If the match cleared a specific confidence score, the system would alert store employees and provide instructions to the employee (e.g. “observe and provide customer service” or “notify police”) based on the match information. These match alerts generally did not give the actual confidence score to store employees. However, as the FTC notes at length in its complaint, the system frequently generated false-positive matches that “subjected consumers to surveillance, removal from stores, and emotional and reputational harm,” among other harms. Rite Aid instructed its store employees not to tell shoppers or the media about the use of this FRT.

The FTC’s allegations roll up into 2 hefty counts against Rite Aid, with the allegations summarized and below. Notably, despite the fact that the FTC could have obtained monetary penalties against Rite Aid given the agency’s decision to enforce a prior order, the consent order did not require Rite Aid to pay money (presumably because the company is in bankruptcy proceedings).

1. Unfair facial recognition technology practices

The FTC contends that Rite Aid failed to:

Assess, monitor, or mitigate any risks of incorrectly identifying consumers, with such errors occurring more frequently based on race or gender. As a result, Black, Asian, Latinx, and women consumers were at higher risk of being misidentified and generating a false positive by Rite Aid’s FRT.

Assess, monitor, or mitigate any risks of incorrectly identifying consumers, with such errors occurring more frequently based on race or gender. As a result, Black, Asian, Latinx, and women consumers were at higher risk of being misidentified and generating a false positive by Rite Aid’s FRT. Reasonably assess or even inquire about the accuracy of the FRT before deploying the technology. In fact, both of its vendors’ contracts expressly disclaimed any warranty as to the accuracy or reliability of the results.

Enforce image quality controls, increasing the likelihood of false-positive match alerts. The FTC alleges that Rite Aid’s image quality policies demonstrated that it understood how poor image quality could lead to false alerts, and yet Rite Aid did not place any controls or oversight to ensure that enrollment photos complied with the policy.

Appropriately train or oversee store employees responsible for operating the FRT, including how to interpret and act on the match alerts. The available training materials also did not adequately address the possibility of false-positive matches.

Regularly monitor or test the accuracy of the technology after deployment. According to the complaint, Rite Aid inadequately examined the accuracy of match alerts, documented outcomes, monitored the frequency of false-positive matches, and addressed issues with problematic enrollments.

2. Unfair failure to implement or maintain a comprehensive information security program mandated by a previous FTC Order in 2010

The FTC also claimed that Rite Aid violated the 2010 Order, which mandated the implementation and maintenance of a comprehensive information security program and documentation of the compliance. This violation included failures to:

Use reasonable steps to assess and select service providers who could meet the security standards for the personal information shared by Rite Aid. (Rite Aid also failed to maintain risk assessment documentation for these service providers.)

Conduct periodic security assessments of its service providers to ensure that they continued to meet the company’s security standards.

Contractually require their service providers to establish appropriate safeguards for the personal information shared by Rite Aid.

The Stipulated Order

The order contains 16 stipulations that Rite Aid agrees to in order to settle the case. In addition to submitting an annual certification of compliance, reporting any “covered incidents” (such as data breaches) to the FTC, and maintaining accurate records, Rite Aid:

1. Cannot use any Facial Recognition or Analysis System (defined broadly as “an automated biometric security or surveillance system that analyzes or uses depictions or images, descriptions, recordings, copies, measurements, or geometry of or related to an individual’s face to generate an output”) for 5 years.

2. Must delete covered biometric information and destroy any AI models or algorithms derived from that information (also called model disgorgement). Rite Aid also must notify any third parties with these data or models and instruct them to do the same.

3. Must establish and implement an Automated Biometric Security or Surveillance System Monitoring Program if Rite Aid continues to use the FRT surveillance system in their stores after the 5-year ban or if they want to use another Automated Biometric Security or Surveillance System not subject to the ban.. (Requirements for the monitoring program start on p. 7 of the order.)

4. Must establish and implement procedures to provide consumers with notice and a means for submitting complaints related to the outputs of the AI system if Rite Aid continues to use the FRT surveillance system in their stores after the 5-year ban or if they want to use another Automated Biometric Security or Surveillance System not subject to the ban. (Requirements for the notice and complaint procedures start on p. 13 of the order.)

5. Must have retention limits for its biometric data prior to implementing any Automated Biometric Security or Surveillance System.

6. Must post clear and conspicuous notices at the retail locations and online platforms disclosing the company’s use of any Automated Biometric Security or Surveillance System that uses biometric information collected from consumers. The notices must contain information about:

a. The specific types of biometric information collected,

b. The types of outputs generated by the AI,

c. All purposes for which Rite-Aid uses the FRT and its outputs, including any actions that store employees may take on account of the outputs, and

d. The timeframe for deletion of each type of biometric information used, as established in the (also mandated) data retention policies.

7. Cannot misrepresent its compliance with these orders.

8. Must establish and maintain a comprehensive information security program for vendors that protects the personal information shared by Rite Aid. (Requirements for the security program start on p. 17 of the order.)

9 & 10. Periodically undergo a security assessment by an independent third party, report the results to the FTC, and cooperate with the assessor.

Key Takeaways



In an accompanying statement to the enforcement action, Commissioner Alvaro Bedoya said, “Section 5 of the FTC Act requires companies using technology to automate important decisions about people’s lives…to take reasonable measures to identify and prevent foreseeable harms,” He explained that the settlement with Rite Aid “offers a strong baseline for what an algorithmic fairness program should look like.”

Companies looking to ensure compliance and avoid attention from regulators for consumer-facing AI systems—especially those engaging with high-risk applications of AI that use personal and/or biometric information to make automated decisions about an individual—should look at this order for guidance. It highlights some of the FTC’s priorities in data privacy and AI governance enforcement, and it provides concrete steps companies can take to avoid AI bias and protect personal information. Here are some specific takeaways from this enforcement action:

Consider the what, how, and where—that is, the context—when deploying AI. As the FTC and other regulators warned companies in 2023, existing laws and authorities that protect against discrimination apply to new technologies, like AI, just as they apply to other practices. In addition to assessing the AI itself for any potential disparate outcomes based on race, ethnicity, or gender presentation, companies should be cognizant of the context in which the AI is being deployed. Here, the FTC noted that Rite Aid not only failed to assess false-positive rates across races and gender, but it also failed to consider how the locations it targeted for deploying its AI surveillance system—locations considered “urban” and “along public transportation routes”—would have disproportionate impacts on racial and ethnic minority communities.

FTC penalties for noncompliance in AI cases can include data deletion and algorithmic disgorgement. We have seen the FTC order algorithmic disgorgement (or the deletion or destruction of algorithms or models) before, such as in their enforcement actions against Everalbum, Cambridge Analytica, and Ring. The Rite Aid enforcement action demonstrates that disgorgement continues to be a remedy that the FTC is actively seeking, which can have meaningful effects for companies.

Conducting regular risk assessments is important for high risk AI that produces outputs that could potentially harm consumers. Provision 3 in the stipulated order can serve as a compliance checklist for companies looking to implement “automated biometric security or surveillance systems” or similar AI technology that makes automated decisions about consumers. In particular, the FTC is concerned about the risks of “physical, financial, or reputational harm to consumers” (including stigma and severe emotional distress) and states that these risk assessments should not only identify and address risks, but also consider if there will be a disproportionate impact on consumers based on race, ethnicity, gender, sex, age, or disability, alone or in combination.

Ensure that your company has a strong training program for employees operating high risk AI systems. Employees tasked with operating and/or acting on outputs from AI, like Rite Aid’s FRT surveillance system, should undergo regular training and understand the limitations of the technology. The FTC’s complaint details how the in-store employees should have been trained on:

how to evaluate the quality of the images enrolled into the model,

how to visually compare the images (as the human operator) to see if their assessment agrees with the AI’s outcome, and

how to understand the effects of bias that might be inherent in the data and model.

Companies should be transparent about any use of AI that involves consumer personal information, especially biometric data. Provision 4 in the order explains what details should be in a consumer notice and how Rite Aid can set up a formal complaint procedure for affected consumers. For example, the notice should include the specific types of biometric information collected, the type of outputs generated, the purpose of the data collection and use, and the data retention policy. These requirements highlight the importance of transparency and consumer communication when deploying AI tools like automated biometric security or surveillance systems.

Companies need to scrutinize contracts with vendors handling biometric data or other personal information. The FTC emphasizes the importance of vendor oversight and due diligence at every stage of the procurement process, from the initial selection to ongoing retention. Conducting periodic assessments of vendors’ capability to safeguard consumers’ personal information and retaining documentation of these efforts can help a company remain in compliance with FTC policy guidelines.. On December 19, 2023, the Federal Trade Commission (“FTC”) announced that it reached a settlement with Rite Aid Corporation and Rite Aid Headquarters Corporation (collectively, “Rite Aid”) to resolve allegations that the companies violated Section 5 of the FTC Act (as well as a prior settlement with the agency) by failing to implement reasonable procedures to prevent harm to consumers while using facial recognition technology. As part of the settlement, Rite Aid agreed to cease using “Facial Recognition or Analysis Systems” (defined below) for five years and establish a monitoring program to address certain risks if it seeks to use such systems for certain purposes in the future.

According to the FTC’s complaint, Rite Aid “used facial recognition technology in hundreds of its retail pharmacy locations to identify patrons that it had previously deemed likely to engage in shoplifting or other criminal behavior.” The FTC claimed that the technology sent alerts to Rite Aid’s employees when patrons were matched with entries in the company’s “watchlist database.” Rite Aid employees allegedly took action against patrons who triggered the matches by, for example, subjecting them to in-person surveillance. The FTC claimed that Rite Aid failed to consider or address foreseeable harm to patrons by such conduct, including failing to (1) test the technology’s accuracy, (2) enforce image quality standards necessary for the technology to function accurately, (3) take reasonable steps to train employees, and (4) “take steps to assess or address risks that its . . . [the] technology would disproportionately harm consumers because of their race, gender, or other demographic characteristics.”

The proposed consent order places a number of restrictions and obligations on Rite Aid, including with respect to its use of a “Facial Recognition or Analysis System,” which it defines as “an Automated Biometric Security or Surveillance System that analyzes or uses depictions or images, descriptions, recordings, copies, measurements, or geometry of or related to an individual’s face to generate an Output.” An “Automated Biometric Security or Surveillance System,” in turn, is defined as “any machine-based system, including any computer software, application, or algorithm, that analyzes or uses Biometric Information of, from, or about individual consumers to generate an Output that relates to those consumers, notwithstanding any assistance by a human being in such analysis or use, and that is used in whole or in part for a Security or Surveillance Purpose,” subject to a few exceptions.

Among other restrictions, the proposed consent order requires that Rite Aid:

not deploy or use any Facial Recognition or Analysis System for five years, either in a retail store or an online retail platform;

delete all photos and videos of consumers used in a Facial Recognition or Analysis System, including any data, models, or algorithms derived from such information;

prior to deploying an Automated Biometric Security or Surveillance System in the future:

Establish and maintain a monitoring program, that among things, identifies and addresses risks that “will result, in whole or in part, in physical, financial, or reputational harm to consumers” and “any such harms [that] will disproportionately affect consumers based on race, ethnicity, gender, sex, age, or disability, alone or in combination; Develop mandatory notice and complaint procedures that include providing written notice to consumers whose biometric information will be enrolled in the system; Develop a written retention schedule that, among other things, sets a time frame of deletion for biometric information that is no greater than five years, subject to certain exceptions; and



implement a comprehensive information security program that includes safeguards based on the “volume and sensitivity” of the information that is at risk and the likelihood that the risk could result in unauthorized collection or misuse.

The proposed FTC consent order is subject to a 30-day public comment period following publication in the Federal Register.

Rite Aid filed for relief under Chapter 11 of the Bankruptcy Code on October 15, 2023. Accordingly, the settlement is also subject to approval by the U.S. Bankruptcy Court overseeing the company’s bankruptcy proceeding.

This settlement was described by the FTC as the first enforcement action by the agency that addresses alleged discrimination through the use of automated decision-making technologies.