The man calling Ruth Card sounded just like her grandson Brandon. So when he said he was in jail, with no wallet or cellphone, and needed cash for bail, Card scrambled to do whatever she could to help. “It was definitely this feeling of … fear,” she said. “That we’ve got to help him right now.”

Card, 73, and her husband, Greg Grace, 75, dashed to their bank in Regina, Saskatchewan, and withdrew 3,000 Canadian dollars ($2,207 in U.S. currency), the daily maximum. They hurried to a second branch for more money. But a bank manager pulled them into his office: Another patron had gotten a similar call and learned the eerily accurate voice had been faked, Card recalled the banker saying. The man on the phone probably wasn’t their grandson.

That’s when they realized they’d been duped.

“We were sucked in,” Card said in an interview with The Washington Post. “We were convinced that we were talking to Brandon.”

Advertisement

As impersonation scams in the United States rise, Card’s ordeal is indicative of a troubling trend. Technology is making it easier and cheaper for bad actors to mimic voices, convincing people, often the elderly, that their loved ones are in distress. In 2022, impostor scams were the second most popular racket in America, with over 36,000 reports of people being swindled by those pretending to be friends and family, according to data from the Federal Trade Commission. Over 5,100 of those incidents happened over the phone, accounting for over $11 million in losses, FTC officials said.

Advancements in artificial intelligence have added a terrifying new layer, allowing bad actors to replicate a voice with an audio sample of just a few sentences. Powered by AI, a slew of cheap online tools can translate an audio file into a replica of a voice, allowing a swindler to make it “speak” whatever they type.

Experts say federal regulators, law enforcement and the courts are ill-equipped to rein in the burgeoning scam. Most victims have few leads to identify the perpetrator and it’s difficult for the police to trace calls and funds from scammers operating across the world. And there’s little legal precedent for courts to hold the companies that make the tools accountable for their use.

Advertisement

“It’s terrifying,” said Hany Farid, a professor of digital forensics at the University of California at Berkeley. “It’s sort of the perfect storm … [with] all the ingredients you need to create chaos.”

Although impostor scams come in many forms, they essentially work the same way: a scammer impersonates someone trustworthy — a child, lover or friend — and convinces the victim to send them money because they’re in distress.

But artificially generated voice technology is making the ruse more convincing. Victims report reacting with visceral horror when hearing loved ones in danger.

It’s a dark impact of the recent rise in generative artificial intelligence, which backs software that creates texts, images or sounds based on data it is fed. Advances in math and computing power have improved the training mechanisms for such software, spurring a fleet of companies to release chatbots, image-creators and voice-makers that are strangely lifelike.

Advertisement

AI voice-generating software analyzes what makes a person’s voice unique — including age, gender and accent — and searches a vast database of voices to find similar ones and predict patterns, Farid said.

It can then re-create the pitch, timbre and individual sounds of a person’s voice to create an overall effect that is similar, he added. It requires a short sample of audio, taken from places such as YouTube, podcasts, commercials, TikTok, Instagram or Facebook videos, Farid said.

“Two years ago, even a year ago, you needed a lot of audio to clone a person’s voice,” Farid said. “Now … if you have a Facebook page … or if you’ve recorded a TikTok and your voice is in there for 30 seconds, people can clone your voice.”

Companies such as ElevenLabs, an AI voice synthesizing start-up founded in 2022, transform a short vocal sample into a synthetically generated voice through a text-to-speech tool. ElevenLabs software can be free or cost between $5 and $330 per month to use, according to the site, with higher prices allowing users to generate more audio.

Advertisement

ElevenLabs burst into the news following criticism of its tool, which has been used to replicate voices of celebrities saying things they never did, such as Emma Watson falsely reciting passages from Adolf Hitler’s “Mein Kampf.” ElevenLabs did not return a request for comment, but in a Twitter thread the company said it’s incorporating safeguards to stem misuse, including banning free users from creating custom voices and launching a tool to detect AI-generated audio.

But such safeguards are too late for victims like Benjamin Perkin, whose elderly parents lost thousands of dollars to a voice scam.

Share this article Share

His voice-cloning nightmare started when his parents received a phone call from an alleged lawyer, saying their son had killed a U.S. diplomat in a car accident. Perkin was in jail and needed money for legal fees.

Advertisement

The lawyer put Perkin, 39, on the phone, who said he loved them, appreciated them and needed the money. A few hours later, the lawyer called Perkin’s parents again, saying their son needed $21,000 in Canadian dollars (U.S. $15,449) before a court date later that day.

Perkin’s parents later told him the call seemed unusual, but they couldn’t shake the feeling they’d really talked to their son.

The voice sounded “close enough for my parents to truly believe they did speak with me,” he said. In their state of panic, they rushed to several banks to get cash and sent the lawyer the money through a bitcoin terminal.

When the real Perkin called his parents that night for a casual check-in, they were confused.

It’s unclear where the scammers got his voice, although Perkin has posted YouTube videos talking about his snowmobiling hobby. The family has filed a police report with Canada’s federal authorities, Perkin said, but that hasn’t brought the cash back.

Advertisement

“The money’s gone,” he said. “There’s no insurance. There’s no getting it back. It’s gone.”

Will Maxson, an assistant director at the FTC’s division of marketing practices, said tracking down voice scammers can be “particularly difficult” because they could be using a phone based anywhere in the world, making it hard to even identify which agency has jurisdiction over a particular case.

Maxson urged constant vigilance. If a loved one tells you they need money, put that call on hold and try calling your family member separately, he said. If a suspicious call comes from a family member’s number, understand that, too, can be spoofed. Never pay people in gift cards, because those are hard to trace, he added, and be wary of any requests for cash.

Eva Velasquez, the chief executive of the Identity Theft Resource Center, said it’s difficult for law enforcement to track down voice-cloning thieves. Velasquez, who spent 21 years at the San Diego district attorney’s office investigating consumer fraud, said police departments might not have enough money and staff to fund a unit dedicated to tracking fraud.

Advertisement

Larger departments have to triage resources to cases that can be solved, she said. Victims of voice scams might not have much information to give police for investigations, making it tough for officials to dedicate much time or staff power, particularly for smaller losses.

“If you don’t have any information about it,” she said, “where do they start?”

Farid said the courts should hold AI companies liable if the products they make result in harms. Jurists, such as Supreme Court Justice Neil M. Gorsuch, said in February that legal protections that shield social networks from lawsuits might not apply to work created by AI.

For Card, the experience has made her more vigilant. Last year, she talked with her local newspaper, the Regina Leader-Post, to warn people about these scams. Because she didn’t lose any money, she didn’t report it to the police.

Above all, she said, she feels embarrassed.. You may very well get a call in the near future from a relative in dire need of help, asking you to send them money quickly. And you might be convinced it’s them because, well, you know their voice.

Artificial intelligence changes that. New generative A.I. tools can create all manner of output from simple text prompts, including essays written in a particular author’s style, images worthy of art prizes, and—with just a snippet of someone’s voice to work with—speech that sounds convincingly like a particular person.

In January, Microsoft researchers demonstrated a text-to-speech A.I. tool that, when given just a three-second audio sample, can closely simulate a person’s voice. They did not share the code for others to play around with; instead, they warned that the tool, called VALL-E, “may carry potential risks in misuse…such as spoofing voice identification or impersonating a specific speaker.”

But similar technology is already out in the wild—and scammers are taking advantage of it. If they can find 30 seconds of your voice somewhere online, there’s a good chance they can clone it—and make it say anything.

“Two years ago, even a year ago, you needed a lot of audio to clone a person’s voice. Now…if you have a Facebook page…or if you’ve recorded a TikTok and your voice is in there for 30 seconds, people can clone your voice,” Hany Farid, a digital forensics professor at the University of California at Berkeley, told the Washington Post.

‘The money’s gone’

The Post reported this weekend on the peril, describing how one Canadian family fell victim to scammers using A.I. voice cloning—and lost thousand of dollars. Elderly parents were told by a “lawyer” that their son had killed an American diplomat in a car accident, was in jail, and needed money for legal fees.

The supposed attorney then purportedly handed the phone over to the son, who told the parents he loved and appreciated them and needed the money. The cloned voice sounded “close enough for my parents to truly believe they did speak with me,” the son, Benjamin Perkin, told the Post.

The parents sent more than $15,000 through a Bitcoin terminal to—well, to scammers, not to their son, as they thought.

“The money’s gone,” Perkin told the paper. “There’s no insurance. There’s no getting it back. It’s gone.”

One company that offers a generative A.I. voice tool, ElevenLabs, tweeted on Jan. 30 that it was seeing “an increasing number of voice cloning misuse cases.” The next day, it announced the voice cloning capability would no longer be available to users of the free version of its tool VoiceLab.

Fortune reached out to the company for comment but did not receive an immediate reply.

“Almost all of the malicious content was generated by free, anonymous accounts,” it wrote. “Additional identity verification is necessary. For this reason, VoiceLab will only be available on paid tiers.” (Subscriptions start at $5 per month.)

Card verification won’t stop every bad actor, it acknowledged, but it will make users less anonymous and “force them to think twice.”. A couple in Canada reportedly lost $21,000 from a scammer claiming to be a lawyer and their son.

Benjamin Perkin told The Washington Post his parents thought the AI-generated voice was him.

The rise of AI is making it easier for scammers to make people think they're talking to loved ones.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

A couple in Canada were reportedly scammed out of $21,000 after they received a call from someone claiming to be a lawyer who said their son was in jail for killing a diplomat in a car accident.

Benjamin Perkin told The Washington Post the caller put an AI-generated voice that sounded like him on the phone with his parents to ask for money. The alleged lawyer called his parents again after the initial call, and told them Perkin needed $21,000 for legal fees before going to court.

Perkin told the Post the voice was "close enough for my parents to truly believe they did speak with me."

His parents collected the cash and sent the scammer money through Bitcoin, Perkin said, but they later admitted they thought the phone call sounded strange. They realized they had been scammed after Perkin called to check in later that evening.

Advertisement

Perkin did not immediately respond to a reachout from Insider to discuss what happened.

He told the Post his family filed a police report with Canadian authorities, but that, "The money's gone. There's no insurance. There's no getting it back."

The Post reported that while Perkin doesn't know how the scammers found his voice, he has posted videos about snowmobiling on YouTube.

Related stories

The rise of more powerful AI tools is coinciding with a rise in scams involving people impersonating other people. The most commonly reported scam last year was imposter scams, the Federal Trade Commission found. The FTC saw fraud reports from 2.4 million people in 2022, which was lower than in 2021. However, the amount of money lost was higher, with $8.8 billion reported lost.

Advertisement

Scams involving AI technology predate the emergence of ChatGPT and other AI bots going viral right now. In 2019, the managing director at a British energy company reportedly wired over $240,000 to an account in Hungary after he thought his boss asked him to do so in a phone call.

In January, ElevenLabs, a research lab exploring voice cloning and tools for synthetic speech, shared a Twitter thread addressing people who "use our tech for malicious purposes."

The startup tweeted that it was releasing a tool to let people verify if an audio sample was made using the company's technology, and that its VoiceLab would only be accessible with payment.

A day before writing the thread, ElevenLabs tweeted that it was aware of "an increasing number of voice cloning misuse cases," after releasing its Beta platform.

Advertisement

Motherboard found that members on the anonymous site, 4chan, were using ElevenLabs' technology to generate voices that sound like celebrities to say racist and inappropriate things.

The FTC has created a new Office of Technology to investigate the potential uses of AI that companies are promising, and to see if companies are mitigating the risks their products can cause.

"We're also concerned with the risk that deepfakes and other AI-based synthetic media, which are becoming easier to create and disseminate, will be used for fraud," FTC spokesperson Juliana Gruenwald previously told Insider.

Gruenwald also told Insider that the FTC "has already seen a staggering rise in fraud on social media."

Advertisement

"AI tools that generate authentic-seeming videos, photos, audio, and text could supercharge this trend, allowing fraudsters greater reach and speed," she said.. TL;DR AI voice-generating software is allowing scammers to mimic the voice of loved ones.

These impersonations have led to people being scammed out of $11 million over the phone in 2022.

The elderly make up a majority of those who are targeted.

AI has been a central topic in the tech world for a while now, as Microsoft continues to infuse its products with ChatGPT and Google attempts to keep up by pushing out its own AI products. While AI has the potential to do some genuinely impressive stuff — like generating images based on a single line of text — we’re starting to see more of the downside of the barely regulated technology. The latest example of this is AI voice generators being used to scam people out of their money.

AI voice generation software has been making a lot of headlines as of late, mostly for stealing the voices of voice actors. Initially, all that was required was a few sentences for the software to convincingly reproduce the sound and tone of the speaker. The technology has since evolved to the point where just a few seconds of dialogue is enough to accurately mimic someone.

In a new report from The Washington Post, thousands of victims are claiming that they’ve been duped by imposters pretending to be loved ones. Reportedly, imposter scams have become the second most popular type of fraud in America with over 36,000 cases submitted in 2022. Of those 36,000 cases, over 5,000 victims were conned out of their money through the phone, totaling $11 million in losses according to FTC officials.

One story that stood out involved an elderly couple who sent over $15,000 through a bitcoin terminal to a scammer after believing they had talked to their son. The AI voice had convinced the couple that their son was in legal trouble after killing a U.S. diplomat in a car accident.

Like with the victims in the story, these attacks appear to mostly target the elderly. This comes as no surprise as the elderly are among the most vulnerable when it comes to financial scams. Unfortunately, the courts have not yet made a decision on whether companies can be held liable for harm caused by AI voice generators or other forms of AI technology.

Comments. . "We were sucked in. We were convinced that we were talking to Brandon."

Bail Out

Ruthless scammers are always looking for the next big con, and they might've found it: using AI to imitate your loved ones over the phone.

When a 73-year-old Ruth Card heard what she thought was the voice of her grandson Brandon on the other end of the line saying he needed money for bail, she and her husband rushed to the bank.

"It was definitely this feeling of... fear," Card told The Washington Post. "That we've got to help him right now."

The couple withdrew the maximum of 3,000 Canadian dollars at one bank and went to another for more. Fortunately, a vigilant bank manager flagged them down and warned them that another customer had gotten a similar phone call that sounded like it was from a loved one — but it turned out the voice had been faked.

"We were sucked in," Card said. "We were convinced that we were talking to Brandon."

Legal Trouble

Not all were as lucky. The 39-year-old Benjamin Perkin told WaPo how his elderly parents were swindled out of thousands of dollars with the help of an AI impersonator.

Perkin's parents had received a phone call from a lawyer, who claimed that their son killed a US diplomat in a car crash and needed money for legal fees. The apparent lawyer then let Perkin speak on the phone — and the voice sounded just like him.

This convinced them. When the lawyer later called back asking for CAD $21,000, his parents went to the bank and sent the money through BitCoin.

"The money's gone," Perkin told the paper. "There's no insurance. There's no getting it back. It's gone."

Easy Pickings

Voice cloning scams have been a threat for several years now. But the growing ubiquity of powerful and easy-to-use AI means that the technology's potential to be abused easily outpaces an unwitting public's ability to keep up with the tricks of bad actors — not realizing they could be targeting them already.

"Two years ago, even a year ago, you needed a lot of audio to clone a person's voice," Hany Farid, a professor of forensics at UC Berkeley, told WaPo. "Now... if you have a Facebook page... or if you've recorded a TikTok and your voice is in there for 30 seconds, people can clone your voice."

Take ElevenLabs, whose AI voice synthesis service costs as little as $5 per month, and can produce results so convincing that a journalist used it to break into his own bank account. It's even spawned an entire genre of memes impersonating President Joe Biden. ElevenLabs' voice cloning has only been around since 2022. Imagine the damage it — and competitors looking to ride the coattails of its success — could do in just a few more years.

More on voice cloning: Voice Actors Enraged By Companies Stealing Their Voices With AI. Cha mẹ của Benjamin Perkin (Canada) nhận được cuộc gọi từ con trai, thực chất là AI giả giọng, nói mình đang bị bắt giam và cần gấp 15.000 USD.

Cơn ác mộng của gia đình Perkin, 39 tuổi, bắt đầu khi một người tự nhận là luật sư gọi cho cha mẹ anh, nói anh đã gây ra vụ tai nạn xe hơi, khiến một nhà ngoại giao Mỹ tử vong. Người này cho biết Perkin đang ở trong tù và cần tiền cho các chi phí pháp lý.

Để tăng độ tin cậy, người này chuyển máy cho Perkin, thực chất là kết nối với thiết bị AI giả giọng. Trong đó, "Perkin" nói rất cần tiền, chỉ còn biết tin tưởng vào cha mẹ. "Giọng nói đủ gần gũi khiến cha mẹ tôi tin đó là tôi", Perkin kể với Washington Post.

Vài giờ sau, "luật sư" hối thúc cha mẹ anh chuyển tiền, nên họ đã ra ngân hàng rút và gửi đi 15.449 USD thông qua một hệ thống chuyển đổi sang Bitcoin. Cha mẹ anh nói họ có cảm giác cuộc gọi "có gì đó bất thường", nhưng vẫn làm theo vì nghĩ đã nói chuyện với con trai. Tối hôm đó, khi Perkin gọi điện, tất cả mới vỡ lẽ.

Perkin nghi ngờ các video anh đăng trên YouTube là nguồn âm thanh cho kẻ lừa đảo huấn luyện AI. "Tiền đã mất. Không có bảo hiểm. Lấy lại là không thể", anh nói.

Bà Ruth Card. Ảnh: Postmedia

Tương tự, một buổi sáng, bà Ruth Card, 73 tuổi ở Regina (Canada), nhận được cuộc gọi từ người lạ. Người này nói cháu bà, anh Brandon, bị tạm giam, không có smartphone để liên lạc và cần một số tiền để được tại ngoại.

"Trong đầu tôi lúc đó chỉ lóe lên suy nghĩ rằng tôi phải giúp thằng bé ngay lập tức", bà nói với Washington Post.

Bà cùng chồng đến ngân hàng và rút 2.207 USD - mức tối đa bà có thể rút hàng ngày. Cả hai định qua ngân hàng thứ hai để lấy số tiền tương tự. May mắn cho họ là vị giám đốc ngân hàng đã gọi cả hai vào văn phòng và cho biết: một khách hàng khác cũng nhận được cuộc gọi giống hệt, cũng bị giả giọng người thân với mức "chính xác đến kỳ lạ". Cả hai gọi điện cho cháu trai và đúng là anh không bị bắt.

"Chúng tôi bị cuốn theo câu chuyện mà không tìm hiểu kỹ. Khi đó, tôi tin chắc đang nói chuyện với Brandon mà không nghi ngờ", bà Card cho hay.

Các vụ lừa đảo công nghệ tăng mạnh thời gian qua, nhưng câu chuyện của bà Card hay Perkin cho thấy xu hướng mới đáng lo ngại: kẻ gian đang lợi dụng AI bắt chước giọng nói cho mục đích lừa tiền. Công nghệ này ngày càng rẻ, dễ tiếp cận khiến số nạn nhân ngày càng tăng, chủ yếu nhắm đến người lớn tuổi.

Lừa đảo qua giọng nói tạo bởi AI ngày càng phổ biến. Minh họa: Washington Post

Theo dữ liệu từ Ủy ban Thương mại Liên bang Mỹ (FTC), trong năm 2022, mạo danh là hình thức lừa đảo phổ biến thứ hai ở Mỹ với hơn 36.000 báo cáo. Kẻ gian thường giả mạo bạn bè hoặc gia đình để khiến nạn nhân mắc lừa. Riêng lừa đảo qua điện thoại chiếm hơn 5.100 trường hợp, gây thiệt hại hơn 11 triệu USD.

Sự tiến bộ của AI thời gian qua thúc đẩy nhiều lĩnh vực phát triển, nhưng cũng là công cụ để kẻ xấu khai thác. Chỉ bằng một mẫu âm thanh trong một vài câu nói thu thập được, kẻ gian có thể dùng trí tuệ nhân tạo chuyển thành bản sao giọng nói của một người. Công cụ sau đó "nói" bất cứ thứ gì theo yêu cầu và trở thành phương tiện lừa đảo.

Giới chuyên gia đánh giá, công cụ AI giả giọng nói đang tràn lan, nhưng các cơ quan quản lý vẫn loay hoay kiểm soát. Trong khi đó, hầu hết nạn nhân đều khó xác định thủ phạm vì kẻ lừa đảo hoạt động trên khắp thế giới. Các công ty tạo ra AI cũng chưa phải chịu trách nhiệm về việc chúng bị kẻ khác lạm dụng.

"Thật đáng sợ. Mọi thứ tạo thành cơn bão, đưa nạn nhân lạc vào sự hỗn loạn", giáo sư Hany Farid tại Đại học California nhận xét. "Kẻ gian sẽ buộc nạn nhân phải phản ứng nhanh, khiến họ không đủ bình tĩnh để xử lý vấn đề, đặc biệt là khi nghe tin người thân gặp nguy hiểm".

Cũng theo ông, phần mềm AI ngày nay đủ thông minh để phân tích giọng nói của một người. "Chỉ cần một bản ghi âm từ Facebook, TikTok, giọng nói của bạn sẽ được sao chép chỉ trong 30 giây", giáo sư Farid nói.

ElevenLabs, công ty đứng sau VoiceLab - công cụ AI tái tạo giọng nói, cảnh báo ngày càng nhiều phần mềm giả giọng có mặt trên thị trường, dẫn đến tình trạng lạm dụng.

Trong khi đó, Will Maxson, hiện làm việc tại Ủy ban Thương mại liên bang Mỹ FTC, cho biết việc theo dõi kẻ lừa đảo giọng nói "đặc biệt khó khăn" vì chúng có thể sử dụng điện thoại và ở bất kỳ đâu. Theo ông, nếu nhận cuộc gọi từ người lạ hoặc từ người thân nhờ hỗ trợ, người dùng cần kiểm tra bằng cách gọi lại cho chính người đang gặp sự cố, cũng như gọi cho các thành viên khác trong gia đình để xác minh thông tin.

Bảo Lâm (theo Washington Post)