A seven-second self-driving Tesla video posted to Twitter this past weekend shows a likely illegal driving event in San Francisco that, unfortunately, happens all the time in ordinary situations. It’s the poster’s reaction that has left many online unsettled.

In the video, we watch from inside the car’s cabin as the robo-Tesla edges close to a pedestrian who enters an intersection. The software display clearly shows the software picking up on the presence of the completely unprotected and vulnerable human, but Tesla’s software, it appears, doesn’t trigger compliance with the relevant law that says cars have to yield to pedestrians.

The text of the tweet couldn’t be more jolly about the victimless (in this case) moving violation, calling it "One of the most bullish / exciting things I've seen on Tesla Full Self-Driving Beta 11.4.1," and describes not yielding as proceeding "like a human would," rather than “slamming on the brakes."

Whole Mars Catalog, which posted the video, is the online identity of a Tesla and SpaceX fan apparently named Omar Qazi. The Whole Mars Catalog Twitter account has hundreds of thousands of followers, and draws approving replies from Elon Musk.

For a rundown of this not-at-all obscure law, check out the third second of the video, in which a sign can be seen which says "STATE LAW YIELD TO [PICTURE OF PEDESTRIAN] WITHIN CROSSWALK," just to the right of the pedestrian in the crosswalk not being yielded to.

Elon Musk recently touted the latest update to Tesla Full Self-Driving beta, version 11.4.1, as a steep improvement. It’s been slowly rolling out to Tesla consumers this month, and It’s supposedly so good that it should be called version 12, except that 12 will, Musk claims, be the release of an "end-to-end AI" version of FSD, whatever that means.

The video has some Twitter users concerned:

The Whole Mars Catalog Twitter account is, so far, full-throated in its defense of FSD 11.4.1, claiming that "people who don't live in cities aren't getting this," and that prior FSD updates would have caused the Tesla in question to stop within the crosswalk in this scenario. "That is wrong, continuing forward is right," claims Whole Mars Catalog. Indeed, stopping within a crosswalk blocks it, and no, that isn’t safe for pedestrians. The issue, however, is that this Tesla quite clearly has ample time to stop well before the crosswalk.

That a California driver may well also ignore the law in a similar situation is irrefutable (the author of this article is a reluctant Los Angeles driver). However, whether or not we should program our robotic cars to also drive this way should, perhaps, be a matter for public debate, and, maybe, for the legal system to decide. Such a democratically-informed decision could perhaps be made before this new norm is rolled out as a software update, and exuberantly lapped up by fans of a billionaire who then get to experiment with it on public roads.. Recently, a Tesla fan account shared a video of a Tesla using so-called Full Self- Driving on a two-lane road. As the video plays, you can see the car approach a marked crosswalk. On the center screen, the car’s sensors detect a pedestrian crossing, but unlike the cars traveling in the opposite direction, the Tesla doesn’t stop and yield to the pedestrian. Instead, it continues driving like normal.


Advertisement

“One of the most bullish / exciting things I’ve seen on Tesla Full Self-Driving Beta 11.4.1. It detected the pedestrian, but rather than slamming on the brakes it just proceeded through like a human would knowing there was enough time to do so,” they wrote.

Advertisement

Sorry, but that’s not remotely exciting or amazing in any way. And while it may be what some humans do, it’s also against the law. A pedestrian crossing at a marked crosswalk has the right of way, and you’re supposed to yield to them. The fact that the Tesla didn’t stop is a major problem. Why doesn’t the software know to yield to pedestrians?

Advertisement

Is that just too complex of a problem to solve yet? Is it because Tesla thinks it knows better than the people who make the laws? Either way, it’s clear that FSD shouldn’t be allowed to operate in areas where pedestrians may be present.

The fact that Tesla fans look at that video and see amazing technology and not a major flaw with FSD is also terrifying even if it’s not exactly surprising. They seem to think that Teslas should be allowed to break the law if the car thinks it can do so safely. In this one instance, no one got hurt, but that’s not the point. The fact that you’re a special fancy person in a special fancy car doesn’t give you the right to ignore the rules of the road. And it’s not like FSD has a perfect track record for safety.

Advertisement

At a time when pedestrian deaths continue to rise, we need to be doing more to keep people safe, not cheering on irresponsible uses of half-baked technology.. A short video of a Tesla with Full Self-Driving activated is doing the rounds online, adding more controversy to the ADAS that has received more than its fair share of criticism ever since its first deployment in October 2020.

The footage shared by Whole Mars Catalog on Twitter shows a Tesla with the latest FSD Beta 11.4.1. update driving in sunny weather with perfect visibility in a suburban area in California.

As the Tesla approached a pedestrian crossing at 28 mph, we can see that a pedestrian had already started to cross the road from the left when the car was roughly 50 yards away. It's worth noting that the crossing was well marked with signs on both sides of the road, in addition to having the traditional lines painted on the asphalt.

So what did the Tesla do? It "saw" the pedestrian, then hesitated a bit – notice that the speed started to decrease at one point – but instead of applying the brakes it continued driving, passing the pedestrian by as they had almost reached the middle of the crossing.

YouTuber Whole Mars Catalog appeared to be thrilled with how FSD Beta proceeded, though.

"One of the most bullish / exciting things I've seen on Tesla Full Self-Driving Beta 11.4.1. It detected the pedestrian, but rather than slamming on the brakes it just proceeded through like a human would knowing there was enough time to do so," he tweeted.

As Jalopnik pointed out, that's not at all exciting, it's clearly dangerous behavior – and an obvious violation of traffic laws. A driver approaching a marked crosswalk is required to yield to pedestrians, and the fact the Tesla didn't even attempt to stop is a big problem – especially since the car's display shows the system detected the pedestrian.

In a series of replies to his original tweet, Whole Mars Catalog argued the Tesla "started slowing a little bit in case it decided it needed to brake but saw it was safe to proceed."

That's clearly not a get out of jail free card, and the fact no one was hurt in this instance does not mean it was the right thing to do. Also, the fact some drivers do that sometimes does not mean FSD Beta should imitate dangerous human behavior.

It's pretty clear that the video exposes a major flaw with the FSD Beta system, and some of the replies in the Twitter thread also reveal that some Tesla fans believe it's not that big of a deal to ignore the rules of the road. Nothing further from the truth.. . Tesla released a new version of its controversial "Full Self-Driving Beta" software last month. Among the updates in version 11.4 are new algorithms determining the car's behavior around pedestrians. But alarmingly, a video posted to Twitter over the weekend shows that although the Tesla system can see pedestrians crossing the road, a Tesla can choose not to stop or even slow down as it drives past.

The video was posted by the Whole Mars Catalog account, a high-profile pro-Tesla account with more than 300,000 followers. The tweet, which has been viewed 1.7 million times, featured a five-second video clip with the accompanying text:

One of the most bullish / exciting things I've seen on Tesla Full Self-Driving Beta 11.4.1. It detected the pedestrian, but rather than slamming on the brakes it just proceeded through like a human would knowing there was enough time to do so.

The person posting the video then clarified that it was filmed in San Francisco and that anyone not OK with this driving behavior must be unfamiliar with city life. (As someone who has lived in big cities all his life, I am definitely not OK with cars not stopping for pedestrians at a crosswalk.)

Most partially automated driving systems like General Motors' Super Cruise or Ford's BlueCruise are geofenced to a controlled operational domain—usually restricted-access divided-lane highways. Tesla has taken a different approach, though, and allows users to unleash its FSD beta software on surface streets.

Not everyone is as comfortable with Tesla drivers road-testing unfinished software around other road users. In February, the National Highway Traffic Safety Administration told Tesla to issue a recall for nearly 363,000 vehicles with the software installed.

Advertisement

The agency had four principal complaints, including that the "FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution."

The version 11.4 update in April was supposed to improve how the cars behaved, but there's now more evidence that the FSD Beta still leads to Teslas breaking traffic laws. Section 7 of California's Driver's Handbook, which deals with laws and rules of the road, says that pedestrians are considered vulnerable road users and that "pedestrians have the right-of-way in marked or unmarked crosswalks. If there is a limit line before the crosswalk, stop at the limit line and allow pedestrians to cross the street."

This is not the first time Tesla's software has been programmed to break traffic laws, either.

FSD is “make or break” for Tesla

Tesla CEO Elon Musk has repeatedly talked about the importance of FSD to his company, saying that it is "make or break" for Tesla and that it's the difference between Tesla being "worth a lot of money or worth basically zero."

FSD Beta has been implicated in a number of crashes and is the subject of several of the open federal investigations into Tesla's electric vehicles. The option now costs $15,000, and each time the automaker declares another feature "complete," it allows the company to recognize some of the deferred revenue it has been collecting as payments for the software.

Despite that bold stance in public, Tesla has been far more circumspect when dealing with authorities—in 2020, it told the California Department of Motor Vehicles that it did not expect FSD to become significantly more capable and that it would never pass beyond so-called SAE level 2, which requires an alert human in the driver's seat who remains liable for the car's actions.

Or, as author Ed Niedermeyer more concisely put it, "Full Self-Driving" is not, and never will be, actually self-driving."

Tesla is holding its annual shareholder meeting later today in Texas.