The families of two young girls who allegedly died as a result of a viral TikTok challenge have sued the social media platform, claiming its “dangerous” algorithms are to blame for their children’s deaths.

Parents of two girls who died in a 2021 “blackout challenge” on TikTok, which encouraged users to choke themselves until they passed out, filed a suit on Tuesday in the Los Angeles county superior court.

Represented by the Social Media Victims Law Center (SMVLC), a legal resource for parents of children harmed by social media addiction and abuse, they allege the platform’s “dangerous algorithm intentionally and repeatedly” pushed videos of the challenge into the children’s feeds, incentivizing them to participate in the challenge that ultimately took their lives.

“TikTok needs to be held accountable for pushing deadly content to these two young girls,” said Matthew P Bergman, founding attorney of SMVLC. “TikTok has invested billions of dollars to intentionally design products that push dangerous content that it knows is dangerous and can result in the deaths of its users.”

One victim, eight-year-old Lalani Erika Renee Walton of Temple, Texas, is described as “an extremely sweet and outgoing young girl” who “loved dressing up as a princess and playing with makeup”. She died on 15 July 2021 in what police determined was “a direct result of attempting TikTok’s ‘Blackout Challenge’”, according to the complaint.

Lalani Erika Renee Walton. Photograph: Courtesy of the family

Lalani had received a phone for her eighth birthday in April 2021 and “quickly became addicted to watching TikTok videos”, the complaint said. She often posted videos of herself singing and dancing, in hopes of becoming “TikTok famous”.

In July 2021, her family began noticing bruising on Lalani’s neck, which she explained away as an accident. Unbeknown to them, she had begun participating in the blackout challenge, which had first showed up on her feed weeks before.

On the day of her death, Lalani had spent hours on a family road trip watching videos, including posts of the challenge.

“She was also under the belief that if she posted a video of herself doing the Blackout Challenge, then she would become famous and so she decided to give it a try,” the complaint said. “Lalani was eight years old at the time and did not appreciate or understand the dangerous nature of what TikTok was encouraging her to do.”

The other victim named in the suit, nine-year-old Arriani Jaileen Arroyo of Milwaukee, Wisconsin, received a phone when she was seven years old and used TikTok multiple times a day, according to the complaint. She “gradually became obsessive” about posting dance videos on TikTok and became “addicted” to the app.

In January 2021, Arriani’s family discussed with her an incident of a young TikTok user dying as the result of a challenge, but the Arriani assured them she would never participate in dangerous videos.

Arriani Jaileen Arroyo. Photograph: Courtesy of the family

However, on 26 February 2021 she was found not breathing by her five-year-old brother. She was rushed to a local hospital but ultimately taken off life support.

“TikTok unquestionably knew that the deadly Blackout Challenge was spreading through their app and that their algorithm was specifically feeding the Blackout Challenge to children, including those who have died,” the complaint reads.

The lawsuit lists a number of complaints against TikTok, including that its algorithm promotes harmful content, allows underage users on the app, and it fails to warn users or their legal guardians of the app’s addictive nature.

TikTok did not immediately respond to request for comment.

The company has been criticized in the past for allowing dangerous challenges to spread. Doctors reported that the 2021 “milk crate challenge”, which encouraged users to stack and climb milk crates, led to dislocated shoulders, ACL tears and even spinal cord injuries. In 2020, a 15-year-old girl died after participating in the “Benadryl challenge”, in which users took a large amount of anti-histamines in an attempt to produce hallucinogenic effects. In 2020, two minors were charged with assault after participating in the “skull breaker” challenge, which caused one victim to have a seizure.

Attorneys for the SMVLC claimed the company knowingly allowed such content to proliferate on the platform because it increased engagement, user numbers and ultimately profit.

“TikTok prioritized greater corporate profits over the health and safety of its users and, specifically, over the health and safety of vulnerable children TikTok knew or should have known were actively using its social media product,” they said.

The Walton and Arroyo families are seeking an unspecified amount in damages and have requested a jury trial to take place in California.. The parents of two girls who said their children died as a result of a “blackout challenge” on TikTok are suing the company, claiming its algorithm intentionally served the children dangerous content that led to their deaths.

The girls were 8 and 9 when they died last year after viewing the challenge, which encouraged users to choke themselves until they passed out, according to the lawsuit, which was filed on Thursday in Superior Court in Los Angeles County.

The suit claims TikTok knew or should have known that its product was “addictive,” that it was directing children to harmful content and that it failed to take significant action to stop those videos or to warn children and parents about them.

The complaint cites in particular TikTok’s “For You” page, which the complaint says shows a stream of videos selected by an algorithm developed by TikTok that is based on a user’s demographic, “likes” and prior activity on the app. The suit seeks unspecified damages.. TikTok’s recommendation algorithm pushes self-harm and eating disorder content to teenagers within minutes of them expressing interest in the topics, research suggests.

The Center for Countering Digital Hate (CCDH) found that the video-sharing site will promote content including dangerously restrictive diets, pro-self-harm content and content romanticising suicide to users who show a preference for the material, even if they are registered as under-18s.

For its study the campaign group set up accounts in the US, UK, Canada and Australia, registered with ages of 13, the minimum age for joining the service. It created “standard” and “vulnerable” accounts, the latter containing the term “loseweight” in their usernames, which CCDH said reflected research showing that social media users who seek out eating disorder content often choose usernames containing related language.

The accounts “paused briefly” on videos about body image, eating disorders and mental health, and also liked them. This took place over a 30-minute initial period when the accounts launched, in an attempt to capture the effectiveness of TikTok’s algorithm that recommends content to users.

On “standard” accounts, content about suicide followed within nearly three minutes and eating disorder material was shown within eight minutes.

“The results are every parent’s nightmare,” said Imran Ahmed, CCDH’s chief executive. “Young people’s feeds are bombarded with harmful, harrowing content that can have a significant cumulative impact on their understanding of the world around them, and their physical and mental health.”

The group said the majority of mental health videos presented to its standard accounts via the For You feed – the main way TikTok users experience the app – consisted of users sharing their anxieties and insecurities.

Body image content was more harmful, the report said, with accounts registered for 13-year-olds being shown videos advertising weight loss drinks and “tummy-tuck” surgery. One animation that appeared in front of the standard accounts carried a piece of audio stating “I’ve been starving myself for you” and had more than 100,000 likes. The report said the accounts were shown self-harm or eating disorder videos every 206 seconds.

The researchers found that videos relating to body image, mental health and eating disorders were shown to “vulnerable” accounts three times more than to standard accounts. The vulnerable accounts received 12 times as many recommendations for self-harm and suicide-related videos as the standard accounts, the report said.

The recommended content was more extreme for the vulnerable accounts, including methods of self-harm and young people discussing plans to kill themselves. CCDH said a mental health or body image-related video was shown every 27 seconds, although the content was dominated by mental health videos, which CCDH defined as videos about anxieties, insecurities and mental health conditions, excluding eating disorders, self-harm and suicide.

The group said its research did not differentiate between content with a positive intent – such as content discovering recovery – or negative content.

A spokesperson for TikTok, which is owned by the Chinese firm ByteDance and has more than 1 billion users worldwide, said the CCDH study did not reflect the experience or viewing habits of real-life users of the app.

skip past newsletter promotion Sign up to TechScape Free weekly newsletter Alex Hern's weekly dive in to how technology is shaping our lives Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

“We regularly consult with health experts, remove violations of our policies and provide access to supportive resources for anyone in need,” they said. “We’re mindful that triggering content is unique to each individual and remain focused on fostering a safe and comfortable space for everyone, including people who choose to share their recovery journeys or educate others on these important topics.”

TikTok’s guidelines ban content that promotes behaviour that could lead to suicide and self-harm, as well as material that promotes unhealthy eating behaviours or habits.

The UK’s online safety bill proposes requiring social networks to take action against so-called “legal but harmful” content being shown to children.

A DCMS spokesperson said “We are putting a stop to unregulated social media causing harm to our children. Under the Online Safety Bill, tech platforms will need to prevent under-18s from being exposed to illegal content assisting suicide and protect them from other harmful or age-inappropriate material, including the promotion of self-harm and eating disorders, or face huge fines.”