Tesla worker killed in fiery crash may be first ‘Full Self-Driving’ fatality Evidence suggests the advanced driver-assistance system was engaged during a fatal crash that killed recruiter Hans von Ohain in 2022 Warning: This graphic requires JavaScript. Please enable JavaScript for the best experience. Scroll to continue The following footage obtained by The Washington Post shows Colorado authorities responding to a car crash on May 16, 2022, in Evergreen, Colo. This story is best experienced with sound. Enable audio

Share Comment on this story Comment Add to your saved stories Save

EVERGREEN, Colo.

Hans von Ohain and Erik Rossiter were on their way to play golf one afternoon in 2022 when von Ohain’s Tesla suddenly swerved off Upper Bear Creek Road. The car’s driver-assistance software, Full Self-Driving, was struggling to navigate the mountain curves, forcing von Ohain repeatedly to yank it back on course.

“The first time it happened, I was like, ‘Is that normal?’” recalled Rossiter, who described the five-mile drive on the outskirts of Denver as “uncomfortable.” “And he was like, ‘Yeah, that happens every now and then.’”

Hours later, on the way home, the Tesla Model 3 barreled into a tree and exploded in flames, killing von Ohain, a Tesla employee and devoted fan of CEO Elon Musk. Rossiter, who survived the crash, told emergency responders that von Ohain was using an “auto-drive feature on the Tesla” that “just ran straight off the road,” according to a 911 dispatch recording obtained by The Washington Post. In a recent interview, Rossiter said he believes that von Ohain was using Full Self-Driving, which — if true — would make his death the first known fatality involving Tesla’s most advanced driver-assistance technology.

Story continues below advertisement Advertisement Story continues below advertisement Advertisement

Tesla owners have long complained of occasionally erratic behavior by the cars’ software, including sudden braking, missed road markings and crashes with parked emergency vehicles. Since federal regulators began requiring automakers to report crashes involving driver-assistance systems in 2021, they have logged more than 900 in Teslas. A Post analysis found at least 40 crashes that resulted in serious or fatal injuries.

Press Enter to skip to end of carousel How The Post determined Full Self-Driving likely was engaged The crash appears in the National Highway Traffic Safety Administration’s database of crashes reported to involve driver assistance software. Tesla reported the crash to NHTSA after receiving a complaint from an unspecified source. A purchase order obtained by The Post shows the car was equipped with Full Self-Driving, and the driver’s widow said he used it frequently, especially on the road where the crash occurred. A passenger who survived the crash said the driver used Full Self-Driving earlier in the day, and that he believes the feature was engaged at the time of the crash. According to 911 dispatch audio obtained by The Post, the survivor told emergency responders that an “auto-drive” feature had been in use. A state police investigator told The Post that crash dynamics were consistent with the feature’s use. 1 / 4 End of carousel

Most involved Autopilot, which is designed for use on controlled-access highways. No fatal crash has been definitively linked to the more sophisticated Full Self-Driving, which is programmed to guide the car almost anywhere, from quiet suburban roads to busy city streets.

Two years ago, a Tesla shareholder tweeted that there “has not been one accident or injury” involving Full Self-Driving, to which Musk responded: “Correct.” But if that was accurate at the time, it no longer appears to be so. A Tesla driver who caused an eight-car pileup with multiple injuries on the San Francisco-Oakland Bay Bridge in 2022 told police he was using Full Self-Driving. And The Post has linked the technology to at least two serious crashes, including the one that killed von Ohain.

[The final 11 seconds of a fatal Tesla Autopilot crash]

Von Ohain and Rossiter had been drinking, and an autopsy found that von Ohain died with a blood alcohol level of 0.26 — more than three times the legal limit — a level of intoxication that would have hampered his ability to maintain control of the car, experts said. Still, an investigation by the Colorado State Patrol went beyond drunken driving, seeking to understand what role the Tesla software may have played in the crash.

“ Next thing I remember is waking up in the vehicle. ” — Erik Rossiter (Julia Wall and Whitney Shefte/TWP)

The question is critical as automakers race toward the promise of a driverless future. For private vehicles, that day is far from here. But critics say features like Full Self-Driving already are giving drivers a false sense of confidence about taking their eyes off the road — or getting behind the wheel after drinking — evincing the dangers of letting consumers test an evolving, experimental technology on the open road.

Tesla did not respond to multiple requests for comment. The company, which has released Full Self-Driving to about 400,000 customers, acknowledges that the software is in “beta” mode — meaning still in development, constantly learning and being modified. But Tesla argues that its public release is an essential step toward reducing America’s 40,000 annual road deaths. “The more automation technology offered to support the driver, the safer the driver and other road users,” Tesla tweeted in December.

Story continues below advertisement Advertisement Story continues below advertisement Advertisement

At the same time, Tesla user manuals cite a long list of conditions under which Full Self-Driving may not function properly, including narrow roads with oncoming cars and curvy roads. The company has long maintained that drivers must control their cars and that Tesla is not liable for distracted or drunken driving.

[Tesla drivers run Autopilot where it’s not intended — with deadly consequences]

Multiple lawsuits have begun challenging the view that drivers are solely responsible when Tesla’s software allegedly causes crashes or fails to prevent them. So far, Tesla has prevailed. Last fall, a California jury found Tesla not liable for a 2019 Autopilot crash in which survivors said the car suddenly veered off the road. At least nine more cases are expected to go to trial this year.

play Play now NaN min Follow on Podcast episode Spotify Apple Google Amazon

Von Ohain’s widow, Nora Bass, said she has been unable to find a lawyer willing to take his case to court because he was legally intoxicated. Nonetheless, she said, Tesla should take at least some responsibility for her husband’s death.

“Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human,” Bass said. “We were sold a false sense of security.”

Hans von Ohain’s Tesla was on fire after the crash. (Obtained by The Washington Post)

Von Ohain used Full Self-Driving nearly every time he got behind the wheel, Bass said, placing him among legions of Tesla boosters heeding Musk’s call to generate data and build the technology’s mastery. While Bass refused to use the feature herself — she said its unpredictability stressed her out — her husband was so confident in all it promised that he even used it with their baby in the car.

“It was jerky, but we were like, that comes with the territory of” new technology, Bass said. “We knew the technology had to learn, and we were willing to be part of that.”

“ Now it feels like we were just guinea pigs. ” — Nora Bass (Julia Wall and Whitney Shefte/TWP)

Von Ohain, a Marine veteran originally from Cincinnati, joined Tesla in late 2020 as a recruiter for engineers, attracted to the company’s mission of bringing electric and autonomous vehicles to the masses, Bass said. He also was inspired by the idea of working for Musk, she said — a “brilliant man” who built a company that promised to save lives and make the roadways safer.

Von Ohain “had this opportunity to be part of a company that is working on insanely advanced technology, and we had always thought Elon Musk was interesting,” Bass said. “Hans was so interested in brilliant minds.”

Story continues below advertisement Advertisement Story continues below advertisement Advertisement

At the time, Tesla had just introduced Full Self-Driving, and would eventually release it to a wider group of owners who had been monitored by the carmaker and declared safe drivers. Like many Tesla employees, von Ohain received the feature — then a $10,000 option — free with his employee discount, according to Bass and a purchase order reviewed by The Post.

Though still in its beta phase, the technology is “the difference between Tesla being worth a lot of money and being worth basically zero,” Musk has said, noting his customers’ — and investors’ — enthusiasm for a fully autonomous car. Many major automakers were developing advanced driver-assistance technology, but Tesla was more aggressive in pushing sophisticated features out to an eager public.

Von Ohain’s parents, Chris and Pam, at their home in Cincinnati. His father said he was disappointed in Tesla for not treating “their own employee who died in their vehicle with a little more empathy.”

For years, Musk had preached the benefits of pursuing autonomous driving. In 2019, he predicted that it would one day be so reliable that drivers “could go to sleep” — though, for now, Tesla’s user agreement requires the driver to stay engaged and ready to take over from Full Self-Driving at all times.

In 2022, Tesla recalled more than 50,000 vehicles amid concerns that Full Self-Driving caused the car to roll through stop signs without coming to a full halt. Even as Musk tweeted months later that Tesla had made Full Self-Driving Beta available to anyone in North America who bought it, complaints continued to pile up: Drivers reported that cars would stop short, blow through stop signs or suddenly veer off the road when lane markings were unclear.

“We test as much as possible in simulation and with [quality assurance] drivers, but reality is vastly more complex,” Musk tweeted last spring about a new version of the software. Tesla employees would get it first, he said, with wider release to come “as confidence grows.”

[Tesla conducts largest-ever recall for ‘insufficient’ safety controls after exclusive Post report on Autopilot]

“ I’m glad it wasn’t Nora and his daughter in that car. ” — Erik Rossiter (Julia Wall and Whitney Shefte/TWP)

On the day of the crash, von Ohain and Rossiter played 21 holes of golf, downing multiple drinks along the way. Though an autopsy would later show that von Ohain was legally drunk, Rossiter said he seemed composed and “by no means intoxicated” as they got in the Tesla and headed home.

Rossiter, who was found to have a similar blood alcohol level, can recall only shreds of the crash: A bright orange glow. Careening off the road. Jumping out of the car and trying to pull his friend out. The driver’s-side door blocked by a fallen tree.

As Rossiter yelled for help on the deserted mountain road, he remembers, his friend was screaming inside the burning car.

A photo of Hans von Ohain on Erik Rossiter’s wall.

Colorado State Patrol Sgt. Robert Madden, who oversaw the agency’s investigation, said it was one of “the most intense” vehicle fires he had ever seen. Fueled by thousands of lithium-ion battery cells in the car’s undercarriage, according to the investigation report, the fire is what killed von Ohain: His cause of death was listed as “smoke inhalation and thermal injuries.” Madden said he probably would have survived the impact alone.

At the scene of the crash, Madden said, he found “rolling tire marks,” meaning the motor continued to feed power to the wheels after impact. There were no skid marks, Madden said, meaning von Ohain appeared not to have hit the brakes.

Story continues below advertisement Advertisement Story continues below advertisement Advertisement

“Given the crash dynamics and how the vehicle drove off the road with no evidence of a sudden maneuver, that fits with the [driver-assistance] feature” being engaged, Madden said.

Colorado police were unable to access data from the car because of the intensity of the fire, according to the investigation report, and Tesla said it could not confirm that a driver-assistance system had been in use because it “did not receive data over-the-air for this incident.” Madden said the remote location may have hindered communications.

(Julia Wall and Whitney Shefte/TWP)

However, Tesla did report the crash to the National Highway Traffic Safety Administration. According to NHTSA, Tesla received notification of the crash through an unspecified “complaint” and alerted federal authorities that a driver-assistance feature had been in use at least 30 seconds before impact. Because of the extensive fire damage, NHTSA could not confirm whether it was Full Self-Driving or Autopilot.

In December, Tesla acknowledged problems with driver inattention, issuing a recall for nearly all of its 2 million U.S. cars to add more-frequent alerts. Bass said that von Ohain knew he needed to pay attention but that his focus naturally flagged with Full Self-Driving.

“You’re told that this car should be smarter than you, so when it’s in Full Self-Driving, you relax,” she said. “Your reaction time is going to be less than if we were not in Full Self-Driving.”

Erik Rossiter, the passenger who survived the Tesla crash, on the road where it occurred. Rossiter told emergency responders at the scene that von Ohain was using an “auto drive feature on the Tesla.”

Charred remnants of the Tesla after the crash. Von Ohain's wife, Nora Bass, said her husband believed in the promise of Full Self-Driving and used it often, despite its sometimes startling behavior. (Obtained by The Washington Post)

Alcohol also dramatically slows reaction time, and von Ohain’s intoxication probably factored heavily in the crash, said Ed Walters, who teaches autonomous vehicle law at Georgetown University. If the technology was acting up on the way to the golf course, as Rossiter claims, von Ohain should have known that he needed to remain fully alert on the drive home, Walters said.

“This driver, when sober, was able to pull the car back on the road and was able to correct for any problems in the Tesla safely,” Walters said. “People need to understand that whatever kind of car they’re driving, whatever kind of software, they need to be paying attention. They need to be sober and they need to be careful.”

(Julia Wall and Whitney Shefte/TWP)

Still, Andrew Maynard, a professor of advanced technology transitions at Arizona State University, said reports of the car’s frequent swerving raise questions about Tesla’s decision to release Full Self-Driving.

“The FSD technology isn’t quite ready for prime time yet,” Maynard said, adding that the value of testing the technology on the open road should be weighed against the risks of pushing it out too quickly to drivers who overestimate its capabilities.

[Why Tesla Autopilot shouldn’t be used in as many places as you think]

“ I have so many questions. ” — Nora Bass (Julia Wall and Whitney Shefte/TWp)

Nearly two years later, mangled car parts and charred battery cells are still strewn along Upper Bear Creek Road. Madden closed the Colorado State Patrol investigation, unable to determine whether Full Self-Driving played a role.

In a recent interview, Madden said he worries about the proliferation of sophisticated driver-assistance systems. “Autonomous vehicles are something of the future, and they are going to be here,” he said. “So the more we know, the more we understand, the safer we can continue into the future with this” technology.

Meanwhile, Tesla has yet to publicly acknowledge the death of an employee driving one of its cars.

To its workforce, the company has said little about what happened to von Ohain, making few efforts to console those who knew him, according to a former employee who spoke on the condition of anonymity for fear of retribution. Von Ohain’s replacement was hired within a few weeks, the person said.

(Julia Wall and Whitney Shefte/TWP)

“Once Hans passed away and time went by, there wasn’t any more discussion about him,” said the former employee, a member of von Ohain’s team who soon resigned.

To von Ohain’s widow, Tesla’s silence seemed almost cruel.

Though the company eventually helped cover the cost of her move back home to Ohio, Bass said, Tesla’s first communication with the family after the crash was a termination notice she found in her husband’s email.

About this story Additional development by Jake Crump. Editing by Lori Montgomery and Karly Domb Sadof. Video production by Jessica Koscielniak. Design editing by Betty Chavarria. Photo editing by Monique Woo. Video graphics by Sarah Hashemi. Copy editing by Anne Kenderdine and Martha Murdock.. A Tesla employee — and "devoted" fan of its CEO Elon Musk — named Hans von Ohain was killed after his Model 3 crashed into a tree and erupted in flames back in 2022.

His friend and fellow passenger Erik Rossiter, who survived the collision, has since told The Washington Post that von Ohain had the car's Full Self-Driving feature turned on at the time of the fatal accident.

If confirmed, the crash could be the first known death involving the feature, an optional $15,000 add-on that has already drawn plenty of attention from regulators.

WaPo also confirmed the vehicle was equipped with the feature — von Ohain received it for free with his employee discount — and his widow also said he frequently made use of it.

Despite the company's misleading marketing, Tesla vehicles are still far from being able to drive themselves, and cautions on its website that drivers must be "ready to take immediate action including braking."

Full Self-Driving (FSD) expands on the company's Autopilot driver assistance software, and is designed to make decisions on behalf of the user while driving on both highways and busy city streets.

While the National Highway Traffic Safety Administration is already investigating Autopilot following a series of accidents in which Teslas have smashed into emergency response vehicles that were pulled over with sirens or flares, no fatal crash has been definitively linked to FSD — though von Ohain's death, as WaPo reports, sure looks suspicious.

According to a Washington Post analysis last year, the number of fatal crashes involving Autopilot mode has surged. Out of over 700 crashes involving the feature since 2019, at least 17 were fatal.

An autopsy of von Ohain's body found that he died with a blood alcohol level of 0.26, which is over three times the legal limit.

Nonetheless, even if alcohol wasn't involved, experts have pointed out that Tesla's misleading marketing may be giving drivers a false sense of security behind the wheel.

In spite of Tesla's outsize claims about the tech, Tesla holds that its FSD feature is still in "beta," meaning that it's still actively being developed.

Its decisionmaking on the road can be highly suspect. In a video uploaded just last week, a Tesla owner had to override the feature after it attempted to make an abrupt left turn, steering straight into oncoming traffic.

We've also come across videos of Teslas with the feature turned on ignoring red lights, smashing into a police car, and struggling in snowy conditions.

Von Ohain's death raises important ethical questions, especially when it comes to culpability. Is Tesla's misleading marketing to blame, or was it a case of an driver's reckless actions?

Earlier this week, the lawyer of a Tesla driver who at first denied having killed a woman with his Tesla in a hit-and-run, is now saying that he couldn't remember if he did — and if he did, he must've been "using Tesla's full self-driving capability."

"Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human," von Ohain's widow Nora Bass told WaPo. "We were sold a false sense of security."

Bass told the newspaper that personally, she found the feature too unpredictable and "jerky."

A lot is riding on the feature, with Musk arguing in 2022 that FSD is "the difference between Tesla being worth a lot of money and being worth basically zero."

Even with its full weight behind developing the feature, the software is still a long way from realizing its stated goal of full autonomy.

Musk has repeatedly promised that Tesla is going to achieve Level 5 autonomy in a matter of less than a year, a point at which a car is fully autonomous and doesn't need a steering wheel or brake pedal.

However, today the feature still hasn't surpassed Level 2 autonomy, requiring the driver to take over at any time.

According to WaPo, Tesla has yet to publicly acknowledge von Ohain's death.

More on FSD: Tesla Driver Says He's Not Sure If He Killed a Pedestrian Because He Was on Autopilot. The horrible fatal crash of a Tesla employee using Full Self-Driving Beta has been reported in detail for the first time to highlight responsibility in those accidents.

The Crash

The Washington Post released a new report on the crash today, which happened back in 2022.

Hans von Ohain, a recruiter at Tesla, and his friend Erik Rossiter set out outside Denver, Colorado, in the former’s Tesla Model 3 to go golfing.

During the drive there, Rossiter says that von Ohain was driving on FSD beta, Tesla’s driver-assist system that takes over all the driving controls but the driver needs to keep their hands on the steering wheel and be ready to take control at all times.

Rossiter said that FSD Beta swerved several times during the drive there and von Ohain had to take control.

They played 21 holes and drank alcohol during the day before driving back. Rossiter said he seemed composed and “by no means intoxicated” when getting into the car for the drive back.

The Washington Post described the crash:

Hours later, on the way home, the Tesla Model 3 barreled into a tree and exploded in flames, killing von Ohain, a Tesla employee and devoted fan of CEO Elon Musk. Rossiter, who survived the crash, told emergency responders that von Ohain was using an “auto-drive feature on the Tesla” that “just ran straight off the road,” according to a 911 dispatch recording obtained by The Washington Post. In a recent interview, Rossiter said he believes that von Ohain was using Full Self-Driving, which — if true — would make his death the first known fatality involving Tesla’s most advanced driver-assistance technology.

While Rossiter admittedly doesn’t have a great recollection of what happened, he did say he remembers getting out of the car, a big orange glow, and then trying to get his friend out of the car as he was screaming inside of the burning car. A fallen tree was blocking the driver’s door.

An autopsy of Von Ohain found that he died with a blood alcohol level of 0.26 — more than three times the legal limit.

Colorado State Police determined that intoxication was the main factor behind the accident, but it also conducted an investigation into the possible role of Tesla’s Full Self-Driving Beta.

The Responsibility

Von Ohain’s widow Nora Bass wants Tesla to take responsibility for her husband’s death:

“Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security.”

She hasn’t been able to find a lawyer to take the case because he was intoxicated.

Colorado State Patrol Sgt. Robert Madden, who led the investigation, has rolling tire marks at the site of the crash, which means that the motor kept sending power to the wheels at the time of impact.

There were also no skid marks found.

Madden said:

“Given the crash dynamics and how the vehicle drove off the road with no evidence of a sudden maneuver, that fits with the [driver-assistance] feature”

We don’t have access to the logs. The police were not able to recover it after the fire, and Tesla reportedly told the police that it didn’t receive the logs over the air. Therefore, it couldn’t confirm if any driver-assist features were activated at the time of the crash.

Electrek’s Take

That’s horrible. I can’t imagine trying to drag your screaming friend out of a burning car. I am sorry for Von Ohain’s loved ones.

Based on the information we have here, it does seem like Von Ohain was intoxicated and overconfident in FSD Beta. The feature failed badly, and he couldn’t take control in time to avoid the fatal crash.

They are both at fault. Von Ohain, rest in peace, had no excuse for getting behind the wheel intoxicated, and it sounds like Tesla’s FSD Beta failed badly.

But if we dig a little bit deeper, it is an interesting situation.

To be honest, the fact that he was a Tesla employee makes this whole situation a lot more complicated. It means that he should have known very well that you need to pay attention on FSD Beta and be ready to take control at all times.

Now, it might be because of his intoxication that he decided that it would be a good idea to use FSD Beta on winding mountain roads while intoxicated, or he might have been taking chances with FSD Beta even when not intoxicated, which is what his wife is pointing to about a “false sense of security.”

This is definitely something where Tesla can improve: managing expectations when it comes to FSD Beta, which is not easy to do when you literally call it “Full Self-Driving.”. A Tesla employee died in what may've been the first death tied to Tesla's Full Self-Driving tech.

Hans von Ohain died in 2022 after his Tesla veered off a mountain road and into a tree.

Von Ohain's wife told The Washington Post she felt her family were "guinea pigs" of the tech.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

Hans von Ohain, a Tesla recruiter, died in a car crash on a Colorado mountain road after his Tesla Model 3 veered off the road and barreled into a tree, bursting into flames, according to a new report from The Washington Post.

Erik Rossiter, a friend of Ohain's who was in the vehicle at the time of the 2022 crash and survived the incident, told first responders Ohain had been using the "auto-drive feature on the Tesla" and the vehicle "just ran straight off the road," the outlet reported.

If what Rossiter said is true, the incident — which Tesla has so far refused to acknowledge publicly — would be the first known fatality linked to the car company's Full Self-Driving technology.

While Ohain had been intoxicated at the time of the crash, with a blood-alcohol level more than three times the legal limit, investigators found the incident was not a typical drunken-driving crash. Sgt. Robert Madden of Colorado State Patrol told the Post there were no skid marks, which would have indicated the vehicle attempted to brake before impact, but there were "rolling tire marks," meaning power was still being deployed to the wheels after the crash.

Advertisement

Madden said that "given the crash dynamics and how the vehicle drove off the road with no evidence of a sudden maneuver, that fits with the [driver-assistance] feature" being engaged.

Madden also described the subsequent fire, which engulfed the car, as one of the "most intense" vehicle fires he had encountered, due largely to the lithium-ion battery cells housed in the undercarriage of the Tesla that Ohain was driving.

Related stories

Ohain's cause of death was determined to be smoke inhalation and thermal injuries, per the Post, and Madden said he probably would have survived the crash had it not been for the intensity of the flames.

Nora Bass, Ohain's widow, told the Post her husband believed in Elon Musk's vision for the future of autonomous vehicles so much that he was willing to tolerate the "jerky" experience to help improve Tesla's self-driving technology.

Advertisement

But since Tesla has so far been silent about Ohain's death, she told the outlet she felt she and her husband were "just guinea pigs" ironing out the kinks in the tech with a false promise of safety.

"Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human," Bass told the Post. "We were sold a false sense of security."

Bass and representatives for Tesla didn't immediately respond to a request for comment from Business Insider.

Tesla's disclaimer for its current self-driving technology reads: "Autopilot, Enhanced Autopilot, and Full Self-Driving Capability are intended for use with a fully attentive driver, who has their hands on the wheel and is prepared to take over at any moment. While these features are designed to become more capable over time, the currently enabled features do not make the vehicle autonomous."

Advertisement

Tesla's Full Self-Driving capability is still in beta testing.

There have been numerous crashes, some fatal, linked to Tesla's Autopilot technology, which is designed for use on highways, though Tesla has not been found responsible for the crashes.

But the Full Self-Driving technology — meant to maneuver Tesla vehicles through nearly any scenario — has not been definitively tied to any deaths. The Post reported that a Tesla driver who was behind an eight-car pileup causing multiple injuries on the San Francisco-Oakland Bay Bridge in November 2022 said he was using Full Self-Driving and that Ohain's death could be the first known fatality associated with the technology.. A Tesla employee and Elon Musk superfan may have become the first person to die in a crash caused by the company's self-driving tech, an investigation has claimed.

Hans von Ohain, 33, was killed in a fiery wreck in Colorado in 2022 after his Tesla swerved violently off a winding country road.

Although the young father had been drinking before the fatal crash, detectives expanded their investigation into the death after it became apparent it was no ordinary case of drunk driving.

'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human,' von Ohain's widow Nora Bass told the Washington Post.

'We were sold a false sense of security.'

Hans von Ohain (left), 33, was killed in a fiery wreck in Colorado in 2022 after his Tesla swerved violently off a winding country road

Von Ohain's Tesla Model 3 plowed into a tree as he and a friend returned from a round of golf, killing the young father in a fireball while his passenger barely escaped with his life

The vehicle had allegedly swerved erratically earlier in the day, which von Ohain allegedly said 'happens every now and then' on his electric car

Tesla has promoted its self-driving technology, Full Self-Driving, as the solution to the 40,000 annual road deaths in the US each year.

But the company insists it is still in its testing phase, and is constantly learning how to navigate new road conditions - including the twisting country roads that von Ohain lost his life on.

Drivers are warned they must keep their hands on the car's steering wheel at all times.

The 33-year-old, a former Marine who was said to be a passionate fan of his boss Elon Musk, was returning from an afternoon round of golf with a friend when the crash occurred on May 16, 2022.

His passenger Erik Rossiter, who barely escaped the wreck with his life, said that hours earlier the car had already driven erratically by itself, causing the recruiter to grab the wheel several times.

'The first time it happened, I was like, 'Is that normal?'' he recalled asking von Ohain, describing the trip as 'uncomfortable.'

'And he was like, 'Yeah, that happens every now and then.'

Von Ohain's Tesla Model 3 plowed into a tree just hours later, killing the young father in a fireball captured in horror police surveillance footage.

In a 911 dispatch call heard by the Washington Post, Rossiter told 911 that von Ohain was 'using an auto-drive feature on the Tesla' that 'just ran straight off the road.'

Rossiter said he recalled only fragments of the tragedy, including the chilling memory of his friend screaming inside his burning car.

Notably, the Full Self-Driving system is different to regular auto-drive, and is intended to navigate fully from point A to B - while auto-drive is only intended for situations such as on a highway.

Rossiter told investigators that he believes that they were using Full Self-Driving, which would make von Ohain's death the first ever caused by the experimental tech.

The 33-year-old, a former Marine and beloved fan of his boss Elon Musk, may have become the first person to ever die while using Tesla's Full Self-Driving tech

Von Ohain was described as a 'devoted father to his little Ray' (pictured together), and an 'amazing husband, dedicated Marine and best friend to many'

While an autopsy found von Ohain had been drinking, his widow Nora Bass (left) questioned whether Tesla's automated navigation was to blame. 'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human,' she said. 'We were sold a false sense of security'

Automakers have been mandated to report crashes involving driver-assistance systems since 2021, and in that time over 900 crashes involving Tesla's have been recorded.

This included at least 40 that resulted in serious or fatal injuries, according to Washington Post analysis.

While most involved auto-pilot, von Ohain's may have been a rare case of a Full-Self Driving crash, with another driver reportedly blaming the tech on an eight-car pileup on the San Francisco-Oakland Bay Bridge in 2022.

And in the face of Musk's promotion of Teslas as the future of the car industry, its new tech has faced challenges - including recalling over 363,000 vehicles following fears the Full Self-Driving system caused cars to roll through stop signs.

After von Ohain's horror crash, an autopsy found he had a blood alcohol level of 0.26 - over three times the legal limit.

However, while investigators may have concluded that his intoxication impaired his ability to control the car, Colorado State Patrol officers also looked into the vehicle's navigation systems.

Tesla has rolled out Full Self-Driving to around 400,000 vehicles, and tweeted in December that it will only help keep motorists safer on the roads, because 'the more automation technology offered to support the driver, the safer the driver and other road users.'

The company has publicly insisted the bugs are not entirely worked out in the system, as it is in its 'beta' phase and is constantly learning.

It also maintains that drivers must remain in control of their car even when Full Self-Driving is turned on, and it is not liable for distracted or drunk driving accidents.

However, von Ohain's widow Nora Bass said that he would use the system almost every time he got behind the wheel, and was happy to be contributing to its huge data collection on the roads.

Bass said she never used it as she didn't trust the tech, but her husband was so confident he would use it even when driving their young son Ray around.

'It was jerky, but we were like, that comes with the territory of new technology,' Bass said. 'We knew the technology had to learn, and we were willing to be part of that.'

The Tesla employee's widow Nora Bass (right) said the company should bear some blame, and said using the experimental Full Self-Driving tech 'feels like we were just guinea pigs'

Von Ohain is survived by his young son (pictured together) and wife

Although a slew of lawsuits have been levied at the car company, including nine reportedly set for trial over the coming year, von Ohain's widow said she has struggled to find any lawyers willing to take her case.

While admitting that her husband was drunk at the time, Bass believes Tesla should bear some blame for the accident.

'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human,' his Bass said. 'We were sold a false sense of security.

'Now, it feels like we were just guinea pigs.'

She added that in spite of warnings from Tesla to remain in control of the vehicle, the natural tendency in an autopilot car would be to switch off, even slightly.

'You’re told that this car should be smarter than you, so when it’s in Full Self-Driving, you relax,' she said. 'Your reaction time is going to be less than if we were not in Full Self-Driving.'

Bass said her husband was overjoyed to become an employee of Musk, who he saw a as a 'brilliant man' advancing society, particularly through forwarding electric car expansion.

He 'had this opportunity to be part of a company that is working on insanely advanced technology, and we had always thought Elon Musk was interesting,' Bass said. 'Hans was so interested in brilliant minds.'

He reportedly received the Full-Self-Driving system early as a perk from his job as an engineer recruiter, enjoying it for free while customers forked out $10,000 for the feature.

But now investigators appear to believe the technology he was so excited to be a part of may have caused his demise.

Colorado State Patrol Sgt. Robert Madden, who oversaw the agency’s investigation, said the lithium battery cells may have contributed to the fireball, describing it as one of the 'most intense' vehicle fires he had seen.

Madden concluded that von Ohain likely would have survived the crash, but died as he burned up in the car. His autopsy listed 'smoke inhalation and thermal injuries' as the cause of death.

The chief investigator concluded that von Ohain likely would have survived the crash, but died as he burned up in the car. His autopsy listed 'smoke inhalation and thermal injuries' as the cause of death

The investigator determined that Full Self-Driving was potentially the cause of the crash, largely due to 'rolling tire marks' at the crash site that indicate the engine continued to power the wheels after striking the tree.

Because there were no skid marks leading to the tree, Madden said von Ohain appeared not to have hit the brakes.

'Given the crash dynamics and how the vehicle drove off the road with no evidence of a sudden maneuver, that fits with the [driver-assistance] feature' being engaged, Madden said.

However, decisively proving Full Self-Driving was the cause has proven difficult, not least because data from the car was lost in the fireball.

Tesla also reportedly said it could not determine if the driving assistance tech was in use because it 'did not receive data over-the-air for this incident' - a factor that Madden said may be due to the remote country location of the wreck.

The investigator has since closed the case into von Ohain's death, unable to conclusively find whether Full Self-Driving played a role in the crash.. EVERGREEN, Colo. — After more than eight months of investigating, the Colorado State Patrol has finished its investigation into a May 2022 Tesla crash and subsequent fire in Evergreen that killed one person.

Hans Von Ohain, 33, died at the scene after his 2021 Tesla Model 3 went off the road, slammed into a tree and caught fire. A passenger made it out of the car, but Von Ohain was unable to exit the Tesla.

“The death of Mr. Von Ohain was tragic,” Colorado State Patrol Sgt. Rob Madden said.

What’s still unknown is why Von Ohain could not get out of the car and if the car was using an auto-drive function at the time of the crash.

The CSP investigator writes in the report, “I am unable to conclusively determine why (Von Ohain) did not exit the vehicle.”

As for the auto-drive function, Madden said due the remote location of the crash and the fire, investigators were unable to obtain any electronic data from the car. They did, however, determine that Von Ohain had a blood alcohol content (BAC) of 0.264, which is more than three times the legal limit.

“The driver of the vehicle was under the influence of alcohol, and his driving actions caused his death,” Madden said. “That is difficult to say because he has a grieving family.”

Madden stressed that everyone who owns and drives an electric vehicle should read the manual and know how to get out of the car if the electronics fail.

“This is a lesson we can all learn from,” he said.

Denver7 Investigates previously reported on this crash last year and discovered multiple examples of fires after electric cars were involved in crashes and drivers and passengers having trouble exiting the car.

“I think your story is a great example of what is needed to change the public intelligence on the new vehicles that are being sold in the United States,” Madden said. “These vehicles are very intelligent. That requires additional thought on how to use the car.”

The National Highway Traffic Safety Administration (NHTSA) is investigating Tesla’s automated driving system. Earlier this month, after pressure from NHTSA, Tesla announced a recall of more than 350,000 of its vehicles with “full driving” or “auto drive system.” The recall is a software update because NHTSA says the current software allows vehicles to exceed the speed limit or travel through intersections in an unlawful or unpredictable manner.

While it’s not proven that the auto-drive function played a role in Von Ohain’s crash, his vehicle would have been part of that recall.. A deadly single-car crash in Evergreen involving a Tesla is now part of a federal investigation that is looking into the safety of electric vehicles.

Denver7 Investigates confirmed that the National Highway Traffic Safety Administration (NHTSA) has collected data and information from the crash as part of its investigation.

The crash happened in the late evening of May 16 on Upper Bear Creek Road when the car went off the road and slammed into a tree. The car then caught fire.

A passenger made it out of the vehicle, but the driver, 33-year-old Hans von Ohain, died at the scene.

A report from Clear Creek County, who assisted on the call, notes that the passenger said the car was in its “auto drive” function. Radio communication from dashcam footage of the accident obtained by Denver7 Investigates backs up that report.

Questions still remain regarding if and how the auto drive function played a role in the crash and if the lithium ion batteries in the Tesla contributed to the intensity of the fire. Colorado State Patrol, who is leading the investigation, has yet to finalize its report from the crash but is working to answer those questions.

Colorado State Patrol Sgt. Rob Madden said the investigation will attempt to determine why the driver could not open his door, what his condition was after impact and how the electronics operated after the fire started.

“Those are all questions we may not have answers to,” he said.

Paul Shoemaker, a Castle Rock firefighter and paramedic, also owns Next Level Extrication, a company that trains emergency workers on how to save lives in car crashes.

Shoemaker provided training to the Evergreen Fire Protection District after this crash and said he believes the driver was awake and alert but was unable to open the door.

Shoemaker also studies post-crash fires in electric vehicles and other examples where drivers could not get out of the vehicle. He cited an accident in British Columbia where a driver kicked the window out of the driver-side door in a Tesla that had caught fire in the road.

“His reaction was to kick the window out,” he said. “He never opened the door.”

Shoemaker added that the driver probably would not have survived, otherwise.

While every vehicle is different, Shoemaker said all electric cars have a manual release to get out of the car when the electric systems fail. However, he estimates less than 1% of people know how to get out of the car in an emergency situation.

The same car can also have different ways to get out if a person is in the front seat or the back seat. Shoemaker suggests electric car vehicle owners look through their manuals to see how they can get out of the car should the electronics fail.

NHTSA investigation

Earlier this year, published reports announced NHTSA was expanding its investigation into the auto drive function on several Tesla models, including the 2021 Model 3 involved in the Evergreen crash.

“They have all the information from this crash, and they are looking at this at their level,” Madden said.

He added that he does not know if the Tesla involved in the crash in Evergreen was being driven by the auto drive or auto pilot function. CSP is also investigating whether alcohol played a role in the crash.

The hope is that a complete report will be ready by the end of the year.

“This report is extremely complex, and that is what we owe to the family and to friends and to the public to give them answers,” Madden said.

NHTSA said it does comment on ongoing investigations. Tesla has not responded to multiple requests for comment.. Tesla’s Full Self-Driving software can’t drive your car by itself. Despite its name, the advanced driver assist program requires drivers keep their eyes on the road at all times and be prepared to take the wheel at a moment’s notice. Now, the widow of a driver killed in a crash that involved FSD has accused the automaker of selling a “false sense of security” with the software.



Tesla Had A Very Interesting Week CC Share Subtitles Off

English view video Tesla Had A Very Interesting Week

In 2022, Tesla employee Hans von Ohain was driving his Model 3 electric vehicle alongside Erik Rossiter. The pair had been out playing golf one afternoon and had a few drinks before heading home. On the drive back, von Ohain reportedly let his Tesla take control of the ride by initiating its FSD software, reports the Washington Post.

Advertisement

However, the drive ended in disaster when the Model 3 careered off the road and burst into flames, killing von Ohain and injuring Rossiter. As the Post explains:

“The Tesla Model 3 barreled into a tree and exploded in flames, killing von Ohain, a Tesla employee and devoted fan of CEO Elon Musk. Rossiter, who survived the crash, told emergency responders that von Ohain was using an “auto-drive feature on the Tesla” that “just ran straight off the road,” according to a 911 dispatch recording obtained by The Washington Post. In a recent interview, Rossiter said he believes that von Ohain was using Full Self-Driving, which — if true — would make his death the first known fatality involving Tesla’s most advanced driver-assistance technology.”

Advertisement

While no crash has so far been definitively linked to the FSD program, the Post identified numerous other collisions in which drivers claimed the software was initiated. This included a 2022 crash that caused a massive pileup in San Francisco and at least two serious crashes, including the crash that killed von Ohain.

Advertisement

Von Ohain’s crash is complicated, though, as a postmortem following his death revealed that he was three times over the legal blood alcohol limit to drive. Still, police investigating the crash have sought to uncover the role FSD played in von Ohain’s death. The Post reports:

Von Ohain’s widow, Nora Bass, said she has been unable to find a lawyer willing to take his case to court because he was legally intoxicated. Nonetheless, she said, Tesla should take at least some responsibility for her husband’s death. “Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human,” Bass said. “We were sold a false sense of security.”

Advertisement

In the aftermath of the crash, investigators found that the Tesla continued to feed power to the wheels after impact. They also didn’t identify any signs that Von Ohain nor the car itself had applied the brakes to try and stop the Model 3 as it came off the road. This, Colorado State Patrol Sgt. Robert Madden told the Post, was a clear sign that “fits with the [driver-assistance] feature being engaged.”

Due to the intensity of the fire and the destruction of the car that it cause, Colorado investigators have been unable to access data from the car to determine if FSD really was engaged. What’s more, Tesla said that it “...could not confirm that a driver-assistance system had been in use because it did not receive data over-the-air for this incident,” reports the Post.

Advertisement

Tesla did report the crash to the National Highway Traffic Safety Administration as part of its continued reporting of crashes involving its Autopilot and FSD systems. However, NHTSA could not confirm which program was involved in the crash.. Supporters of self-driving car technologies claim that autonomous features like Tesla’s Full Self-Driving system make cars safer. However, regardless of how safe they are – and opinions are split on the Tesla package – someone was always going to be the first to die in an FSD-related accident.

Earlier this week, the Washington Post reported that the person was Tesla employee Hans von Ohain, who was killed in a fiery collision when his Model 3 left the road and burst into flames after smashing into a tree. However, Tesla CEO Elon has taken to social media to dispute The Post’s story, claiming that Ohain’s car wasn’t equipped with FSD capability.

The Post said that the purchase order for von Ohain’s EV showed it was equipped with features only available to buyers who purchased the FSD system. Additionally, friends and family of the driver said he used the car’s autonomous capabilities wherever he went, proudly showing them off to passengers. But Musk insists von Ohain’s car wasn’t equipped with Tesla’s top-line driver-assist package.

Related: Tesla Employee Killed In Crash Had Full-Self Driving On, Claims Passenger

“He was not on FSD,” Musk wrote on X. “The software had unfortunately never been downloaded. I say ‘unfortunately’, because the accident probably would not have happened if FSD had been engaged.”

In a separate tweet, Tesla’s policy boss, Rohan Patel, seconded Musk’s comments about von Ohain’s car, again insisting that the FSD beta software package wasn’t downloaded to the Model 3 before it crashed in Evergreen, Colorado.

Von Ohain was found to have been over the drink-drive limit at the time of the crash, as was his passenger, who survived the accident. The pair had been drinking during the day at a golf course and the passenger told emergency responders that the driver had been using the “auto-drive feature on the Tesla” and that the Model 3 “just ran straight off the road,” The Post reported.. Tesla likes to boast about FSD's safety, but a report about a fatal crash in 2022 raises concerns about how drivers interact with the system

The passenger in a 2022 crash, which tragically claimed the life of a Tesla engineering recruiter behind the wheel of a Model 3, has alleged that the driver was utilizing “Full Self-Driving” (FSD) when the vehicle careened off the road.

The fiery crash that claimed Hans von Ohain’s life occurred near Denver in 2022 as he and his friend, Erik Rossiter, were returning home after a game of golf. Both men’s blood alcohol levels were over the legal limit. Rossiter’s claim that FSD was engaged raises questions about the system’s safety and the way drivers interact with it.

Read: Tesla Autopilot Recall Fix Sparks Complaints From Both Owners And Regulators

Von Ohain’s widow, Nora Bass, described her husband as a true believer in both Tesla and its CEO, Elon Musk, in a recent interview with The Washington Post. She claimed that he used FSD at every opportunity he had, though she found it too jerky and unsettling to use herself.

That description was echoed by Rossiter, who stated that his friend used FSD both on the way to and from the golf course. He added that the ride there was “uncomfortable” due to the car driving jerkily, and von Ohain had to repeatedly correct its course as the Model 3 struggled to navigate the winding roads outside of Denver, Colorado.

Challenges Surrounding FSD and Data Recovery

Rossiter described his memory of the accident as spotty. While NHTSA documents confirm that Tesla’s driver assistance systems were engaged 30 seconds prior to the accident, there is little more official data available. The car was too badly burned in the accident to recover data, and von Ohain was driving in too rural an area for over-the-air data to confirm an FSD timeline.

In fact, official data cannot even confirm whether it was FSD or Autopilot that was engaged, and the distinction may be meaningful. Although investigations have been conducted regarding the use of Autopilot in accidents with victims, Tesla maintains that no fatal accidents have been recorded while FSD was engaged.

That has helped the automaker justify the technology’s use on public roads, despite still officially being in “beta” mode. Tesla points to America’s notably large number of on-road fatalities as evidence that driver assistance technologies, such as FSD, need to be developed as quickly as possible, and therefore should continue to be allowed on its roads, even before they are officially out of beta testing.

However, the nature of FSD, as a Level 2+ driver assistance system, and Tesla’s insistence that it is a “full self-driving” system, make it prone to misuse, according to critics. Indeed, Bass said that she thinks the automaker bears some responsibility for her husband’s death, despite his inebriation.

“You’re told that this car should be smarter than you, so when it’s in Full Self-Driving, you relax,” she told The Washington Post. “Your reaction time is going to be less than if we were not in Full Self-Driving.”

However, due to von Ohain’s level of intoxication, she stated that she has been unable to find a lawyer to take his case to court. Nonetheless, experts agree that consumers need to be aware that their vehicle cannot yet fully drive for them. Others, like Arizona State University professor of advanced technology transitions, Andrew Maynard, are even more direct and claim that FSD “isn’t quite ready for prime time yet.”

Criticism of Tesla’s Handling and Support

Tesla’s stance on FSD isn’t the only aspect drawing criticism. Von Ohain’s parents and his widow expressed disappointment with the way the company treated them following his loss. Bass stated that the company’s silence regarding her husband was almost cruel, and the first communication they received from Tesla was a termination notice addressed to the deceased von Ohain.. The tragic fatal crash of a Tesla employee allegedly using Full Self-Driving (FSD) Beta in 2022 has been extensively reported. However, one such recent report has revealed the intricacies of Hans von Ohain’s accident, shedding light on accountability in such incidents.

Ohain died in a car crash after his Tesla Model 3 veered off course, crashed into a tree, and burst into flames, according to a new report by The Washington Post. However, Ohain wasn’t alone in the car.. Benzinga - by Chris Katje, Benzinga Staff Writer.

Tesla Inc (NASDAQ:TSLA) recruiter Hans von Ohain died in a fatal automotive accident in 2022. New footage and evidence from The Washington Post points to the accident being the potential first full self-driving fatality from the electric vehicle (EV) company.

What Happened: Von Ohain and passenger Erik Rossiter were involved in the accident that saw the Tesla Model 3 hit a tree and burst into flames. Rossiter survived the crash and later told emergency responders that the driver had used an "auto-drive feature on the Tesla."

Rossiter's testimony and evidence suggest that Autopilot wasn’t being used. Instead, von Ohain and Rossiter used the full self-driving, function.

Both the driver and passenger in this particular accident had been drinking before leaving a golf course. An autopsy of von Ohain showed a blood alcohol level of 0.26 — significantly higher than the legal limit.

Nora Bass, the widow of von Ohain, shifted blame to Tesla CEO Elon Musk and expressed frustration over not being able to find a lawyer who would take the case.

"Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human," Bass said. "We were sold a false sense of security."

An order reviewed by the Washington Post showed von Ohain received full self-driving for free with an employee discount. Customers paid $10,000 at the time for the feature.

Given the state of the vehicle after the accident, police were unable to access data from the car. Tesla could not confirm if full self-driving had been in use, stating it "did not receive data over-the-air for this incident."

here or remove ads . 3rd party Ad. Not an offer or recommendation by Investing.com. See disclosureor

The Austin, Texas-based company reported the accident to the National Highway Traffic Safety Administration (NHTSA) saying a driver-assistance feature had been used before the impact.

According to The Washington Post, Tesla has not publicly acknowledged the death of their former employee. Bass said the company's first communication with her after the accident was a termination notice found in von Ohain's email.

Benzinga did not hear back on a request for comment from Tesla.

Related Link: Tesla Q4 Earnings Highlights: Revenue Miss, EPS Miss, Model Y A Global Bestseller, Next-Gen Vehicle Update And More

Why It’s Important: Tesla is the leader in EV production and deliveries globally. Implementing self-driving technology remains one of the company’s main objectives.

Over 900 accidents using driver-assistance systems in Tesla vehicles have been reported to federal regulators since 2021. Among the 900 accidents, 40 included serious or fatal injuries.

Full self-driving is available for around 400,000 Tesla drivers. The technology is currently in "beta mode," with ongoing development continuing to improve the system.

Musk previously said that full self-driving could be reliable in the future that drivers "could go to sleep."

Tesla's user agreement for full self-driving requires that drivers are ready to take over the vehicle at all times.

A previous federal investigation into a 2021 car crash involving a Tesla Model S was blamed on drunk driving instead of the use of Autopilot.

There are several pending investigations and potential lawsuits against Tesla involving the use of Autopilot and full self-driving.

TSLA Price Action: Tesla shares are down 2% to $183.99 on Tuesday versus a 52-week trading range of $152.37 to $299.29.

here or remove ads . 3rd party Ad. Not an offer or recommendation by Investing.com. See disclosureor

Read Next: Elon Musk Says Autopilot Would ‘Almost Certainly Have’ Saved Driver In 2019 Model 3 Crash After Tesla Prevails In Court

© 2024 Benzinga.com. Benzinga does not provide investment advice. All rights reserved.

Read the original article on Benzinga. A Tesla recruiter lost his life in a tragic incident in 2022 while using the electric vehicle's (EV) Full Self-Driving (FSD) feature. The fatal accident occurred on a Colorado mountain road when Hans von Ohain's Tesla Model 3 veered off the road, collided with a tree, and subsequently caught fire.

Surviving passenger and friend Erik Rossiter informed emergency responders that the victim had activated the "auto-drive feature" before the crash. In a report, The Washington Post wrote that this would be Tesla's first driver-assistance fatality if the incident gets officially connected with FSD. Even though von Ohain was legally drunk, officials are investigating Tesla's software's participation in the disaster.

Tesla Driver-Assistance Features Under Scrutiny

Since 2021, federal officials have recorded over 900 Tesla driver-assistance system crashes, with at least 40 causing serious or fatal injuries. The majority included Tesla's Autopilot, meant for highway use, although full self-driving, intended for more situations, has not been related to any fatalities.

(Photo : Spencer Platt/Getty Images)The inside of a Tesla vehicle is viewed as it sits parked in a new Tesla showroom and service center in Red Hook, Brooklyn on July 5, 2016 in New York City.

Tesla calls FSD "beta" but highlights its importance for road safety after releasing it to 400,000 customers. Tesla user guides listed scenarios where FSD may not work, and the EV manufacturer emphasizes driver control. The reported Tesla accidents cast doubt on autonomous driving and driver-assistance technology safety, emphasizing regulatory attention and the necessity to comprehend changing technology on public roadways.

Though Tesla has not yet commented on the issue, it warns that Autopilot, Enhanced Autopilot, and Full Self-Driving Capability are designed to be utilized by a fully aware driver, grasping the wheel and ready to take control at any time, according to Business Insider. "While these features are designed to become more capable over time, the currently enabled features do not make the vehicle autonomous," Tesla stated.

Read Also: Sensitive US Military Data Exposed in DOD Email Leak, Affecting More Than 20K Individuals

Is Tesla Deceiving The Public with FSD?

Hans von Ohain, a Marine veteran and Tesla employee since 2020, was a believer in the potential of Tesla's full self-driving technology and frequently utilized the feature. Given Elon Musk's claims about the technology's capabilities, his widow, Nora Bass, expressed worries about the false sense of security FSD provides, per The Post.

Bass highlighted her struggle to secure legal representation due to her husband's state of intoxication. However, she maintained that Tesla shares some responsibility for her spouse's demise, emphasizing that regardless of Hans's level of intoxication, Musk claimed that the car could operate autonomously and was essentially superior to a human. "We were sold a false sense of security."

Tesla's self-driving claims are under investigation by the government. Despite Elon Musk's frequent promises regarding autonomous vehicles, Tesla's history is now tainted by the FSD tragedies. One of multiple Tesla lawsuits is led by Nabilah Hussain, a lawyer alleging that the EV firm advertised a non-existent fully self-driving device. "Tesla is marketing and trying to sell you a product that doesn't exist," Hussain said in the article.

Tesla has not officially recognized von Ohain's death, raising issues about the company's duty and contact with employees and their families. The incident highlights persistent technical and ethical concerns around incorporating autonomous driving technology into daily use.

Related Article: SpaceX Fined, Musk Firm Pays for Nearly Amputated Employee Accident

ⓒ 2024 TECHTIMES.com All rights reserved. Do not reproduce without permission.. Next Article

The crash took place in 2022

Tesla employee with full self-driving enabled killed in car crash

By Dwaipayan Roy 12:41 pm Feb 14, 202412:41 pm

What's the story Back in 2022, Tesla employee and Elon Musk fan Hans von Ohain, tragically died when his Model 3 crashed and caught fire. Now, Erik Rossiter, a survivor of the accident, has claimed that the Full Self-Driving (FSD) feature was active during the crash. If true, this could be the first death involving FSD, a feature that has already caught regulators' attention.

Investigation

FSD feature under scrutiny

The Washington Post has confirmed that von Ohain's car had FSD, which he got for free as an employee perk. His widow, Nora Bass, mentioned he used it often. However, Tesla's vehicles are not fully autonomous yet, and drivers must be prepared to take control. The National Highway Traffic Safety Administration is already investigating Tesla's Autopilot after several accidents involving emergency response vehicles.

Fault

Autopilot fatalities and misleading marketing

Last year, Washington Post found that fatal crashes involving Tesla's Autopilot mode have increased since 2019, with at least 17 out of more than 700 crashes being deadly. Von Ohain's autopsy showed a blood alcohol level of 0.26, more than three times than legally permissible. Experts argue that Tesla's misleading marketing might give drivers a false sense of security, even without alcohol involved.

Problems

Ethical questions and Tesla's responsibility

Von Ohain's death raises questions about responsibility. Is Tesla's misleading marketing at fault, or was it the driver's reckless behavior? Bass told the Washington Post, "Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security." Tesla has not publicly acknowledged von Ohain's death, and the FSD facility is still in development, far from achieving full autonomy.

Future

FSD is Tesla's trump card

Tesla's future is dependent on FSD's success. In 2022, Musk claimed that FSD is "the difference between Tesla being worth a lot of money and being worth basically zero." He claims that the firm will achieve Level 5 autonomy in less than a year, at which point, the car will not require a steering wheel or brake pedal. However, the facility is yet to surpass Level 2 autonomy and needs the driver to take over the wheels at any time.. Evidence suggests that Tesla’s advanced driver-assistance system, Full Self-Driving (FSD), was engaged during a fatal crash that killed Tesla employee Hans von Ohain in Colorado in 2022. If this proves accurate, a Tesla employee could prove to be the very first fatality caused by Elon Musk’s “self-driving” software.

The Washington Post reports that on May 16, 2022, Hans von Ohain was killed when his Tesla Model 3 crashed into a tree and caught fire in Evergreen, Colorado. Von Ohain worked as a recruiter at Tesla and was an avid fan of CEO Elon Musk. His passenger, Erik Rossiter, survived the crash.

Rossiter told 911 dispatchers that von Ohain had activated an “auto-drive feature” on the Tesla, which led the car to veer off the road on its own. In a later interview, Rossiter clarified that he believes von Ohain was using Tesla’s Full Self-Driving feature at the time of the crash.

Full Self-Driving is Tesla’s most advanced driver-assistance technology, designed to guide the vehicle on roads from quiet suburbs to busy cities with little input from the driver. Over 400,000 Tesla owners have access to the FSD software, which remains in ongoing beta testing.

If Rossiter’s account proves true, this would likely be the first known fatality involving Full Self-Driving. In late 2021, federal regulators began requiring automakers to report crashes involving driver-assistance systems. Since then, they have logged over 900 crashes in Tesla EVs, including at least 40 serious or fatal injuries. Most crashes involved Tesla’s simpler Autopilot system.

According to the police report, there were no skid marks at the Colorado crash site, suggesting von Ohain did not brake before impact. The car continued powering its wheels after hitting the tree, pointing to the advanced driver-assistance system being active at the time.

An autopsy showed von Ohain had a blood alcohol level over three times the legal limit. Experts say this level of intoxication would have seriously hampered his ability to maintain control. However, the sophisticated self-driving capabilities von Ohain believed were engaged may have given him undue confidence in the car’s ability to correct itself.

Tesla has faced growing complaints over unreliable behavior by its driver-assistance software, including sudden swerving or braking. Lawsuits claim Tesla should share responsibility when its technology causes crashes or fails to prevent them. So far, Tesla has avoided liability by arguing that drivers must stay alert and in control.

Breitbart News has reported on dangerous situations caused by Musk’s “full self-driving” software such as a video of a Tesla running a red light while driving itself. In another case, a Tesla caused an eight-car pileup on the San Francisco Bay Bridge.

Read more at the Washington Post here.

Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship.. . The untimely demise of Tesla recruiter Hans von Ohain in a collision involving a Model 3 on a route in Colorado’s mountains has prompted serious concerns about the reliability and security of Tesla’s autonomous driving technology. Ohain’s buddy Erik Rossiter, who was in the car when it crashed and erupted into flames, reportedly told The Washington Post that Ohain was using the Tesla’s auto-drive option when it went off the road and smashed into a tree.

Investigators observed that although Ohain’s blood alcohol content was more than three times the legal limit at the time of the disaster, the occurrence did not follow the usual pattern of a drunk driving crash. It was possible that the driver-assistance feature was activated at the time of the collision because there were rolling tire marks and no skid marks.

The subsequent intense fire that consumed the car and resulted in the determination of Ohain’s cause of death as a result of heat injuries and smoke inhalation raised questions over the safety of Tesla’s lithium-ion battery cells, which are located in the vehicle’s underside.

Nora Bass, the widow of Ohain, acknowledged her husband’s faith in Elon Musk’s autonomous car vision, but she also chastised Tesla for its lack of comment on the incident, implying that they were “just guinea pigs” testing the technology under false pretenses of safety.

Tesla stresses that a fully attentive driver who is ready to take control at any time is required for its present self-driving technology. Although Tesla’s Autopilot technology—which is intended to be used on highways—has been implicated in multiple collisions, the Full Self-Driving technology, which is intended to guide Tesla vehicles through almost any situation, has not been conclusively linked to any fatalities.

The death of Hans von Ohain underscores the ongoing challenges and ethical dilemmas surrounding the development and deployment of autonomous driving technologies, raising important questions about responsibility, transparency, and the true capabilities of these systems.