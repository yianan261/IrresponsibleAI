Tesla likes to boast about FSD's safety, but a report about a fatal crash in 2022 raises concerns about how drivers interact with the system

The passenger in a 2022 crash, which tragically claimed the life of a Tesla engineering recruiter behind the wheel of a Model 3, has alleged that the driver was utilizing “Full Self-Driving” (FSD) when the vehicle careened off the road.

The fiery crash that claimed Hans von Ohain’s life occurred near Denver in 2022 as he and his friend, Erik Rossiter, were returning home after a game of golf. Both men’s blood alcohol levels were over the legal limit. Rossiter’s claim that FSD was engaged raises questions about the system’s safety and the way drivers interact with it.

Read: Tesla Autopilot Recall Fix Sparks Complaints From Both Owners And Regulators

Von Ohain’s widow, Nora Bass, described her husband as a true believer in both Tesla and its CEO, Elon Musk, in a recent interview with The Washington Post. She claimed that he used FSD at every opportunity he had, though she found it too jerky and unsettling to use herself.

That description was echoed by Rossiter, who stated that his friend used FSD both on the way to and from the golf course. He added that the ride there was “uncomfortable” due to the car driving jerkily, and von Ohain had to repeatedly correct its course as the Model 3 struggled to navigate the winding roads outside of Denver, Colorado.

Challenges Surrounding FSD and Data Recovery

Rossiter described his memory of the accident as spotty. While NHTSA documents confirm that Tesla’s driver assistance systems were engaged 30 seconds prior to the accident, there is little more official data available. The car was too badly burned in the accident to recover data, and von Ohain was driving in too rural an area for over-the-air data to confirm an FSD timeline.

In fact, official data cannot even confirm whether it was FSD or Autopilot that was engaged, and the distinction may be meaningful. Although investigations have been conducted regarding the use of Autopilot in accidents with victims, Tesla maintains that no fatal accidents have been recorded while FSD was engaged.

That has helped the automaker justify the technology’s use on public roads, despite still officially being in “beta” mode. Tesla points to America’s notably large number of on-road fatalities as evidence that driver assistance technologies, such as FSD, need to be developed as quickly as possible, and therefore should continue to be allowed on its roads, even before they are officially out of beta testing.

However, the nature of FSD, as a Level 2+ driver assistance system, and Tesla’s insistence that it is a “full self-driving” system, make it prone to misuse, according to critics. Indeed, Bass said that she thinks the automaker bears some responsibility for her husband’s death, despite his inebriation.

“You’re told that this car should be smarter than you, so when it’s in Full Self-Driving, you relax,” she told The Washington Post. “Your reaction time is going to be less than if we were not in Full Self-Driving.”

However, due to von Ohain’s level of intoxication, she stated that she has been unable to find a lawyer to take his case to court. Nonetheless, experts agree that consumers need to be aware that their vehicle cannot yet fully drive for them. Others, like Arizona State University professor of advanced technology transitions, Andrew Maynard, are even more direct and claim that FSD “isn’t quite ready for prime time yet.”

Criticism of Tesla’s Handling and Support

Tesla’s stance on FSD isn’t the only aspect drawing criticism. Von Ohain’s parents and his widow expressed disappointment with the way the company treated them following his loss. Bass stated that the company’s silence regarding her husband was almost cruel, and the first communication they received from Tesla was a termination notice addressed to the deceased von Ohain.. Evidence suggests that Tesla’s advanced driver-assistance system, Full Self-Driving (FSD), was engaged during a fatal crash that killed Tesla employee Hans von Ohain in Colorado in 2022. If this proves accurate, a Tesla employee could prove to be the very first fatality caused by Elon Musk’s “self-driving” software.

The Washington Post reports that on May 16, 2022, Hans von Ohain was killed when his Tesla Model 3 crashed into a tree and caught fire in Evergreen, Colorado. Von Ohain worked as a recruiter at Tesla and was an avid fan of CEO Elon Musk. His passenger, Erik Rossiter, survived the crash.

Rossiter told 911 dispatchers that von Ohain had activated an “auto-drive feature” on the Tesla, which led the car to veer off the road on its own. In a later interview, Rossiter clarified that he believes von Ohain was using Tesla’s Full Self-Driving feature at the time of the crash.

Full Self-Driving is Tesla’s most advanced driver-assistance technology, designed to guide the vehicle on roads from quiet suburbs to busy cities with little input from the driver. Over 400,000 Tesla owners have access to the FSD software, which remains in ongoing beta testing.

If Rossiter’s account proves true, this would likely be the first known fatality involving Full Self-Driving. In late 2021, federal regulators began requiring automakers to report crashes involving driver-assistance systems. Since then, they have logged over 900 crashes in Tesla EVs, including at least 40 serious or fatal injuries. Most crashes involved Tesla’s simpler Autopilot system.

According to the police report, there were no skid marks at the Colorado crash site, suggesting von Ohain did not brake before impact. The car continued powering its wheels after hitting the tree, pointing to the advanced driver-assistance system being active at the time.

An autopsy showed von Ohain had a blood alcohol level over three times the legal limit. Experts say this level of intoxication would have seriously hampered his ability to maintain control. However, the sophisticated self-driving capabilities von Ohain believed were engaged may have given him undue confidence in the car’s ability to correct itself.

Tesla has faced growing complaints over unreliable behavior by its driver-assistance software, including sudden swerving or braking. Lawsuits claim Tesla should share responsibility when its technology causes crashes or fails to prevent them. So far, Tesla has avoided liability by arguing that drivers must stay alert and in control.

Breitbart News has reported on dangerous situations caused by Musk’s “full self-driving” software such as a video of a Tesla running a red light while driving itself. In another case, a Tesla caused an eight-car pileup on the San Francisco Bay Bridge.

Read more at the Washington Post here.

Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship.. Next Article

The crash took place in 2022

Tesla employee with full self-driving enabled killed in car crash

By Dwaipayan Roy 12:41 pm Feb 14, 202412:41 pm

What's the story Back in 2022, Tesla employee and Elon Musk fan Hans von Ohain, tragically died when his Model 3 crashed and caught fire. Now, Erik Rossiter, a survivor of the accident, has claimed that the Full Self-Driving (FSD) feature was active during the crash. If true, this could be the first death involving FSD, a feature that has already caught regulators' attention.

Investigation

FSD feature under scrutiny

The Washington Post has confirmed that von Ohain's car had FSD, which he got for free as an employee perk. His widow, Nora Bass, mentioned he used it often. However, Tesla's vehicles are not fully autonomous yet, and drivers must be prepared to take control. The National Highway Traffic Safety Administration is already investigating Tesla's Autopilot after several accidents involving emergency response vehicles.

Fault

Autopilot fatalities and misleading marketing

Last year, Washington Post found that fatal crashes involving Tesla's Autopilot mode have increased since 2019, with at least 17 out of more than 700 crashes being deadly. Von Ohain's autopsy showed a blood alcohol level of 0.26, more than three times than legally permissible. Experts argue that Tesla's misleading marketing might give drivers a false sense of security, even without alcohol involved.

Problems

Ethical questions and Tesla's responsibility

Von Ohain's death raises questions about responsibility. Is Tesla's misleading marketing at fault, or was it the driver's reckless behavior? Bass told the Washington Post, "Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security." Tesla has not publicly acknowledged von Ohain's death, and the FSD facility is still in development, far from achieving full autonomy.

Future

FSD is Tesla's trump card

Tesla's future is dependent on FSD's success. In 2022, Musk claimed that FSD is "the difference between Tesla being worth a lot of money and being worth basically zero." He claims that the firm will achieve Level 5 autonomy in less than a year, at which point, the car will not require a steering wheel or brake pedal. However, the facility is yet to surpass Level 2 autonomy and needs the driver to take over the wheels at any time.. A deadly single-car crash in Evergreen involving a Tesla is now part of a federal investigation that is looking into the safety of electric vehicles.

Denver7 Investigates confirmed that the National Highway Traffic Safety Administration (NHTSA) has collected data and information from the crash as part of its investigation.

The crash happened in the late evening of May 16 on Upper Bear Creek Road when the car went off the road and slammed into a tree. The car then caught fire.

A passenger made it out of the vehicle, but the driver, 33-year-old Hans von Ohain, died at the scene.

A report from Clear Creek County, who assisted on the call, notes that the passenger said the car was in its “auto drive” function. Radio communication from dashcam footage of the accident obtained by Denver7 Investigates backs up that report.

Questions still remain regarding if and how the auto drive function played a role in the crash and if the lithium ion batteries in the Tesla contributed to the intensity of the fire. Colorado State Patrol, who is leading the investigation, has yet to finalize its report from the crash but is working to answer those questions.

Colorado State Patrol Sgt. Rob Madden said the investigation will attempt to determine why the driver could not open his door, what his condition was after impact and how the electronics operated after the fire started.

“Those are all questions we may not have answers to,” he said.

Paul Shoemaker, a Castle Rock firefighter and paramedic, also owns Next Level Extrication, a company that trains emergency workers on how to save lives in car crashes.

Shoemaker provided training to the Evergreen Fire Protection District after this crash and said he believes the driver was awake and alert but was unable to open the door.

Shoemaker also studies post-crash fires in electric vehicles and other examples where drivers could not get out of the vehicle. He cited an accident in British Columbia where a driver kicked the window out of the driver-side door in a Tesla that had caught fire in the road.

“His reaction was to kick the window out,” he said. “He never opened the door.”

Shoemaker added that the driver probably would not have survived, otherwise.

While every vehicle is different, Shoemaker said all electric cars have a manual release to get out of the car when the electric systems fail. However, he estimates less than 1% of people know how to get out of the car in an emergency situation.

The same car can also have different ways to get out if a person is in the front seat or the back seat. Shoemaker suggests electric car vehicle owners look through their manuals to see how they can get out of the car should the electronics fail.

NHTSA investigation

Earlier this year, published reports announced NHTSA was expanding its investigation into the auto drive function on several Tesla models, including the 2021 Model 3 involved in the Evergreen crash.

“They have all the information from this crash, and they are looking at this at their level,” Madden said.

He added that he does not know if the Tesla involved in the crash in Evergreen was being driven by the auto drive or auto pilot function. CSP is also investigating whether alcohol played a role in the crash.

The hope is that a complete report will be ready by the end of the year.

“This report is extremely complex, and that is what we owe to the family and to friends and to the public to give them answers,” Madden said.

NHTSA said it does comment on ongoing investigations. Tesla has not responded to multiple requests for comment.