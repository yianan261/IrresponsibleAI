For many online, Lensa AI is a cheap, accessible profile picture generator. But in digital art circles, the popularity of artificial intelligence-generated art has raised major privacy and ethics concerns.

Lensa, which launched as a photo editing app in 2018, went viral last month after releasing its â€œmagic avatarsâ€ feature. It uses a minimum of 10 user-uploaded images and the neural network Stable Diffusion to generate portraits in a variety of digital art styles. Social media has been flooded with Lensa AI portraits, from photorealistic paintings to more abstract illustrations. The app claimed the No. 1 spot in the iOS App Storeâ€™s â€œPhoto & Videoâ€ category earlier this month.

But the appâ€™s growth â€” and the rise of AI-generated art in recent months â€” has reignited discussion over the ethics of creating images with models that have been trained using other peopleâ€™s original work.

Lensa is tinged with controversy â€” multiple artists have accused Stable Diffusion of using their art without permission. Many in the digital art space have also expressed qualms over AI models producing images en masse for so cheap, especially if those images imitate styles that actual artists have spent years refining.

For a $7.99 service fee, users receive 50 unique avatars â€” which artists said is a fraction of what a single portrait commission normally costs.

Companies like Lensa say theyâ€™re â€œbringing art to the masses,â€ said artist Karla Ortiz. â€œBut really what theyâ€™re bringing is forgery, art theft [and] copying to the masses.â€

In an email to NBC News on Wednesday, Prisma Labs CEO Andrey Usoltsev clarified that bringing art to the masses "was never part of the company's mission" and stated that the "democratization of access" to technology like Stable Diffusion is "quite an incredible milestone."

"What was once available only to techy well-versed users is now out there for absolutely everyone to enjoy. No specific skills are required," Usoltsev said.

"As AI technology becomes increasingly more sophisticated and accessible, it is likely that we will see AI-powered tools and features being widely integrated into consumer-facing apps at a rapid scale. We'd like to be a part of this ongoing conversation and steer the use of such technology in a safe and ethical way."

Prisma issued a lengthy Twitter thread on Tuesday morning, in which it addressed concerns of AI art replacing art by actual artists.

The thread did not address accusations that many artists didnâ€™t consent to the use of their work for AI training.

â€œAs cinema didnâ€™t kill theater and accounting software hasnâ€™t eradicated the profession, AI wonâ€™t replace artists but can become a great assisting tool,â€ the company tweeted. â€œWe also believe that the growing accessibility of AI-powered tools would only make man-made art in its creative excellence more valued and appreciated, since any industrialization brings more value to handcrafted works.â€

The company said that AI-generated images â€œcanâ€™t be described as exact replicas of any particular artwork.â€

Usoltsev said that he could not provide further comment regarding the "third party research and methodologies" used by Stability AI, which developed Stable Diffusion.

For some artists, AI models are a creative tool. Several have pointed out that the models are helpful for generating reference images that are otherwise difficult to find online. Other writers have posted about using the models to visualize scenes in their screenplays and novels. While the value of art is subjective, the crux of the AI art controversy is the right to privacy.

Ortiz, who is known for designing concept art for movies like â€œDoctor Strange,â€ also paints fine art portraits. When she realized that her art was included in a dataset used to train the AI model that Lensa uses to generate avatars, she said it felt like a â€œviolation of identity.â€

Prisma Labs deletes user photos from the cloud services it uses to process the images after it uses them to train its AI, the company told TechCrunch. The companyâ€™s user agreement states that Lensa can use the photos, videos and other user content for â€œoperating or improving Lensaâ€ without compensation.

In its Twitter thread, Lensa said that it uses a â€œseparate model for each user, not a one-size-fits-all monstrous neural network trained to reproduce any face.â€ The company also stated that each userâ€™s photos and â€œassociated modelâ€ are permanently erased from its servers as soon as the userâ€™s avatars are generated.

The fact that Lensa uses user content to further train its AI model, as stated in the appâ€™s user agreement, should alarm the public, artists who spoke with NBC News said.

â€œWeâ€™re learning that even if youâ€™re using it for your own inspiration, youâ€™re still training it with other peopleâ€™s data,â€ said Jon Lam, a storyboard artist at Riot Games. â€œAnytime people use it more, this thing just keeps learning. Anytime anyone uses it, it just gets worse and worse for everybody.â€

Image synthesis models like Google Imagen, DALL-E and Stable Diffusion are trained using datasets of millions of images. The models learn associations between the arrangement of pixels in an image and the imageâ€™s metadata, which typically includes text descriptions of the image subject and artistic style.

The model can then generate new images based on the associations it has learned. When fed the prompt â€œbiologically accurate anatomical description of a birthday cake,â€ for example, the model Midjourney generated unsettling images that looked like actual medical textbook material. Reddit users described the images as â€œbrilliantly weirdâ€ and â€œlike something straight out of a dream.â€

The San Francisco Ballet even used images generated by Midjourney to promote this seasonâ€™s production of the Nutcracker. In a press release earlier this year, the San Francisco Balletâ€™s chief marketing officer Kim Lundgren said that pairing the traditional live performance with AI-generated art was the â€œperfect way to add an unexpected twist to a holiday classic.â€ The campaign was widely criticized by artist advocacy groups.

A spokesperson for the ballet said the campaign was a "chance to experiment with today's technological tools," and that nearly 30 people were involved in creating it.

"In the spirit of Bay Area ingenuity, we tried something new," the spokesperson said. "SF Ballet remains deeply connected to and proudly a part of the diverse artistic communities of the Bay Area."

Ortiz said that images like the ones used in the San Francisco Ballet's campaign "look so good due to the nonconsensual data they gathered from artists and the public."

He was referring to the Large-scale Artificial Intelligence Open Network (LAION), a nonprofit organization that releases free datasets for AI research and development. LAION-5B, one of the datasets used to train Stable Diffusion and Google Imagen, includes publicly available images scraped from sites like DeviantArt, Getty Images and Pinterest.

Many artists have spoken out against models that have been trained with LAION because their art was used in the set without their knowledge or permission. When an artist used the site Have I Been Trained, which allows users to check if their images were included in LAION-5B, she found her own face and medical records. Ars Technica reported that â€œthousands of similar patient medical record photosâ€ were also included in the dataset.

â€œAnd now we are facing the same problem the music industry faced with websites like Napster, which was maybe made with good intentions or without thinking about the moral implications.â€ artist mateusz urbanowicz

Artist Mateusz Urbanowicz, whose work was also included in LAION-5B, said that fans have sent him AI-generated images that bear striking similarities to his watercolor illustrations.

Itâ€™s clear that LAION is â€œnot just a research project that someone put on the internet for everyone to enjoy,â€ he said, now that companies like Prisma Labs are using it for commercial products.

â€œAnd now we are facing the same problem the music industry faced with websites like Napster, which was maybe made with good intentions or without thinking about the moral implications.â€

The art and music industry abide by stringent copyright laws in the United States, but the use of copyrighted material in AI is legally murky. Using copyrighted material to train AI models might fall under fair use laws, The Verge reported. Itâ€™s more complicated when it comes to the content that AI models generate, and itâ€™s difficult to enforce, which leaves artists with little recourse.

â€œThey just take everything because itâ€™s a legal gray zone and just exploiting it,â€ Lam said. â€œBecause tech always moves faster than law, and law is always trying to catch up with it.â€

Usoltsev asserted that Lensa is â€œfully GDPR and CCAP complaint.â€ To the best of his knowledge, he said, â€œthe commercial use of the model doesnâ€™t represent any legal violations.â€

Thereâ€™s also little legal precedent for pursuing legal action against commercial products that use AI trained on publicly available material. Lam and others in the digital art space say they hope that a pending class action lawsuit against GitHub Copilot, a Microsoft product that uses an AI system trained by public code on GitHub, will pave the way for artists to protect their work. Until then, Lam said heâ€™s wary of sharing his work online at all.

Lam isnâ€™t the only artist worried about posting his art. After his recent posts calling out AI art went viral on Instagram and Twitter, Lam said that he received â€œan overwhelming amountâ€ of messages from students and early career artists asking for advice.

The internet â€œdemocratizedâ€ art, Ortiz said, by allowing artists to promote their work and connect with other artists. For artists like Lam, who has been hired for most of his jobs because of his social media presence, posting online is vital for landing career opportunities. Putting a portfolio of work samples on a password-protected site doesnâ€™t compare to the exposure gained from sharing it publicly.

â€œIf no one knows your art, theyâ€™re not going to go to your website,â€ Lam added. â€œAnd itâ€™s going to be increasingly difficult for students to get their foot in the door.â€

Adding a watermark may not be enough to protect artists â€” in a recent Twitter thread, graphic designer Lauryn Ipsum listed examples of the â€œmangled remainsâ€ of artistsâ€™ signatures in Lensa AI portraits.

Some argue that AI art generators are no different from an aspiring artist who emulates anotherâ€™s style, which has become a point of contention within art circles.

Days after illustrator Kim Jung Gi died in October, a former game developer created an AI model that generates images in the artistâ€™s unique ink and brush style. The creator said the model was an homage to Kimâ€™s work, but it received immediate backlash from other artists. Ortiz, who was friends with Kim, said that the artistâ€™s â€œwhole thing was teaching people how to draw,â€ and to feed his lifeâ€™s work into an AI model was â€œreally disrespectful.â€

Urbanowicz said heâ€™s less bothered by an actual artist whoâ€™s inspired by his illustrations. An AI model, however, can churn out an image that he would â€œnever makeâ€ and hurt his brand â€” like if a model was prompted to generate â€œa store painted with watercolors that sells drugs or weaponsâ€ in his illustration style, and the image was posted with his name attached.

â€œIf someone makes art based on my style, and makes a new piece, itâ€™s their piece. Itâ€™s something they made. They learned from me as I learned from other artists,â€ he continued. â€œIf you type in my name and store [in a prompt] to make a new piece of art, itâ€™s forcing the AI to make art that I donâ€™t want to make.â€

Many artists and advocates also question if AI art will devalue work created by human artists.

Lam worries that companies will cancel artist contracts in favor of faster, cheaper AI-generated images.

Urbanowicz pointed out that AI models can be trained to replicate an artistâ€™s previous work, but will never be able to create the art that an artist hasnâ€™t made yet. Without decades of examples to learn from, he said, the AI images that looked just like his illustrations would never exist. Even if the future of visual art is uncertain as apps like Lensa AI become more common, heâ€™s hopeful that aspiring artists will continue to pursue careers in creative fields.

â€œOnly that person can make their unique art,â€ Urbanowicz said. â€œAI cannot make the art that they will make in 20 years.â€. At 19, when I began drafting my webcomic, I had just been flung into adulthood. I felt a little awkward, a little displaced. The glittering veneer of social media, which back then was mostly Facebook, told me that everyone around me had their lives together while I felt like a withering ball of mediocrity. But surely, I believed, I could not be the only one who felt that life was mostly an uphill battle of difficult moments and missed social cues.

I started my webcomic back in 2011, before â€œrelatableâ€ humor was as ubiquitous online as it is today. At the time, the comics were overtly simple, often drawn shakily in Microsoft Paint or poorly scanned sketchbook pages. The jokes were less punchline-oriented and more of a question: Do you feel this way too? I wrote about the small daily struggles of missed clock alarms, ill-fitting clothes and cringe-worthy moments.

I had hoped, at most, for a small, niche following, but to my elation, I had viral success. My first comic to reach a sizable audience was about simply not wanting to get up in the morning, and it was met with a chorus of â€œthis is so me.â€ I felt as if I had my finger on the pulse of the collective underdog. To have found this way of communicating with others and to make it my work was, and remains, among the greatest gifts and privileges of my life.

But the attention was not all positive. In late 2016, I caught the eye of someone on the 4chan board /pol/. There was no particular incident that prompted the harassment, but in hindsight, I was a typical target for such groups. I am a woman, the messaging in the comics is feminist leaning, and importantly, the simplicity of my work makes it easy to edit and mimic. People on the forum began reproducing my work and editing it to reflect violently racist messages advocating genocide and Holocaust denial, complete with swastikas and the introduction of people getting pushed into ovens. The images proliferated online, with sites like Twitter and Reddit rarely taking them down.. If your Instagram is awash in algorithmically generated portraits of your friends, you arenâ€™t alone. After adding a new avatar generation tool based on Stable Diffusion, the photo editing app Lensa AI went viral over the last few days, with users sharing their uncanny AI-crafted avatars (and the horrible misfires) in stories and posts.

Lensaâ€™s fun, eminently shareable avatars mark the first time that many people have interacted with a generative AI tool. In Lensaâ€™s case, itâ€™s also the first time theyâ€™ve paid for computer-generated art.

Stable Diffusion itself is free and a lot of people are playing around with it for research purposes or just for fun. But Lensa and other services like it â€” Avatar AI and Profilepicture.AI, to name a few â€” are making money by selling the computing cycles required to run the prompts and spit out a set of images. That certainly changes the equation a little.

Lensa is built on Stable Diffusionâ€™s free, open source image generator but acts as a middleman. Send Lensa 10-20 selfies and $7.99 ($3.99 if you sign up for a free trial) and the app does the heavy lifting for you behind the scenes, handing back a set of stylized portraits in an array of styles like sci-fi, fantasy and anime. Anyone with sufficient processing power can install Stable Diffusion on a machine, download some models and get similar results, but Lensaâ€™s avatars are impressive and Instagram-ready enough that droves of people are more than happy to pay for the convenience.

Since we introduced our AI-generated avatars here, the hype has gone over the roof. Among the first and most voted names of whom we should do next, you named Casey Neistat.

So we've braced ourselves, prepared more bandwidth on servers, and are happy to show @Casey as seen by AI pic.twitter.com/FjhLG7WTdU â€” Prisma Labs (@PrismaAI) November 29, 2022

While the tech world has celebrated the advancements of AI image and text generators this year â€” and artists have watched the proceedings warily â€” your average Instagram user probably hasnâ€™t struck up a philosophical conversation with ChatGPT or fed DALL-E absurdist prompts. That also means that most people havenâ€™t grappled with the ethical implications of free, readily available AI tools like Stable Diffusion and how theyâ€™re poised to change entire industries â€” if we let them.

From my experience over the weekend on Instagram, for every 10 Lensa avatars thereâ€™s one Cassandra in the comments scolding everyone for paying for an app that steals from artists. Those concerns arenâ€™t really overblown. Stable Diffusion, the AI image generator that powers Lensa, was originally trained on 2.3 billion captioned images â€” a massive cross-section of the visual internet. Swept up in all of that is all kinds of stuff, including watermarked images, copyrighted works and a huge swath of pictures from Pinterest, apparently. Those images also include many thousands of photos pulled from Smugmug and Flickr, illustrations from DeviantArt and ArtStation and stock images from sites like Getty and Shutterstock.

These AI photos generated by @PrismaAI are kinda crazy ğŸ˜³ğŸ”¥ pic.twitter.com/A0CtkX4he2 â€” Ken Walker (@TheKenFolk) November 29, 2022

Individual artists didnâ€™t opt in to appearing in the training dataset, nor can they opt out. According to LAION, the nonprofit that created the huge datasets to begin with, the troves of data are â€œsimply indexes to the internet,â€ lists of URLs to images across the web paired with the alt text that describes them. If youâ€™re an EU citizen and the database contains a photo of you with your name attached, you can file a takedown request per the GDPR, Europeâ€™s groundbreaking privacy law, but thatâ€™s about it. The horse has already left the barn.

Weâ€™re in the earliest stages of grappling with what this means for artists, whether itâ€™s independent illustrators and photographers or massive copyright-conscious corporations that get swept up in the AI modeling process. Some models using Stable Diffusion push the issue even further. Prior to a recent update, Stable Diffusion Version 2, anyone could craft a named template designed to mimic a specific artistâ€™s distinct visual style and mint new images ad infinitum at a pace that no human could compete with.

We are excited to announce the release of Stable Diffusion Version 2! Stable Diffusion V1 changed the nature of open source AI & spawned hundreds of other innovations all over the world. We hope V2 also provides many new possibilities! Link â†’ https://t.co/QOSSmSRKpG pic.twitter.com/z0yu3FDWB5 â€” Stability AI (@StabilityAI) November 24, 2022

Andy Baio, who co-founded a festival for independent artists, has a thoughtful interview up on his blog delving into these concerns. He spoke with an illustrator who discovered an AI model specifically designed to replicate her work. â€œMy initial reaction was that it felt invasive that my name was on this tool,â€ she said. â€œâ€¦ If I had been asked if they could do this, I wouldnâ€™t have said yes.â€

By September, Dungeons & Dragons artist Greg Rutkowski was so popular as a Stable Diffusion prompt used to generate images in his detailed fantasy style that he worried his actual art would be lost in the sea of algorithmic copies. â€œWhat about in a year? I probably wonâ€™t be able to find my work out there because [the internet] will be flooded with AI art,â€ Rutkowski told MITâ€™s Technology Review.

Those worries, echoed by many illustrators and other digital creatives, are reverberating on social media as many people encounter these thorny issues â€” and the existential threat they seem to pose â€” for the first time.

â€œI know a lot of people have been posting their Lensa/other AI portraits lately. I would like to encourage you not to do so or, better yet, not to use the service,â€ voice actor Jenny Yokobori wrote in a popular tweet thread about Lensa. In another, Riot Games artist Jon Lam shared his own discomfort with AI-generated art. â€œWhen AI artists steal/co-opt art from us I donâ€™t just see art, I see people, mentors and friends. I donâ€™t expect you to understand.â€

Personally, I was sick and stuck at home over the weekend, where I spent more time than usual idly scrolling on social media. My Instagram stories were a blur of flattering digital illustrations that cost cents a piece. Lensa has clearly tapped into something special there, appealing to both the vain impulse to effortlessly collect 50 stylish self-portraits and the allure of polling your friends about which are your exact likeness (most, from my experience) and which are hilarious mutations that only a computer doing its best human impersonation could spin up.

Some friends, mostly artists and illustrators, pushed back, encouraging everyone to find an artist to pay instead. Some creative people in my circles paid too and itâ€™s hard to fault them. For better or worse, itâ€™s genuinely amazing what the current cohort of AI image generators can do, particularly if you just tuned in.

Soon, weâ€™ll all be paying attention. In the name of story research and vanity, I downloaded Lensa and gave the app a try. Iâ€™d only paid once for an artist to make me a profile picture in the past and that was just one image, all the way back in 2016. Now for less than 10 bucks I had a set of 50 epic avatars generated from my most me photos, but these were extra me. Me in various futuristic jumpsuits stepping out of the pages of a graphic novel, me in purple robes looking like an intergalactic saint, me, me, me.

I see the appeal. A handful of friends remarked on how the pictures made them feel, hinting at the gender euphoria of being seen the way they see themselves. I wouldnâ€™t fault anyone for exploring this stuff; itâ€™s all very interesting and at least that complicated. I like my avatars, but part of me wishes I didnâ€™t. I donâ€™t plan to use them.

I thought about my own art, the photography I sell when I remember to stock my online store â€” mostly mountain landscapes and photos of the night sky. I thought back to a handful of the prints Iâ€™ve sold and the effort I had to put in to get the shots. One of my favorite photos involved special permission from the National Park Service and a five-hour backpack trek up to a remote fire lookout in Washington. Many entailed lonely hours of tending my tripod alone in the freezing cold, tracking the Milky Way as it rotated above a dark horizon like the hand of a clock.

The AI models already have enough training fodder to faithfully recreate photos of one tucked-away mountain spot nearby that only local nightsky photographers seem to know about. Three years ago, when I shot photos there, I had to snag a competitive campsite and drive for miles up a potholed forest service road only to wait in the dark for hours. I cooked an expired packet of ramen noodles with a small camp stove to stay warm, tucked the feathers back into my jacket and jumped at everything that made a noise in the dark.

I donâ€™t make a living off of my art. But it still feels like a loss to think that those experiences and the human process they represent â€” learning how to predict a cluster of ancient stars in the blackness, slipping on wet stones and chipping my hotshoe, keeping extra batteries warm in a down pocket â€” could be worth less in the future.. Weâ€™ve filed lawÂ­suits chalÂ­lengÂ­ing AI image genÂ­erÂ­aÂ­tors for using artistsâ€™ work withÂ­out conÂ­sent, credit, or comÂ­penÂ­saÂ­tion.

Because AI needs to be fair & ethÂ­iÂ­cal for everyÂ­one.

This is Joseph Saveri and Matthew Butterick. In NovemÂ­ber 2022, we teamed up to file a lawÂ­suit chalÂ­lengÂ­ing GitHub CopiÂ­lot, an AI codÂ­ing assiÂ­tant built on unpreceÂ­dented open-source softÂ­ware piracy. In July 2023, we filed lawÂ­suits on behalf of book authors chalÂ­lengÂ­ing ChatÂ­GPT and LLaMA.

In JanÂ­uÂ­ary 2023, on behalf of three wonÂ­derÂ­ful artist plainÂ­tiffsâ€”Sarah AnderÂ­sen, Kelly McKÂ­erÂ­nan, and Karla Ortiz, we filed an iniÂ­tial comÂ­plaint against StaÂ­bilÂ­ity AI, DeviantArt, and MidÂ­jourÂ­ney for their use of StaÂ­ble DifÂ­fuÂ­sion, a 21st-cenÂ­tury colÂ­lage tool that remixes the copyÂ­righted works of milÂ­lions of artists whose work was used as trainÂ­ing data. LockÂ­ridge Grindal Nauen P.L.L.P. joined us as co-counÂ­sel.

In NovemÂ­ber 2023, we filed an amended comÂ­plaint, adding seven new plainÂ­tiffs and one new defenÂ­dant, RunÂ­way AI.

In April 2024, we filed an iniÂ­tial comÂ­plaint against Google for its trainÂ­ing of ImaÂ­gen.

Case updates are posted regÂ­uÂ­larly. You can also get updates by email.

Our plainÂ­tiffs are wonÂ­derÂ­ful, accomÂ­plished artists who have stepped forÂ­ward to repÂ­reÂ­sent a class of thouÂ­sandsâ€”posÂ­siÂ­bly milÂ­lionsâ€”of felÂ­low artists affected by genÂ­erÂ­aÂ­tive AI.

Sarah AnderÂ­sen is a carÂ­toonÂ­ist and illusÂ­traÂ­tor. She gradÂ­uÂ­ated from the MaryÂ­land InstiÂ­tute ColÂ­lege of Art in 2014. She curÂ­rently lives in PortÂ­land, OreÂ­gon. Her semi-autoÂ­biÂ­oÂ­graphÂ­iÂ­cal comic strip, Sarahâ€™s ScribÂ­bles, finds the humor in livÂ­ing as an introÂ­vert. Her graphic novel FANGS was nomÂ­iÂ­nated for an EisÂ­ner Award. Sarah also wrote The Alt-Right ManipÂ­uÂ­lated My Comic. Then A.I. Claimed It for the New York Times.

Kelly McKÂ­erÂ­nan is an indeÂ­penÂ­dent artist based in Nashville. They gradÂ­uÂ­ated from KenÂ­neÂ­saw State UniÂ­verÂ­sity in 2009 and have been a full-time artist since 2012. Kelly creÂ­ates origÂ­iÂ­nal waterÂ­color and acryla gouache paintÂ­ings for galÂ­leries, priÂ­vate comÂ­misÂ­sions, and their online store. In addiÂ­tion to mainÂ­tainÂ­ing a large social-media folÂ­lowÂ­ing, Kelly shares tutoÂ­riÂ­als and teaches workÂ­shops, travÂ­els across the US for events and comic-cons, and also creÂ­ates illusÂ­traÂ­tions for books, comics, games, and more.

Karla Ortiz is a Puerto Rican, interÂ­naÂ­tionÂ­ally recÂ­ogÂ­nized, award-winÂ­ning artist. With her excepÂ­tional design sense, realÂ­isÂ­tic renÂ­ders, and charÂ­acÂ­ter-driÂ­ven narÂ­raÂ­tives, Karla has conÂ­tributed to many big-budÂ­get projects in the film, teleÂ­viÂ­sion and video-game indusÂ­tries. Karla is also a regÂ­uÂ­lar illusÂ­traÂ­tor for major pubÂ­lishÂ­ing and role-playÂ­ing game comÂ­paÂ­nies. Karlaâ€™s figÂ­uÂ­raÂ­tive and mysÂ­teÂ­riÂ­ous art has been showÂ­cased in notable galÂ­leries such as Spoke Art and Hashimoto ConÂ­temÂ­poÂ­rary in San FranÂ­cisco; Nucleus Gallery, ThinkÂ­space, and Maxwell AlexanÂ­der Gallery in Los AngeÂ­les; and Galerie Arludik in Paris. She curÂ­rently lives in San FranÂ­cisco with her cat Bady.

Hawke SouthÂ­worth is an illusÂ­traÂ­tion and 3D artist who has been active in the online art comÂ­muÂ­nity since 2006. He attended Laguna ColÂ­lege of Art and Design in 2014. Since then, his focus has ranged from charÂ­acÂ­ter and creaÂ­ture designs, to worldÂ­buildÂ­ing, ARPG comÂ­muÂ­nity buildÂ­ing, and game develÂ­opÂ­ment. He curÂ­rently works as a freeÂ­lance designer and runs an up-and-comÂ­ing Art RoleÂ­playÂ­ing Game webÂ­site, feaÂ­turÂ­ing his own creaÂ­ture designs and an eclecÂ­tic fanÂ­tasy world that users can interÂ­act with.

GrzeÂ­gorz Rutkowski is a PolÂ­ish fine artist, digÂ­iÂ­tal painter, illusÂ­traÂ­tor, and conÂ­cept artist. He started his proÂ­fesÂ­sional career in 2009. He has worked for comÂ­paÂ­nies like WizÂ­ards of the Coast, Ubisoft, BlizÂ­zard, DisÂ­ney, CD ProÂ­jekt RED, Games WorkÂ­shop and many more. He lives in Poland with his wife and two daughÂ­ters.

GreÂ­gory Manchess has creÂ­ated artÂ­work for National GeoÂ­graphic MagÂ­aÂ­zine, Time, Atlantic Monthly, and The SmithÂ­sonÂ­ian. His large porÂ­trait of AbraÂ­ham LinÂ­coln and seven other paintÂ­ings are highÂ­lighted in the AbraÂ­ham LinÂ­coln PresÂ­iÂ­denÂ­tial Library and Museum. Manchess wrote and illusÂ­trated his first â€˜widescreen novelâ€™ Above the TimÂ­berÂ­line, released in 2017 to stelÂ­lar reviews. The SociÂ­ety of IllusÂ­traÂ­tors preÂ­sented him with their highÂ­est honor, the HamilÂ­ton King Award, in 1999. Today, GreÂ­gory lecÂ­tures at uniÂ­verÂ­siÂ­ties and colÂ­leges nationÂ­wide and gives workÂ­shops in paintÂ­ing at the NorÂ­man RockÂ­well Museum in StockÂ­bridge MA, and The AteÂ­lier in MinÂ­neapoÂ­lis. He teaches at the IllusÂ­traÂ­tion MasÂ­ter Class in SavanÂ­nah GA and online with SmArt School.

Artist and author GerÂ­ald Brom has been conÂ­tributÂ­ing his macabre and often bizarre visions to all facets of the creÂ­ative indusÂ­tries for forty years, includÂ­ing major motion picÂ­tures, top-tier video games, comics, and books. He has also writÂ­ten and illusÂ­trated six horÂ­ror novÂ­els, includÂ­ing such popÂ­uÂ­lar titles as The Child Thief, KramÂ­pus the Yule Lord, Lost Gods, and SlewÂ­foot.

Jingna Zhang is a BeiÂ­jing-born, US-based SinÂ­gaÂ­porean phoÂ­togÂ­raÂ­pher whose award-winÂ­ning works have appeared in Vogue, Time, and Harperâ€™s Bazaar. Inspired by the Pre-Raphaelites and anime, her romanÂ­tic images interÂ­weave Asian influÂ­ences with painterly art styles, and feaÂ­ture colÂ­labÂ­oÂ­raÂ­tors such as Coco Rocha, SugÂ­izo, and Michelle Yeoh. Prior to phoÂ­togÂ­raÂ­phy, Jingna was an agent and conÂ­sulÂ­tant for conÂ­cept artists and illusÂ­traÂ­tors with clients spanÂ­ning LucasArts, AmaÂ­zon PubÂ­lishÂ­ing, and Sony Music Japan. Jingna is curÂ­rently the founder of Cara, a platÂ­form for disÂ­covÂ­erÂ­ing human artists.

Julia Kaye is an LA-based stoÂ­ryÂ­board artist & reviÂ­sionÂ­ist with over six years of indusÂ­try expeÂ­riÂ­ence, and the award-winÂ­ning author of two critÂ­iÂ­cally acclaimed graphic novÂ­els: Super Late Bloomer and My Life in TranÂ­siÂ­tion. Her comÂ­mitÂ­ment to activism has led to colÂ­labÂ­oÂ­raÂ­tions with non-profit orgaÂ­niÂ­zaÂ­tions such as The Trevor ProÂ­ject and Trans LifeÂ­line. Her work has appeared on Webtoon, GoComics, BuzÂ­zfeed, and the DisÂ­ney aniÂ­mated show Big City Greens.

Adam Ellis is a comic artist and illusÂ­traÂ­tor who lives in New York City. When heâ€™s not drawÂ­ing heâ€™s watchÂ­ing docÂ­uÂ­menÂ­taries about cults and dreamÂ­ing about one day ownÂ­ing a backÂ­yard big enough to keep chickÂ­ens.

Hope LarÂ­son is a New York Times bestÂ­selling, multiâ€“EisÂ­ner Awardâ€“winÂ­ning carÂ­toonÂ­ist and comics creÂ­ator. She has authored conÂ­temÂ­poÂ­rary midÂ­dle-grade graphic novÂ­els includÂ­ing the Eagle Rock series feaÂ­turÂ­ing All SumÂ­mer Long, All Together Now, and All My Friends, fanÂ­tasy stoÂ­ries such as Salt Magic (illusÂ­trated by Rebecca Mock), adapted Madeleine Lâ€™Engleâ€™s clasÂ­sic, A WrinÂ­kle in Time into a graphic novel, writÂ­ten BatÂ­girl for DC Comics, and co-creÂ­ated the Goldie Vance series for BOOM! StuÂ­dios. Her most recent work, Be That Way, is a YA novel that blends prose, illusÂ­traÂ­tion, and comics. She lives in Asheville, North CarÂ­olina with her famÂ­ily.

Jess Fink is an illusÂ­traÂ­tor and graphic novÂ­elÂ­ist from New York. Author of the award-winÂ­ning graphic novel series Chester 5000 (pubÂ­lished by Top Shelf). Author of We Can Fix It, A Time Travel MemÂ­oir (pubÂ­lished by Top Shelf). IllusÂ­traÂ­tor of the award-winÂ­ning table top role playÂ­ing game Star Crossed (writÂ­ten by Alex Roberts, pubÂ­lished by Bully PulÂ­pit). Their work has been feaÂ­tured in varÂ­iÂ­ous comic antholoÂ­gies and pubÂ­liÂ­caÂ­tions. As an illusÂ­traÂ­tor they mainly focus on romance and humor.

StaÂ­bilÂ­ity AI, founded by Emad Mostaque, is based in LonÂ­don.

StaÂ­bilÂ­ity AI funded LAION, a GerÂ­man orgaÂ­niÂ­zaÂ­tion that is creÂ­atÂ­ing ever-larger image datasetsâ€”withÂ­out conÂ­sent, credit, or comÂ­penÂ­saÂ­tion to the origÂ­iÂ­nal artistsâ€”for use by AI comÂ­paÂ­nies.

StaÂ­bilÂ­ity AI is the develÂ­oper of StaÂ­ble DifÂ­fuÂ­sion. StaÂ­bilÂ­ity AI trained StaÂ­ble DifÂ­fuÂ­sion using the LAION dataset.

StaÂ­bilÂ­ity AI also released DreamÂ­StuÂ­dio, a paid app that packÂ­ages StaÂ­ble DifÂ­fuÂ­sion in a web interÂ­face.

DeviantArt was founded in 2000 and has long been one of the largest artist comÂ­muÂ­niÂ­ties on the web.

As shown by Simon WilliÂ­son and Andy Baio, thouÂ­sandsâ€”and probÂ­aÂ­bly closer to milÂ­lionsâ€”of images in LAION were copied from DeviantArt and used to train StaÂ­ble DifÂ­fuÂ­sion.

Rather than stand up for its comÂ­muÂ­nity of artists by proÂ­tectÂ­ing them against AI trainÂ­ing, DeviantArt instead chose to release DreamUp, a paid app built around StaÂ­ble DifÂ­fuÂ­sion. In turn, a flood of AI-genÂ­erÂ­ated art has inunÂ­dated DeviantArt, crowdÂ­ing out human artists.

MidÂ­jourÂ­ney was founded in 2021 by David Holz in San FranÂ­cisco. MidÂ­jourÂ­ney offers a text-to-image genÂ­erÂ­aÂ­tor through DisÂ­cord and a web app.

Though holdÂ­ing itself out as a â€œresearch labâ€, MidÂ­jourÂ­ney has culÂ­tiÂ­vated a large audiÂ­ence of payÂ­ing cusÂ­tomers who use MidÂ­jourÂ­neyâ€™s image genÂ­erÂ­aÂ­tor proÂ­fesÂ­sionÂ­ally. Holz has said he wants MidÂ­jourÂ­ney to be â€œfocused toward makÂ­ing everyÂ­thing beauÂ­tiÂ­ful and artisÂ­tic lookÂ­ing.â€

To that end, Holz has admitÂ­ted that MidÂ­jourÂ­ney is trained on â€œa big scrape of the interÂ­netâ€. Though when asked about the ethics of masÂ­sive copyÂ­ing of trainÂ­ing images, he said â€œThere are no laws specifÂ­iÂ­cally about that.â€

RunÂ­way was founded in New York in 2018 by AnasÂ­taÂ­sis GerÂ­maniÂ­dis, AleÂ­janÂ­dro MataÂ­mala-Ortiz and CristÃ³bal ValenÂ­zuela. RunÂ­way also employs Patrick Esser, forÂ­merly a memÂ­ber of the ComÂ­pVis research group at LudÂ­wig MaxÂ­iÂ­mÂ­ilÂ­ian UniÂ­verÂ­sity in Munich, where he was a prinÂ­ciÂ­pal develÂ­oper of the techÂ­nolÂ­ogy underÂ­lyÂ­ing StaÂ­ble DifÂ­fuÂ­sion.

Google creÂ­ated and released ImaÂ­gen, an image-difÂ­fuÂ­sion model that Google admits was trained on LAION-400M.

If youâ€™re a memÂ­ber of the press or the pubÂ­lic with other quesÂ­tions about this case or related topÂ­ics, conÂ­tact stablediffusion_inquiries@saverilawfirm.com. (Though please donâ€™t send conÂ­fiÂ­denÂ­tial or privÂ­iÂ­leged inforÂ­maÂ­tion.)

If youâ€™d like to receive occaÂ­sional email updates on the progress of the case, click here to sign up.. A group of artists has filed a first-of-its-kind copyright infringement lawsuit against the developers of popular AI art tools, but did they paint themselves into a corner?

Depending upon who you ask, AI-generated art is either poised to thrust a photorealistic dagger into the throats of millions of commercial artists or is merely a kitschy fad with no lasting aesthetic value. While the truth almost certainly lies somewhere in between, thereâ€™s no denying the legitimate concerns expressed by many artists about the manner in which current AI tools were trained and their potential to disrupt creative livelihoods.

Andersen v. Stability AI Ltd.

On January 13, three of those artists, Sarah Andersen, Kelly McKernan and Karla Ortizâ€”along with a proposed class of potentially millions moreâ€”brought the first copyright infringement lawsuit of its kind (complaint here) against the developers of several popular AI art generation tools. The defendants include Stability AI (developer of Stable Diffusion and DreamStudio), Midjourney (which packaged the open-source Stable Diffusion into its own Discord and web-based applications) and DeviantArt (which recently launched the DreamUp app, also based on Stable Diffusionâ€™s platform). Open AI, which created the popular Dall-E 2 program, wasnâ€™t named in the lawsuit, likely because the content of its training data hasnâ€™t been made public.

Over the past week, the plaintiffsâ€™ lawsuit has been the subject of thousands of articles which have largely parroted the complaintâ€™s key talking point: that AI image generators are nothing more than â€œa 21st-century collage tool that remixes the copyright works of millions of artists whose work was used as training data.â€

After the initial headlines have run their course, plaintiffsâ€™ legal team will be left with the decidedly more difficult task of actually proving their case. Unfortunately, I think members of the artistic community (and putative class members) will be disheartened to learn that many of the facts and legal theories underlying plaintiffsâ€™ allegations donâ€™t square with the soundbites, and may even hurt artists more than they help.

When Artists Attack (Midjourney)

Diffusion Models Donâ€™t Create Collages

As a factual matter, the complaint misrepresents how AI diffusion models like Stable Diffusion actually work. These tools arenâ€™t like some sort of DJ Earworm megamix sampling bits and pieces from an image database and mashing them together. As I noted in an earlier post, the programs use data from training images and their associated text to identify the essential qualities of objects and to discover relationships between their fundamental elements. Once trained, the tools can create, not â€œseemingly new images,â€ as plaintiffs assert, but entirely new works from the ground up.

To use a simple example, letâ€™s say the AI program analyzes data from millions of text-image pairs containing the word â€œtables.â€ The tool will learn that a table is a piece of furniture with one or more legs and a flat top. It will learn that tables most commonly have four legs, but not always; that tables found in a dining room are used for seated persons to eat meals, while tables found next to a bed are used to hold objects like alarm clocks; that tables donâ€™t need to be made from any particular material or have a particular color, etc.

â€œA photorealistic dining table made out of old license platesâ€ (Midjourney)

The tool can then apply its knowledge of tables to the knowledge it has acquired about aesthetic choices, styles and perspectives, all en route to creating a new image thatâ€™s never existed before. Thereâ€™s no â€œcutting and pastingâ€ involved. (If youâ€™re interested in doing a deeper dive into how all of this works, I recommend following Andres Guadamuzâ€™s blog on the topic.)

With a proper understanding of the technology, we can see that the complaintâ€™s repeated description of Stable Diffusion as a â€œ21st-Century collage tool,â€ while perhaps catchy, simply isnâ€™t accurate.

Stable Diffusion Doesnâ€™t Store Copies of Training Images

The complaint also mischaracterizes Stable Diffusion by asserting that images used to train the model are â€œstored at and incorporatedâ€ into the tool as â€œcompressed copies.â€ The current Stable Diffusion model uses about 5 gigabytes of data. None of it includes copies of images. It would be physically impossible to copy the 5 billion images used to train the dataset into a 5GB file in a way that would allow the tool to spit out representations of those images, no matter how small you tried to compress them.

The complaint gives the example of a text prompt for the phrase â€œa dog wearing a baseball cap while eating ice cream,â€ and claims that the system â€œstrugglesâ€ with generating an image based on this prompt because although there are many photos of dogs, baseball caps and ice cream among the training images, there are â€œunlikely to be images that combine all three.â€ A latent diffusion system canâ€™t illustrate this combination of objects, plaintiffs assert, â€œbecause it can never exceed the limitations of its Training Images.â€

This might be true if Stable Diffusion actually had to locate a particular image in the way a search engine does, but because thatâ€™s not how the tool works, it took me about fifteen seconds to type this prompt into Midjourney and get a serviceable result:

The result of my Midjourney prompt â€œA dog wearing a baseball cap while eating ice cream.â€ Iâ€™m not sure whatâ€™s up with the chalupa next to the ice cream.

Reproduction Rights vs. Derivative Rights

With a better understanding of how the technology in fact works, we can turn to the complaintâ€™s legal theories.

Inputs

When it comes to potentially infringing actions committed by the developers of AI tools, the most likely candidate is the initial scraping (i.e., reproduction) of images in order to train the model. Whether this conduct constitutes actionable infringement or is instead a protected fair use will hinge in large part on the courtâ€™s assessment of whether the images were copied and used for a transformative purpose (akin to Googleâ€™s Book Search tool or the Turnitin plagiarism detection software), weighed against the commercial nature of these tools and their ability to negatively impact the licensing market for the underlying works.

As I noted in my prior article on AI art, AI tools arenâ€™t copying images so much to access their creative expression as to identify patterns in the images and captions. In addition, the original images scanned into those databases, unlike Googleâ€™s display of book snippets, are never shown to end users. This arguably makes the use of copyrighted works by Stable Diffusion even more transformative than Google Book Search.

Surprisingly, plaintiffsâ€™ complaint doesnâ€™t focus much on whether making intermediate stage copies during the training process violates their exclusive reproduction rights under the Copyright Act. Given that the training images arenâ€™t stored in the software itself, the initial scraping is really the only reproduction thatâ€™s taken place.

Outputs

Nor does the complaint allege that any output images are infringing reproductions of any of the plaintiffsâ€™ works. Indeed, plaintiffs concede that none of the images provided in response to a particular text prompt â€œis likely to be a close match for any specific image in the training data.â€

Instead, the lawsuit is premised upon a much more sweeping and bold assertionâ€”namely that every image thatâ€™s output by these AI tools is necessarily an unlawful and infringing â€œderivative workâ€ based on the billions of copyrighted images used to train the models.

This allegation is factually flawed and legally suspect; itâ€™s also overreaching in a way that could actually undermine the work of many artists who are members of the proposed class. But before we get there, we need to ask a fundamental question: Whatâ€™s a derivative work?

Letâ€™s Talk About Derivative Works

Subject to fair use and other defenses, a copyright owner has the exclusive right to prepare derivative works based upon the copyrighted work.

You might assume that the concept of a â€œderivative workâ€ under copyright law would be simple to define. Youâ€™d be wrong. Sure, thereâ€™s a definition included in the 1976 Copyright Act itself, but barrels of ink have been spilled by copyright lawyers, scholars and judges trying to make sense of what it actually means.

The Copyright Act Definition is Broad, But . . .

Letâ€™s start with the Copyright Actâ€™s definition:

A â€œderivative workâ€ is a work based upon one or more preexisting works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which a work may be recast, transformed, or adapted. 17 U.S.C. Â§ 101

Notice that a derivative work is defined as one thatâ€™s â€œbased uponâ€ one or more preexisting works. This is likely the concept that plaintiffsâ€™ complaint is latching onto when it asserts that the output produced by AI art tools is â€œnecessarilyâ€ derivative of the dataset of preexisting copyrighted works.

Derivative Works Must Incorporate a Portion of the Underlying Work

While the definition of â€œderivative workâ€ is admittedly broad, it doesnâ€™t mean that any new work thatâ€™s loosely â€œderivedâ€ from another work qualifies as a derivative workâ€”let alone an infringing work. As I noted in a few Twitter and Mastodon threads last week (follow me here and here), whether or not a particular image generated by an AI art tool is a â€œderivative workâ€ is ultimately irrelevant to the question of infringement.

The legislative history of the 1976 Copyright Act explains that to violate an artistâ€™s derivative works right, a new work must incorporate a portion of the underlying work. It gives the example of a musical composition inspired by a novel, which would not normally constitute infringement. In the Ninth Circuitâ€”the jurisdiction in which the class action lawsuit against Stability AI and the other defendants has been filedâ€”the court in Litchfield v. Spielberg held that a work is not an infringing derivative work unless itâ€™s â€œsubstantially similarâ€ to a preexisting work. Only then has the former â€œincorporatedâ€ the latter.

Interestingly, while plaintiffs allege that each image output by Stable Diffusion is â€œnecessarily a derivative workâ€ of all the training images, they concede that â€œnone of the Stable Diffusion output images provided in response to a particular Text Prompt is likely to be a close match for any specific image in the training data.â€ This acknowledgement seems to throw a giant wrench into plaintiffsâ€™ derivative work claim which, as I noted earlier, is really the crux of their lawsuit.

Stable Diffusion can certainly be coaxed into generating a work thatâ€™s derivative of elements found in another workâ€”some of which may be separately copyrightable. In particular, images of popular characters like Mickey Mouse, Homer Simpson and SpongeBob SquarePants that are all over the internet can be easily generated in unique settings. In essence, the tool can learn the essential elements of SpongeBob in the same way it learns the essential attributes of a table.

SpongeBob SquareDetective (Midjourney)

This also explains why fragments of artistsâ€™ signatures or pieces of Getty Imagesâ€™ watermark may show up in output images. There are ways to temper the effect of â€œovertrainingâ€ models on ubiquitous images, but itâ€™s important to remember that even if a generated output image is original, it may still be infringing if it substantially incorporates protectable elements from another work.

It should be clear by now that determining whether particular output images are infringing requires an image-by-image comparison, and a class action lawsuit really isnâ€™t the best vehicle for that sort of thing.

The Danger in Overreaching

Thereâ€™s another, more fundamental problem with plaintiffsâ€™ argument. If every output image generated by AI tools is necessarily an infringing derivative work merely because it reflects what the tool has learned from examining existing artworks, what might that say about works generated by the plaintiffs themselves? Works of innumerable potential class members could reflect, in the same attenuated manner, preexisting artworks that the artists studied as they learned their skill.

If adopted by the court, plaintiffsâ€™ broad assertion would have dire implications for the very artists the lawsuit is trying to protect. Any time artists produce creative works, they draw upon what theyâ€™ve seen and experienced throughout their lives. In this sense, all human creative work is derivative. Many artists specifically work to build upon or remix styles, techniques and even expressions first created by other artists.

A Matter of Style

For this reason, the complaintâ€™s related suggestion that a tool capable of creating art in the â€œstyleâ€ of another artist is unlawful is also dangerous. Style is certainly an element that can and should be considered within an overall substantial similarity analysis, but prohibiting works that are merely â€œinspired byâ€â€”or even copyâ€”preexisting art techniques would artificially stifle human creative development.

The courts that have considered this issue have held that style is an ingredient of expression, but that standing alone, it isnâ€™t protectable. So even if one artist were the first to think up the style of anime, for example, â€œhe could only have a protectible copyright interest in his specific expression of that idea; he could not lay claim to all anime that ever was or will be produced.â€

In another example, a court found that the defendantsâ€™ movie poster unlawfully copied the plaintiffsâ€™ New Yorker cartoon. Thatâ€™s because the two works not only used the same â€œsketchy, whimsicalâ€ style, but did so using the same subject matter, the same theme and even identical concrete details in a substantially similar way. Thatâ€™s improper. On the other hand, one artist is perfectly free to use the same subject and style as another artist â€œso long as the second artist does not substantially copy [the first artistâ€™s] specific expression of his idea.â€

The Right of Publicity Claim

Note that plaintiffs have also asserted a separate right of publicity claim under California law based on Stable Diffusionâ€™s ability to generate outputs in a particular artistâ€™s â€œstyleâ€ when the artistâ€™s name is invoked in a prompt. The reason the tools can recognize certain artistsâ€™ names and styles is the same reason they recognize objects of furnitureâ€”this is factual information thatâ€™s often included in the captions or alt text associated with training images.

Itâ€™s an interesting theory, but I donâ€™t think it ultimately works unless thereâ€™s some evidence that the defendants used the plaintiffsâ€™ names in a manner thatâ€™s directly connected to the promotion of the tools or in some other overtly commercial way. If itâ€™s really just a backdoor attempt to control the artistic works used in the training sets, the claim is likely preempted by the Copyright Act.

Vicarious Liability

Itâ€™s also important to remember that any potentially infringing derivative images output by Stable Diffusion are created in response to prompts input by the individuals using the programs, not by the defendants themselves. While the plaintiffs have asserted a claim for vicarious infringement based on usersâ€™ conduct, it doesnâ€™t strike me as particularly strong.

To be vicariously liable for another personâ€™s copyright infringement, a defendant must have the right and ability to supervise the direct infringer. Stability AI and the other defendants canâ€™t control the prompts that are input by users.

Public-facing tools like Midjourney should be eligible to invoke the DMCAâ€™s â€œsafe harborâ€ provisions with respect to user-generated content in public Discord servers. (Midjourney has set up a notice and takedown system for this purpose.) But software installations of Stable Diffusion on usersâ€™ local machines are a lot like the peer-to-peer software at issue in MGM v. Grokster (which rejected a vicarious liability claim). In that case, the Supreme Court held that â€œnone of the communication between defendants and users provides a point of access for filtering or searching for infringing files, since infringing material and index information do not pass through defendantsâ€™ computers.â€

Also, as I explained a while back in an article about YouTube stream-ripping software, just because a particular tool may be used for an infringing purpose doesnâ€™t automatically mean that the manufacturer faces secondary liability for distributing the tool. The manufacturer would need to take additional action that contributed to or induced infringement by users. Notably, in 1984â€™s â€œBetamaxâ€ opinion, Sony Corp. of America v. Universal City Studios, the U.S. Supreme Court expressly held that so long as a technology is capable of â€œsubstantial noninfringing uses,â€ the manufacturer couldnâ€™t be held liable â€œsimply because the equipment is used by some individuals to make unauthorized reproductions of [plaintiffsâ€™] works.â€

If vicarious liability is to be imposed on Sony in this case, it must rest on the fact that it has sold equipment with constructive knowledge of the fact that its customers may use that equipment to make unauthorized copies of copyrighted material. There is no precedent in the law of copyright for the imposition of vicarious liability on such a theory. Sony Corp. of Am. v. Universal City Studios, Inc., 464 U.S. 417, 419 (1984).

The Limits of Law

The true concern driving plaintiffsâ€™ lawsuit is summarized in the complaintâ€™s introduction, which asserts that â€œPlaintiffs and the Class seek to end this blatant and enormous infringement of their rights before their professions are eliminated by a computer program powered entirely by their hard work.â€

This concern is understandable, but itâ€™s not one that plaintiffsâ€™ lawsuit is equipped to solve. The plaintiffsâ€™ lawyers have stated that â€œEven assumÂ­ing nomÂ­iÂ­nal damÂ­ages of $1 per image, the value of this misÂ­apÂ­proÂ­priÂ­aÂ­tion would be roughly $5 bilÂ­lion.â€ Sure, thatâ€™s a big number in the aggregate, but it would collectively net the three named plaintiffs less than $250 based on the 242 works they claim were used as training images. (Note that McKernan and Ortiz donâ€™t appear to own any registered copyrights, which is required for them to be proper plaintiffs in the first place.) More importantly, no damages award would put the AI-generated genie back in the bottle.

I also have to believe that an â€œethically sourcedâ€ AI tool, trained only with images obtained via an opt-in or licensing model, could be programmed to produce output that would rival that of the models at issue in the lawsuit. Such a tool might avoid the legal or ethical baggage of Stable Diffusion, but would do nothing to stop the proliferation of AI-generated art.

While the law isnâ€™t likely to provide much comfort to the plaintiffs, history may. Consider that way back in 1859, French art critic Charles Baudelaire grumbled that if a then-new technological development were â€œallowed to supplement art in some of its functions, it will soon have supplanted or corrupted it altogether, thanks to the stupidity of the multitude which is its natural ally.â€

Baudelaire was talking about photography, but he may as well have been venting about AI-generated art. The truth is that many new creative innovationsâ€”from the advent of Photoshop, to the widespread adoption of digital photography, to the introduction of 3D computer-generated animationâ€”have been met with resistance. Ultimately, each of these technologies was embraced as a tool, not of displacement or corruption, but supplementation, allowing artists to unlock entirely new forms and styles of creative expression.

Iâ€™ll be keeping a close eye on legal developments in Andersen v. Stability AI Ltd. and will post updates here on Copyright Lately. In the meantime, Iâ€™d love to know what you think about the new lawsuit. Drop me a comment below or @copyrightlately on social media.. A group of artists â€” Sarah Andersen, Kelly McKÂ­erÂ­nan, and Karla Ortiz â€” have filed a class-action lawsuit against Midjourney and Stability AI, companies behind AI art tools Midjourney and Stable Diffusion, and DeviantArt, which recently launched its own artificial intelligence art generator, DreamUp.

The suit alleges that these companies â€œviolated the rights of millions of artistsâ€ by using billions of internet images to use train its AI art tool without the â€œconsent of artists and without compensating any of those artists.â€ These companies â€œbenefit commercially and profit richly from the use of copyrighted images,â€ the suit alleges. â€œThe harm to artists is not hypothetical,â€ the suit says, noting that works created by generative AI art are â€œalready sold on the internet, siphoning commissions from the artists themselves.â€

The three plaintiffs are artists whose work has been used to train these generative AI tools. Andersen is the creator of popular webcomic Sarahâ€™s Scribbles. McKÂ­erÂ­nan is a full-time artist whose works have been featured in galleries and who makes illustrations for comics, books, and games. Ortiz is a concept artist and illustrator whose clients include Marvel Film Studios and Wizards of the Coast.

1/ As I learned more about how the deeply exploitative AI media models practices I realized there was no legal precedent to set this right. Letâ€™s change that.



Read more about our class action lawsuit, including how to contact the firm here: https://t.co/yvX4YZMfrG â€” Karla Ortiz (@kortizart) January 15, 2023

Attorney Matthew Butterick filed the suit, working with Joseph Saveri Law Firm, a California-based firm specializing in antitrust and class-action law. The Stable Diffusion litigation blog describes Stable Diffusion, and generative AI like it, as â€œa parasite that, if allowed to proliferate, will cause irreparable harm to artists, now and in the future.â€

The group is seeking a jury trial and unspecified damages.

AI-generated art has grown in prominence since last summer, as tools like Stable Diffusion and Midjourney became available for broad use. And as AI-generated art has become more prominent, artists have pushed back. AI art tools are trained with billions of images from the internet, and artists often donâ€™t have a way of opting in or out. Stable Diffusion, for example, was trained using LAION-5B, a database of 5.85 billion text-image pairs with sources including Flickr, DeviantArt, Wikimedia, and overwhelmingly, Pinterest. Using an AI art tool like Stable Diffusion is as easy as typing in a thread of words. You can even create an image in the style of an artist whose work was scraped.

Artists have since begun sharing tools and resources for determining whether their work was scraped as part a dataset used to train Midjourney and Stable Diffusion.

Here are two documents containing the names of 100s of artists whose work has been trained on for Midjourney & Stable Diffusion. If you see your name, you have grounds to join the class action lawsuit. Contact stablediffusion_inquiries@saverilawfirm.com - please share w/ artists! â€” Kelly McKernan (@Kelly_McKernan) January 16, 2023

Whether or not AI art tools violate copyright law can be difficult to determine. Images within these massive databases, that these AI tools â€œlearnâ€ from, may be protected by fair use doctrine. As The Verge reported, itâ€™s a complicated matter of evaluating both the â€œinputsâ€ (the images scraped from these databases) and the â€œoutputsâ€ (the images that the AI art generators create), for copyright law violations.

In response to the suit, a Stability AI spokesperson told Polygon, â€œPlease note that we take these matters seriously. Anyone that believes that this isnâ€™t fair use does not understand the technology and misunderstands the law.â€

Polygon has reached out to Midjourney, DeviantArt, and Matthew Butterick and will update this story when they respond.. NEW YORK (AP) â€” Countless artists have taken inspiration from â€œThe Starry Nightâ€ since Vincent Van Gogh painted the swirling scene in 1889.

Now artificial intelligence systems are doing the same, training themselves on a vast collection of digitized artworks to produce new images you can conjure in seconds from a smartphone app.

The images generated by tools such as DALL-E, Midjourney and Stable Diffusion can be weird and otherworldly but also increasingly realistic and customizable â€” ask for a â€œpeacock owl in the style of Van Goghâ€ and they can churn out something that might look similar to what you imagined.

But while Van Gogh and other long-dead master painters arenâ€™t complaining, some living artists and photographers are starting to fight back against the AI software companies creating images derived from their works.

Two new lawsuits â€” one this week from the Seattle-based photography giant Getty Images â€” take aim at popular image-generating services for allegedly copying and processing millions of copyright-protected images without a license.

Getty said it has begun legal proceedings in the High Court of Justice in London against Stability AI â€” the maker of Stable Diffusion â€” for infringing intellectual property rights to benefit the London-based startupâ€™s commercial interests.

Another lawsuit in a U.S. federal court in San Francisco describes AI image-generators as â€œ21st-century collage tools that violate the rights of millions of artists.â€ The lawsuit, filed on Jan. 13 by three working artists on behalf of others like them, also names Stability AI as a defendant, along with San Francisco-based image-generator startup Midjourney, and the online gallery DeviantArt.

The lawsuit alleges that AI-generated images â€œcompete in the marketplace with the original images. Until now, when a purchaser seeks a new image â€˜in the styleâ€™ of a given artist, they must pay to commission or license an original image from that artist.â€

Companies that provide image-generating services typically charge users a fee. After a free trial of Midjourney through the chatting app Discord, for instance, users must buy a subscription that starts at $10 per month or up to $600 a year for corporate memberships. The startup OpenAI also charges for use of its DALL-E image generator, and StabilityAI offers a paid service called DreamStudio.

Stability AI said in a statement that â€œAnyone that believes that this isnâ€™t fair use does not understand the technology and misunderstands the law.â€

In a December interview with The Associated Press, before the lawsuits were filed, Midjourney CEO David Holz described his image-making service as â€œkind of like a search engineâ€ pulling in a wide swath of images from across the internet. He compared copyright concerns about the technology with how such laws have adapted to human creativity.

â€œCan a person look at somebody elseâ€™s picture and learn from it and make a similar picture?â€ Holz said. â€œObviously, itâ€™s allowed for people and if it wasnâ€™t, then it would destroy the whole professional art industry, probably the nonprofessional industry too. To the extent that AIs are learning like people, itâ€™s sort of the same thing and if the images come out differently then it seems like itâ€™s fine.â€

The copyright disputes mark the beginning of a backlash against a new generation of impressive tools â€” some of them introduced just last year â€” that can generate new visual media, readable text and computer code on command.

They also raise broader concerns about the propensity of AI tools to amplify misinformation or cause other harm. For AI image generators, that includes the creation of nonconsensual sexual imagery.

Some systems produce photorealistic images that can be impossible to trace, making it difficult to tell the difference between whatâ€™s real and whatâ€™s AI. And while some have safeguards in place to block offensive or harmful content, experts fear itâ€™s only a matter of time until people utilize these tools to spread disinformation and further erode public trust.

â€œOnce we lose this capability of telling whatâ€™s real and whatâ€™s fake, everything will suddenly become fake because you lose confidence of anything and everything,â€ said Wael Abd-Almageed, a professor of electrical and computer engineering at the University of Southern California.

As a test, the AP submitted a text prompt on Stable Diffusion featuring the keywords â€œUkraine warâ€ and â€œGetty Images.â€ The tool created photo-like images of soldiers in combat with warped faces and hands, pointing and carrying guns. Some of the images also featured the Getty watermark, but with garbled text.

AI can also get things wrong, like feet and fingers or details on ears that can sometimes give away that theyâ€™re not real, but thereâ€™s no set pattern to look out for. Those visual clues can also be edited. On Midjourney, users often post on the Discord chat asking for advice on how to fix distorted faces and hands.

With some generated images traveling on social networks and potentially going viral, they can be challenging to debunk since they canâ€™t be traced back to a specific tool or data source, according to Chirag Shah, a professor at the Information School at the University of Washington, who uses these tools for research.

â€œYou could make some guesses if you have enough experience working with these tools,â€ Shah said. â€œBut beyond that, there is no easy or scientific way to really do this.â€

For all the backlash, there are many people who embrace the new AI tools and the creativity they unleash. Some use them as a hobby to create intricate landscapes, portraits and art; others to brainstorm marketing materials, video game scenery or other ideas related to their professions.

Thereâ€™s plenty of room for fear, but â€œwhat can else can we do with them?â€ asked the artist Refik Anadol this week at the World Economic Forum in Davos, Switzerland, where he displayed an exhibit of climate-themed work created by training AI models on a trove of publicly available images of coral.

At the Museum of Modern Art in New York, Anadol designed â€œUnsupervised,â€ which draws from artworks in the museumâ€™s prestigious collection â€” including â€œThe Starry Nightâ€ â€” and feeds them into a digital installation generating animations of mesmerizing colors and shapes in the museum lobby.

The installation is â€œconstantly changing, evolving and dreaming 138,000 old artworks at MoMAâ€™s archive,â€ Anadol said. â€œFrom Van Gogh to Picasso to Kandinsky, incredible, inspiring artists who defined and pioneered different techniques exist in this artwork, in this AI dream world.â€

Anadol, who builds his own AI models, said in an interview that he prefers to look at the bright side of the technology. But he hopes future commercial applications can be fine-tuned so artists can more easily opt out.

â€œI totally hear and agree that certain artists or creators are very uncomfortable about their work being used,â€ he said.

For painter Erin Hanson, whose impressionist landscapes are so popular and easy to find online that she has seen their influence in AI-produced visuals, the concern is not about her own prolific output, which makes $3 million a year.

She does, however, worry about the art community as a whole.

â€œThe original artist needs to be acknowledged in some way or compensated,â€ Hanson said. â€œThatâ€™s what copyright laws are all about. And if artists arenâ€™t acknowledged, then itâ€™s going to make it hard for artists to make a living in the future.â€

â€”â€”

Oâ€™Brien reported from Providence, Rhode Island.. Have you noticed that many of your friends are suddenly fairy princesses or space travelers? Is your Instagram feed overrun with Renaissance-style paintings of people who were definitely born in the â€™90s? If so, you are entitled to an explanation of what exactly is going on here (and itâ€™s not time travel).

In the past week, users have flocked to Lensa AI, an app that uses your selfies and artificial intelligence to create portraits in a variety of styles. Created by the company Prisma Labs, the app is generating images â€” and controversy.

What is Lensa AI?

Even if you havenâ€™t heard of Lensa AI, youâ€™ve possibly seen its work this week. As of Wednesday, it was the most popular iPhone app in the United States in Appleâ€™s app store. Lensa takes your selfies, studies them and churns out original, computer-generated portraits of you â€” or anyone whose photos you feed it.

Do I have to pay for it?

You do. Right now, you can get 50 avatars â€” 10 images in five styles â€” for $3.99 during a one-week trial period. (For $35.99 you can subscribe to Lensa AI for the year, which gets you a 51 percent discount on future avatars.) â€œMagic Avatars consume tremendous computation power to create amazing avatars for you,â€ according to Lensaâ€™s checkout page. â€œItâ€™s expensive, but we made it as affordable as possible.â€ Fair warning: Prices have been fluctuating as the app has gotten more and more popular and may have changed since this article was published.. A spokesperson for Stability.AI told MIT Technology Review: â€We are listening to artists and the community and working with collaborators to improve the dataset. This involves allowing people to opt out of the model and also to opt in when they are not already included.â€

But Karla Ortiz, an artist and a board member of the Concept Art Association, an advocacy organization for artists working in entertainment, says she doesnâ€™t think Stability.AI is going far enough.

The fact that artists have to opt out means â€œthat every single artist in the world is automatically opted in and our choice is taken away,â€ she says.

â€œThe only thing that Stability.AI can do is algorithmic disgorgement, where they completely destroy their database and they completely destroy all models that have all of our data in it,â€ she says.

The Concept Art Association is raising $270,000 to hire a full-time lobbyist in Washington, DC, in hopes of bringing about changes to US copyright, data privacy, and labor laws to ensure that artistsâ€™ intellectual property and jobs are protected. The group wants to update laws on intellectual property and data privacy to address new AI technologies, require AI companies to adhere to a strict code of ethics, and work with labor unions and industry groups that deal with creative work.. . . Robots would come for humansâ€™ jobs. That was guaranteed. The assumption generally was that they would take over manual labor, lifting heavy pallets in a warehouse and sorting recycling.

Now significant advances in generative artificial intelligence mean robots are coming for artists, too.

A.I.-generated images, created with simple text prompts, are winning art contests, adorning book covers, and promoting â€œThe Nutcracker,â€ leaving human artists worried about their futures.

The threat can feel highly personal. An image generator called Stable Diffusion was trained to recognize patterns, styles and relationships by analyzing billions of images collected from the public internet, alongside text describing their contents.