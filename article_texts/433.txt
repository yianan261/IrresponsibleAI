. Gun violence in Chicago has surged since late 2015, and much of the news media attention on how the city plans to address this problem has focused on the Strategic Subject List, or S.S.L.

The list is made by an algorithm that tries to predict who is most likely to be involved in a shooting, either as perpetrator or victim. The algorithm is not public, but the city has now placed a version of the list — without names — online through its open data portal, making it possible for the first time to see how Chicago evaluates risk.

We analyzed that information and found that the assigned risk scores — and what characteristics go into them — are sometimes at odds with the Chicago Police Department’s public statements and cut against some common perceptions.

■ Violence in the city is less concentrated at the top — among a group of about 1,400 people with the highest risk scores — than some public comments from the Chicago police have suggested.. A police officer stands at the corner of a busy intersection, scanning the crowd with her body camera. The feed is live-streamed into the Real Time Crime Center at department headquarters, where specialized software uses biometric recognition to determine if there are any persons of interests on the street.

Data analysts alert the officer that a man with an abnormally high threat score is among the crowd; the officer approaches him to deliver a “custom notification”, warning him that the police will not tolerate criminal behavior. He is now registered in a database of potential offenders.

Overhead, a light aircraft outfitted with an array of surveillance cameras flies around the city, persistently keeping watch over entire sectors, recording everything that happens and allowing police to stop, rewind and zoom in on specific people or vehicles …

None of this is techno-paranoia from the mind of Philip K Dick or Black Mirror, but rather existing technologies that are already becoming standard parts of policing.

The California city of Fresno is just one of the police departments in the US already using a software program called “Beware” to generate “threat scores” about an individual, address or area. As reported by the Washington Post in January, the software works by processing “billions of data points, including arrest reports, property records, commercial databases, deep web searches and the [person’s] social media postings”.

A brochure for Beware uses a hypothetical example of a veteran diagnosed with PTSD, indicating they also take into account health-related data. Scores are colour-coded so officers can know at a glance what level the threat is: green, yellow or red.

Chicago's computer-generated heat list profiled potential criminals – essentially suspects for crimes not yet committed

This is just one of many new technologies facilitating “data-driven policing”. The collection of vast amounts of data for use with analytics programs allows police to gather data from just about any source and for just about any reason.

The holy grail is ‘predictive policing’

“Soon it will be feasible and affordable for the government to record, store and analyze nearly everything we do,” writes law professor Elizabeth Joh in Harvard Law & Policy Review. “The police will rely on alerts generated by computer programs that sift through the massive quantities of available information for patterns of suspicious activity.”

The holy grail of data-fueled analytics is called “predictive policing”, which uses statistical models to tell officers where crime is likely to happen and who is likely to commit it.

In February 2014, the Chicago police department (CPD) attracted attention when officers pre-emptively visited residents on a computer-generated “heat list”, which marked them as likely to be involved in a future violent crime. These people had done nothing wrong, but the CPD wanted to let them know that officers would be keeping tabs on them.

Essentially, they were already considered suspects for crimes not yet committed.

From Fresno to New York, and Rio to Singapore, data analysts sit at the helm of futuristic control rooms, powered by systems such as IBM’s Intelligent Operations Center and Siemens’ City Cockpit. Monitors pipe in feeds from hundreds or even thousands of surveillance cameras in the city.

Is this really the promise of a ‘smart city’?

These analysts have access to massive databases of citizens’ records. Sensors installed around the city detect pedestrian traffic and suspicious activities. Software programs run analytics on all this data in order to generate alerts and “actionable insights”.

Neither IBM nor the Chicago Police Department responded to a request to comment. But it this is the new model of how policing should be done in an era of “smart cities”?

These analytics can be used to great effect, potentially enhancing the ability of police to make more informed, less biased decisions about law enforcement. But they are often used in dubious ways and for repressive purposes. It is unclear – especially for analytics tools developed and sold by private tech companies – how exactly they even work.

What data are they using? How are they weighing variables? What values and biases are coded into them? Even the companies that develop them can’t answer all those questions, and what they do know can’t be divulged because of trade secrets.

So when police say they are using data-driven techniques to make smarter decisions, what they really mean is they are relying on software that spits out scores and models, without any real understanding of how. This demonstrates a tremendous faith in the veracity of analytics.

Those affected lose their right to individualized treatment, as systems treat them as a mere collection of data points

It is absurd for police not to know what decisions, weights, values and biases are baked into the analytics they use. It obscures the factors and decisions that influence how police operate – even to the police themselves, and means these methods are not open to the scrutiny of public oversight and accountability.

Yet even in the face of controversy, police departments continue to use and defend these technologies when questioned by the public, the media and politicians.

Contrary to the common defense trotted out by technocrats, the judgments produced by the algorithms of big data analytics are not objective arbiters of discretion and punishment. Rather, they easily become ways of laundering subjective, biased decisions into ostensibly objective judgments.

Many researchers and advocates have argued that techniques like predictive policing don’t so much see the future as entrench and justify longstanding practices of racial profiling in policing certain urban neighborhoods, aggressively targeting already disempowered groups.

Those affected lose their right to individualized treatment and understanding, as technical systems treat people as a mere collection of data points.

Moreover, by integrating these technologies deeper into their operations, police departments are opening the way for corporations to have disproportionate influence over what policing means in society. Technologies are not just neutral tools, and they are not divorced from politics; they are designed with certain values and goals in mind. And when large corporations and entrepreneurial startups establish those values and goals, you are likely to end up with policing modeled after technocratic motivations for total efficiency and control.

It is telling that the companies creating these analytics programs sell modified versions of their software to more than just police departments. As the Intercept reported in October 2015, the same types of analytics are used from marketing to the military; in essence, the Pentagon uses a data program to monitor drones in the same way businesses monitor customers.

IBM has its own vision of an internet-connected world, and its “Smarter Planet” concept happens to align with the goals of business, the military and the police. While its program has already been sold to law enforcement, it is also developing systems to process refugees and even predict terrorist attacks.

Many researchers have documented how police forces are increasingly technologized and militarized. Enthusiastic and rapid adoption of data-driven analytics has happened at the expense of meaningful public input, safeguards against misconduct and other basic procedures of accountability.

The question remains: are these technologies accurate, appropriate and legitimate?

Our governments and police departments are already deploying this technology, and need to be held to account. This technology encourages police officers to view the public as a threat, as criminals in waiting. These opaque procedures and questionable outcomes need to be scrutinized, before the efficiencies of connected data are used to justify taking power away from citizens and further concentrating it in police forces.. For the last four years, the Chicago Police Department has kept a list of people they believe are most likely to be involved in a shooting. The list—known as the "heat list" or the Strategic Subject List—was developed using a secret algorithm and contains the names of over a thousand people at any given time.

In record-breaking year for gun violence rates, Superintendent Eddie Johnson has praised the list and the department’s use of big data in predicting crimes. In May, the CPD reported three out of four shooting victims in 2016 were on the Strategic Subject List. The number of people arrested in connection to shootings was even more impressive: 80 percent were on the SSL, they say.

Though the department has been quick to tout the list’s accuracy, there is no way to independently verify their claims. The names on the list are private and the department won’t even explain what variables are used to determine a person’s ranking on the list. In other words, we’ve had to take the department at their word that their big data program works.

That changed this week with the release of a report by RAND Corporation in the Journal of Experimental Criminology. In the first independent audit of the department’s SSL, researchers found a 2013 version of the list to be not nearly as valuable as the department claims.

“Individuals on the SSL are not more or less likely to become a victim of a homicide or shooting than the [control] group,” the authors write. Police use of the list also had no effect on citywide violence levels in Chicago.

While the study’s authors found that individuals on the SSL were indeed more likely to be arrested for a shooting, the researchers guessed that this was happening because officers were using the list as leads to close cases. Superintendent Johnson has said as recently as last month, however, that the list is not being used to target people for arrest.

“One of the major findings [of the study] was that the police on the ground, the people in the field, do not get a lot of training about how to use this list and what it means,” says lead author Jessica Saunders.

When asked for a comment yesterday afternoon, CPD spokesman Anthony J. Guglielmi said he was unaware of the report. In a statement released today, which includes a point-by-point response, police emphasize that the SSL has changed significantly since the 2013 version that is the subject of RAND's analysis.

"The evaluation was conducted on an early model of the algorithm that is no longer in use today… We are currently using SSL Version 5, which is more than 3 times as accurate as the version reviewed by RAND," the statement says.

But Saunders says that her findings can still apply to the tool CPD is using today.

“The findings of this study are probably not changed by making the list better,” says Saunders. “What we really found was that they didn’t know what to do with the list and there was no intervention tied to the list. So in my opinion, it almost doesn’t matter how good the list is, if you don’t know what to do with it.”

Saunders says that the CPD must carefully consider what interventions it uses on people on the list in order to prevent crime. Tactics such as call-ins and home visits, which the CPD sometimes uses in conjunction with the list, cannot be effective if they are not done across the board.

In its official statement, CPD says this intervention strategy has likewise evolved along with the SSL since 2013: they are now used in every police district, and metrics on the interventions are now "fully integrated within our CompStat accountability framework and weekly Compstat meetings."

Still, those who study big data policing say this week’s report from RAND is troubling.

“I think there’s a real question now after [the] RAND [report],” says Andrew Ferguson, a law professor at the University of the District of Columbia in Washington. “We don’t know how effective these lists are except for what the police tell us. This is one of the first analyses of the risk factors.”

Police departments and criminal justice organizations across the country are increasingly using algorithms like Chicago’s to predict the locations and perpetrators of future crimes. And in an era marked by the police shootings of young black men, big data has been held up as a way to avoid racial profiling and reduce violence.

But few cities make their algorithms available to the public or to organizations that work with communities most at risk for violence. This week’s RAND study is one of only two independent evaluations of predictive policing programs that have been done nationwide.

Given the shroud of secrecy that covers big data policing, many have questioned the algorithms’ accuracy and fairness. A ProPublica investigation earlier this year found a risk assessment algorithm used in Florida to have significant racial disparities and to be only slightly more accurate than a coin flip.

The Electronic Frontier Foundation and the American Civil Liberties Union of Illinois have both voiced concerns about how Chicago’s Strategic Subject List handles the issue of race. The Chicago Police Department has said that race is not one of the 11 weighted variables it uses to determine a person’s ranking on the list, but other variables they are using may code for race in less explicit ways. For example, a person’s address in a highly segregated neighborhood in Chicago could indicate a person’s wealth and race.

“The RAND analysis should be the beginning, not the end of determining whether or not these systems work,” says Ferguson. “The underlying idea of prioritizing police resources on those most at risk makes a ton of sense. The downside of getting that prediction wrong means a lot of wasted resources. So I think we need to figure out whether it’s possible to prioritize risk and then really decide whether police are really the right remedy once we’ve identified risks through big data policing.”. . . More on: Human Rights

David O’Connor is an intern for the Digital and Cyberspace Policy Program at the Council on Foreign Relations.

The problem of policing has always been that it’s after-the-fact. If law enforcement officers could be at the right place at the right time, crime could be prevented, lives could be saved, and society would surely be safer. In recent years, predictive policing technology has been touted as just such a panacea. References to Minority Report are apparently obligatory when writing about the topic, but disguise a critical problem: predictive policing isn’t sci-fi; it’s a more elaborate version of existing, flawed practices.

Predictive policing is an umbrella term to describe law enforcement’s use of new big data and machine learning tools. There are two types of tools: person-based and location-based systems. Person-based systems like Chicago’s Strategic Subject List use a variety of risk factors, including social media analysis, to identify likely offenders. A 2015 report stated the Chicago Police D epartment had assembled a list of “roughly 400 individuals identified by certain factors as likely to be involved in violent crime.” This raises a host of civil liberties questions about what degree of latitude police should be granted to perform risk analysis on people with no criminal records. In the future, these questions will become ever more pressing as revelations of threat scores, StingRays and facial recognition technology continue to grab headlines.

In the present, however, the majority of publicly-known predictive policing algorithms are location-based. Twenty of the nation’s fifty largest police departments are known to use such algorithms, all of which rely on historical crime data—things like 911 calls and police reports. Based on data trends, these algorithms direct police to locations that are likely to experience crime at a particular time. Unfortunately, the Department of Justice has estimated that less than half of violent crimes and even fewer household property crimes are reported to the police. An algorithm trying to make predictions based on historical data isn’t actually looking at crime; it’s looking at how police respond to crimes they know about.

This merely reinforces the biases of existing policing practices. In October, the Human Rights Data Analysis Group released a study that applied a predictive policing algorithm to the Oakland Police Department’s drug crime records from 2010. The study found that the algorithm would dispatch officers “almost exclusively to lower income, minority neighborhoods”—despite the fact that drug users are estimated to be widely dispersed throughout the city. The predictive algorithm essentially sent cops to areas they had already made arrests, not identifying new areas where drugs might appear.

The algorithm the researchers analyzed was written by PredPol, one of the largest suppliers of predictive policing systems in the United States, and was chosen for being one of the few algorithms openly published in a scientific journal. PredPol says it uses “only three data points in making predictions: past type of crime, place of crime and time of crime. It uses no personal information about individuals or groups of individuals, eliminating any personal liberties and profiling concerns.” Ironically, these parsimonious standards ensure that the algorithm cannot improve on the historical record; it can only reinforce it.

Some systems, like IBM’s, wisely incorporate other data points like weather and proximity of liquor stores. Unlike PredPol, however, the vast majority of these algorithms are trade secrets and not subject to independent review. The secrecy around the software makes it harder for police departments and local governments to make fully informed decisions. It also bars the public from participating in the decision-making process and sows distrust.

That’s not to say that police departments shouldn’t use software to analyze their data. In fact, a 2015 study found predictive policing technology had significantly aided law enforcement in Los Angeles and Kent, England. In Norcross, Georgia, police claim that they saw a 15 percent reduction in robberies and burglaries within four months of deploying PredPol. The Atlanta Police Department was similarly enthused.

Further development of the technology is inevitable, so local governments and police departments should develop appropriate standards and practices. For starters, these algorithms should not be called ‘predictive.’ They aren’t crystal balls; they’re making forecasts based on limited data. Less obvious data points, like broken streetlights or the presence of trees, should be incorporated to refine these forecasts. As in St. Louis, they shouldn’t be used for minor crimes. Person-based algorithmic forecasts should never be accepted as meeting the reasonable suspicion requirement for detaining an individual, and only data specialists should have access to the software to reduce the chances of abuse. Police should by default incorporate racial impact assessments into their data and programs should be open to academic review. The potential for mission creep is enormous; reviews must be regular to ensure that software is being used appropriately. Most importantly, city governments and police departments should conduct a transparent dialogue with the public about what data is being collected, particularly in the cases of cell phone surveillance and social media analysis, and citizens should be able to see what data has been compiled on them, be it photos or a threat score or biometrics.

It’s a natural choice for police departments with shrinking budgets and huge troves of data to turn to machines for help. However, officials must recognize that nerds won’t solve the problem of policing; data must be a supplement to traditional, people-focused police work. Data-driven predictions have suffered many prominent setbacks in 2016. We should be cautious about using them to arrest people.. William Isaac and Kristian Lum

From Los Angeles to New York, there is a quiet revolution underway within police departments across the country.

Just as major tech companies and political campaigns have leveraged data to target potential customers or voters, police departments have increasingly partnered with private firms or created new divisions to develop software to predict who will commit a crime or where crime will be committed before it occurs. While it may sound eerily reminiscent of the Tom Cruise movie Minority Report, the dream of having a tool that, in theory, could more efficiently allocate policing resources would be a boon for departments struggling with shrinking budgets and increasing pressure to be more responsive to their communities.

Policing the USA

Not surprisingly, a 2012 survey by the Police Executive Research Forum found that 70% of roughly 200 police agencies planned to implement the use of predictive policing technology in the next two to five years. The technology policy institute Upturn found that at least 20 of the 50 largest police agencies currently use predictive policing technology, and another 150 departments are testing the tools. But, in the rush to adopt these new tools, police departments have failed to address very serious Fourth Amendment concerns and questions of whether predictive policing reveals new "hot spots" unknown to the police or simply concentrates police effort in already over-policed communities.

Guilt by association, not evidence

Contrary to Hollywood’s depiction of predictive policing, police departments do not have a team of psychics who see crimes before they occur. In reality, the predictions are made by a computer program that uses historical police records to make estimates about future crime or criminals. Those predictions then become part of policing strategies that, among other things, send police commanders to people's homes to monitor their activity. In some cases, the suspects receiving additional monitoring have yet to be implicated in any crimes.

In the more commonly used place-based predictive policing, departments are provided estimates of potential crime “hot spots” or locations where a high number of recorded crimes are expected to occur. Departments allocate additional officers to patrol these areas. While these examples may seem like only minor modifications to standard policing practices, predictive policing tools bring about a fundamental shift in how police decide that certain people and communities are deserving of greater police scrutiny and who is accountable for these decisions.

Why whites must join fight for black justice: Voices

The Fourth Amendment, largely prohibits unreasonable searches and seizures. Many prominent civil liberties groups such as the American Civil Liberties Union have argued that the growth of predictive policing will allow officers in the field to stop suspects under the guise of “Big Data” rather than more stringent legal standards , such as reasonable suspicion. An even more egregious violation of the Fourth Amendment could come through the use of social media by law enforcement to monitor the contacts of known criminals. That could easily open the door to an even larger network of people being monitored who have actually committed no crime, but are seen by law enforcement as guilty by association. How often do we "friend" or "follow" folks whose past is largely a mystery to us?

Use of algorithms shows little crime reduction

More concerning is the lack of solid, independent evidence that predictive policing tools actually reduce crime or reveal hidden patterns in crime that were not already known to law enforcement. While one study co-written by the founders of the proprietary predictive policing software PredPol reported evidence of a small reduction in property crime through the use of their tool, a study conducted by RAND Corporation found no decline in crime rates after the deployment of a similar tool in Shreveport, La. Another peer reviewed study assessing the Chicago Police department’s algorithm that cranks out a suspicious subjects list, or “heat list,” also found no evidence that the program decreased the number of homicides in the city. What the study did find was that the program disproportionately targeted the city’s black residents.

In our new study, we apply a predictive policing algorithm to publicly available data of drug crimes in the city of Oakland, Calif. Predictive policing offered no insight into patterns of crime beyond what the police already knew. In fact, had the city of Oakland implemented predictive policing, the algorithm would have repeatedly sent the police back to exactly the same locations where police had discovered crime in the past. Through over-policing, these tools have the potential to cause greater deterioration of community-police relations, particularly in already over-policed communities.

While the promise of predictive policing is alluring, the reality is that predictive policing tools raise serious legal and technical issues and appear ineffective at reducing crime.

We need much more public scrutiny and research into the efficacy and community impact of predictive policing tools before they are further deployed across the country. Police departments and private vendors should release findings of their internal assessments (if conducted) as well as related data for third-party replication and assessment. Departments and private vendors should also engage a wider group of community stakeholders when departments conduct internal testing of new predictive tools and aggressively develop intervention and training protocols for patrol officers that minimize negative police interactions if they intend to use predictive policing tools.

The reality is that many police departments – like many government agencies and large businesses – lack the institutional experience and resources to seamlessly evolve into a data-driven organization. This does not mean departments should reject data-driven approaches. Instead they should become more aware of flaws and biases embedded within data and ensure that decision-makers understand the limits of its use.

Future efforts of data-driven policing should be inclusive — a process that leverages all of a city's resources to improve community safety.

William Isaac is a doctoral candidate in the Department of Political Science at Michigan State University. Kristian Lum is the lead statistician at the Human Rights Data Analysis Group.. Struggling to reduce its high murder rate, the city of Chicago has become an incubator for experimental policing techniques. Community policing, stop and frisk, "interruption" tactics — the city has tried many strategies. Perhaps most controversial and promising has been the city’s futuristic "heat list" — an algorithm-generated list identifying people most likely to be involved in a shooting.

The hope was that the list would allow police to provide social services to people in danger, while also preventing likely shooters from picking up a gun. But a new report from the RAND Corporation shows nothing of the sort has happened. Instead, it indicates that the list is, at best, not even as effective as a most wanted list. At worst, it unnecessarily targets people for police attention, creating a new form of profiling.

It unnecessarily targets people for police attention

Funded through a $2 million grant from the National Institute of Justice, the list’s algorithm identifies people by looking not only at arrests, but also whether someone is socially connected with a known shooter or shooting victim. The program also has a kind of pre-crime feature in which police visit people on the list before any crime has been committed.

One of the list’s most promising aspects was that it wasn’t just a police officer who would visit. Social workers would show up, too — employees of the Chicago Violence Reduction Strategy group at John Jay College. The list was designed to let Chicago police engage with at-risk (and potentially dangerous) citizens, but also to provide social services, such as access to counseling, to people who were in danger.

"We want to show them the carrot and the stick," said Christopher Mallette, executive director of the Chicago Violence Reduction Strategy group, in a conversation with The Verge last year. "We want them to know they can get help — but we also want them to know that if they don’t keep in line, there’s a jail cell waiting for them."

"We want to show them the carrot and the stick."

CPD wasn’t shy about touting the importance of the list, later rebranded as the Strategic Subjects List, or SSL. In 2014, the CPD official in charge of the program, Commander Jonathan Lewin, told The Verge: "This will inform police departments around the country and around the world on how best to utilize predictive policing to solve problems. This is about saving lives."

But the study from RAND, which was granted extraordinary access to CPD when it launched the list in 2013, found that the program has saved no lives at all. The RAND researchers were allowed to view the list, sit in on internal meetings, and generally observe how the tool was being used. They discovered that CPD wasn’t using the list as a way to provide social services; instead, CPD was using it as a way to target people for arrest.

"The individuals on the SSL were considered to be ‘persons of interest’ to the CPD," according to the report. "Overall ... there was no practical direction about what to do with individuals on the SSL, little executive or administrative attention paid to the pilot, and little to no follow-up with district commanders."

CPD was using it as a way to target people for arrest.

John S. Hollywood, one of the report’s authors, explained to The Verge that there were as many as 11 different violence reduction initiatives going on within CPD at the time the list was being rolled out. "The list just got lost," he said.

It was no surprise, then, that when Hollywood and his colleagues compiled data to figure out whether the list changed the city’s murder rate or reduced the likelihood that someone on the list might be involved in a shooting, they found it made no significant difference.

"[A]t-risk individuals were not more or less likely to become victims of a homicide or shooting as a result of the SSL, and this is further supported by city-level analysis finding no effect on the city homicide trend," according to the report.

Instead of being used to prevent violence, the list essentially served as a way to find suspects after the fact. "We do find, however, that SSL subjects were more likely to be arrested for a shooting," the report said.

The list essentially served as a way to find suspects after the fact

CPD’s Lewin declined to comment about the report, but CPD issued a press release in response. It stressed that RAND "evaluated a very early version" of the list, "which has since evolved greatly and has been fully integrated with the Department’s management accountability process." It also points out that "the prediction model discussed in the report is the very early, initial model (Version 1), developed in August, 2012. We are now using Version 5, which is significantly improved."

Hollywood agreed that the list was in an early stage when it was evaluated, and that it’s possible that it has improved. (CPD has agreed to allow RAND researchers to evaluate an updated version of the list, Hollywood said.)

The RAND report is significant, however, as a rare look at the effectiveness of a major predictive policing tool that was touted as the future of policing — and may instead be a failed experiment.

"Creating a data-driven ‘most-wanted’ list misses the value."

Andrew G. Ferguson, a law professor and predictive policing expert at the University of the District of Columbia, summarized the problems identified in the RAND report.

"Just creating a data-driven ‘most-wanted’ list misses the value of big data prediction," Ferguson said in an email. "The ability to identify and proactively intervene in the lives of at risk youth is a positive, but you have to commit to the intervention piece.". Photo: Erin Hooley/Chicago Tribune

Chicago is a leader in using predictive policing, the use of data and algorithms to inform its overall strategy and on-the-ground decision making. One such tool that’s been used since 2012 is the Strategic Subject List, a computer model that purports to identify which Chicagoans are most likely to be involved in violence, either as a victim or perpetrator.

The idea of the SSL, city officials say, is to use data and technology to solve some of the city’s most intractable problems. Gun violence is one of them—this July saw 76 gun deaths, putting the city on pace to surpass last year’s homicide number. The official known uses of the SSL, which assigns individuals a score from 1 to 500, are to connect individuals to social services and to serve as an “investigative resource” for police.

If all this sounds vague, it’s because there’s very little information available about how police are using the list. Asked to provide details which social services are offered and how, or how the SSL might assist in investigations, city and police officials either declined to comment or referred reporters to other people who could not answer. In a Chicago magazine analysis, official police documents contradict multiple claims made by city officials, and some officials contradicted one another or the little public data available.

The Chicago Police Department fought to keep the list secret until this summer, when it lost a lengthy legal dispute with the Sun-Times and was forced to release a version of the database based on arrest records from August 2012 to July 2016. Among the main takeaways: only 3.6 percent of people on the SSL were a “party to violence,” meaning they have been involved in a shooting or murder as either the victim or offender.

Illustrations: Patrick Sier

So far only one independent analysis of the SSL has been undertaken; in a study published earlier this year, the RAND Corporation said the program had little impact on violence. At the time, police officials said the version of the SSL used by police had changed significantly since the 2013 version analyzed by RAND, but did not provide details on how.

This secrecy is one reason the tool has made national headlines and drawn comparisons to the dystopian Tom Cruise film Minority Report, where police use technology to predict crimes and arrest people before they commit them. “This is basically government decision-making turned over to an algorithm without any transparency about it,” says Karen Sheley, director of the American Civil Liberties Union of Illinois’ Police Practices Project.

In the past, police Superintendent Eddie Johnson has said the SSL has helped the CPD become “very good at predicting who will be the perpetrators or victims of gun violence,” and officials have doubled down on saying the SSL is only a risk assessment tool.

Chicago magazine delved into five findings about the list.

1. Police say they are not using the list to question or arrest people, but official documents show otherwise

Frank Giancamilli, CPD spokesperson, said in an email response on August 4 that the list was “simply a tool that calculates risk” and “is not used for enforcement and does not establish probable cause for arrest or even questioning.”

But documents show it will have a significant impact on police actions on the ground. In a contract approved this past February to increase CPD’s use of data-driven strategies in enforcement, “predictive analytics and SSL” are described as the driver of a planned “total overhaul” of mission assignments for a pilot program in two police districts. That could include, for example, district sergeants deciding which areas receive specialized units or rapid response cars.

Even before this contract was signed, the department used multiple press releases to highlight arrests of people on the SSL, a position that is contrary to statements that the list is used to help rehabilitate people, discourage them from criminal activity, and connect them with social services. The press releases regularly list the number of people arrested, nature of the charges, and number of arrestees with SSL scores. While the department has insisted the list is not used in enforcement, it denied a public records request from the Sun-Times for the list, claiming “criminals could still use the list to ‘thwart’ the police.” How criminals would “thwart” police in their use of a tool they have stated is only for social service outreach was not made clear.

Since late June, the Chicago Police Department and a city of Chicago spokesperson did not respond to repeated email requests for comments, as well as several phone calls, on how they will deploy resources based on the SSL.

2. Data suggests that more people on the SSL are being arrested than approached for social services

Social service outreach based on the SSL is managed by the Custom Notifications program at John Jay College. The program is under the umbrella of the Chicago Violence Reduction Strategy, a violence intervention model much like that used by CeaseFire (CeaseFire itself, and its parent group Cure Violence, are not affiliated with the SSL), which employs four staff members from John Jay College to apply the group’s intervention framework on the ground in Chicago.

According to executive director of the Chicago Violence Reduction Strategy program, Chris Mallette, the program sends out police officers, community members, and personal friends to convince an individual (typically gang-affiliated) that they are risking their life or their freedom by being involved in violence. “The primary goal is getting to who are the hot people right now, and who do we think is going to be hot in the future,” he says, noting that the SSL is only part of that equation, but that “everybody we talk to has a decent or high Strategic Subject List score.”

In a Department of Justice report describing strategies for custom notifications, Chicago’s program is cited as an example for using social network analysis to determine the “impact players” (people most responsible for driving violence) as targets for intervention. However, the RAND evaluation of the program found this strategy ineffective. In a response to the RAND evaluation, CPD stated that the SSL model had changed, moving away from looking at social networks to only looking at an individual’s interactions with law enforcement.

Regardless, police have declined to release data on how many people are involved in the Custom Notifications program. A Freedom of Information request from Freddy Martinez, a data activist who runs the nonprofit Lucy Parsons Labs, revealed that in 2016, 1,024 notifications were attempted, 558 were completed, and only 26 people attended a call-in, where police officers, social workers, and others offer support services. (Each of the notification attempts, says Mallette, could involve a visit to someone’s house. A completed notification, meanwhile, would include a face-to-face meeting.) The department makes it clear that for those who have interacted with the Custom Notifications program and are later charged with a crime, “the highest possible charges will be pursued.”

To put this in perspective, CPD has stated that 280 individuals with SSL scores were arrested in four gang raids during a six-month span in 2016.

Illustration: Patrick Sier

3. 56 percent of black men in the city ages 20 to 29 have an SSL score

The newest data release about the SSL showed that nearly 400,000 people were assigned scores on the list, using primarily eight variables: age, the number of times an individual has been shot, the number of times they were the victim of assault or battery, gang affiliation, arrests for violent offenses, drugs, or weapons, and whether someone is getting arrested more frequently over time.

(While gang affiliation was a factor in the original data released by the Sun-Times in May, the mayor's office said that the most recent version as of this summer apparently does not. On the other hand, in August, CPD confirmed via email to Chicago that the gang database was one of eight factors used in the algorithm.)

Illustration: Patrick Sier

The Chicago Police Department has stated that the inputs don’t include race, gender or location, and that the resulting scores do not overestimate or underestimate risk for any specific demographics. Still, the outputs show definite patterns. The majority of black men in Chicago ages 20 to 29 have an SSL score, meaning that 56 percent of them are considered at risk for taking part in violence in some way.

Kade Crockford, director of the Technology for Liberty Program at the American Civil Liberties Union of Massachusetts and a regular blogger about policing and technology, says the ACLU is skeptical of police claims that the list is unbiased. “In Chicago, like other large metro areas in the United States, police have focused extra special attention on young black men,” says Crockford. “The danger with the addition of technology is that some people think because a computer has told you to profile a bunch of young black men, this is race-neutral and couldn’t be a racially discriminatory program.”

Crockford says that computer programs and algorithms are as biased as the people who create them, citing examples like facial recognition programs created by white programmers that can’t recognize black people and how Facebook algorithms that determine hate speech favor white men over black children.

The city has declined multiple requests to clarify what happens to people who are placed on the list or have a high SSL number, though documents referenced above indicate that more police may be deployed to areas where individuals have high SSL scores.

4. The list is based on arrests—not convictions

The list uses the four most recent years of CPD arrest records—rather than convictions—as an input for the SSL, meaning people may end up on the list for crimes they have not committed.

Convicted in Cook, a project of Smart Chicago Collaborative’s Civic Works Project, reviewed felony cases filed in Cook County Court between 2005 and 2009 and found that about 6 percent of the felony cases filed never ended in a conviction. The number is relatively low but still significant. Additionally, nationwide about 90 percent of felony convictions result from a plea deal, a matter of contention among criminal justice reformers.

In addition, Crockford and other activists are concerned that individuals who have served their time will continue to be under suspicion. “Even when people have finished their sentences and served their time,” says Crockford, the list shows “you can never really escape the collateral consequences. It makes it really difficult to imagine how someone can escape from the criminal system.”

We also know that there is racial bias in arrest rates. For example, while white and black people do drugs at a similar rate, the latter are arrested and incarcerated at much higher rates for drug-related offenses, even in Chicago where politicians have attempted to curb low-level drug arrests.

5. Arrests are focused in already heavily policed areas

The general pattern of enforcement in Chicago is that more officers patrol the city’s minority and heavily low-income South and West side than other areas of the city.

While geographic information may not be directly included in the Strategic Subject List, the data shows that the most recent arrests of people with an SSL score map directly on the areas that are heavily policed, as evidenced through “contact cards,” the forms that police fill out after a street stop.

That creates a troubling cycle: Are police using SSL scores to determine where officers are assigned, which then leads to more arrests and higher SSL scores? Without more information from the police department about how police use the data, we can’t know.

Note: Counts of Chicago residents with SSL scores were generated by selecting only individuals of a given race, sex, and age whose last listed residential police district was not 31, which is a catchall for non-Chicago districts. Illustration: Patrick Sier

As predictive policing has gained traction around the country, Chicago remains at the forefront. The SSL is a complex example of what happens when data is used to evaluate human behavior, and whether the public can determine how police use that information.

For Miles Wernick, the head of the team of researchers based at the Illinois Institute of Technology who created the algorithm on which the SSL is based, it’s important that people understand that the initial interest of the list was in social service outreach more than in any punitive measures. “It’s really critical that when people use these sort of tools they use them in ways that are appropriate,” says Wernick. “It should never be used to arrest people, harass people, or take any sort of punitive actions based on some computer algorithm.”