Updated 3PM ET

Tesla is taking PR very seriously after one of its vehicles in autonomous mode killed a passenger recently.

The crash occurred at 9:27 AM on Highway 101 near Mountain View, California. Walter Huang was in the driver's seat of the Model X, which was in autonomous mode. The car hit a concrete highway divider, marked with black and yellow chevrons, at full force. Huang didn't take any action. The SUV crumpled like a tin can, and Huang didn't make it.

The investigation into the fatal #Tesla crash continued today with the #CHP & #NTSB digging through the scorched EV ? https://t.co/rfdgY88bn7 pic.twitter.com/vd2YzFmAZ0 — Dean C. Smith (@DeanCSmith) March 29, 2018

Other information has been hard to come by, due to the severity of the damage. So far we don't know if his death was a result of negligence, a fatal nap, or simply being distracted by the fireworks of warning lights, and sounds. But one thing is clear: the crash proves that audio and visual cues on the dashboard could after all be insufficient to prevent a crash.

Huang wasn't the first to die in a Tesla with Autopilot active. In 2016, Joshua Brown crashed his Model S into a truck, marking the first fatal collision while Autopilot was engaged.

The timing for this particular crash isn't exactly ideal (from Tesla's perspective). Uber is already doing damage control after its self-driving car killed a pedestrian in Arizona on March 19, four days before Huang's fatal collision.

Interestingly, officials aren't too pleased about Tesla's PR offensive. On Sunday, a spokesperson for the U.S. National Transportation Safety Board (NTSB) told the Washington Post:

At this time the NTSB needs the assistance of Tesla to decode the data the vehicle recorded. In each of our investigations involving a Tesla vehicle, Tesla has been extremely cooperative on assisting with the vehicle data. However, the NTSB is unhappy with the release of investigative information by Tesla.

Presumably, investigators aren't happy because they'd like to get as much information as they can, then release a report.

But Tesla might have jumped the gun. Not complying with the NTSB's investigation processes and deadlines might end up having their technological advancements (and security improvements) screech to a halt.

After the Uber car's crash, the company was banned from further testing in Arizona (though other companies were allowed to continue). Many people feared that the crash would fray the public's trust in autonomous vehicles, and that largely has not come to pass, at least not yet.

But if the crashes continue, that could change. The market for autonomous cars could dry up before the technology becomes reliable enough to make them widespread.

Tesla's Autopilot is Level 2 autonomy, while Uber's self-driving car is a Level 4. So the technology isn't even really the same. Still, a turn in the tide of public opinion could sweep both up with it.

Autonomous vehicles aren't the best at sharing the unpredictable road with imprecise humans. Yes, once fully autonomous vehicles roll out all over the country and make up 100 percent of the vehicles on the road, American roads will inevitably become safer.

But we're not there yet. If crashes like these keep happening, and the public loses trust, we might never be.

Update: Tesla CEO Elon Musk took to Twitter to respond to comments from NTSB and reiterate Tesla's priorities:

Lot of respect for NTSB, but NHTSA regulates cars, not NTSB, which is an advisory body. Tesla releases critical crash data affecting public safety immediately & always will. To do otherwise would be unsafe. — Elon Musk (@elonmusk) April 2, 2018. . In the fall of 2016, Tesla beamed new software over the air to cars on the road in the United States and elsewhere that added safeguards to its Autopilot system to prevent drivers from looking away from the road or keeping their hands off the steering wheel for long periods of time.

The move came in the wake of a crash in Florida in which an Ohio man died when his Model S sedan hit a tractor-trailer while Autopilot was engaged. Federal investigators found that the driver’s hands had been on the steering wheel for only a few seconds in the minute before the crash.

When the upgrades were released, Tesla’s chief executive, Elon Musk, said the new Autopilot system was “really going to be beyond what people expect” and would make the Tesla Model S sedan and the Model X sport utility vehicle the safest cars on the road “by far.”

Now, however, Tesla’s semiautonomous driving system is coming under new scrutiny after the company disclosed late on Friday that a fatal crash on March 23 in California occurred while Autopilot was engaged.. Tesla confirmed that its self-driving feature, Autopilot, was engaged in a fatal accident that occurred in California. Walter Huang, the driver of the Model X, struck a compressed traffic attenuator and was killed despite having Autopilot engaged. Prior to the data being revealed, the man's family made complaints to local outlet Mercury News that he had reportedly brought his Model X to Tesla several times over complaints about the Autopilot system.

Tesla's Autopilot is far from being crowned a perfect driver. Complaints about the company's latest hardware revision, called AP2, have been made by drivers for many months, often claiming that they felt the system was not as accurate as the company's first iteration, AP1. The updated technology has often shown past difficulty of being able to avoid traveling into other lanes; however, Tesla's over-the-air firmware updates have seemingly improved upon them since the introduction of AP2.

Huang's family reportedly told local news that Huang made several complaints to his local Tesla dealer regarding the vehicle veering off the road with Autopilot engaged. What seemingly makes matters worse is that the dealer was allegedly told that it wasn't just any stretch of road Huang experienced the problem with, but the same stretch of road where the accident occurred.

A Tesla spokesperson told local news that they could find no record suggesting that Huang ever reported the Autopilot performance complaints to Tesla.

The manufacturer's investigation over the crash revealed that Huang had his hands off of the steering wheel when the accident occurred, and despite receiving warnings for over six seconds prior to the crash, no action was said to be taken by the driver. Tesla goes on to defend Autopilot by stating that it is 3.7 times less likely to be involved in a fatal accident than a human driver.. SAN FRANCISCO — A Tesla Model S that crashed into a stopped fire truck at high speed was operating in Autopilot mode, the driver of the car told Utah police officials.

Tesla says it continues to work with police on the investigation, and has not yet released details of the incident based on the car's computer logs.

The driver of the vehicle, a 28-year-old woman from Lehi, Utah, slammed into the truck in South Jordan, Utah on Friday. The woman also told police she was looking at her phone prior to the collision and estimated her speed at 60 mph, which is consistent with eyewitness accounts, according to a police statement issued late Monday.

The result of the violent crash was an accordioned front end for the electric car, but only a broken foot for the driver, according to Sgt. Sam Winkler of the South Jordan Police Department.

The driver of the United Fire Authority mechanic truck was evaluated for whiplash and was not checked into the hospital.

Tesla said the company's previous response to the crash still stood, which noted that Autopilot — a semi-autonomous system that works like a souped-up cruise control — requires constant vigilance and is not meant to take over driving responsibilities while the driver focuses on other chores.

Winkler said that South Jordan police was continuing to investigate the crash, and would be working with Tesla to gather vehicle information from the Model S's computers over the coming days. Police officials also said they were getting technical assistance from National Transportation Safety Board officials.

Eyewitness accounts indicate the Model S did not slow down or swerve as it rammed into the back of the truck, which was stopped at a traffic light in the far right lane.

More:Elon Musk shakes up Tesla as another Model S faces crash queries

More:Tesla crash that killed two Florida teens probed by NTSB investigators

Autopilot has been in the crosshairs of federal crash investigators, dating back to a 2016 crash of Tesla Model S in Autopilot mode that killed its driver after the car failed to stop for a tractor trailer that cut across its path.

More recently, the NTSB was called in to review details of a March crash in which a Tesla Model X slam into a highway divider in Mountain View, Calif. The driver died.

Tesla has said the California driver ignored the car's warnings to take back control of the vehicle. But the driver's family is considering suing on the grounds that Tesla ignored the driver's previously raised concerns about Autopilot acting up on that same stretch of Silicon Valley highway.

NTSB and National Highway Traffic Safety Administration officials also are investigating a recent Tesla Model S crash in Florida in which two teens died and one was injured.

The car hit a concrete barrier at high speed in a residential neighborhood and burst into flames. Autopilot is not thought to be a factor, but investigators are looking into the ensuing battery fire.

Just prior to Utah police announcing that the driver indicated Autopilot had been in use, Tesla CEO Elon Musk posted a series of tweets that emphasized the safety of his product.

"What’s actually amazing about this accident is that a Model S hit a fire truck at 60mph and the driver only broke an ankle," Musk tweeted (although initially reported as an ankle injury, South Jordan officials said the injury was a broken foot). "An impact at that speed usually results in severe injury or death."

Musk also lamented media coverage that he said glossed over the 40,000 annual U.S. road deaths, and acknowledged that while no technology is perfect "a system that, on balance, saves lives & reduces injuries should be released."

Follow USA TODAY tech reporter Marco della Cava on Twitter.. ‘We have never seen this level of damage to a Model X in any other crash.’

Electric carmaker Tesla has confirmed its “Autopilot” feature was engaged during a fatal crash last week, a development set to exacerbate concerns over the safety of futuristic vehicles.

Autopilot is still far from a completely autonomous driving system, which would not require any involvement by a human.

Autopilot is considered part of the second of five levels of autonomous driving, with the fifth being fully autonomous – something once featured in futuristic cartoons but which has moved closer to reality.

Car crash

A Tesla Model X – the latest model – collided with a highway barrier near the town of Mountain View in California on March 23, catching fire before two other cars struck it.

The driver was identified by The Mercury News as a 38-year-old man, Wei Huang, an engineer for Apple. He later died in hospital.

Tesla issued a blog post late Friday saying the driver had activated the Autopilot but ignored several warnings.

“In the moments before the collision … Autopilot was engaged with the adaptive cruise control follow-distance set to minimum,” Tesla said.

“The driver had received several visual and one audible hands-on warning earlier in the drive and the driver’s hands were not detected on the wheel for six seconds prior to the collision.

“The driver had about five seconds and 150 meters (164 yards) of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken.”

Tesla added the reason the car sustained such great damage was because a highway barrier “had been crushed in a prior accident without being replaced”.

“We have never seen this level of damage to a Model X in any other crash,” it said.

The company, founded 15 years ago by Elon Musk, sought to downplay fears over its technology.

“Over a year ago, our first iteration of Autopilot was found by the US government to reduce crash rates by as much as 40 percent,” it said.

Pedestrian killed

In January last year, the US Transportation Department closed an investigation into the fatal 2016 crash in Florida of a Tesla Model S on Autopilot, finding that no “safety-related defect” had caused that accident, the first of its kind.

The latest fatal Tesla crash came the same week a collision involving an autonomous Uber vehicle in Arizona killed a pedestrian and caused that company to temporarily halt its self-driving car programme.

Circumstances of the two crashes are different: Tesla’s Autopilot is a driver assistance feature, while the Uber vehicle was designed to operate autonomously but with a driver behind the wheel to correct mistakes.

Dashcam footage released by police showed that the operator appeared to be distracted seconds before the car hit the woman.

The nonprofit group Consumer Watchdog has argued that autonomous vehicles are not ready for roads and the public should not be put at risk to test such technology.

After the Uber accident, Democratic Senator Richard Blumenthal said, “Autonomous vehicle technology has a long way to go before it is truly safe for the passengers, pedestrians and drivers.”

Competition

Both Uber and Tesla are rivals in the multibillion-dollar drive to develop vehicles which, in the future, will not need any driver intervention.

Among other contenders, General Motors has asked to test a car with no steering wheel on roads beginning next year. Google-owned Waymo is also intensifying its self-driving efforts.

If the final, fifth stage, of autonomous driving is still distant, microprocessor manufacturer NVIDIA unveiled an artificial intelligence platform to enable that goal several months ago.

The system can perform 320 trillion operations a second, completely independently of a vehicle’s passengers.

California-based NVIDIA provided some technology in the Uber car which crashed in Arizona, prompting the chip firm to suspend its road tests pending more information about the incident.. In this Sept. 29, 2015, file photo, Elon Musk, CEO of Tesla Motors Inc., introduces the Model X car at the company's headquarters in Fremont, Calif. For years, Tesla has boasted that its cars and SUVs are safer than other vehicles on the roads, and Musk doubled down on the claims in a series of tweets this week. (AP Photo/Marcio Jose Sanchez, File)

For years, Tesla has boasted that its cars and SUVs are safer than other vehicles on the roads, and CEO Elon Musk doubled down on the claims in a series of tweets this week.

The electric vehicles are under intense scrutiny from federal investigators, who have been looking into post-crash battery fires and the performance of Tesla's Autopilot semi-autonomous driving system. On Wednesday, they traveled to Utah to open another inquiry into a Tesla crash—their fourth this year—in which a Model S slammed into a firetruck that was stopped at a red light.

A look at the tweets and Tesla's past claims about the safety of its vehicles and Autopilot:

MUSK (from his tweets Monday): "According to (National Highway Traffic Safety Administration), there was an automotive fatality every 86M miles in 2017 ((tilde)40,000 deaths). Tesla was every 320M miles. It's not possible to be zero, but probability of fatality is much lower in a Tesla."

THE FACTS: This is based on a Tesla analysis of U.S. fatal crashes per miles traveled in 2017. The company's math is correct on the fatality rate involving all of the nation's 272 million vehicles, about 150,000 of which are Teslas, according to sales estimates from Ward's Automotive. But Tesla won't say how many fatalities occurred in its vehicles or how many miles they were driven.

We don't know of any Tesla fatalities in 2017, but the numbers can vary widely from year to year. There have been at least three already this year and a check of 2016 NHTSA fatal crash data—the most recent year available—shows five deaths in Tesla vehicles.

In this Dec. 2, 2015, file photo, Tesla Motors Inc. CEO Elon Musk delivers a speech at the Paris Pantheon Sorbonne University as part of the United Nations Climate Change Conference in Paris. For years, Tesla has boasted that its cars and SUVs are safer than other vehicles on the roads, and CEO Elon Musk doubled down on the claims in a series of tweets this week. (AP Photo/Francois Mori, File)

Statistically, experts say Musk's tweet analysis isn't valid. While Teslas could have a lower death rate, it may speak more about the demographics of Tesla drivers than it does about safety of the vehicles, says Ken Kolosh, manager of statistics for the National Safety Council.

Expensive Teslas tend to be driven by middle-age affluent people who are less likely to get in a crash than younger people, Kolosh said. Also, Tesla drivers tend to live in urban areas and travel on roads with lower speeds, where fatality rates are lower, he said.

Musk also is comparing a fleet of older, less-expensive vehicles to his newer and more costly models, Kolosh said. Most Teslas on the road are six years old or less. The average vehicle in the U.S. is 11.6 years old, according to IHS Markit. Older, less-expensive vehicles often aren't maintained like newer ones and would have more mechanical problems.

___

MUSK (from his tweets Monday in reference to the Utah crash): "What's actually amazing about this accident is that a Model S hit a fire truck at 60 mph and the driver only broke an ankle. An impact at that speed usually results in severe injury or death."

THE FACTS: It's true that the driver in the Utah crash sustained minor injuries considering how fast her car was traveling. The same is true for a January freeway crash near Los Angeles in which the driver was not hurt. But not all Tesla crashes end the same way.

In this April 15, 2018, photo, unsold 2018 models sits amid a series of charging stations on a Tesla dealer's lot in the south Denver suburb of Littleton, Colo. For years, Tesla has boasted that its cars and SUVs are safer than other vehicles on the roads, and CEO Elon Musk doubled down on the claims in a series of tweets this week. (AP Photo/David Zalubowski, File)

In March, the driver of a Tesla Model X was killed in California when his SUV hit a barrier while traveling at "freeway speed." NHTSA and the National Transportation Safety Board are investigating that case, in which the Autopilot system was engaged. Autopilot was also engaged in the Utah crash, according to a summary of data from the car.

Last week, the NTSB opened a probe into an accident in which a Model S caught fire after crashing into a wall at a high speed in Florida. Two 18-year-olds were trapped in the vehicle and died in the flames. The agency has said it does not expect Autopilot to be a focus of that investigation.

___

TESLA (from a March 30 press release): "Over a year ago, our first iteration of Autopilot was found by the U.S. government to reduce crash rates by as much as 40 percent."

THE FACTS: The government says it did not assess how effective Autopilot is at reducing crashes. It did mention a 40 percent reduction in crash rates after "Autosteer" was installed in Tesla vehicles, based on data provided by Tesla. Autosteer is the part of Autopilot that keeps the car centered in a lane and can change lanes automatically. NHTSA said it did a "cursory" comparison of crash rates between vehicles with and without Autosteer, but it didn't consider whether drivers were actually using Autosteer, which has to be manually activated.

___

In this April 15, 2018, photo, the sun shines off the rear deck of a roadster on a Tesla dealer's lot in the south Denver suburb of Littleton, Colo. For years, Tesla has boasted that its cars and SUVs are safer than other vehicles on the roads, and CEO Elon Musk doubled down on the claims in a series of tweets this week. (AP Photo/David Zalubowski, File)

TESLA: The company has touted on its website and in press releases that the Model S sedan scored the highest numerical rating of any vehicle in NHTSA's crash tests, and that the Model X was the first SUV to get a five-star rating in every category.

THE FACTS: It's true that the Model S and Model X got five-star crash-test ratings from NHTSA, and the Model S did have the highest numerical score of any vehicle. But in more demanding tests by the Insurance Institute for Highway Safety, the Model S failed to get the industry group's coveted "Top Safety Pick" or "Top Safety Pick Plus" ratings.

The reasons: the Model S got an "Acceptable" rating in a front-end small offset crash test that mimics when the front driver-side corner of a vehicle collides with a tree or another vehicle. Its headlights also were rated "Poor." Vehicles have to get the highest rating of "Good" in five crash tests to be top safety picks. Fourteen large cars from other manufacturers received Top Safety Pick or Top Safety Pick Plus ratings. IIHS has not yet done crash tests on Tesla's Model X or Model 3.

The Model S also had a low rate of medical insurance claims for injuries, tying for seventh in IIHS's most recent rankings. The institute gave it a score of 46, which is 54 percent better than the average score of 100. The Toyota Camry, the top-selling car in America, scored 112. But the Model S had higher collision claim frequencies and was more expensive to fix than gas-powered large luxury cars.

© 2018 The Associated Press. All rights reserved.. It’s the year 2025. Your driverless car has just crashed into a tree at 55mph because its built-in computer valued a pedestrian’s life above your own. Your injuries are the result of a few lines of code that were hacked out by a 26-year-old software programmer in the San Francisco Bay Area back in the heady days of 2018. As you wait for a paramedic drone, bleeding out by the roadside, you ask yourself – where did it all go wrong?

The above scenario might sound fanciful, but death by driverless car isn’t just inevitable – it’s already happening. Most recently, an Uber self-driving car hit and killed a pedestrian in Arizona, while in May 2017, semi-autonomous software failed in a similarly tragic way when Joshua Brown’s Tesla Model S drove under the trailer of an 18-wheel truck on a highway while in Autopilot mode.

Tesla admits that its system sensors failed to distinguish the white trailer against a bright sky, resulting in the untimely death of the 40-year-old Floridian. But Tesla also says that drivers need to keep their hands on the wheel to stop accidents like this from happening, even when Autopilot is activated. Despite the name, it’s a semi-autonomous system.

Uber, on the other hand, may not be at fault, according to a preliminary police report, which lays the blame on the victim.

It’s a sad fact that these tragedies are just a taste what’s to come. In writing this article, I’ve realised how woefully unprepared we are for the driverless future – expected as soon as 2020. What’s more worrying is that this future is already spilling out into our present, thanks to semi-autonomous systems like Tesla’s Autopilot and Uber’s (now halted) self-driving car tests.

Tomorrow’s technology is here today, and with issues like ethics and liability now impossible to avoid, car makers can’t afford not to be ready.

What happens when a driverless car causes an accident and, worse still, kills someone?

Understanding death by computer

To tackle liability, we need to ask how and why a driverless car could kill someone. Unlike humans, cars don’t suffer fatigue, they don’t experience road rage, and they can’t knock back six pints of beer before hitting the highway – but they can still make mistakes.

Tesla’s Model S features semi-autonomous Autopilot technology

Arguably, the most likely cause of “death-by-driverless-car” would be if a car’s sensors were to incorrectly interpret data, causing the computer to make a bad driving decision. While every incident, fatal or otherwise, will result in fixes and improvements, tracing the responsibility would be a long and arduous legal journey. I’ll get to that later.

The second possible cause of death-by-driverless-car is much more difficult to resolve, because it’s all about ethics.

Picture this scenario: You’re riding in a driverless car with your spouse, travelling along a single-lane, tree-lined B-road. There are dozens upon dozens of B-roads like this in the UK. The car is travelling at 55mph, which is below the 60mph national speed limit on this road.

A blind bend is coming up, so your car slows down to a more sensible 40mph. As you travel around the bend, you see that a child has run out onto the road from a public footpath hidden in the trees. His mother panicked and followed him, and now they’re both in the middle of the road. It’s a windy day and your car is electric, so they didn’t hear you coming. The sensors on your car didn’t see either of them until they were just metres away.

There’s no chance of braking in time, so the mother and child are going to die if your car doesn’t swerve immediately. If the car swerves to the left, you go off-road and hit a tree; if the car swerves right, you hit an autonomous truck coming in the opposite direction. It’s empty, so you and your spouse would be the only casualties.

In this situation, the car is forced to make a decision – does it hit the pedestrians, almost certainly killing them, or does it risk the passengers in the probability that they may survive the accident. The answers will have been decided months (or even years) before, when the algorithms were originally programmed into your car’s computer system – and they could very well end your life. While the car won’t know it, this is in effect an ethical decision.

To demonstrate the overwhelming difficulty of coding ethics, have a go at the Massachusetts Institute of Technology’s Moral Machine. It’s a quiz that aims to track how humans react to moral decisions made by self-driving cars. You’re presented with a series of scenarios where a driverless car has to choose between two evils (i.e. killing two passengers or five pedestrians) and you have to choose which one you think is most acceptable. As you’ll quickly realise, it’s really hard.

Which scenario would you choose? MIT’s Moral Machine is a self-driving nightmare

If all of this scares you, you’re not alone. In March, a survey by US motoring organisation AAA revealed that three out of four US drivers are “afraid” of riding in self-driving cars, and 84% of those said that was because they trusted their own driving skills more than a computer’s, in spite of overwhelming evidence that suggests that driverless cars are significantly safer – human error is the biggest killer on our roads, after all.

So why are we so afraid? I asked Prof. Luciano Floridi, Professor of Philosophy and Ethics of Information at the University of Oxford, that very question.

“I think it’s an ancestral fear of the unknown, fear of machines. It was the Golem and Frankenstein, and now it’s a technology that we don’t fully control in the case of driverless cars,” Professor Floridi tells me. “It also seems to be, perhaps, a mis-selling on the side of the industry, where total safety is sold as a possibility, which we know is not the case. Nothing is 100% safe, you can only be safer.”

So even though driverless cars are safer and reduce a person’s chance of death, drivers struggle to accept the idea that they could be killed by a machine, rather than by human error.

The greater good vs your safety – who wins?

The Massachusetts Institute of Technology recently surveyed 2,000 people and found that 76% of respondents would expect a driverless car to prioritise the safety of a group of ten pedestrians over a single passenger. Very noble, right? In ethics, this is called “utilitarianism” – that’s shorthand for when decisions are made for the greater good.

But humans are animals, and that means evolution has wired us to protect ourselves and those who are important to us, often at the expense of the “greater good”, whatever that may be.

Tellingly, when MIT’s respondents were asked to rate the morality of that same driverless car – the one that would crash and kill its owner to save the pedestrians – as if they themselves were a passenger in the vehicle, the morality rating of the “ethical” car dropped by a third.

The moral of the story is no great surprise – we all like the idea of utilitarian driverless cars, until we have to ride in one. This self-preserving belief system will have an acute effect on the driverless car industry. For instance, what if car makers start selling their vehicles on an ethical basis? Could some cars be designed to always strive for a “greatest good” result, while others are built to act in the interests of the driver or passengers?

But, while Professor Floridi believes ethics will play some part in marketing driverless vehicles, he isn’t convinced they’ll be sold based on their moral code:

“That would be such a bad move on the advertising side of things. What we will see, however, is some ethical component in advertising at large. If you remember when cars started being advertised as safer because of safety belts, companies saying it was so much safer if you’re in an accident. It will be like that. Not that this ethical code is better than that ethical code.”

Professor Luciano Floridi thinks ethics is one of the biggest roadblocks to the driverless car future

Are driverless cars impossibly complicated?

Unfortunately for us all, Floridi tells me, driverless cars are ill-equipped to tackle such complex decisions. He believes that despite all of the research and development efforts focused on bringing driverless cars onto our roads, computers are inherently unable to handle the complex moral decisions that driving demands.

“The ethics is delicate, too subtle, too open-ended for an ordinary driverless car,” he explains. “The car I need to go from the countryside to London, or from London back to Rome, as I did when I was a young guy with not much money – can you imagine that journey? Around 2,000 miles from Oxford to Rome and back? In a context where anything could happen, it’s very hard to imagine a totally uncontrolled environment with a driverless car being successful, without me having a chance to pick up the wheel.”

Floridi says that it’s “impossible” to create an algorithm that can emulate human ethics on the road, because there are an infinite number of possible scenarios that can arise when driving. He gives the example of a cat in the road:

“You could say ‘stop every time’ there’s a cat – unless what? Unless there’s a big truck driving at 70mph behind you, and you have your kids in the car. At that point you say ‘go and kill the cat’, because your family is at risk. You can multiply this with ‘only if’ and ‘unless’ to an endless number of conditions.”

Why is it so difficult? Well, it’s very easy to computerise a closed world, where we can understand every single condition. But driving is an open-world scenario, where the conditions are random and impossible to predict, which makes it very hard to computerise. It’s because of this that Floridi believes there’s no hope for driverless car software to be sufficiently ethically competent, at least on existing road networks:

“Imagine playing chess with someone who picks up a gun at some point, out of his pocket, and says ‘you can’t make that move,’” jokes Floridi. “That’s a different kind of chess. No computer can do anything about it, no matter what we’ve put into it.”

Floridi says that to tackle this, car companies are already consulting with ethics experts to make sure their software is morally watertight. I asked for an example, and he told me that he attended a meeting with Audi “where ethical issues were the main topic precisely because of the implications of driverless cars. As any advanced technology that we develop requires, at some point the social impact forces an ethical consideration.”

So who pays up?

OK, so we’re back at the roadside, and the paramedic drone has finally arrived. The bleeding has stopped, you’re lucid once again, and you’re ready to tackle the issue of compensation. You didn’t crash your car; your car crashed itself – who is paying up?

“In criminal law and in civil law, the issue is who is in control of the vehicle,” Matthew Claxson, a road injury lawyer at Slater & Gordon, tells me. “So the operator could say, ‘Well, I was reading a newspaper and relied upon the computer to take control of the journey,’ and there would be an argument that the manufacturer of the car would have the liability. Whether they pursue further claims against software manufacturers remains to be seen.”

Related: Apple CarPlay vs Android Auto

If a car part – like a gearbox, for instance – fails in the UK today, Claxson explains, and the fault was the result of dodgy manufacturing, and that car goes on to kill someone, the estates of the person who died would “usually sue the car driver,” or the car operator in the case of a driverless car. The driver’s insurance company then settles the claim, but would then go on to pursue a further action against the car manufacturer “behind the scenes, to recover their loss.”

“It’s one of the advantages of the UK insurance policy that it gives recourse to those who have been seriously injured or killed to a financial settlement that can either assist with the medical treatment, or pay for the funeral costs, or assist the bereaved families in some way,” says Claxson.

But there’s one issue that throws a spanner in the works, and that’s hacking. Earlier this year, it was discovered that the Mitsubishi Outlander hybrid could be controlled by hackers thanks to a vulnerability in the car’s on-board Wi-Fi system. And more recently, Fiat Chrysler had to issue a voluntary recall for 1.4 million vehicles after a hack that could kill the engine remotely was discovered.

Driverless cars could be hacked and crashed – so who pays up then?

So if a driverless car kills somebody because it’s been hacked, then it doesn’t seem fair to pin liability on the driver or the car maker. In that case, who pays up? Claxson suggests the following solution:

“We have a process in this country where if a car is stolen, and it kills or causes injury to somebody, then that thief disappears (i.e. it’s a hit-and-run), we have an organisation known as the Motor Insurers Bureau, which is a fund of last resort that’s set up by the Secretary of Transport in this country. That would step in for those hit-and-run victims and compensate them if they are the innocent party.”

He continues: “One would hope to see a similar mechanism like that in the future, that if a car is hacked and it’s not the fault of the manufacturer or driver, you would hope there would be a similar fund to assist those victims.”

Driverless cars are a liability nightmare

The message here is simple: It’s pretty easy to assign liability when cars are fully autonomous. And even in the case of hacking, compensation can still be paid out. But what happens when a car isn’t fully autonomous, and the driver can still control the vehicle?

After all, when Joshua Brown died after his car’s sensors failed earlier this year, Tesla quickly reminded us of its advice that Autopilot “is an assist feature that requires you to keep your hands on the steering wheel at all times.” Tesla also added: “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert.”

The starting point is working out exactly how autonomous a vehicle is. There are plenty of different methods to do this, but the most popular (so far, anyway) is the SAE system, which has six different levels of vehicle automation:

Level 0 – No Automation – The full-time performance by the human driver of all aspects of driving

Level 1 – Driver Assistance – The driving mode-specific execution of accelerating, decelerating, or steering, using information about the environment

Level 2 – Partial Automation – The driving mode-specific execution by one or more driver assistance systems of both steering and acceleration/deceleration

Level 3 – Conditional Automation – Performance by an automated driving system of all aspects of the driving task, with the expectation that a human driver will respond to a request to intervene

Level 4 – High Automation – The performance by an automated driving system of all aspects of driving, even if a human driver doesn’t respond to a request to intervene

Level 5 – Full Automation – The full-time performance by an automated driving system of the entire driving task, under all road and environmental conditions that can be managed by a human driver

Related: What is Tesla Autopilot?

Even new cars aren’t hitting level 5 autonomy yet, and there are plenty of cars on the road that still haven’t made it past level 0. That means that under existing law, if your car has an accident because you didn’t have your hands on the wheel, even in a semi-autonomous car, you’re almost certainly liable, at least according to Adrian Armstrong, of the Collision Investigation team at forensics firm Keith Borer Consultants:

“Currently, it’s the driver’s fault. All of the safety features that are added, take the Tesla for example, are designed to be an addition to the driver’s input. They are not supposed to replace the driver’s awareness.”

But as the Tesla case has proved, as cars become more autonomous, it becomes significantly more difficult to assign liability.

Nvidia’s DRIVEnet demo, which shows car sensors tracking objects in real-time

The UK Department of Transport currently suggests that liability for a vehicle in autonomous mode rests with the car maker, but when the driver has regained control, the driver should assume liability instead.

The problem is that it’s tough for drivers to stay mentally engaged when they’re not physically engaged, which makes that ‘handover’ period difficult to resolve in terms of liability. Some studies have shown that it can take as long as 35 to 40 seconds for drivers to take effective control of a car when switching back from autonomous mode.

Insurance companies are (unsurprisingly) very keen to crack the matter of liability with awkward situations like the handover dilemma. One insurance firm that’s already making significant headway in the space is AXA, as Daniel O’Byrne, the company’s Public Affairs Manager for the UK, explains:

“We are a partner in three of the government-funded trials that are looking to bring driverless or connected or autonomous vehicles to fruition. We’re taking the modified Bowler Wildcat, which is already capable of being driven under remote control, and was developed militarily by BAE. We’re trying to take it to the next level with the sensors and the decision-making process, and where we come in is looking at that from an insurance liability perspective.”

O’Byrne says that the company is particularly interested in how liability is assigned during the handover period. After all, he tells me the government “eventually wants you to be able to read a book or check e-mails on an iPad”, adding: “I’d be very surprised if by 2020, you can’t just press a button in your car and start reading a book.” But that is a huge problem, because it means the “driver” isn’t paying attention to the road environment:

“If the car hands over control to the human, how long is it reasonable to allow that driver to assume liability again? Can a driver go from reading a book to driving at X miles an hour in five seconds? That’s probably unreasonable. We are looking at that behavioural aspect, the universities are looking at the psychology, and how people react to that handover.”

Watch: Autonomous liability debated

Unlike in the US, there’s no law in the UK that says you have to keep your hands on the wheel. What we have in Britain is the Road Traffic Act, which only compels drivers to pay “due care and attention.” That sounds like frustratingly broad phrasing, but O’Byrne believes this ambiguity could actually give the UK an advantage against other countries when it comes to building a legislative framework.

“We were signatories to, but not ratifiers of, the Vienna Convention,” says O’Byrne, referencing the Vienna Convention on Road Traffic, which says that drivers must always keep their hands on the wheel. “And, in a nutshell, whilst the Road Traffic Act says you have to pay due care and attention, we are not as prescriptive as other countries saying that you have to keep your hands on the wheel at all times, which potentially allows us an easier change in the law than, say, Germany.”

The Venturer Consortium, which is the driverless car trial group that AXA is part of, is currently testing driver’s performance during the handover process in different scenarios. The results of this trial are going to be extremely useful in working out who is liable, and will hopefully reveal how long a driver reasonably has to regain full control of a vehicle.

Once the insurance companies and legislators come to a joint conclusion on this, it should be easier to set a standard length of time for drivers to regain control of a driverless vehicle. That should make it significantly easier to determine who’s liable, because the switchover from autonomous systems to driver control can easily be tracked on a software level. And that means assigning blame is almost as easy as it is today.

Bowler Wildcats are currently modified for driverless car tests in the UK

Are driverless cars worth all this effort?

Let’s be fair: When industry giants like Tesla, Google, and Ford tell you that we’ll be safer with driverless cars, they’re absolutely right. As cars begin to rely less on human input, fewer humans will die on roads.

According to OECD data, a fatality occurs once for every 53.4 million miles driven by cars, as an average across 20 EU countries sampled. In the United States, better-than-average road safety standards reduce that figure to one fatality per 94 million miles. But already, over 130 million miles have been driven with Tesla’s Autopilot activated, with just a single fatality on the books.

Part of the allure of driverless cars is the fact that it snuffs out human error, which AXA believes accounts for 94% of all driving accidents. The CDC says over 30% of road fatalities in the United States involve alcohol. And in the United Kingdom, the Department for Transport puts that figure at around one in six. If we wake up tomorrow and every car is autonomous, drink driving disappears overnight.

It gets even better: According to the Eno Centre for Transportation, if 90% of all vehicles on the roads in the US were autonomous, the number of road accidents would fall from 5.5 million per year to just 1.3 million. That future is not so distant either, with the Institute of Electrical and Electronics Engineers predicting that by 2040, around 75% of all vehicles will be fully autonomous.

The biggest challenge to driverless cars going mainstream is getting the public on side. Without proper solutions for the issues around ethics and liability, public opinion is sure to stay frosty for the foreseeable future.

Car makers can talk up the technology until they’re blue in the face, but if a manufacturer can’t tell a grieving wife why her husband’s hatchback just drove him off a cliff – or even cough up the compensation – then driverless cars will probably be passenger-less too.

This article was originally published in 2016. At that point, Tesla did not respond to our interview request.

Would you welcome a driverless car future? Tell us your thoughts by tweeting us @TrustedReviews.. . Consumer-safety advocates and autonomous-vehicle experts criticized Tesla Inc. for issuing another statement about the death of a customer that pinned the blame on driver inattentiveness.

Days after publishing a second blog post about the crash involving Walter Huang, a 38-year-old who died last month in his Model X, Tesla issued a statement in response to his family speaking with San Francisco television station ABC7. The company said the “only” explanation for the crash was “if Mr. Huang was not paying attention to the road, despite the car providing multiple warnings to do so.”

“I find it shocking,” Cathy Chase, president of the group Advocates for Highway and Auto Safety, said by phone. “They’re claiming that the only way for this accident to have occurred is for Mr. Huang to be not paying attention. Where do I start? That’s not the only way.”

Groups including Advocates for Highway and Auto Safety and Consumer Reports have criticized Tesla for years for naming its driver-assistance system Autopilot, with the latter calling on the company to choose a different moniker back in July 2016. The two organizations share the view of the National Transportation Safety Board, which has urged carmakers to do more to ensure drivers using partially autonomous systems like Autopilot remain engaged with the task of driving. The U.S. agency is in the midst of two active investigations into Autopilot-related crashes.

It’s Tesla’s responsibility to provide adequate safeguards against driver misuse of Autopilot, including by sending visual and audible warnings when the system needs a human to take back over, Chase said. “If they’re not effective in getting someone to re-engage -- as they say that their drivers have to -- then they’re not doing their job.”

High Stakes



The stakes for Tesla’s bid to defend Autopilot are significant. The NTSB’s investigation of the March 23 crash involving Huang contributed to a major selloff in the company’s shares late last month. Chief Executive Officer Elon Musk claimed almost 18 months ago that the system will eventually render Tesla vehicles capable of full self-driving, and much of the value of the $51 billion company is linked to views that it could be an autonomous-car pioneer.

From October: Musk spends a year being wrong about self-driving Teslas

Tesla has declined to say how long drivers can now use Autopilot between visual or audible warnings to have a hand on the wheel. It’s also refused to comment on how many alerts can be ignored before the system disengages, what version of Autopilot software was in Huang’s Model X, or when the car was built.

“Just because a driver does something stupid doesn’t mean they -- or others who are truly blameless -- should be condemned to an otherwise preventable death,” said Bryant Walker Smith, a professor at the University of South Carolina’s School of Law, who studies driverless-car regulations. “One might consider whether there are better ways to prevent drivers from hurting themselves or, worse, others.”

Under Investigations



The NTSB is looking into the crash that killed Huang, as well as a collision in January involving a Tesla Model S that rear-ended a fire truck parked on a freeway near Los Angeles with Autopilot engaged. The agency said after Tesla’s second blog post about the Huang incident that it was unhappy with the company for disclosing details during its investigation.

In its latest statement, Tesla said it is “extremely clear” that Autopilot requires drivers to be alert and have hands on the steering wheel. The system reminds the driver this every time it’s engaged, according to the company.

“Tesla’s response is reflective of its ongoing strategy of doubling down on the explicit warnings it has given to drivers on how to use, and not use, the system,” said Mike Ramsey, an analyst at Gartner Inc. “It’s not the first time Tesla has taken this stance.”

Huang’s wife told ABC7 he had complained before the fatal crash that his Model X had steered toward the same highway barrier he collided with on March 23. The family has hired Minami Tamaki LLP, which said in a statement Wednesday that it believes Tesla’s Autopilot is defective and likely caused Huang’s death. The San Francisco-based law firm declined to comment on Tesla’s statement.

Crash-Rate Claim



The National Highway Traffic Safety Administration, which has the power to order recalls and fine auto manufacturers, found no defect after investigating the May 2016 crash involving a Tesla Model S driven on Autopilot by Josh Brown, a former Navy SEAL. The agency closed its probe in January 2017.

According to data Tesla gave NHTSA investigators prior to its decision against any recall, Autopilot’s steering system may prevent the rate of crashes per million miles driven by about 40 percent, a figure the company cited in its latest statement.

“We empathize with Mr. Huang’s family, who are understandably facing loss and grief, but the false impression that Autopilot is unsafe will cause harm to others on the road,” Tesla said. “The reason that other families are not on TV is because their loved ones are still alive.”

Neither Tesla nor NHTSA has released the underlying data to support the crash-rate reduction claim.

“Tesla explicitly uses data gathered from its vehicles to protect itself, even if it means going after its own customers,” said Ramsey, the Gartner analyst.

by Dana Hull, Ryan Beene and Keith Naughton. On Sunday night, a self-driving car operated by Uber struck and killed a pedestrian, 49-year-old Elaine Herzberg, on North Mill Avenue in Tempe, Arizona. It appears to be the first time an automobile driven by a computer has killed a human being by force of impact. The car was traveling at 38 miles per hour.

An initial investigation by Tempe police indicated that the pedestrian might have been at fault. According to that report, Herzberg appears to have come “from the shadows,” stepping off the median into the roadway, and ending up in the path of the car while jaywalking across the street. The National Transportation Safety Board has also opened an investigation. It’s still hard to know exactly what took place, at this time, without some speculation.

Likewise, it’s difficult to evaluate what this accident means for the future of autonomous cars. Crashes, injuries, and fatalities were a certainty as driverless vehicles began moving from experiment to reality. In 2016, a Tesla operating in its unique “autopilot” mode in Florida crashed into a tractor-trailer that made a left turn in front of the vehicle, killing the Tesla’s driver. At the time, it was the first known fatality from a self-driving vehicle—but at the time of the accident, the car had apparently been warning its driver to disengage the autopilot mode and take control of the vehicle.

Advocates of autonomy tend to cite overall improvements to road safety in a future of self-driving cars. Ninety-four percent of car crashes are caused by driver error, and both fully and partially autonomous cars could improve that number substantially—particularly by reducing injury and death from speeding and drunk driving. Even so, crashes, injuries, and fatalities will hardly disappear when and if self-driving cars are ubiquitous. Robocars will crash into one another occasionally and, as the incident in Tempe illustrates, they will collide with pedestrians and bicyclists, too. Overall, eventually, those figures will likely number far fewer than the 37,461 people who were killed in car crashes in America in 2016.

The problem is, that result won’t be accomplished all at once, but in spurts as autonomous technology rolls out. During that period, which could last decades, the social and legal status of robocar safety will rub up against existing standards, practices, and sentiments. A fatality like the one in Tempe this week seems different because it is different. Instead of a vehicle operator failing to see and respond to a pedestrian in the road, a machine operating the vehicle failed to interpret the signals its sensors received and process them in a way that averted the collision. It’s useful to understand, and even to question the mechanical operation of these vehicles, but the Tempe fatality might show that their legal consequences are more significant than their technical ones.

Arizona Governor Doug Ducey has turned the state into a proving ground for autonomous cars. Ducey, a businessman who was the CEO of the franchised ice-cream shop Cold Stone Creamery before entering politics, signed an executive order in 2015 instructing state agencies to undertake “any necessary steps to support the testing and operation of self-driving cars on public roads within Arizona.” While safety gets a mention, the order cites economic development as its primary rationale. Since then, Uber, Waymo, Lyft, Intel, GM, and others have set up shop there, testing self-driving cars in real-world conditions—a necessity for eventually integrating them into cities.

The 2015 order outlines a pilot program, in which operators are required to “direct the vehicle’s movement if necessary.” On March 1, 2018, Ducey issued an updated order, which allowed fully autonomous operation on public roads without an operator, provided those vehicles meet a “minimal risk condition.” For the purposes of the order, that means that the vehicle must achieve a “reasonably safe state ... upon experiencing a failure” in the vehicle’s autonomous systems. The new order also requires fully autonomous vehicles to comply with registration and insurance requirements, and to meet any applicable federal laws. Furthermore, it requires the state Departments of Transportation and Public Safety, along with all other pertinent state agencies, to take steps to support fully autonomous vehicles. In this case, “fully autonomous” means a level four or five system by SAE standard, or one that a human need not operate at all, but which can be taken over by a human driver if needed.

If Uber’s vehicles are considered to operate at SAE level 4, the company would be required to file notice, within 60 days of the issuance of the new Arizona executive order, that the vehicle and driver meet certain conditions, including licensure, insurance, and the achievement of the “minimal risk condition” described, in order to be allowed on the road. But the order was issued March 1, making Uber’s compliance with it irrelevant for the March 18 pedestrian fatality. Given the preliminary police report that appears to exonerate Uber and its driver from culpability, combined with Arizona’s industry-friendly policies toward autonomous vehicles, the likelihood of any kind of state intervention into the incident seems low. Worse, Tempe police speculate that Herzberg was homeless, lowering the chance that her death will earn substantial scrutiny. Uber has ceased testing of its autonomous fleet in all cities, but soon enough, they will likely resume, just as they did after their last accident, and the preparations for a driverless future will return to the desert.

Arizona residents might not be satisfied with that outcome. After all, they’re the ones who have to live in a robot-car test facility. Even if Uber’s self-driving apparatus did not “fail” under the definition of the order that permits its vehicles to operate on public roads, this feels like a different accident—and a different death—than the hundreds of others that occur every day.

When I reported on Uber’s Tempe autonomous test fleet last November, I observed how different the cars made the city feel. Standing on a median not unlike the one Herzberg allegedly stepped off into the path of Uber’s Volvo SUV, the entire relationship between humans and automobiles seemed to shift—the very texture of urban life is altered when a person cannot look a driver in the eye to gauge their intentions, or when a two-ton machine is run by an array of sensors and computers, whose decisions are foreign to human reasoning.

And that’s under normal operation, absent pedestrian death or its threat. “Will a self-driving car recognize the micro-drama of an unsupervised toddler, or a professor lost in his smartphone?” Ed Finn, a professor of arts and media at Arizona State University, wonders when I ask him about the Uber fatality. An autonomous-vehicle crash feels different, and maybe worse, than a human-caused one partly because of the tangled relationship between driving, liability, and human frailty. When people get into car crashes with one another, vehicular negligence is typically the cause. Determining which party is negligent, and therefore at fault, is central to the common understanding of automotive risk. Negligence means liability, and liability translates the human failing of a vehicle operator into financial compensation—or, in some cases, criminal consequence.

Overall, there’s recognition that self-driving cars implicate the manufacturer of the vehicle more than its driver or operator. That has different implications for a company like GM, which manufactures and sells cars, than Google, which has indicated that it doesn’t have plans to make cars, only the technology that runs them. The legal scholar Bryant Walker Smith has argued that autonomous vehicles represent a shift from vehicular negligence to product liability. The latter legal doctrine covers claims against companies who manufacture and sell a defective or dangerous product. On today’s roads, product liability claims arise in cases like the failure of Bridgestone/Firestone tires in the late 1990s, and the violent rupture of Takata airbags in the late aughts. These situations represent fairly traditional examples of product liability: A company designed, manufactured, or marketed a product that didn’t do what it promised, and harmed people as a result.

But these situations are different from a self-driving car. In Hertzberg’s case, at least according to the initial police report, a defective sensor or computer doesn’t appear to have caused the car or its operator to lose control or otherwise cause the crash. Instead, the operator’s judgment and response was replaced by the machine’s. If it’s true that no driver, human or robot, could have prevented the Tempe crash and death, then it offers a particularly intriguing test of Smith’s theory. Normally, the driver and his or her insurance provider would be the ones litigated for the accident—that’s vehicular negligence. But, the fact that an autonomous vehicle caused the outcome might be enough to shift the liability and compensation to that of product liability on the part of Uber.

The problem is, Smith’s argument is based on a future in which self-driving cars are sold or leased for hire, such that individuals who choose to drive or ride in them have initiated a change in vehicular usage that would entail the shift in legal responsibility and blame. But that’s not the case today, with Uber or anyone. Uber is running tests of their technology, a feat allowed by Arizona’s liberal regulatory policy on autonomous cars. It’s possible that, upon review, the Hertzberg death might neither be construed as vehicular negligence, because a person both is and isn’t driving, nor product liability, because there is no product being leased or sold.

Smith, for his part, holds that negligence still probably covers the kind of situation at issue in the Tempe collision this week, and similar ones that will come up in the future. The test, he says, is whether a natural or legal person had a duty that was violated by acting unreasonably, in a way that causes harm. “If the safety driver were negligent, then Uber could be vicariously negligent as an employer, but other decisions might be evaluated as well, including the decision to test or deploy the vehicle and the training provided to the operator,” Smith explains.

What about the state? Could Arizona be held accountable for allowing autonomous cars to roam public streets without sufficient oversight, including legal guidance for inevitable situations like pedestrian deaths? Smith doubts it. “In general, states are not liable for policy determinations. A state might be liable for not properly maintaining a road, but not for deciding whether or not to build the road.”

Still, since the law is set by precedents pursued by legal action, other interpretations of self-driving liability are possible. A different interpretation might compare operating autonomous test cars to taking dangerous or experimental equipment on city roads. There’s an argument to be made that a pedestrian death at the hands of an autonomous car, even one that would have been unavoidable, is no different from a human-driven car with a new, experimental combustion engine that malfunctions and blows up on a city road or interstate.

Meanwhile, the letter of the Arizona executive order seems to suggest that the human operator is on the hook for any traffic infractions while he or she is in the vehicle, even if it’s in fully autonomous mode. That means that an operator could, in theory, be charged with vehicular manslaughter—although the courts would inevitably have to adjudicate such a matter were the state to bring the charge. The whole situation is muddy and confused, and it might be impossible to understand it in the abstract, before legal precedent is set.

Furthermore, since the autonomous Ubers can and do, at times, pick up ordinary Uber passengers during their transit of Tempe’s streets, their autonomous vehicles might also be subject to the common carrier doctrine—a law that requires common carriers, like buses and taxis, but also hoteliers, insurers, and others (including, more recently, internet-service providers) to be held to a higher standard of care than ordinary operators. But there’s confusion about what a “higher standard of care” means— the Arizona Supreme Court has even held that common carriers in the state are only subject to “reasonable” care anyway, the same as any other agent. Worse, it’s not clear if ride-hail services even count as common carriers in Arizona or elsewhere.

Ducey’s executive order was written to encourage self-driving technology and manufacturing companies to move jobs and commerce to the state. As such, it sacrifices some of Arizona’s citizens’ rights to safety in the present in exchange for economic development, and the possibility of safer roads in the future, when and if autonomous cars become ubiquitous.

That said, an executive order doesn’t really do very much. The governor can direct the state and its agencies to do things, but statutory laws are made by the legislature—the regulation of self-driving vehicles in Arizona can’t really go beyond existing law anyway. Ducey is taking the implicit position that existing law is consistent with automated driving. Should the Hertzberg collision, or another situation, result in litigation, then the interpretation of the law can proceed as usual. “Tragedies have a way of bringing legal issues to a head,” Smith concludes.

There are signs that new laws might eventually intervene, reducing liability for self-driving vehicle operators. New federal legislation under consideration could push complaints arising from autonomous-vehicle collisions or injuries into private arbitration. In addition to reducing citizen’s rights, that move could prevent courts from hearing some cases that might produce new statutory precedents.

In the past, I’ve argued that autonomous cars could erode citizens’ rights to the public streets. Given sufficient economic incentive to pursue public-private partnerships between municipalities and technology companies, cities, counties, and states might choose to adopt industry-friendly regulatory policy in exchange for changes to the urban environment. Eventually, should autonomous cars become widespread, it might become more expedient just to close certain roads to pedestrians, bicyclists, and human drivers so that computer cars can operate at maximum efficiency. It would be a step too far to conclude that the fatality in Tempe rings the death knell for pedestrian and human-driver access to the roads. But it’s happened before: Jaywalking laws were essentially invented to transform streets into places for cars. Uber, Google, and other wealthy companies with big aspirations for autonomous driving might see this fatality as a sign that it’s time to get more serious about legal protection for their interests.. . Source: Tesla, Inc.

The automotive industry is at the beginning of a grand experiment. If completely successful, humanity could be ushered into a new economy where driving is a hobby, only for sunny days along clear roads with a view. The struggles and tedium of the daily commute could be handled by autonomous vehicles, traffic accidents could fall to nil, passengers could focus on working and relaxing in their mobile offices, and the elderly, disabled, and blind could have considerable mobility and autonomy. If a complete failure, automobile companies would have invested billions of dollars in computer vision, sensors, and automated driving systems only to have no effect on or actually increase the number of traffic accidents and fatalities by introducing new risks. This would cause a public backlash, and requiring regulators to impose a slow, costly review process that slows the pace of innovation so that after an initial roll-out to a few hundred thousand vehicles, further roll-outs are halted. Then, autonomous vehicle technology may follow the same path as the U.S. nuclear power industry, which has stopped building new power plants since the Three Mile Island accident in 1979. Which scenario or whether something in between unfolds depends on good design, as well as careful understanding and communication of the safety of autonomous driving technology and the path from partially autonomous to fully autonomous vehicles. And understanding the safety of autonomous vehicles (AV) is a very thorny statistics problem.

Recent fatal and injury-producing crashes involving vehicles with Tesla’s Autopilot and Uber’s self-driving pilot have led to significant disagreement among experts, reporters, automakers, and regulators about safety statistics for partial autonomy technologies[1]. Tesla, in particular, has made recent headlines after two crashes and 1 fatality with its Autopilot-equipped partial autonomy vehicles in the past few months. Tesla claims that its technology is 3.7x safer than the existing U.S. vehicle fleet, stating a fatality rate of 1 death per 86 million miles for conventional vehicles versus 1 death per 320 million miles for Autopilot-equipped vehicles, but many experts question the methodology and data behind these statistics. In this article, I’ll review the data, methods, and the three main criticisms of Tesla’s methodology for conventional vehicle fatality rates, provide my best estimates, and make recommendations for regulators and automakers on the safety of autonomous vehicles. I don’t have access to data to verify the fatality rate for Tesla Autopilot-equipped vehicles but the company has promised to release public Autopilot safety data in future quarters.

1. What’s an Autopilot mile?

One informal complaint I’ve heard among analysts is the question of which miles should be included as an ‘Autopilot mile’ in Tesla’s statistic of 1 fatality per 320 million miles. Some analysts argue that one should only compare miles driven in a vehicle with Autopilot engaged to manually-driven vehicle-miles to obtain a fatality rate. Instead, Tesla’s methodology includes all miles driven with an Autopilot-enabled vehicle, whether or not the functionality was engaged.

I agree with Tesla’s methodology on Autopilot mileage because the road conditions under which a partial autonomy system is rated for operation (highways, clear lane markings, etc) are systematically different from manually-driven miles. If one only used Autopilot-enabled miles in the fatality rate calculation, a comparable baseline of miles for a manual vehicle driven under similar road conditions would be difficult to obtain and there are already considerable gaps in the vehicle mileage data needed to compute good partial autonomy safety statistics (more below).

Because the characteristics of manually-driven miles in Autopilot-enabled vehicles are very different than the miles driven in a manually-driven vehicle — more curves, poor lane markings, rain or poor-visibility weather, etc. — it could be possible that crashes are more likely to occur when an Autopilot-enabled vehicle turned over operation to the driver, because road conditions were worse. If that hypothesis were true, these types of crashes should be included as an Autopilot crash, as it pertains to the road coverage of Autopilot and the hand-off between autonomous and manual control, which is related to Tesla’s design choices.

So, unless the owner of an Autopilot-enabled vehicle never or rarely chose to enable the functionality, the proper comparison for fatality rate safety statistics should be made between Autopilot vehicles and all other vehicles.

2. What are comparable vehicles and fatalities?

Another criticism of Tesla’s Autopilot safety statistics is aimed at its choice of comparable baseline vehicles in the 1 fatality per 86 million miles statistic. Analysts believe this statistic was obtained from the Insurance Institute for Highway Safety (IIHS)’s general statistics on fatal crashes, which includes all fatalities (driver, passenger, pedestrian) from accidents by all vehicle types (automobiles, pickups and SUVs, trucks and buses, motorcycles, etc), to arrive at a 2016 total rate of 1.16 fatalities per 100 million miles, or 1 fatality per 86 million miles. Criticisms that this is an ‘apples to aardvarks’ comparison are fair.

There are two main factors that contribute to vehicle fatalities: the number of crashes per vehicle, and the number of passengers or pedestrians involved in each crash[2]. While the number of crashes per vehicle is related to Autopilot design, the number of passengers in the vehicle is essentially random. The ideal calculation of the fatal crash rate statistic should be the ratio of any crash with one or more driver, passenger, or pedestrian fatality to the number of miles traveled per vehicle. In addition, vehicles of similar classes should be compared, i.e. large luxury sedans vs. the Model S, large luxury SUVs vs. the Model X, and mid-size luxury sedans vs. the Model 3, as the demographics and driving patterns of drivers of these sub-classes of vehicles should be similar.

Unfortunately, these ideal data are not reported by the IIHS, which either reports all fatalities, not broken out by vehicle sub-class, or driver-only fatality rates by vehicle models and sub-classes. If one uses the overall rate of 29–32 driver deaths per million vehicle-years for all vehicles, and 11,000 miles driven per year per vehicle, that corresponds to a driver fatality rate of one per 340 million to 380 million miles[3]. However, the fatality rate for any crash that included pedestrian, passenger, or driver deaths could be higher. According to the 2016 IIHS crash death by type statistics just under 60% of vehicle fatalities involve a driver, when including all vehicle types but excluding fatalities involving truck drivers/passengers, bicyclists, and motorcyclists. If so, the fatal crash rate could be one per 210 million to 230 miles if we included vehicle crashes with other types of fatalities. But we don’t know for sure because the IIHS doesn’t track fatal crash rates by vehicle, only driver death rates. By this last estimate, Autopilot has about the same to 35% lower fatal crash rates than any conventional vehicle at this time.

Why does my estimate differ so much from Tesla’s reported baseline for conventional vehicles of 1 in 86 million miles? Partly because Tesla includes all deaths for all vehicle types for conventional vehicles which is not a fair comparison with Autopilot-equipped vehicles, and partly because that simple arithmetic calculation should not be used because it doesn’t tell the whole truth and misses the bigger business and public policy problem.

3. Lies, Damned Lies, and Statistics

If life is a simulation, a simple arithmetic calculation of the miles per fatality statistic is the outcome of a single run, a single roll of the dice. If a butterfly flapped its wings in China and an autonomous vehicle accident occurred earlier or later in time, a snapshot of the miles per fatality statistic calculated immediately after an accident would cloud our crystal ball. That one fatality per 320 million miles statistic for Autopilot should really be calculated using the same methodology (called Poisson regression) that IIHS used to determine the driver fatality rate of 29–32 per million vehicle-years for conventional vehicles. A Poisson regression is used to model independent random events in time, like fatal accidents, as a function of exposure, such as miles traveled per vehicle to obtain a probabilistic estimate of fatality rate. However, IIHS was only able to obtain a narrow range of driver fatality rate estimates (29–32 per million car-years) for the entire population of vehicles in the U.S. The driver fatality rates for some less-common vehicles makes and models (including some of the large luxury vehicles that could be compared with Tesla Autopilot-enabled vehicles), while shown on the IIHS website, have such large (confidence interval) ranges that the average driver fatality rate figures are useless for decision-making.

Policymakers, automakers, and the general public need to get comfortable with the uncomfortable fact that over 8 billion of miles will have to be driven with partially-autonomous vehicles before we have statistical confidence that autonomous vehicles are 20% better than humans, according to RAND[4]. In the short term, we cannot be certain that partially autonomous vehicles are safer than drivers, but automakers and customers who think the technology can improve over time will take a risk to invest in it or use it.

There are consequences in delaying the roll-out of autonomous vehicle technology. RAND has developed a useful decision analysis tool to allow anyone to compare the timing of a partial-autonomy roll-out, the safety of the technology (from half-as-safe to almost perfect), and the lives saved over the course of several decades, assuming that by 2060, almost perfect fully-autonomous vehicles are rolled out to the majority of the vehicle fleet in the U.S. This model shows that rolling out just as safe or a little safer partially-autonomous vehicles by 2020 will save 160,000 more lives over 50 years than a scenario that waits until 2025 to roll out almost perfect autonomous vehicles. Delaying the roll-out of partially-autonomous vehicles costs lives. This conclusion assumes that (1) automakers make steady progress in improving the safety and reliability of their partially autonomous vehicles and (2) drivers are comfortable enough with monitoring the partially-autonomous vehicles so that new sources of error associated with the transition to and from manual and autonomous control do not increase fatality rates. Automakers and regulators should do everything to verify and ensure that these assumptions are true. RAND finds that the most lives can be saved if partially autonomous vehicles are rolled out after they are 10% better than human drivers, and from the publicly available data, Autopilot appears to be performing at least at that level, if not better.

Recommendations for Regulators and Automakers

NHTSA has been a strong supporter of the transition to autonomous vehicles throughout the years because of its potential benefits for safety and productivity for all and increased mobility for the disabled, blind, and elderly. It would be highly beneficial to development of autonomous vehicle technology if NHTSA coordinates the collection and analysis of safety statistics using the Poisson regression methodology used by IIHS for conventional and partially autonomous vehicles. These statistics should be developed for the specific vehicle classes that are likely to see partial autonomy features in the coming years, i.e. mid-size and large sedans and SUVs. These data are vitally important so that the automotive industry and the general public know where partial autonomy technologies stand relative to conventional vehicles, so that individual drivers can make informed purchasing decisions.

Similarly, automakers should collect and report on safety statistics on partially-autonomous vehicles, including counts of fatal and non-fatal crashes, airbag deployments, fender benders, near-misses, manual overrides of vehicles, as well as miles driven with autonomy-enabled vehicles and non-autonomy-enabled vehicles. Over time, as vehicle fleets gain enough mileage, safety statistics could be computed using the Poisson regression methodology.

According to RAND’s study, ‘developers of this [partial autonomy] technology and third-party testers cannot drive their way to safety’ with small fleets of ~100 vehicles driving 24 hours a day. Large-scale trials of 100s of thousands of vehicles would have to operate for multiple years before to obtain the mileage necessary to have confidence in better than human performance of a partial autonomy system. Researchers have proposed simulation and testing approaches of hardware and software to obtain autonomous driving safety data without on-the-road mileage. For example, Mobileye has been developing open-source rules for how a partially-autonomous vehicle should handle the 37 main pre-crash cases in NHTSA’s accident database. Functional testing to ensure that partially autonomous systems follow these rules would help to clarify system capabilities for drivers and limit manufacturers’ liability (though some legal experts question this approach). Regulators will have to work with manufacturers to develop and review these alternative safety tests and adapt regulations accordingly on the road to a self-driving future.

Notes: The author is a data scientist, former Tesla employee with no involvement with Autopilot development, and holds TSLA stock. This article was written with no input from Tesla. All views are my own.

[1] Some companies (e.g. Waymo/Google) argue that partial autonomy systems will never be safer than human driving and promote the development of fully autonomous vehicles, complete with no steering wheel, only. That debate is outside the scope of this article.

[2] Treatment of crashes involving pedestrians is not just a statistics problem, it is a legal problem outside the scope of this article. With conventional vehicles, the driver is fully responsible for pedestrian crashes, but with partial- and fully-autonomy vehicles, the liability and insurance costs is a shared responsibility between drivers and automakers depending on the level of autonomy claimed or achieved. This is an area that requires a lot of further study and the development of custom insurance products.

[3] Note that the correct number to use of this type of analysis is vehicle-miles traveled per vehicle, as opposed to vehicle-miles traveled per licensed driver as in Figure 4–4 in this FHWA reference (because people could drive multiple cars), or vehicle-miles traveled per capita (because not all people drive), to quantify the exposure to the risk of a fatal car accident per vehicle.

[4] Less on-the-road mileage is needed if safety statistics on other types of road accidents, like non-fatal crashes, fender benders, and near-misses can be obtained. However, data on minor accidents and near-misses are spotty.. NHTSA’s Office of Defects Investigation is opening a Preliminary Evaluation of the design and performance of automated driving systems in the Tesla Model S.

NHTSA recently learned of a fatal highway crash involving a 2015 Tesla Model S, which, according to the manufacturer, was operating with the vehicle’s ‘Autopilot’ automated driving systems activated. The incident, which occurred on May 7 in Williston, Florida, was reported to NHTSA by Tesla. NHTSA deployed its Special Crash Investigations Team to investigate the vehicle and crash scene, and is in communication with the Florida Highway Patrol. Preliminary reports indicate the vehicle crash occurred when a tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway. The driver of the Tesla died due to injuries sustained in the crash.

NHTSA’s Office of Defects Investigation will examine the design and performance of the automated driving systems in use at the time of the crash. During the Preliminary Evaluation, NHTSA will gather additional data regarding this incident and other information regarding the automated driving systems.

The opening of the Preliminary Evaluation should not be construed as a finding that the Office of Defects Investigation believes there is either a presence or absence of a defect in the subject vehicles.. After making repeated statements on an ongoing government investigation into a fatal crash involving Autopilot, Tesla has been kicked out of the probe being conducted by the National Transportation Safety Board for violating its legal agreement with the NTSB. Adding to the drama is the fact that Tesla may have lied before the NTSB publicly announced the decision, saying that it voluntarily left the investigation because it wouldn't allow Tesla to "release information about Autopilot to the public." On Thursday afternoon, Tesla released another statement refuting the NTSB's version of events and again claiming they left the investigation of their own accord).

Tesla's statements artfully package the company's exit from the investigation as a matter of information freedom. But the company's statements on Autopilot incidents, including in its most recent investigations, have consistently placed blame on drivers rather than investigating its own technology.

Last month, a Tesla Model X crashed into a concrete median near Mountain View, California, killing driver Walter Huang, sparking the NTSB investigation. In a statement a week after the crash, Tesla acknowledged that Huang was using Autopilot during the crash, but squarely blamed him for his own death, saying:

The driver had received several visual and one audible hands-on warning earlier in the drive and the driver's hands were not detected on the wheel for six seconds prior to the collision. The driver had about five seconds and 150 meters of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken.

Nowhere in the post did Tesla acknowledge why its Autopilot feature had directed the car to crash into a concrete barrier, except that "the adaptive cruise control follow-distance" was "set to minimum."

In another fatal crash in May 2016, a tractor-trailer crossed in front of a Tesla using Autopilot. The Tesla did not break and the Tesla smashed into the semi, effectively peeling off the car's roof and killing the driver. Tesla acknowledged Autopilot's role in the crash, saying "Neither Autopilot nor the driver noticed the white side of the tractor trailer against a brightly lit sky, so the brake was not applied." Despite this, Tesla ultimately blamed the driver:

When drivers activate Autopilot, the acknowledgment box explains, among other things, that Autopilot 'is an assist feature that requires you to keep your hands on the steering wheel at all times,' and that 'you need to maintain control and responsibility for your vehicle' while using it.

In another Autopilot crash in January, a Tesla slammed into a parked firetruck. Tesla's response was that Autopilot is "intended for use only with a fully attentive driver."

The two deaths and crash fit into a series of accidents and viral videos that show the imperfections of Tesla's Autopilot technology.

The public scrutiny of the crashes has been comparatively reserved to the reaction garnered from the one death caused by Uber's self-driving car, and Tesla's reactions have been much more defensive.

A key aspect of Tesla's responses to its Autopilot crashes is the fact that it fits the bill of a level 2 automation system in the Society of Automotive Engineers's automation 6-level framework. A level 2 system will manage speed and steering in certain conditions but requires that drivers pay attention and be ready to take over — Tesla attempts to enforce this by alerting the driver whenever their hands aren't on the wheel, and eventually disabling Autopilot if a drivers hands aren't on the wheel long enough (although drivers have figured out ways to hack this). A Level 3 car will only alert the driver when it detects a situation it can't handle.

Tesla has conveniently used this fact in its responses to high-profile crashes but has also repeatedly advertised its cars self-driving capabilities as higher than a level 2 — sending mixed signals to drivers and the public.

On its Autopilot website, Tesla touts "Full Self-Driving Hardware on All Cars," despite its insistence that the software is only meant for assistance.

&amp;lt;img src="http://static.digg.com/images/a88e74d8ce754329951658ddd2fbbd98_f067ce2537094ef68757324bf8478870_1_post.png" alt="" /&amp;gt;

On the same page, a video shows a Tesla being operated with no hands, detecting its environment. A caption reads "The person in the driver's seat is only there for legal reasons. He is not doing anything. The car is driving itself."

Only at the bottom of the page, does Tesla specify that "Full Self-Driving Capability" is a software add-on that hasn't actually been released yet, and doesn't apply to Tesla's existing "Enhanced Autopilot."

Adding to the confusion is the fact that Tesla CEO Elon Musk has repeatedly said that Tesla's cars on the market that are equipped for Autopilot will eventually be able to achieve "approximately human-level autonomy," and could possibly facilitate "full automation." In 2016 he called Autopilot "almost twice as good as a person."

Despite Tesla's mixed messaging, Tesla is correct when it says that the "NHTSA has found that [Autopilot] reduces accident rates by 40%." With its current statistics, Tesla says "you are 3.7 times less likely to be involved in a fatal accident."

Despite this, recent Tesla crashes reveal that over-reliance on the new, and still flawed, safety features of Autopilot and semi-autonomous modes can be deadly. Whether Tesla and Musk are willing to publicly emphasize Autopilot's limitations is another question.. . Electric car company Tesla has confirmed that a recent fatal crash involving one of its vehicles occurred while the car was in autopilot mode.

Tesla released a blog post last week to provide more details about the accident on 23 March 2018, which took place in Mountain View, California, and killed the car's driver – Apple software engineer Wei Huang, 38.

The statement said that the Model X sports utility vehicle's autopilot function was engaged, and its adaptive cruise control follow-distance was set to minimum.

Driver failed to take control

It also suggested that the driver was given ample warning to override the function before the collision, but failed to take action.

"The driver had received several visual and one audible hands-on warning earlier in the drive and the driver's hands were not detected on the wheel for six seconds prior to the collision," said the statement.

"The driver had about five seconds and 150 metres of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken."

Related story Tesla's Autopilot reduced crashes by 40 per cent, finds US inquiry

Tesla also placed some of the blame on the road infrastructure, which had not been repaired since an earlier incident.

"The reason this crash was so severe is because the crash attenuator, a highway safety barrier which is designed to reduce the impact into a concrete lane divider, had been crushed in a prior accident without being replaced," it said. "We have never seen this level of damage to a Model X in any other crash."

Autopilot still safer says Tesla

The company assured that its autopilot function makes accidents less likely, not more, but requires drivers to remain vigilant and keep hands on the wheel to help avoid potential collisions.

The company's founder, billionaire entrepreneur Elon Musk, called the Model X the "safest SUV ever" when it launched in 2015. A US government study found that the autopilot function reduces accidents by 40 per cent, initiated after the first death from a crash that occurred while using the autopilot function in July 2016.

"The consequences of the public not using autopilot, because of an inaccurate belief that it is less safe, would be extremely severe," Tesla's statement said. "There are about 1.25 million automotive deaths worldwide. If the current safety level of a Tesla vehicle were to be applied, it would mean about 900,000 lives saved per year."

"We expect the safety level of autonomous cars to be 10 times safer than non-autonomous cars," the statement added, ending with condolences to the victim's family and friends.

Related story Tesla's electric Model X is the "safest SUV ever" says Elon Musk

The news comes at a troubling time for Tesla. Musk has reportedly taken over the management of its Model 3 electric car production after failing to meet first-quarter targets.

It has also been a bad period for autonomous cars. Last month, an Uber taxi killed a woman in the first fatal accident between a pedestrian and a self-driving car, while a crash involving a Tesla Model S that may have been in autopilot mode in January 2018 is currently under investigation.. The safety of Tesla Autopilot came back into focus after it was confirmed that the driver assist system was on during the fatal accident that killed a Model X owner in Mountain View last month.

Now the family of the deceased say that they are prepared to sue Tesla after a media interview.

As we previously reported, the Model X was driving on Autopilot when it entered the median of a ramp on the highway as if it was a lane and hit a barrier about a hundred and fifty meters after going into the median.

The impact was quite severe because there was no crash attenuator since it was already destroyed from a previous crash. The driver was rushed to the hospital, but he, unfortunately, died of his injuries.

Sevonne Huang, the wife of the driver, Walter Huang, gave an interview to ABC7 yesterday.

During the interview, she said that her husband had previously complained about the Autopilot’s behaviour at hat exact location:

Sevonne Huang: “And he want to show me, but a lot of time it doesn’t happen.”

Dan Noyes: “He told you that the car would drive to that same barrier?”

Sevonne: “Yes.”

Noyes: “The same barrier that he finally hit?”

Sevonne: “Yeah, that’s why I saw the news. I knew that’s him.”

The family hired attorney Mark Fong and say that they are prepared to sue Tesla.

Fong commented:

“Unfortunately, it appears that Tesla has tried to blame the victim here. It took him out of the lane that he was driving in, then it failed to break, then it drove him into this fixed concrete barrier. We believe this would’ve never happened had this Autopilot never been turned on.”

Tesla responded to the interview in a statement:

“We are very sorry for the family’s loss. According to the family, Mr. Huang was well aware that Autopilot was not perfect and, specifically, he told them it was not reliable in that exact location, yet he nonetheless engaged Autopilot at that location. The crash happened on a clear day with several hundred feet of visibility ahead, which means that the only way for this accident to have occurred is if Mr. Huang was not paying attention to the road, despite the car providing multiple warnings to do so. The fundamental premise of both moral and legal liability is a broken promise, and there was none here. Tesla is extremely clear that Autopilot requires the driver to be alert and have hands on the wheel. This reminder is made every single time Autopilot is engaged. If the system detects that hands are not on, it provides visual and auditory alerts. This happened several times on Mr. Huang’s drive that day. We empathize with Mr. Huang’s family, who are understandably facing loss and grief, but the false impression that Autopilot is unsafe will cause harm to others on the road. NHTSA found that even the early version of Tesla Autopilot resulted in 40% fewer crashes and it has improved substantially since then. The reason that other families are not on TV is because their loved ones are still alive.”

Electrek’s Take

I can’t blame the family for having this reaction or any kind of reaction after such a tragic loss, but when it comes to the lawsuit, it looks like they are destroying their own case.

As Tesla said in the statement and as it was confirmed by other Tesla owners recreating the circumstances of the crash, an attentive driver would have plenty of time to go back to the correct lane after the car enters the median, which means that Huang was most likely not paying attention.

On top of it, his wife says that he was aware that Autopilot had difficulties handling this specific situation and yet he decided to activate it anyway and apparently not pay close attention.

In my opinion, as long as Tesla is being clear about drivers needing to stay attentive and keep their hands on the steering wheel, there’s not much of a case here that Tesla is responsible for the accident.

But with this said, as Tesla Autopilot improves it seems that some drivers are growing more confident with the driver assist system and are putting too much trust in it.

I think it’s important to remind everyone that as long as Tesla doesn’t claim it’s anything more than a level 2 driving system and it’s not thoroughly tested to be anything more than that, they should always stay vigilant and be ready to take control.. Page Content

​​The page you're looking for doesn't exist.

Check for a typo in the URL, or go to the site home. Maria Smith sat on her front porch in Holbrook and recalled her terrifying experience last December while driving home from college.

A Massachusetts State Police trooper had just stopped the 21-year-old at about 10 p.m. on Route 24 in West Bridgewater. As the trooper approached her driver's side window, Smith reached for her vehicle registration. Then, she was suddenly jolted by a loud collision.

"It just happened so quick," Smith said. "Before I knew it, my car was flying forward. I looked behind me, and my whole back windshield was blown out. There was glass in my hair."

According to court documents, a Weston man driving a Tesla slammed into a Massachusetts State Police cruiser that was stopped in the left lane of the road, propelling the SUV forward into Smith's vehicle before spinning out.

The driver, Nicholas Ciarlone, is now facing a negligent driving charge. Once courthouses reopen, he is scheduled to be arraigned in September.

Ciarlone declined to answer questions about the incident when the NBC10 Boston Investigators approached him outside the Brockton District Court following a clerk magistrate hearing.

However, court documents point to Tesla's emerging driver assistance technology as an element in the crash.

A trooper who responded to the scene wrote that Ciarlone said his Tesla was set to Autopilot mode and he "must not have been paying attention."

"I thought that was terrifying," Smith said. "To think the sensors are not equipped enough to pick up a police car with its sirens and lights on the highway."

When Autopilot is engaged, the car helps with steering and matches your speed to surrounding traffic. All Tesla models come equipped with the feature, which doesn't make the vehicle fully autonomous, but assists with what Tesla describes as the "most burdensome parts of driving."

But there are a growing number of examples in which Autopilot didn't prevent crashes, including:

Nationally-recognized auto safety watchdog Sean Kane said he believes Tesla is testing out Autopilot in real time on public roadways.

"We are all involved in a clinical trial that we didn't sign up for," Kane said.

NBC10 Boston first took a closer look at Autopilot last November, when a Newburyport driver said he accidentally fell asleep for 14 miles behind the wheel as his vehicle navigated the highway.

That driver shared his story to highlight the different ways Tesla owners were taking detours around the car's warning system, which reminds drivers to keep their hands on the wheel. Online videos showed drivers rigging the steering wheel with everything from weights to water bottles to a piece of fruit to trick the software.

Following that report, Massachusetts Sen. Ed Markey had a strong reaction, sending a letter to Tesla CEO Elon Musk.

Markey later called on Tesla to rebrand Autopilot, arguing the name caused drivers to rely too heavily on the technology.

The Massachusetts lawmaker also raised the issue at a hearing in Washington D.C., peppering the head of the National Highway Traffic Safety Administration with questions about the federal agency's oversight.

An NHTSA spokeswoman told NBC10 the agency is aware of the Massachusetts crash and is gathering details from Tesla and law enforcement.

"All forms of distracted driving — including by drivers who abuse their vehicles' advanced safety features — are dangerous and put the driver and other road users at risk," an NHTSA statement reads.

Tesla did not respond to questions from NBC10 Boston about the crash. But the electric carmaker did recently roll out improvements designed to help its vehicles identify stop signs and traffic lights while in Autopilot.

Tesla has also maintained drivers need to pay attention while the vehicle is in Autopilot and be ready to take over at a moment's notice.

Kane believes that message contradicts human nature.

"You can't call something 'Autopilot' and then have the driver fully engaged. That doesn't make any sense at all," he said. "Unfortunately, the regulators are allowing it to happen right before their eyes."

Smith and the state trooper both went to the hospital following the crash in December, but were not seriously hurt.

Smith, who saw the trooper get knocked to the ground next to a highway barrier, told NBC10 Boston it all came down to a matter of inches.

"If my car had pushed forward any more, he probably would've ended up getting crushed by it," she said.. Police are probing possible drunk-driving in the case of a Tesla driver in Arizona who said he was using the Bay Area electric car maker’s controversial “Autopilot” system when his sedan smashed into an unoccupied police vehicle, which then hit an ambulance.

The crash occurred Tuesday on an Arizona highway, according to the state’s Department of Public Safety. “We can confirm the driver indicated to troopers the Tesla was on autopilot at the time of the collision,” the department tweeted, adding that the 23-year-old male driver was being investigated for driving under the influence.

The police sergeant who had driven the department’s SUV was not in it at the time of the crash, and the ambulance occupants were not hurt, the department said. The Tesla driver was hospitalized with serious but not life-threatening injuries, police said.

Tesla did not immediately respond to a request for comment. After a fatal 2018 accident involving a Tesla on Autopilot in Mountain View, the Palo Alto company said that “Autopilot can be safely used on divided and undivided roads as long as the driver remains attentive and ready to take control,” the National Transportation Safety Board noted in a report. In a 2018 blog post, Tesla claimed Autopilot makes crashes “much less likely to occur,” arguing that “No one knows about the accidents that didn’t happen, only the ones that did.”

Crashes involving Tesla’s Autopilot driver-assistance system have sparked multiple investigations by the federal safety board. The agency found a Tesla driver’s over-reliance on the automated system was a factor in a 2016 fatal Model S crash in Florida, and determined that in 2018 in Mountain View, Autopilot steered a Tesla Model X SUV into a Highway 101 barrier, a collision that caused the driver’s death.

After another fatal Florida crash, between a Model 3 sedan and a truck in March 2019, the agency blamed the driver’s over-reliance on automation and Tesla’s design of the Autopilot system as well as “the company’s failure to limit the use of the system to the conditions for which it was designed,” it said in a report.

The report noted that after the 2016 Florida crash, which involved a collision between a Tesla and a truck, the agency recommended that Tesla and five other car makers using automated systems develop technology to “more effectively sense the driver’s level of engagement and alert the driver when engagement is lacking.” However, while the five other companies responded with descriptions of their planned solutions, “Tesla was the only manufacturer that did not officially respond,” the report said.

The agency also found Autopilot was a factor when a Model S slammed into the back of a fire truck on I-405 in Culver City near Los Angeles in 2018. The driver was also to blame in the non-injury collision, for using Autopilot in “ways inconsistent with guidance and warnings from the manufacturer,” the agency reported.. Police in North Carolina have filed charges against a driver whose Tesla crashed into a police car early Wednesday morning, Raleigh's CBS 17 television reports. The driver admitted to officers that he had activated the Autopilot technology on his Model S and was watching a movie on his phone at the time of the crash.

"A Nash County deputy and a trooper with the Highway Patrol were on the side of the road while responding to a previous crash when the Tesla slammed into the deputy’s cruiser," CBS 17 reports. "The impact sent the deputy’s cruiser into the trooper’s vehicle—which pushed the trooper and deputy to the ground."

Thankfully, no one was seriously injured by the crash.

The driver was charged with a violation of the state's "move over" law and with having a television in the car.

It's an important reminder that no car on the market today is fully self-driving. Drivers need to pay attention to the road at all times, regardless of what kind of car they have or what kind of driver-assistance technology their car has.

Advertisement

Tesla could use better driver monitoring technology

In the last year, there have been at least three similar incidents involving Tesla vehicles crashing into police cars. This happened in Arizona in July and in Connecticut and Massachusetts last December.

To be fair, this isn't just a Tesla problem. Studies have found that driver-assistance systems like Autopilot—from Tesla and other automakers—are not good at stopping for stationary vehicles. A study earlier this month found that driver assistance systems from BMW, Kia, and Subaru failed to consistently stop for stationary vehicles on a test track.

Still, Tesla clearly has room for improvement. Obviously, it would be good if Autopilot could actually detect stopped vehicles. But Tesla could also use better driver monitoring technology.

Tesla vehicles use a steering wheel torque sensor to try to detect whether a driver is paying attention. This kind of sensor is easy to defeat. It's also possible to keep a hand on the wheel without actually paying attention to the road.

Tesla could learn from Cadillac, whose Super Cruise technology includes an eye-tracking camera that verifies that the driver is looking at the road. An eye-tracking system like this would likely prevent incidents like Wednesday's crash in North Carolina. If the driver had tried to watch a movie while Autopilot was engaged, the system would have detected that he was not watching the road, warned the driver, and eventually deactivated itself.