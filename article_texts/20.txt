Updated 3PM ET

Tesla is taking PR very seriously after one of its vehicles in autonomous mode killed a passenger recently.

The crash occurred at 9:27 AM on Highway 101 near Mountain View, California. Walter Huang was in the driver's seat of the Model X, which was in autonomous mode. The car hit a concrete highway divider, marked with black and yellow chevrons, at full force. Huang didn't take any action. The SUV crumpled like a tin can, and Huang didn't make it.

The investigation into the fatal #Tesla crash continued today with the #CHP & #NTSB digging through the scorched EV ? https://t.co/rfdgY88bn7 pic.twitter.com/vd2YzFmAZ0 — Dean C. Smith (@DeanCSmith) March 29, 2018

Other information has been hard to come by, due to the severity of the damage. So far we don't know if his death was a result of negligence, a fatal nap, or simply being distracted by the fireworks of warning lights, and sounds. But one thing is clear: the crash proves that audio and visual cues on the dashboard could after all be insufficient to prevent a crash.

Huang wasn't the first to die in a Tesla with Autopilot active. In 2016, Joshua Brown crashed his Model S into a truck, marking the first fatal collision while Autopilot was engaged.

The timing for this particular crash isn't exactly ideal (from Tesla's perspective). Uber is already doing damage control after its self-driving car killed a pedestrian in Arizona on March 19, four days before Huang's fatal collision.

Interestingly, officials aren't too pleased about Tesla's PR offensive. On Sunday, a spokesperson for the U.S. National Transportation Safety Board (NTSB) told the Washington Post:

At this time the NTSB needs the assistance of Tesla to decode the data the vehicle recorded. In each of our investigations involving a Tesla vehicle, Tesla has been extremely cooperative on assisting with the vehicle data. However, the NTSB is unhappy with the release of investigative information by Tesla.

Presumably, investigators aren't happy because they'd like to get as much information as they can, then release a report.

But Tesla might have jumped the gun. Not complying with the NTSB's investigation processes and deadlines might end up having their technological advancements (and security improvements) screech to a halt.

After the Uber car's crash, the company was banned from further testing in Arizona (though other companies were allowed to continue). Many people feared that the crash would fray the public's trust in autonomous vehicles, and that largely has not come to pass, at least not yet.

But if the crashes continue, that could change. The market for autonomous cars could dry up before the technology becomes reliable enough to make them widespread.

Tesla's Autopilot is Level 2 autonomy, while Uber's self-driving car is a Level 4. So the technology isn't even really the same. Still, a turn in the tide of public opinion could sweep both up with it.

Autonomous vehicles aren't the best at sharing the unpredictable road with imprecise humans. Yes, once fully autonomous vehicles roll out all over the country and make up 100 percent of the vehicles on the road, American roads will inevitably become safer.

But we're not there yet. If crashes like these keep happening, and the public loses trust, we might never be.

Update: Tesla CEO Elon Musk took to Twitter to respond to comments from NTSB and reiterate Tesla's priorities:

Lot of respect for NTSB, but NHTSA regulates cars, not NTSB, which is an advisory body. Tesla releases critical crash data affecting public safety immediately & always will. To do otherwise would be unsafe. — Elon Musk (@elonmusk) April 2, 2018. Maria Smith sat on her front porch in Holbrook and recalled her terrifying experience last December while driving home from college.

A Massachusetts State Police trooper had just stopped the 21-year-old at about 10 p.m. on Route 24 in West Bridgewater. As the trooper approached her driver's side window, Smith reached for her vehicle registration. Then, she was suddenly jolted by a loud collision.

"It just happened so quick," Smith said. "Before I knew it, my car was flying forward. I looked behind me, and my whole back windshield was blown out. There was glass in my hair."

According to court documents, a Weston man driving a Tesla slammed into a Massachusetts State Police cruiser that was stopped in the left lane of the road, propelling the SUV forward into Smith's vehicle before spinning out.

The driver, Nicholas Ciarlone, is now facing a negligent driving charge. Once courthouses reopen, he is scheduled to be arraigned in September.

Ciarlone declined to answer questions about the incident when the NBC10 Boston Investigators approached him outside the Brockton District Court following a clerk magistrate hearing.

However, court documents point to Tesla's emerging driver assistance technology as an element in the crash.

A trooper who responded to the scene wrote that Ciarlone said his Tesla was set to Autopilot mode and he "must not have been paying attention."

"I thought that was terrifying," Smith said. "To think the sensors are not equipped enough to pick up a police car with its sirens and lights on the highway."

When Autopilot is engaged, the car helps with steering and matches your speed to surrounding traffic. All Tesla models come equipped with the feature, which doesn't make the vehicle fully autonomous, but assists with what Tesla describes as the "most burdensome parts of driving."

But there are a growing number of examples in which Autopilot didn't prevent crashes, including:

Nationally-recognized auto safety watchdog Sean Kane said he believes Tesla is testing out Autopilot in real time on public roadways.

"We are all involved in a clinical trial that we didn't sign up for," Kane said.

NBC10 Boston first took a closer look at Autopilot last November, when a Newburyport driver said he accidentally fell asleep for 14 miles behind the wheel as his vehicle navigated the highway.

That driver shared his story to highlight the different ways Tesla owners were taking detours around the car's warning system, which reminds drivers to keep their hands on the wheel. Online videos showed drivers rigging the steering wheel with everything from weights to water bottles to a piece of fruit to trick the software.

Following that report, Massachusetts Sen. Ed Markey had a strong reaction, sending a letter to Tesla CEO Elon Musk.

Markey later called on Tesla to rebrand Autopilot, arguing the name caused drivers to rely too heavily on the technology.

The Massachusetts lawmaker also raised the issue at a hearing in Washington D.C., peppering the head of the National Highway Traffic Safety Administration with questions about the federal agency's oversight.

An NHTSA spokeswoman told NBC10 the agency is aware of the Massachusetts crash and is gathering details from Tesla and law enforcement.

"All forms of distracted driving — including by drivers who abuse their vehicles' advanced safety features — are dangerous and put the driver and other road users at risk," an NHTSA statement reads.

Tesla did not respond to questions from NBC10 Boston about the crash. But the electric carmaker did recently roll out improvements designed to help its vehicles identify stop signs and traffic lights while in Autopilot.

Tesla has also maintained drivers need to pay attention while the vehicle is in Autopilot and be ready to take over at a moment's notice.

Kane believes that message contradicts human nature.

"You can't call something 'Autopilot' and then have the driver fully engaged. That doesn't make any sense at all," he said. "Unfortunately, the regulators are allowing it to happen right before their eyes."

Smith and the state trooper both went to the hospital following the crash in December, but were not seriously hurt.

Smith, who saw the trooper get knocked to the ground next to a highway barrier, told NBC10 Boston it all came down to a matter of inches.

"If my car had pushed forward any more, he probably would've ended up getting crushed by it," she said.. Electric car company Tesla has confirmed that a recent fatal crash involving one of its vehicles occurred while the car was in autopilot mode.

Tesla released a blog post last week to provide more details about the accident on 23 March 2018, which took place in Mountain View, California, and killed the car's driver – Apple software engineer Wei Huang, 38.

The statement said that the Model X sports utility vehicle's autopilot function was engaged, and its adaptive cruise control follow-distance was set to minimum.

Driver failed to take control

It also suggested that the driver was given ample warning to override the function before the collision, but failed to take action.

"The driver had received several visual and one audible hands-on warning earlier in the drive and the driver's hands were not detected on the wheel for six seconds prior to the collision," said the statement.

"The driver had about five seconds and 150 metres of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken."

Related story Tesla's Autopilot reduced crashes by 40 per cent, finds US inquiry

Tesla also placed some of the blame on the road infrastructure, which had not been repaired since an earlier incident.

"The reason this crash was so severe is because the crash attenuator, a highway safety barrier which is designed to reduce the impact into a concrete lane divider, had been crushed in a prior accident without being replaced," it said. "We have never seen this level of damage to a Model X in any other crash."

Autopilot still safer says Tesla

The company assured that its autopilot function makes accidents less likely, not more, but requires drivers to remain vigilant and keep hands on the wheel to help avoid potential collisions.

The company's founder, billionaire entrepreneur Elon Musk, called the Model X the "safest SUV ever" when it launched in 2015. A US government study found that the autopilot function reduces accidents by 40 per cent, initiated after the first death from a crash that occurred while using the autopilot function in July 2016.

"The consequences of the public not using autopilot, because of an inaccurate belief that it is less safe, would be extremely severe," Tesla's statement said. "There are about 1.25 million automotive deaths worldwide. If the current safety level of a Tesla vehicle were to be applied, it would mean about 900,000 lives saved per year."

"We expect the safety level of autonomous cars to be 10 times safer than non-autonomous cars," the statement added, ending with condolences to the victim's family and friends.

Related story Tesla's electric Model X is the "safest SUV ever" says Elon Musk

The news comes at a troubling time for Tesla. Musk has reportedly taken over the management of its Model 3 electric car production after failing to meet first-quarter targets.

It has also been a bad period for autonomous cars. Last month, an Uber taxi killed a woman in the first fatal accident between a pedestrian and a self-driving car, while a crash involving a Tesla Model S that may have been in autopilot mode in January 2018 is currently under investigation.. The company said the driver, Wei Huang, 38, a software engineer for Apple, had received several visual and audible warnings to put his hands back on the steering wheel but had failed to do so, even though his Model X S.U.V. had the modified version of the software. His hands were not detected on the wheel for six seconds before his Model X slammed into a concrete divider near the junction of Highway 101 and 85 in Mountain View, and neither Mr. Huang nor the Autopilot activated the brakes before the crash.

The accident renews questions about Autopilot, a signature feature of Tesla vehicles, and whether the company has gone far enough to ensure that it keeps drivers and passengers safe.

“At the very least, I think there will have to be fundamental changes to Autopilot,” said Mike Ramsey, a Gartner analyst who focuses on self-driving technology. “The system as it is now tricks you into thinking it has more capability than it does. It’s not an autonomous system. It’s not a hands-free system. But that’s how people are using it, and it works fine, until it suddenly doesn’t.”

On Saturday, Tesla declined to comment on the California crash or to make Mr. Musk or another executive available for an interview. In its blog post on Friday about the crash, the company acknowledged that Autopilot “does not prevent all accidents,” but said the system “makes them much less likely to occur” and “unequivocally makes the world safer.”

For the company, the significance of the crash goes beyond Autopilot. Tesla is already reeling from a barrage of negative news. The value of its stock and bonds has plunged amid increasing concerns about how much cash it is using up and the repeated delays in the production of the Model 3, a battery-powered compact car that Mr. Musk is counting on to generate much-needed revenue.