COLORADO SPRINGS, Colo. (KRDO) - Colorado Springs attorney Zachariah Crabill thought he was filing a motion with cited cases that would favor his client’s argument, only to find out many of the cases were made up by Artificial Intelligence software ChatGPT.

Crabill, a licensed Colorado attorney for about a year and a half, was working on his first civil litigation case. He was defending a client that was accused of breaching a car payment agreement.

In a court document, admitting his mistake, Crabill said it was the first motion to set aside a summary judgment he had ever researched, drafted, and filed by himself. He had heard of ChatGPT, an artificial intelligence chatbot developed by OpenAI, trained to follow instructions in a prompt and provide a detailed response. So he turned to AI technology to help find case laws that would strengthen his client’s argument.

“I felt my lack of experience in legal research and writing, and consequently, my efficiency in this regard could be exponentially augmented to the benefit of my clients by expediting the time-intensive research portion of drafting,” Crabill said in a court document.

Ramsey Lama, a former Fremont County judge turned defense attorney, said law research can be very time-intensive, taking 20 to 30 hours for some cases. While he has never used ChatGPT, he said he can see how it would be useful in the legal industry.

“What previously took a lawyer maybe 40 hours or 20 hours to do, maybe a tool like this could condense it to two hours or maybe 30 minutes,” Lama said. “A quicker result for the client is better for the client. A quicker result for the client saves the client money.”

This was Crabill’s hope. At first, he asked ChatGPT about existing Colorado laws. He said the responses were accurate and he trusted the technology.

However, when Crabill started researching cases to cite in his motion, ChatGPT betrayed him without him even realizing it. The AI technology spit out dozens of cases similar to his client’s, only the cases didn’t exist.

Sean Williams, the director of the Technical Communication and Information Design Program at the University of Colorado-Colorado Springs, said this is a common mistake many people make when using AI technology.

“After we’re confident and comfortable with the information that we have, then we assume that the information it's giving us is correct,” Williams said. “This case shows it wasn't true and that's the downside.”

One of the cases ChatGPT cited for Crabill was Gonzales v. Allstate Ins. Co. from 2014. ChatGPT said in that case the defendant didn’t appear at a hearing but the court found her absence excusable. But this case doesn’t exist, at least not in its entirety. There is a Gonzales v. Allstate Ins. Co. case, but it’s from 2002 and involves a dispute over an insurance policy after a crash happened outside of the country.

“Based on the accuracy of prior validated responses, and the apparent accuracy of the case law citations, it never even dawned on me that this technology could be deceptive,” Crabill said in court documents.

Crabill claims he filed the motion without knowing he cited fake cases. On the day of the hearing, he realized his mistake. In a message to one of the Baker Law Firm’s legal aides, he said he thinks all the case cites from ChatGPT are “garbage” and he can’t find them in LexisNexis.

The judge overseeing the hearing couldn’t find the cases either and denied the motion due to the false citations. He then threatened to file a complaint against the attorney. The Office of Attorney Regulations couldn’t confirm if a complaint had been filed against Crabill.

Lama said Crabill violated his “duty of candor to the tribunal” — a lawyer’s obligation to not make material misrepresentations of fact or law. While Lama sees the benefits of AI technology, there are clear dangers, he said.

“Somebody uses software and it generates inaccurate information and they just submit it to the court — there's a danger right there on display,” Lama said.

Lama even joked he doesn’t want AI to “put (him) out of work.”

However, Williams is more optimistic. While he said the technology poses a threat to existing jobs, he also said it will create new ones.

“There's definitely a future for it in the workforce because it's already being used,” Williams said referencing a marketing company that was using ChatGPT to write copy drafts.

“It gives us the opportunity to see things or to imagine things that might not have occurred to us otherwise,” Williams said.

But the creativity of AI can also bring “falseness,” he said.

“In this particular case, it took existing data, existing information, and skewed it,” Williams said.. Zachariah Crabill, a rookie lawyer, claims he was fired after using OpenAI's ChatGPT to help him write a motion.

The 29-year-old said the AI-generated information was riddled with fake lawsuits.

Still, Crabill believes that AI has the power to make lawyers more productive.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

Thinking about using OpenAI's ChatGPT on the job? Be careful about false information — or risk getting fired.

This summer, Zachariah Crabill, a 29-year-old lawyer who previously worked at Baker Law Group, was fired after he used ChatGPT at work, he confirmed to Insider.

Crabill said he was feeling stressed about mounting deadlines and internal workplace dynamics when his bosses at the Colorado-based law firm added more work to his plate in May.

To get through it all, he turned to ChatGPT, which he had used before and trusted as an accurate research tool. He asked the chatbot to bolster a motion he had written with details from Colorado case law.

Advertisement

"When ChatGPT saved me hours of work, it was a tiny ray of sunlight in an otherwise abysmal situation," Crabill said in an email to Insider. "My experience is not unique, sadly I've heard many attorneys say they too were 'thrown to wolves' early in their career."

Once the motion was complete, the lawyer submitted it to his boss to review and, ultimately, filed it with the Colorado court — but he missed the critical step of checking the AI chatbot's work.

His excitement quickly turned into horror when he realized ChatGPT created multiple fake lawsuit citations in the motion.

Related stories

"I think all my cases cited from chatGPT are garbage … I can't even find the cases in Lexis…" Crabill said regarding the motion, according to screenshots of his text messages reviewed by Law Week Colorado.

Advertisement

The errors can likely be chalked up to hallucinations, when ChatGPT generates seemingly convincing responses that are actually not grounded in fact.

He told the judge he used the AI chatbot to help strengthen the document. The judge later reported him to a statewide office.

Soon after, he was fired, The Washington Post first reported. Crabill maintained to Insider that using ChatGPT was not the reason he was fired, though he didn't respond when asked for further clarification.

Despite losing his job, Crabill said he still believes AI has the power to make lawyers more productive. He has started his own company that offers legal services through AI.

Advertisement

"I still use ChatGPT in my day-to-day, much like most people use Google on the job," Crabill said.

Baker Law Group didn't respond to Insider's request for comment before publication.

Crabill's ChatGPT snafu isn't the first time lawyers used AI to the detriment of their employer, clients, or their jobs. In June, a New York law firm was fined $5,000 because one its lawyers used ChatGPT to write a court brief that referenced nonexistent cases and opinions.

Workers in other industries, too, claim they are being let go for using AI on the job.

Advertisement

In June, Tina Sendin, a marketing professional, claimed she got fired by a client after she used an AI writing tool to generate articles.. Zachariah Crabill was two years out of law school, burned out and nervous, when his bosses added another case to his workload this May. He toiled for hours writing a motion until he had an idea: Maybe ChatGPT could help? Within seconds, the artificial intelligence chatbot had completed the document. Crabill sent it to his boss for review and filed it with the Colorado court.

“I was over the moon excited for just the headache that it saved me,” he told The Washington Post. But his relief was short-lived. While surveying the brief, he realized to his horror that the AI chatbot had made up several fake lawsuit citations.

Crabill, 29, apologized to the judge, explaining that he’d used an AI chatbot. The judge reported him to a statewide office that handles attorney complaints, Crabill said. In July, he was fired from his Colorado Springs law firm. Looking back, Crabill wouldn’t use ChatGPT, but says it can be hard to resist for an overwhelmed rookie attorney.

Advertisement

“This is all so new to me,” he said. “I just had no idea what to do and no idea who to turn to.”

Business analysts and entrepreneurs have long predicted that the legal profession would be disrupted by automation. As a new generation of AI language tools sweeps the industry, that moment appears to have arrived.

Stressed-out lawyers are turning to chatbots to write tedious briefs. Law firms are using AI language tools to sift through thousands of case documents, replacing the work of associates and paralegals. AI legal assistants are helping lawyers analyze documents, memos and contracts in minutes.

The AI legal software market could grow from $1.3 billion in 2022 to upward of $8.7 billion by 2030, according to an industry analysis by the market research firm Global Industry Analysts. A report by Goldman Sachs in April estimated that 44 percent of legal jobs could be automated away, more than any other sector except for administrative work.

Advertisement

But these money-saving tools can come at a cost. Some AI chatbots are prone to fabricating facts, causing lawyers to be fired or fined, or to have cases thrown out. Legal professionals are racing to create guidelines for the technology’s use, to prevent inaccuracies from bungling major cases. In August, the American Bar Association launched a year-long task force to study the impacts of AI on law practice.

“It’s revolutionary,” said John Villasenor, a senior fellow at the Brookings Institution’s center for technological innovation. “But it’s not magic.”

AI tools that quickly read and analyze documents allow law firms to offer cheaper services and lighten the workload of attorneys, Villasenor said. But this boon can also be an ethical minefield when it results in high-profile errors.

In the spring, Lydia Nicholson, a Los Angeles housing attorney, received a legal brief relating to their client’s eviction case. But something seemed off. The document cited lawsuits that didn’t ring a bell. Nicholson, who uses they/them pronouns, did some digging and realized many were fake.

Advertisement

They discussed it with colleagues and “people suggested, ‘Oh, that seems like something that AI could have done,’” Nicholson said in an interview.

Nicholson filed a motion against the Dennis Block law firm, a prominent eviction firm in California, pointing out the errors. A judge agreed after an independent inquiry and issued the group a $999 penalty. The firm blamed a young, newly hired lawyer at its office for using “online research” to write the motion and said she had resigned shortly after the complaint was made. Several AI experts analyzed the briefing and proclaimed it “likely” generated by AI, according to the media site LAist.

The Dennis Block firm did not return a request for comment.

Share this article Share

It’s not surprising that AI chatbots invent legal citations when asked to write a brief, said Suresh Venkatasubramanian, a computer scientist and the director of the Center for Technology Responsibility at Brown University.

Advertisement

“What’s surprising is that they ever produce anything remotely accurate,” he said. “That’s not what they’re built to do.”

Rather, chatbots like ChatGPT are designed to make conversation, having been trained on vast amounts of published text to compose plausible-sounding responses to just about any prompt. So when you ask ChatGPT for a legal brief, it knows that legal briefs include citations — but it hasn’t actually read the relevant case law, so it makes up names and dates that seem realistic.

Judges are struggling with how to deal with these errors. Some are banning the use of AI in their courtroom. Others are asking lawyers to sign pledges to disclose if they have used AI in their work. The Florida Bar is weighing a proposal to require attorneys to have a client’s permission to use AI.

One point of discussion among judges is whether honor codes requiring attorneys to swear to the accuracy of their work apply to generative AI, said John G. Browning, a former Texas district court judge.

Browning, who chairs the State Bar of Texas’s task force on AI, said his group is weighing a handful of approaches to regulate use, such as requiring attorneys to take professional education courses in technology or considering specific rules for when evidence generated by AI can be included.

Advertisement

Lucy Thomson, a D.C.-area attorney and cybersecurity engineer who is chairing the American Bar Association’s AI task force, said the goal is to educate lawyers about both the risks and potential benefits of AI. The bar association has not yet taken a formal position on whether AI should be banned from courtrooms, she added, but its members are actively discussing the question.

“Many of them think it’s not necessary or appropriate for judges to ban the use of AI,” Thomson said, “because it’s just a tool, just like other legal research tools.”

In the meantime, AI is increasingly being used for “e-discovery”— the search for evidence in digital communications, such as emails, chats or online workplace tools.

While previous generations of technology allowed people to search for specific keywords and synonyms across documents, today’s AI models have the potential to make more sophisticated inferences, said Irina Matveeva, chief of data science and AI at Reveal, a Chicago-based legal technology company. For instance, generative AI tools might have allowed a lawyer on the Enron case to ask, “Did anyone have concerns about valuation at Enron?” and get a response based on the model’s analysis of the documents.

Advertisement

Wendell Jisa, Reveal’s CEO, added that he believes AI tools in the coming years will “bring true automation to the practice of law — eliminating the need for that human interaction of the day-to-day attorneys clicking through emails.”

Jason Rooks, the chief information officer for a Missouri school district, said he began to be overwhelmed during the coronavirus pandemic with requests for electronic records from parents litigating custody battles or organizations suing schools over their covid-19 policies. At one point, he estimates, he was spending close to 40 hours a week just sifting through emails.

Instead, he hit on an e-discovery tool called Logikcull, which says it uses AI to help sift through documents and predict which ones are most likely to be relevant to a given case. Rooks could then manually review that smaller subset of documents, which cut the time he spent on each case by more than half. (Reveal acquired Logikcull in August, creating a legal tech company valued at more than $1 billion.)

Advertisement

But even using AI for legal grunt work such as e-discovery comes with risks, said Venkatasubramanian, the Brown professor: “If they’ve been subpoenaed and they produce some documents and not others because of a ChatGPT error — I’m not a lawyer, but that could be a problem.”

Those warnings won’t stop people like Crabill, whose misadventures with ChatGPT were first reported by the Colorado radio station KRDO. After he submitted the error-laden motion, the case was thrown out for unrelated reasons.

He says he still believes AI is the future of law. Now, he has his own company and says he’s likely to use AI tools designed specifically for lawyers to aid in his writing and research, instead of ChatGPT. He said he doesn’t want to be left behind.. A Colorado lawyer has been suspended for a year for using an AI chatbot to draft a legal document.

A judge ordered Zachariah Crabill to a one year and one day suspension after Crabill allegedly "cited case law that he found through the artificial intelligence platform ChatGPT" to to draft a motion in a civil case in May. The platform them produced cases that were incorrect and fictitious.

Crabill allegedly did not alert the court to the existence of the incorrect cases after he filed the motion. He also did not withdraw the motion.

A ruling handed down notes that when the judge asked Crabill about possible inaccuracies, Crabill attributed the mistakes to a legal intern. Six days after the hearing, he filed an affidavit explaining that he used the platform when he drafted the motion.

The presiding judge noted that Crabill violated several of the Colorado Bar Association's Rules of Professional Conduct. If he successfully completes a 90-day suspension and two subsequent years of probation, the rest of the year suspension will be forgiven.