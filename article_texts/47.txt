ARTICLE TITLE: LinkedIn Search Prefers Male Names
The question of whether a computer can be biased or not may seem frivolous, but it could make all the difference when it comes to being found online.

Now, an investigation by a US newspaper has suggested that this bias may be present on the biggest professional networking site in the world.

It found that a search of common female names on LinkedIn returned suggestions for related male names.

An investigation by a US newspaper has suggested that gender bias may be present on the biggest professional networking site in the world, by suggesting male names when searching for female professionals. LinkedIn has denied its algorithms use gender

LINKEDIN 'SEARCH BIAS' An investigation has claimed LinkedIn's search function carries a gender bias. When searching for a common female names, such as 'Andrea Jones', it also returned suggestions for male equivalents, such as 'Andrew Jones'. The bias was found not to work the other way, suggesting female equivalents in searches for male professionals. LinkedIn has said its algorithms are not based on gender but on previous searches of its 450 million users. Advertisement

According to the report in The Seattle Times, the same pattern works for at least a dozen common female names in the US.

What’s more, the apparent bias seems to be a one way street.

When searching for common male names, there are no suggestions of women with a similar name.

It claims that a search for the common name ‘Stephanie Williams’ suggests ‘Stephen Williams’.

Other examples include a search for ‘Andrea Jones’ which brings back many details for ‘Andrew Jones’.

MailOnline was able to replicate some of the results.

LinkedIn is the largest professional networking platform in the world, claiming more than 450 million users. It is set to be bought by software giant Microsoft in a deal worth an estimated £17.7 billion ($26.2 bn).

Responding to the claims, LinkedIn said that searches are based on common search queries by its 450 million users, based on similarly spelled names, having nothing to do with a user’s sex.

The investigation claims that a search for common female names results in suggestions for male professionals, such as a search for ‘Andrea Jones’ brings back many suggestions for ‘Andrew Jones’ (pictured)

LinkedIn is the largest professional networking platform in teh world, claiming more than 450 million users. It is set to be bought by software giant Microsoft in a deal worth an estimated £17.7 billion ($26.2 bn)

CAN COMPUTERS BE BIASED? When teaching machines how to process language, programmers can use word embedding algorithms. These programs enable computers to use machine learning to process language based on learned examples. An example is when a computer has to find related words using the 'she is to he' comparison. This can be used to find accurate pairs of words like she:he, such as sister:brother or queen:king. But using real world sources, such as news articles and websites, can lead to gender biases creeping in. For example, occupations associated with 'he' can be philosopher, fighter pilot, or boss. But occupations associated with 'she' included homemaker, socialite, receptionist and hairdresser. Researchers are trying to combat this bias by teaching machines to ignore certain relationships between words. Advertisement

A spokesperson for the networking platform told MailOnline: ‘The search algorithm is guided by relative frequencies of words appearing in past queries and member profiles; it's not to do with gender.

‘To fix unintended spelling suggestions that are similar sounding we rolled out a change that explicitly recognises people's names so that the algorithm doesn't try to correct them into another name – of the same or different gender.

‘As with all machine-learned systems, there are always edge cases and we are constantly working hard to improve and create the best possible experience for our members.’

As machine learning algorithms are increasingly used to deal with complex queries, instances of inherent bias are coming to light.

One example is algorithms used to predict rates of recidivism in former criminals in the US, based on social and societal factors.

Some reports have claimed these predictive algorithms may skew results cording to race, with African Americans facing more negative outcomes.

While other examples include a technique called word embedding, which teaches machines how to process language by finding relationships between words.

But when the computer searches real world sources, the embedding approach can pick up on inherent gender stereotypes.. Prior to the update, searches for 100 of the most common male names in the US did not result in prompts suggesting female versions of those names, the Seattle Times said.