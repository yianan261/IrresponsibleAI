Washington, DC CNN —

The Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday.

The Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly.

The Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. CNN Business was first to report last month the driver’s claim that “full self-driving” was active.

The National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The agency typically conducts special investigations into about 100 crashes a year.

The pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla had previously restricted access to drivers with high scores on its safety rating system.

“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals, but despite Tesla’s name for it, it requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some Tesla drivers but also alarmed others with its flaws. Drivers are warned on an in-car screen by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”

Tesla generally does not engage with the professional news media and did not respond to CNN’s request for comment.

“We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years,” Tesla said this month in an update to its vehicle safety data.

Traffic safety experts have long questioned the merits of Tesla’s findings, which show fewer crashes when the driver-assist technologies are active, because among other things they’re generally used on highways where crashes are already rarer.

Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement.

The agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety.

Bryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features.

“How many more crashes will there be before NHTSA releases findings?” Reimer said.

Reimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.”

This story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.. This is not good. This is not good at all.

Found Footage

New footage just dropped of a Tesla self-driving accident that caused a pileup on the San Francisco Bay Bridge — a serious accident that even resulted in a toddler being injured.

Obtained by The Intercept via a California Public Records Act request, the security camera footage of the November 24, 2022 crash seems to corroborate eyewitness accounts, with the car abruptly switching into the fast lane of one of the bridge's underpasses before braking, leading to an eight-car pileup.

As the Intercept's report notes, nine people — including a two-year-old child — were injured in the accident.

I obtained surveillance footage of the self-driving Tesla that abruptly stopped on the Bay Bridge, resulting in an eight-vehicle crash that injured 9 people including a 2 yr old child just hours after Musk announced the self-driving feature. Full story: https://t.co/LaEvX9TzxW pic.twitter.com/i75jSh2UpN — Ken Klippenstein (@kenklippenstein) January 10, 2023

The Thanksgiving Day accident occurred just hours after Tesla CEO took to Twitter to announce that the company's controversial full self-driving (FSD) beta testing would be available to any North American driver who requested it.

According to a California Highway Patrol report, the driver told police that he'd been using FSD when suddenly, the car's left turn signal and brakes activated. The driver also seemed to suggest that the car drove itself into the left-most lane before "slowing to a stop" right in front of another car, catalyzing the pileup.

Tell Me Why

This latest example of the growing problem surrounding Tesla's Autopilot and FSD capabilities is particularly egregious because of its timing on Thanksgiving Day — and on the very day Musk expanded it to all North American users.

Add in this new footage and this accident becomes a picture-perfect example of why federal authorities and consumers alike are so concerned about Tesla's automation efforts, which federal regulators suspect to be way more deadly than assisted driving software offered by other electric vehicle manufacturers.

The Intercept report notes that in spite of this increasingly-alarming problem, however, there still aren't any federal regulations about autonomous vehicle testing on the books, though some states have begun implementing their own restrictions.

More on Tesla: Tesla Sighs and Lets Customers Have a Normal Steering Wheel Instead of Weird Misshapen Yoke Thing