Washington CNN —

A driver told authorities that their Tesla’s “full-self-driving” software braked unexpectedly and triggered an eight-car pileup in the San Francisco Bay Area last month that led to nine people being treated for minor injuries including one juvenile who was hospitalized, according to a California Highway Patrol traffic crash report.

CNN Business obtained the report detailing the crash through a public records request Wednesday. California Highway Patrol reviewed videos that show the Tesla vehicle changing lanes and slowing to a stop.

California Highway Patrol said in the Dec. 7 report that it could not confirm if “full self-driving” was active at the time of the crash. A highway patrol spokesperson told CNN Business on Wednesday that it would not determine if “full self-driving” was active, and Tesla would have that information.

The crash occurred about lunchtime on Thanksgiving, snarling traffic on Interstate 80 east of the Bay Bridge as two lanes of traffic were closed for about 90 minutes as many people traveled to holiday events. Four ambulances were called to the scene.

The pileup took place just hours after Tesla CEO Elon Musk had announced that Tesla’s driver-assist software “full self-driving” was available to anyone in North America who requested it. Tesla had previously restricted access to drivers with high safety scores on its rating system.

“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals. It requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some drivers but also alarmed others with its limitations. Drivers are warned by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”

The report states that the Tesla Model S was traveling at about 55 mph and shifted into the far left-hand lane, but then braked abruptly, slowing the car to about 20 mph. That led to a chain reaction that ultimately involved eight vehicles to crash, all of which had been traveling at typical highway speeds.

Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive.”

The agency has received hundreds of complaints from Tesla drivers. Some have described near crashes and concerns about their safety. This summer NHTSA upgraded the investigation to what it calls an engineering analysis, an indication that it’s seriously considering a recall.

NHTSA told CNN Business a few days after the Thanksgiving Day crash that it was gathering addition information from Tesla and law enforcement about the crash.. The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month.

The crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.

It is the latest in a series of accidents blamed on Tesla technology. The electric automaker’s chief executive, Elon Musk, has heavily promoted “Full Self-Driving” (FSD) software, sold as $15,000 add-on to Tesla vehicles, but it faces legal, regulatory and public scrutiny.

After the San Francisco accident, the driver told police the FSD software malfunctioned.

The police report said the vehicle was traveling at 55mph when it shifted lane but braked abruptly, slowing the car to about 20mph. That led to another vehicle hitting the Tesla and a chain reaction of crashes, according to Reuters.

However, police were unable to determine if the software was in operation or that the driver’s account was accurate. The report was made public after a records request.

The crash occurred hours after Musk said Tesla would make FSD software available to anyone in North America who requested it. It previously offered the system only to drivers with high safety scores.

The police report said that if FSD malfunctioned, the driver should have manually taken control. Tesla has repeatedly said its advanced self-driving technology requires “active driver supervision” and its vehicles “are not autonomous”.

Drivers are also warned when they install FSD that it “may do the wrong thing at the worst time”.

The National Highway Traffic Safety Administration (NHTSA), which is investigating Tesla after reports of braking “without warning, at random, and often repeatedly in a single drive”, did not immediately comment on the San Francisco crash.

Last summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if “full self-driving” is an accurate description of the technology – and said Tesla must do more to prevent misuse.. . The U.S. government’s highway safety agency said Thursday it will send teams to investigate two November crashes in California and Ohio involving Teslas that may have been operating on automated driving systems.

The probes bring to 35 the number of crashes investigated by the National Highway Traffic Safety Administration since 2016 in which either Tesla’s “Full Self-Driving” or “Autopilot” systems likely were in use. Nineteen people were killed in the crashes.

The California crash occurred on Thanksgiving Day involving eight vehicles on the San Francisco-Oakland Bay Bridge. The driver told authorities that the Tesla Model S was using the company’s “Full Self-Driving” software, according to Highway Patrol report obtained by CNN.

The Ohio crash happened Nov. 18 near Toledo, when a Tesla Model 3 crashed into an Ohio Highway Patrol SUV stopped on a roadway with its emergency lights flashing.

A message was left Thursday seeking comment from Tesla on the latest NHTSA action. The company based in Austin, Texas, has disbanded its media relations department.

The agency said Thursday that it sent the team to the California crash after gathering information from law enforcement officers and Tesla.

The eight-vehicle crash happened about noon, closing two lanes and clogging traffic on the holiday. Nine people were treated for minor injuries including a child who was hospitalized, according to CNN, which got a copy of the crash report through a public records request.

The Tesla Model S driver reportedly told the California Highway Patrol that the company’s “Full Self-Driving” system was operating when the crash occurred, and that it braked unexpectedly while traveling at 55 miles per hour (88.5 kilometers per hour). The Model S shifted into the far left lane, then braked to 20 mph, causing the pileup, CNN said the report stated.

In the crash near Toledo, an Ohio State Highway Patrol trooper and another motorist were injured when a Tesla Model 3 struck the rear of the police SUV parked on a road with its emergency lights flashing.

The Toledo Blade reported that the highway patrol report didn’t address whether any automated systems were operating.

NHTSA has been investigating Tesla automated systems for 6 1/2 years without taking enforcement action.

Michael Brooks, executive director of the nonprofit Center for Auto Safety, a watchdog group, said it’s long past time for the agency to seek a recall.

“The question is what’s the threshold number of injuries and deaths and cars driving stupidly that we have to see before NHTSA finds that there’s some sort of defect in these cars?” Brooks asked.

He said NHTSA has only published information on one of the Tesla crash investigations and called on the agency to be more transparent.

A message was left for a NHTSA spokeswoman seeking comment on the length of the investigations.

In addition to the crash investigation teams, NHTSA already has two formal investigations open into Tesla’s advanced driver assistance systems. One involves complaints that the vehicles brake unexpectedly for no reason, and the other stems from crashes into emergency vehicles parked on roadways with lights flashing.

NHTSA hasn’t made public any enforcement actions on either probe. The agency also has ordered automakers and tech companies to report crashes involving automated systems.

In the “phantom braking” investigation, the agency said that more than 750 Tesla owners have complained that cars operating on the partially automated driving systems have suddenly stopped on roadways for no apparent reason.

In opening the probe, the agency said it was looking into vehicles equipped with automated driver-assist features such as adaptive cruise control and “Autopilot,” which allows them to automatically brake and steer within their lanes.

“Complainants report that the rapid deceleration can occur without warning, and often repeatedly during a single drive cycle,” the agency said.

Many owners wrote in their complaints that they feared a rear-end crash on a freeway.

Although Tesla calls one of its systems “Full Self Driving,” even CEO Elon Musk concedes that it’s not ready to drive itself. The other system, called “Autopilot,” keeps a car centered in its lane and a distance away from vehicles in front of it. In both cases Tesla says these are driver-assist systems and that drivers must be ready to intervene at all times.

Critics say humans become too reliant on the systems when they’re not capable of handling all situations.. Highway surveillance footage from Thanksgiving Day shows a Tesla Model S vehicle changing lanes and then abruptly braking in the far-left lane of the San Francisco Bay Bridge, resulting in an eight-vehicle crash. The crash injured nine people, including a 2-year-old child, and blocked traffic on the bridge for over an hour.

The video and new photographs of the crash, which were obtained by The Intercept via a California Public Records Act request, provides the first direct look at what happened on November 24, confirming witness accounts at the time. The driver told police that he had been using Tesla’s new “Full Self-Driving” feature, the report notes, before the Tesla’s “left signal activated” and its “brakes activated,” and it moved into the left lane, “slowing to a stop directly in [the second vehicle’s] path of travel.”

Just hours before the crash, Tesla CEO Elon Musk had triumphantly announced that Tesla’s “Full Self-Driving” capability was available in North America, congratulating Tesla employees on a “major milestone.” By the end of last year, Tesla had rolled out the feature to over 285,000 people in North America, according to the company.

Tesla Full Self-Driving Beta is now available to anyone in North America who requests it from the car screen, assuming you have bought this option. Congrats to Tesla Autopilot/AI team on achieving a major milestone! — Elon Musk (@elonmusk) November 24, 2022

The National Highway Traffic Safety Administration, or NHTSA, has said that it is launching an investigation into the incident. Tesla vehicles using its “Autopilot” driver assistance system — “Full Self-Driving” mode has an expanded set of features atop “Autopilot” — were involved in 273 known crashes from July 2021 to June of last year, according to NHTSA data. Teslas accounted for almost 70 percent of 329 crashes in which advanced driver assistance systems were involved, as well as a majority of fatalities and serious injuries associated with them, the data shows. Since 2016, the federal agency has investigated a total of 35 crashes in which Tesla’s “Full Self-Driving” or “Autopilot” systems were likely in use. Together, these accidents have killed 19 people.

In recent months, a surge of reports have emerged in which Tesla drivers complained of sudden “phantom braking,” causing the vehicle to slam on its brakes at high speeds. More than 100 such complaints were filed with NHTSA in a three-month period, according to the Washington Post.

The child injured in the crash was a 2-year-old who suffered an abrasion to the rear left side of his head as well as a bruise, according to the incident detail report obtained by The Intercept. In one photograph of the crash, a stroller is parked in front of the car in which the child was injured.

Photo: California Highway Patrol

As traditional car manufacturers enter the electric vehicle market, Tesla is increasingly under pressure to differentiate itself. Last year, Musk said that “Full Self-Driving” was an “essential” feature for Tesla to develop, going as far as saying, “It’s really the difference between Tesla being worth a lot of money or worth basically zero.”

The term “Full Self-Driving” has been criticized by other manufacturers and industry groups as misleading and even dangerous. Last year, the autonomous driving technology company Waymo, owned by Google’s parent company, announced that it would no longer be using the term.

“Unfortunately, we see that some automakers use the term ‘self-driving’ in an inaccurate way, giving consumers and the general public a false impression of the capabilities of driver assist (not fully autonomous) technology,” Waymo wrote in a blog post. “That false impression can lead someone to unknowingly take risks (like taking their hands off the steering wheel) that could jeopardize not only their own safety but the safety of people around them.”

Though Waymo doesn’t name any names, the statement was “clearly motivated by Musk’s controversial decision to use the term ‘Full Self Driving,’” according to The Verge.

Along the same lines, the premier lobbying group for self-driving cars recently rebranded from the “Self-Driving Coalition for Safer Streets” to the “Autonomous Vehicle Industry Association.” The change, the industry group said, reflected its “commitment to precision and consistency in how the industry, policymakers, journalists and the public talk about autonomous driving technology.”

Secretary of Transportation Pete Buttigieg has also been critical of the emerging driver assistance technologies, which he stresses have not replaced the need for an alert human driver. “I keep saying this until I’m blue in the face: Anything on the market today that you can buy is a driver assistance technology, not a driver replacement technology,” Buttigieg said. “I don’t care what it’s called. We need to make sure that we’re crystal clear about that — even if companies are not.”

Though the language may be evolving, there are still no federal restrictions on the testing of autonomous vehicles on public roads, though states have imposed limits in certain cases. Tesla has not announced any changes to the program or its branding, but the crash was one of multiple that month. Several days prior to the Bay Bridge accident, on November 18 in Ohio, a Tesla Model 3 crashed into a stopped Ohio State Highway Patrol SUV which had its hazard lights flashing. The Tesla is likewise suspected of having been in self-driving mode and is also being investigated by NHTSA.

NHTSA is also investigating a tweet by Musk in which he said that “Full Self-Driving” users would soon be given the option to turn off reminder notifications for drivers to keep their hands on the steering wheel. “Users with more than 10,000 miles on FSD Beta should be given the option to turn off the steering nag,” a Twitter user posted on New Year’s Eve, tagging Musk.

“Agreed, update coming in Jan,” Musk replied.

Additional reporting by Beth Bourdon.. This is not good. This is not good at all.

Found Footage

New footage just dropped of a Tesla self-driving accident that caused a pileup on the San Francisco Bay Bridge — a serious accident that even resulted in a toddler being injured.

Obtained by The Intercept via a California Public Records Act request, the security camera footage of the November 24, 2022 crash seems to corroborate eyewitness accounts, with the car abruptly switching into the fast lane of one of the bridge's underpasses before braking, leading to an eight-car pileup.

As the Intercept's report notes, nine people — including a two-year-old child — were injured in the accident.

I obtained surveillance footage of the self-driving Tesla that abruptly stopped on the Bay Bridge, resulting in an eight-vehicle crash that injured 9 people including a 2 yr old child just hours after Musk announced the self-driving feature. Full story: https://t.co/LaEvX9TzxW pic.twitter.com/i75jSh2UpN — Ken Klippenstein (@kenklippenstein) January 10, 2023

The Thanksgiving Day accident occurred just hours after Tesla CEO took to Twitter to announce that the company's controversial full self-driving (FSD) beta testing would be available to any North American driver who requested it.

According to a California Highway Patrol report, the driver told police that he'd been using FSD when suddenly, the car's left turn signal and brakes activated. The driver also seemed to suggest that the car drove itself into the left-most lane before "slowing to a stop" right in front of another car, catalyzing the pileup.

Tell Me Why

This latest example of the growing problem surrounding Tesla's Autopilot and FSD capabilities is particularly egregious because of its timing on Thanksgiving Day — and on the very day Musk expanded it to all North American users.

Add in this new footage and this accident becomes a picture-perfect example of why federal authorities and consumers alike are so concerned about Tesla's automation efforts, which federal regulators suspect to be way more deadly than assisted driving software offered by other electric vehicle manufacturers.

The Intercept report notes that in spite of this increasingly-alarming problem, however, there still aren't any federal regulations about autonomous vehicle testing on the books, though some states have begun implementing their own restrictions.

More on Tesla: Tesla Sighs and Lets Customers Have a Normal Steering Wheel Instead of Weird Misshapen Yoke Thing. Washington, DC CNN —

The Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday.

The Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly.

The Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. CNN Business was first to report last month the driver’s claim that “full self-driving” was active.

The National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The agency typically conducts special investigations into about 100 crashes a year.

The pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla had previously restricted access to drivers with high scores on its safety rating system.

“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals, but despite Tesla’s name for it, it requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some Tesla drivers but also alarmed others with its flaws. Drivers are warned on an in-car screen by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”

Tesla generally does not engage with the professional news media and did not respond to CNN’s request for comment.

“We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years,” Tesla said this month in an update to its vehicle safety data.

Traffic safety experts have long questioned the merits of Tesla’s findings, which show fewer crashes when the driver-assist technologies are active, because among other things they’re generally used on highways where crashes are already rarer.

Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement.

The agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety.

Bryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features.

“How many more crashes will there be before NHTSA releases findings?” Reimer said.

Reimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.”

This story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.. I’m not sure how much you keep up with bridge-related holiday car crashes, but there was a huge one this past Thanksgiving on the San Francisco Bay Bridge. This was a genuine pileup, with eight vehicles involved and nine people injured. That’s already big news, but what makes this bigger big news is that the pileup seems to have been caused by a Tesla that was operating under the misleadingly-named Full Self-Driving beta software, according to the driver. As you likely already know, the inclusion of the nouns “Tesla” and the string of words “full self-driving” is internet click-catnip, but that’s not really what I want to focus on here. What this crash really demonstrates are the inherent conceptual – not technological – problems of all Level 2 semi-automated driving systems. Looking at what happened in this crash, it’s hard not to see it as an expensive, inconvenient demonstration of something called “the vigilance problem.” I’ll explain.

First, let’s go over just what happened. Thanks to a California Public Records Act request from the website The Intercept, video and photographs of the crash are available, as is the full police report of the incident. The crash happened on I-80 eastbound, in the lower level of the Bay Bridge. There’s five lanes of traffic there, and cars were moving steadily at around 55 mph; there appeared to be no obstructions and good visibility. Nothing unusual at all.

A Tesla was driving in the second lane from the left, and had its left turn signal on. The Tesla began to slow, despite no traffic anywhere ahead of it, then pulled into the leftmost lane and came to a complete stop — on the lower level of a bridge, with traffic all around it going between 50 and 60 mph or so. The results were grimly predictable, with cars stopping suddenly behind the now-immobile Tesla, leading to the eight-car crash.

Here’s what it looked like from the surveillance cameras:

ADVERTISEMENT

…and here’s the diagram from the police report:

According to the police report, here’s what the Tesla (referred to in the report as V-1) said of what happened

“He was driving V-1 on I-80 eastbound traveling at 50 miles per hour in the #1 lane. V-1 was in Full Auto mode when V-1 slowed to 20 miles per hour when he felt a rear impact… He was driving V-1 on I-80 eastbound in Full Self Driving Mode Beta Version traveling at approximately 55 miles per hour…When V-1 was in the tunnel, V-1 moved from the #2 lane into the #1 lane and started slowing down unaccountably.”

So, the driver’s testimony was that the car was in Full Self-Driving (FSD) mode, and it would be easy to simply blame all of this on the demonstrated technological deficiencies of FSD Beta. This could be an example of “phantom braking,” where the system becomes confused and attempts to stop the car even when there are no obstacles in its path. It could have been the system disengaged for some reason and attempted to get the driver to take over, or it could be caused by any number of technological issues, but that’s not really what the underlying problem is.

This is the sort of wreck that, it appears, would be extremely unlikely to happen to a normal, unimpaired driver (unless, say, the car depleted its battery, though the police report states that the Tesla was driven away, so it wasn’t that) because there was really no reason for it to happen at all. It’s about the simplest driving situation possible: full visibility, moderate speed, straight line, light traffic. And, of course, if the driver was using this Level 2 system as intended – remember, even though the system is called Full Self-Driving, it is still only a semi-automated system that requires a driver’s full, nonstop attention and a readiness to take over at any moment, which is something the “driver” of this Tesla clearly did not do.

ADVERTISEMENT

Of course, Tesla knows this and we all technically know this and the police even included a screengrab from Tesla’s site that states this in its report:

We all know this basic fact about L2 systems, that they must be watched nonstop, but what we keep seeing is that people are just not good at doing this. This is a drum I’ve been banging for years and years, and sometimes I think to myself: “Enough already, people get it,” but then I’ll see a crash like this, where a car just does something patently idiotic and absurd and entirely, easily preventable if the dingus behind the wheel would just pay the slightest flapjacking bit of attention to the world outside, and I realize that, no, people still don’t get it.

So I’m going to say it again. While, yes, Tesla’s system was the particular one that appears to have failed here, and yes, the system is deceptively named in a way that encourages this idiotic behavior, this is not a problem unique to Tesla. It’s not a technical problem. You can’t program your way out of the problem with Level 2; in fact, the better the Level 2 system seems to be, the worse the problem gets. That problem is that human beings are simply no good at monitoring systems that do most of the work of a task and remaining ready to take over that task with minimal to no warning.

ADVERTISEMENT

This isn’t news to people who pay attention. It’s been proven since 1948, when N.H. Mackworth published his study The Breakdown of Vigilance During Prolonged Visual Search which defined what has come to be known as the “vigilance problem.” Essentially, the problem is that people are just not great at paying close attention to monitoring tasks, and if a semi-automated driving system is doing most of the steering, speed control, and other aspects of the driving task, the human in the driver’s seat’s job changes from one of active control to one of monitoring for when the system may make an error. The results of the human not performing this task well are evidenced by the crash we’re talking about.

I think it’s not unreasonable to think of Level 2 driving as potentially impaired driving, because the mental focus of the driver when engaging with the driving task from a monitoring approach is impaired when compared to an active driver.

I know lots of people claim that systems like these make driving safer – and they certainly can, in a large number of contexts. But they also introduce significant and new points of failure that simply do not need to be introduced. The same safety benefits can be had if the Level 2 paradigm was flipped, where the driver was always in control, but the semi-automated driving system was doing the monitoring, and was ready to take over if it detected dangerous choices by the human driver. This would help in situations of a tired or distracted or impaired driver, but would be less sexy in that the act of driving wouldn’t feel any different than normal human driving.

If we take anything away from this wreck, it shouldn’t be that Tesla’s FSD Beta is the real problem here. It’s technically impressive in many ways though certainly by no means perfect; it’s also not the root of what’s wrong, which is Level 2 itself. We need to stop pretending this is a good approach, and start being realistic about the problems it introduces. Cars aren’t toys, and as much fun as it is to show off your car pretending to drive itself to your buddies, the truth is it can’t, and when you’re behind the wheel, you’re in charge — no question, no playing around.

If you want to read about this even more, for some reason, I might know of a book you could get. Just saying.

ADVERTISEMENT

New IIHS Study Confirms What We Suspected About Tesla’s Autopilot And Other Level 2 Driver Assist Systems: People Are Dangerously Confused

Level 3 Autonomy Is Confusing Garbage

Support our mission of championing car culture by becoming an Official Autopian Member.

ADVERTISEMENT

Got a hot tip? Send it to us here. Or check out the stories on our homepage.