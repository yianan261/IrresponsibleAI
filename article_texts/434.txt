The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month.

The crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.

It is the latest in a series of accidents blamed on Tesla technology. The electric automaker’s chief executive, Elon Musk, has heavily promoted “Full Self-Driving” (FSD) software, sold as $15,000 add-on to Tesla vehicles, but it faces legal, regulatory and public scrutiny.

After the San Francisco accident, the driver told police the FSD software malfunctioned.

The police report said the vehicle was traveling at 55mph when it shifted lane but braked abruptly, slowing the car to about 20mph. That led to another vehicle hitting the Tesla and a chain reaction of crashes, according to Reuters.

However, police were unable to determine if the software was in operation or that the driver’s account was accurate. The report was made public after a records request.

The crash occurred hours after Musk said Tesla would make FSD software available to anyone in North America who requested it. It previously offered the system only to drivers with high safety scores.

The police report said that if FSD malfunctioned, the driver should have manually taken control. Tesla has repeatedly said its advanced self-driving technology requires “active driver supervision” and its vehicles “are not autonomous”.

Drivers are also warned when they install FSD that it “may do the wrong thing at the worst time”.

The National Highway Traffic Safety Administration (NHTSA), which is investigating Tesla after reports of braking “without warning, at random, and often repeatedly in a single drive”, did not immediately comment on the San Francisco crash.

Last summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if “full self-driving” is an accurate description of the technology – and said Tesla must do more to prevent misuse.. Washington CNN —

A driver told authorities that their Tesla’s “full-self-driving” software braked unexpectedly and triggered an eight-car pileup in the San Francisco Bay Area last month that led to nine people being treated for minor injuries including one juvenile who was hospitalized, according to a California Highway Patrol traffic crash report.

CNN Business obtained the report detailing the crash through a public records request Wednesday. California Highway Patrol reviewed videos that show the Tesla vehicle changing lanes and slowing to a stop.

California Highway Patrol said in the Dec. 7 report that it could not confirm if “full self-driving” was active at the time of the crash. A highway patrol spokesperson told CNN Business on Wednesday that it would not determine if “full self-driving” was active, and Tesla would have that information.

The crash occurred about lunchtime on Thanksgiving, snarling traffic on Interstate 80 east of the Bay Bridge as two lanes of traffic were closed for about 90 minutes as many people traveled to holiday events. Four ambulances were called to the scene.

The pileup took place just hours after Tesla CEO Elon Musk had announced that Tesla’s driver-assist software “full self-driving” was available to anyone in North America who requested it. Tesla had previously restricted access to drivers with high safety scores on its rating system.

“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals. It requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some drivers but also alarmed others with its limitations. Drivers are warned by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”

The report states that the Tesla Model S was traveling at about 55 mph and shifted into the far left-hand lane, but then braked abruptly, slowing the car to about 20 mph. That led to a chain reaction that ultimately involved eight vehicles to crash, all of which had been traveling at typical highway speeds.

Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive.”

The agency has received hundreds of complaints from Tesla drivers. Some have described near crashes and concerns about their safety. This summer NHTSA upgraded the investigation to what it calls an engineering analysis, an indication that it’s seriously considering a recall.

NHTSA told CNN Business a few days after the Thanksgiving Day crash that it was gathering addition information from Tesla and law enforcement about the crash.. Highway surveillance footage from Thanksgiving Day shows a Tesla Model S vehicle changing lanes and then abruptly braking in the far-left lane of the San Francisco Bay Bridge, resulting in an eight-vehicle crash. The crash injured nine people, including a 2-year-old child, and blocked traffic on the bridge for over an hour.

The video and new photographs of the crash, which were obtained by The Intercept via a California Public Records Act request, provides the first direct look at what happened on November 24, confirming witness accounts at the time. The driver told police that he had been using Tesla’s new “Full Self-Driving” feature, the report notes, before the Tesla’s “left signal activated” and its “brakes activated,” and it moved into the left lane, “slowing to a stop directly in [the second vehicle’s] path of travel.”

Just hours before the crash, Tesla CEO Elon Musk had triumphantly announced that Tesla’s “Full Self-Driving” capability was available in North America, congratulating Tesla employees on a “major milestone.” By the end of last year, Tesla had rolled out the feature to over 285,000 people in North America, according to the company.

Tesla Full Self-Driving Beta is now available to anyone in North America who requests it from the car screen, assuming you have bought this option. Congrats to Tesla Autopilot/AI team on achieving a major milestone! — Elon Musk (@elonmusk) November 24, 2022

The National Highway Traffic Safety Administration, or NHTSA, has said that it is launching an investigation into the incident. Tesla vehicles using its “Autopilot” driver assistance system — “Full Self-Driving” mode has an expanded set of features atop “Autopilot” — were involved in 273 known crashes from July 2021 to June of last year, according to NHTSA data. Teslas accounted for almost 70 percent of 329 crashes in which advanced driver assistance systems were involved, as well as a majority of fatalities and serious injuries associated with them, the data shows. Since 2016, the federal agency has investigated a total of 35 crashes in which Tesla’s “Full Self-Driving” or “Autopilot” systems were likely in use. Together, these accidents have killed 19 people.

In recent months, a surge of reports have emerged in which Tesla drivers complained of sudden “phantom braking,” causing the vehicle to slam on its brakes at high speeds. More than 100 such complaints were filed with NHTSA in a three-month period, according to the Washington Post.

The child injured in the crash was a 2-year-old who suffered an abrasion to the rear left side of his head as well as a bruise, according to the incident detail report obtained by The Intercept. In one photograph of the crash, a stroller is parked in front of the car in which the child was injured.

Photo: California Highway Patrol

As traditional car manufacturers enter the electric vehicle market, Tesla is increasingly under pressure to differentiate itself. Last year, Musk said that “Full Self-Driving” was an “essential” feature for Tesla to develop, going as far as saying, “It’s really the difference between Tesla being worth a lot of money or worth basically zero.”

The term “Full Self-Driving” has been criticized by other manufacturers and industry groups as misleading and even dangerous. Last year, the autonomous driving technology company Waymo, owned by Google’s parent company, announced that it would no longer be using the term.

“Unfortunately, we see that some automakers use the term ‘self-driving’ in an inaccurate way, giving consumers and the general public a false impression of the capabilities of driver assist (not fully autonomous) technology,” Waymo wrote in a blog post. “That false impression can lead someone to unknowingly take risks (like taking their hands off the steering wheel) that could jeopardize not only their own safety but the safety of people around them.”

Though Waymo doesn’t name any names, the statement was “clearly motivated by Musk’s controversial decision to use the term ‘Full Self Driving,’” according to The Verge.

Along the same lines, the premier lobbying group for self-driving cars recently rebranded from the “Self-Driving Coalition for Safer Streets” to the “Autonomous Vehicle Industry Association.” The change, the industry group said, reflected its “commitment to precision and consistency in how the industry, policymakers, journalists and the public talk about autonomous driving technology.”

Secretary of Transportation Pete Buttigieg has also been critical of the emerging driver assistance technologies, which he stresses have not replaced the need for an alert human driver. “I keep saying this until I’m blue in the face: Anything on the market today that you can buy is a driver assistance technology, not a driver replacement technology,” Buttigieg said. “I don’t care what it’s called. We need to make sure that we’re crystal clear about that — even if companies are not.”

Though the language may be evolving, there are still no federal restrictions on the testing of autonomous vehicles on public roads, though states have imposed limits in certain cases. Tesla has not announced any changes to the program or its branding, but the crash was one of multiple that month. Several days prior to the Bay Bridge accident, on November 18 in Ohio, a Tesla Model 3 crashed into a stopped Ohio State Highway Patrol SUV which had its hazard lights flashing. The Tesla is likewise suspected of having been in self-driving mode and is also being investigated by NHTSA.

NHTSA is also investigating a tweet by Musk in which he said that “Full Self-Driving” users would soon be given the option to turn off reminder notifications for drivers to keep their hands on the steering wheel. “Users with more than 10,000 miles on FSD Beta should be given the option to turn off the steering nag,” a Twitter user posted on New Year’s Eve, tagging Musk.

“Agreed, update coming in Jan,” Musk replied.

Additional reporting by Beth Bourdon.. Washington, DC CNN —

The Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday.

The Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly.

The Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. CNN Business was first to report last month the driver’s claim that “full self-driving” was active.

The National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The agency typically conducts special investigations into about 100 crashes a year.

The pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla had previously restricted access to drivers with high scores on its safety rating system.

“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals, but despite Tesla’s name for it, it requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some Tesla drivers but also alarmed others with its flaws. Drivers are warned on an in-car screen by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”

Tesla generally does not engage with the professional news media and did not respond to CNN’s request for comment.

“We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years,” Tesla said this month in an update to its vehicle safety data.

Traffic safety experts have long questioned the merits of Tesla’s findings, which show fewer crashes when the driver-assist technologies are active, because among other things they’re generally used on highways where crashes are already rarer.

Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement.

The agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety.

Bryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features.

“How many more crashes will there be before NHTSA releases findings?” Reimer said.

Reimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.”

This story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.