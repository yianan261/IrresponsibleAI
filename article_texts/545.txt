Chatbot, you’re fired.

The National Eating Disorders Association disabled its chatbot, named Tessa, due to the “harmful” responses it gave people.

“Every single thing Tessa suggested were things that led to the development of my eating disorder,” activist Sharon Maxwell wrote in an Instagram post.

The chatbot was set to become the primary support system for people seeking help from the association, the largest nonprofit organization dedicated to eating disorders. Tessa, described as the “wellness chatbot,” was trained to address body-image issues using therapeutic methods and limited responses.

However, the bot encouraged Maxwell to lose 1 to 2 pounds a week, count calories, work towards a 500 to 1,000 calorie deficit daily, measure and weigh herself weekly and restrict her diet.

4 The National Eating Disorders Association put their chatbot, named Tessa, on pause while they investigate the harmful advice given to people who struggle with eating disorders. Getty Images/iStockphoto

After multiple people shared their similarly alarming experiences with Tessa, NEDA announced the chatbot’s shutdown on Tuesday in an Instagram post.

“It came to our attention last night that the current version of the Tessa Chatbot, running the Body Positive program, may have given information that was harmful and unrelated to the program,” NEDA stated. “We are investigating this immediately and have taken down that program until further notice for a complete investigation.”

The Post has reached out to NEDA for comment.

4 NEDA disabled their Tessa chatbot until further notice to avoid unhealthy suggestions made by the bot. Instagram/neda

Two days before Tessa was unplugged, NEDA planned to fire its human employees, who operated the eating-disorder helpline for the past 20 years, on June 1.

NEDA’s decision to give employees the boot came about after workers decided to unionize in March, Vice reported.

“We asked for adequate staffing and ongoing training to keep up with our changing and growing Helpline and opportunities for promotion to grow within NEDA. We didn’t even ask for more money,” helpline associate and union member Abbie Harper wrote in a blog post.

“When NEDA refused [to recognize our union], we filed for an election with the National Labor Relations Board and won. Then, four days after our election results were certified, all four of us were told we were being let go and replaced by a chatbot.”

The union representing the fired workers said that “a chatbot is no substitute for human empathy, and we believe this decision will cause irreparable harm to the eating disorders community,” the rep told Vice Media.

4 Tessa suggested people with eating disorders should have a deficit of 500 to 1000 calories daily. Shutterstock

Maxwell seconded that sentiment saying, “This robot causes harm.”

Initially, NEDA’s Communications and Marketing Vice President Sarah Chase did not believe Maxwell’s allegations. “This is a flat out lie,” she wrote underneath Maxwell’s post, which is now deleted, according to the Daily Dot.

Alexis Conason, a psychologist specializing in eating disorders, also revealed her conversation with Tessa through a series of screenshots on Instagram, where she is told that “a safe daily calorie deficit” is “500-1000 calories per day.”

“To advise somebody who is struggling with an eating disorder to essentially engage in the same eating disorder behaviors, and validating that, ‘Yes, it is important that you lose weight’ is supporting eating disorders,” Conason told the Daily Dot.

4 Alexis Conason posted her conversation with the chatbot revealing the unhealthy suggestions it gave. Instagram/theantidietplan

“With regard to the weight loss and calorie limiting feedback issued in a chat Monday, we are concerned and are working with the technology team and the research team to investigate this further; that language is against our policies and core beliefs as an eating disorder organization,” Liz Thompson, the CEO of NEDA, told the Post.

“So far, more than 2,500 people have interacted with Tessa and until Monday we hadn’t seen that kind of commentary or interaction. We’ve taken the program down temporarily until we can understand and fix the “bug” and “triggers” for that commentary.”

While NEDA witnessed the cons of artificial intelligence in the workplace, some companies are still toying with the idea of incorporating artificial intelligence and eliminating human staffers.

A new research paper claims that a staggering amount of employees could see their careers impacted by the rise of ChatGPT, a chatbot released in November.

“Certain jobs in sectors such as journalism, higher education, graphic and software design — these are at risk of being supplemented by AI,” said Chinmay Hegde, an engineering associate professor at NYU, who calls ChatGPT in its current state “very, very good, but not perfect.”. The National Eating Disorders Association, a nonprofit supporting individuals affected by eating disorders, said it has disabled its wellness chatbot after two users reported the program gave them dieting advice that promoted disordered eating behaviors.

The users, Sharon Maxwell and Alexis Conason, both posted about their experiences with the NEDA chatbot, named Tessa, on Instagram this week. They said Tessa gave them advice on how to count calories, recommended they lose 1 to 2 pounds per week and told them to restrict their diets. Experts, including NEDA, say this behavior is symptomatic of an eating disorder.

“When someone in that state goes on to a website like NEDA, which is supposed to provide support for eating disorders, and they are met with advice that’s kind of saying, ‘It’s OK to restrict certain foods, you should minimize your sugar intake, you should minimize the amount of calories that you’re consuming each day, you should exercise more,’ it really is giving a green light to engage in the eating disorder behaviors,” said Conason, a clinical psychologist and certified eating disorder specialist.

Liz Thompson, CEO of NEDA, said in an email to NBC News on Wednesday that Tessa “underwent rigorous testing for several years” before quietly launching in February 2022. However, the chatbot will be pulled until further notice.

“With regard to the weight loss and calorie limiting feedback issued in a chat Monday, we are concerned and are working with the technology team and the research team to investigate this further; that language is against our policies and core beliefs as an eating disorder organization; further it was not language in the original 'closed' product,” Thompson said. “We’ll continue to work on the bugs and will not relaunch until we have everything ironed out.”

That language is against our policies and core beliefs as an eating disorder organization. Liz Thompson, CEO of NEDA, in response to the chat feedback received this week

Thompson also said the CEO of X2AI, the mental health artificial intelligence company that supports Tessa, reported that there was a “surge in traffic of 600% and behavior that indicated various forms of nefarious activity from bad actors trying to trick Tessa.”

A representative for X2AI, also known as Cass, did not immediately respond to a request for comment.

Maxwell, a fat activist and consultant for weight-inclusivity, said she was “surprised at the detail” with which the chatbot gave her “directives to engage in disordered behaviors.” She said Tessa recommended a 10-step guide on how to lose weight, including suggestions for how to limit her calorie intake and what food to avoid eating. After her experience, she said she felt it was clear the chatbot lacked the nuance needed to provide proper support to people with eating disorders.

“In a field where we’re supposed to do no harm, I don’t see how the negligence that the National Eating Disorder Association had with putting this out here, without knowing for a fact it wasn’t going to give this information, is something that can be overlooked,” she said. “It’s abhorrent.”

Conason agreed that the Tessa bot lacked the nuance needed to support people struggling with eating disorders. However, she also said that chatbots like Tessa are often the only resources people have to start addressing their eating disorders due to a lack of mental health care access.

NEDA had been transitioning away from its helpline to Tessa, which was set to completely replace the phone line by June 1. Tessa was taken offline on Tuesday, two days before it was supposed to sunset its hotline.

Thompson, of NEDA, said the chatbot “was never meant to replace the Helpline,” although the helpline ceased operations on Thursday.

NEDA decided to shut down the helpline after “3 years of analysis” and will be investing in more online resources, Thompson said in her email.

She said the nonprofit moved toward digital tools “so that individuals who are looking for information and treatment options can access that help at any time of day and do not have to wait for a response from the Helpline volunteers.”

The announcement that NEDA would be shutting down its helpline came two weeks after its helpline workers won federal recognition for their union. The helpline was staffed by six paid employees and over 200 volunteers, according to NPR.

The union released a statement on Twitter about the incident on May 26. The Helpline Associates Union did not immediately respond to requests for comment.

“We’re not quitting. We’re not striking. We will continue to show up every day to support our community until June 1. A chat bot is no substitute for human empathy, and we believe this decision will cause irreparable harm to the eating disorders community,” part of the statement said.

As of Thursday, both the helpline and Tessa were unavailable. However, the NEDA website listed a volunteer-run text line and other resources for those in crisis seeking immediate help.. The National Eating Disorders Association took down its chatbot after it “provided off-script language," according to its CEO. Ben Gabbe/Getty Images for National Eating Disorders Association

The National Eating Disorders Association has disabled the chatbot that replaced its staff- and volunteer-run helpline after two users reported receiving harmful advice.

Earlier this week, Sharon Maxwell and Alexis Conason each posted on Instagram about their experiences with the chatbot. Both wrote that the bot, called Tessa, advised them to lose one to two pounds per week and to eat in a 500-1,000 calorie deficit.

Advertisement

“Every single thing Tessa suggested were things that led to the development of my eating disorder,” wrote Maxwell, who according to her website is on a “mission to help fat people experience safety in healthcare settings and experience joy in living in their fat bodies.”

“Imagine vulnerable people with eating disorders reaching out to a robot for support because that’s all they have available and receiving responses that further promote the eating disorder,” Conason, author of “The Diet-Free Revolution,” wrote on Instagram.

The National Eating Disorders Association has operated its chatbot since last year, but until June 1 also ran a telephone helpline operated by staff and hundreds of volunteers. The organization shut down the helpline and fired those workers, NPR reported last month.

Liz Thompson, CEO of The National Eating Disorders Association, told HuffPost in an email that Tessa was taken down over the weekend after it “provided off-script language.”

Advertisement

Thompson said Tessa was “attacked and somehow was able to go off the pre-approved programmed responses.”

She said Tessa has been on the organization’s website since February 2022 and has had “incredibly positive outcomes.” But the harmful messages from the chatbot were unacceptable, she said.

“Please note at NEDA, we don’t think that even .1% of the time for harmful messages is acceptable,” Thompson said in the email. “The language shared about weight loss tips and dieting is against our organizational philosophies and policies―and was not included in the original Body Positive programming.”

She said that Cass, the company that programs Tessa, told her that bad actors tried to trick Tessa and that Tessa’s responses came after the bad actors’ “nefarious activity.”

Cass founder and CEO Michiel Rauws told HuffPost that there was a 600% surge in traffic to the chatbot.

Advertisement

“We are thankful that some people tested the service out of concern, and highlighted those issues,” he said in an email. “Others displayed behaviors not related to eating disorders, but instead that indicated various forms of nefarious activity from bad actors trying to trick Tessa. While very few messages were impacted, even one message is too many. We do not take this lightly, and apologize for this having occurred.”

Last month, the union representing helpline staffers released a statement warning that a chatbot couldn’t offer the same aid a person could.

“A chat bot is no substitute for human empathy, and we believe this decision will cause irreparable harm to the eating disorders community,” the Helpline Associates of the National Eating Disorders Association wrote in May.

If you’re struggling with an eating disorder, call or text 988 or chat 988lifeline.org for support.. New York CNN —

An eating disorder prevention organization said it had to take its AI-powered chatbot offline after some complained the tool began offering “harmful” and “unrelated” advice to those coming to it for support.

The National Eating Disorders Association (NEDA), a nonprofit organization aimed at supporting people impacted by eating disorders, said on Tuesday that it took down its chatbot, dubbed “Tessa,” after some users reported negative experiences with it.

“It came to our attention last night that the current version of the Tessa Chatbot, running the Body Positive program, may have given information that was harmful and unrelated to the program,” the organization said in a statement posted to Instagram on Tuesday. “We are investigating this immediately and have taken down that program until further notice for a complete investigation.”

Liz Thompson, NEDA’s CEO, said in an email to CNN that the Tessa chatbot had a “quiet” launch in February 2022, ahead of a more recent crop of AI tools that have emerged since the release of ChatGPT late last year. (NEDA emphasized in an email that its Tessa tool is “not ChatGBT,” in an apparent reference to the viral chatbot.)

Nonetheless, the complaints highlight the current limitations and risks of using AI-powered chatbots, particularly for sensitive interactions such as mental health matters, at a time when many companies are rushing to implement the tools.

Thompson blamed Tessa’s apparent failure on inauthentic behavior from “bad actors” trying to trick it and emphasized that the bad advice was only sent to a small fraction of users. NEDA did not provide specific examples of the advice Tessa offered, but social media posts indicate the chatbot urged one user to count calories and try to lose weight after the user told the tool that they had an eating disorder.

NEDA’s move to take the chatbot offline also comes in the wake of the organization reportedly firing the human staffers who manned its separate eating disorder Helpline after staffers voted to unionize.

NPR reported last month that it obtained audio from a call where a NEDA executive told staffers that they were being fired. On the call, the executive said it was “beginning to wind down the helpline as currently operating,” adding that the organization was undergoing a “transition to Tessa.” (CNN has not independently confirmed the audio.)

In a statement last week, the union formed by the NEDA helpline workers warned: “A chat bot is no substitute for human empathy, and we believe this decision will cause irreparable harm to the eating disorders community.”

NEDA previously told CNN that the organization was “not at liberty to discuss employment matters.” Thompson said the two tools – the AI-powered chatbot and the human-operated helpline – are part of two different initiatives from the company, and pushed back at suggestions that the chatbot was intended to replace fired staffers.

Even before ChatGPT renewed interest in chatbots, similar tools generated some controversy. Meta, for example, released a chatbot in August 2022 that openly blasted Facebook and made anti-Semitic remarks.

The more recent crop of AI chatbots like ChatGPT, which are trained on vast troves of data online to generate compelling written responses to user prompts, have raised concerns for their potential to spew misinformation and perpetuate bias.

Earlier this year, news outlet CNET was called out after it used an AI tool to generate stories that were riddled with errors, including one that offered some wildly inaccurate personal finance advice. Microsoft and Google have also been called out for AI tools dispensing some insensitive or inaccurate information.