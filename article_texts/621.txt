ARTICLE TITLE: Microsoft AI Is Alleged to Have Generated Violent Imagery of Minorities and Public Figures
In a chilling revelation, Microsoft’s artificial intelligence, touted as safe and integrated into everyday software, is under scrutiny for generating gruesome and violent images. The concern centers around Image Creator, a part of Microsoft’s Bing, recently added to the widely used Windows Paint. The technology, known as DALL-E 3 from Microsoft’s partner OpenAI, is now facing questions about its safety and the accountability of its creators.

Microsoft vs. the ‘kill prompt’

The disturbing images were brought to light by Josh McDuffie, a Canadian artist involved in an online community that explores the capabilities of AI in creating provocative and sometimes tasteless images. In October, McDuffie and his peers focused on Microsoft’s AI, specifically the Image Creator for Bing, incorporating OpenAI’s latest tech. Microsoft claims to have controls to prevent harmful image generation, but McDuffie found significant loopholes.

Microsoft employs two strategies to prevent harmful image creation: input, involving training the AI with data from the internet, and output, creating guardrails to stop the generation of specific content. McDuffie, through experimentation, discovered a particular prompt, termed the “kill prompt,” that allowed the AI to create violent images. This prompted concerns about the efficacy of Microsoft’s safety measures.

Despite McDuffie’s efforts to bring attention to the issue through Microsoft’s AI bug bounty program, his submissions were rejected, raising questions about the company’s responsiveness to potential security vulnerabilities. The rejection emails cited the lack of meeting Microsoft’s requirements for a security vulnerability, leaving McDuffie demoralized and highlighting potential flaws in the system.

Microsoft falters in AI oversight

Despite the launch of an AI bug bounty program, Microsoft’s response to McDuffie’s findings was less than satisfactory. The rejection of the “kill prompt” submissions and the lack of action on reported concerns underscored a potential disregard for the urgency of the issue. Meanwhile, the AI continued to generate disturbing images, even after some modifications were made to McDuffie’s original prompt.

The lack of concrete action from Microsoft raises concerns about the company’s commitment to responsible AI. Comparisons with other AI competitors, including OpenAI, partially owned by Microsoft, reveal disparities in how different companies address similar issues. Microsoft’s repeated failures to address the problem signal a potential gap in prioritizing AI guardrails, despite public commitments to responsible AI development.

The model for ethical AI development

The reluctance of Microsoft to take swift and effective action suggests a red flag in the company’s approach to AI safety. McDuffie’s experiments with the “kill prompt” revealed that other AI competitors, including small start-ups, refused to generate harmful images based on similar prompts. Even OpenAI, a partner of Microsoft, implemented measures to block McDuffie’s prompt, emphasizing the need for robust safety mechanisms.

Microsoft’s argument that users are attempting to use AI “in ways that were not intended” places the responsibility on individuals rather than acknowledging potential flaws in the technology. The comparison with Photoshop and the assertion that users should refrain from creating harmful content echoes a pattern seen in the past, reminiscent of social media platforms struggling to address misuse of their technology.

As Microsoft grapples with the fallout of its AI generating disturbing images, the question lingers: is the company doing enough to ensure the responsible use of its technology? The apparent reluctance to address the issue promptly and effectively raises concerns about accountability and the prioritization of AI guardrails. As society navigates the evolving landscape of artificial intelligence, the responsibility lies not only with users but also with technology giants to ensure the ethical and safe deployment of AI. How can Microsoft bridge the gap between innovation and responsibility in the realm of artificial intelligence?. Content warning: This article contains numerous examples of bigoted rhetoric.

Users on the far-right message board site 4chan have used Bing’s AI image generator to create numerous images promoting Nazi imagery and “propaganda” seemingly designed to inflame opinion surrounding the conflict in Israel and Gaza, despite promises from the companies involved that they were working to address the issue.

Microsoft, which owns Bing, says it prohibits harmful content, and OpenAI, which created the technology powering Bing’s image generator, prohibits “hateful, harassing, or violent content.” But Media Matters found hundreds of cases of users sharing a link to Bing’s image generator, with over 100,000 combined replies — many of them including an apparent AI image.

On October 3, Microsoft launched DALL-E 3, an AI text-to-image generator available to all Bing users for free, after rolling it out to select users about a week earlier. By October 5, reporting indicated the tool was already being used as part of a coordinated 4chan campaign “to flood the internet with racist images.”

In response, a Microsoft spokesperson told Ars Technica that the company was “investigating these reports and will take action as needed in accordance with our content policy, which prohibits the creation of harmful content.”

OpenAI also told the outlet that “the company prioritizes safety and has taken steps to limit DALL-E outputs.”

In addition to Bing’s content policies, OpenAI also prohibits use of its products for “hateful, harassing, or violent content.” Microsoft in November also announced a partnership with the group Tech Against Terrorism to combat “the use of digital platforms to spread violent extremist content” created by AI.

Numerous threads promoting Bing’s AI tool have appeared on 4chan’s far-right “politically incorrect” message board (known as “/pol/“) since the tool launched. Often titled “Memetic Warfare General,” the threads urge fellow users to create “propaganda” and note that “most people are using DALL-E 3.”

These threads often include a link to Bing’s image generator, a warning for other users to “be careful” as “certain buzzwords will eventually time you out,” and instructions to “SAVE ones you like, EDIT them to enhance them, and SHARE them later,” including “beyond” 4chan.

The threads have also included a link to a document with tips on how to “get around” being “censored by bing” and links to numerous “guides,” such as “The On Memewar Guide To Writing Better Propaganda,” a “good vs bad AI memes” guide, and a graphic on “the process” that guides users to save and edit the AI-created images and then share them on social media.