ARTICLE TITLE: Facial Recognition Researchers Used YouTube Videos of Transgender People without Consent
About five or six years ago, one of Karl Ricanek’s students showed him a video on YouTube. It was a time lapse of a person undergoing hormone replacement therapy, or HRT, in order to transition genders. “At the time, we were working on facial recognition,” Ricanek, a professor of computer science at the University of North Carolina at Wilmington, tells The Verge. He says he and his students were always trying to find ways to break the systems they worked on, and that this video seemed like a particularly tricky challenge. “We were like, ‘Wow there’s no way the current technology could recognize this person [after they transitioned].’”

Ricanek turned to YouTube to find images of transgender people

To tackle the problem, Ricanek did what all good scientists do: he started collecting data. Like all AI systems, facial recognition software requires stacks of information to train on, and although there are a number of sizable and freely available face databases available (ranging in size from thousands to millions of images), there was nothing documenting faces before and after HRT. So, Ricanek turned to the internet — a decision that would later prove to be controversial.

On YouTube, he found a treasure trove. Individuals undergoing HRT often document their progress and post the results online, sometimes keeping regular diaries, and sometimes making time-lapse videos of the entire process. “I shared my videos because I wanted other trans people to see my transition,” says Danielle, who posted her transition video on YouTube years ago. “These types of transition montages were helpful to me, so I wanted to pay it forward,” she tells The Verge.

The videos also happen to be gold for AI researchers, as each contains dozens of varied, true-to-life photos. As Ricanek wrote on a webpage for the dataset he would compile from the videos: “[It] includes an average of 278 images per subject that are taken under real-world conditions, and hence, include variations in pose, illumination, expression, and occlusion.”

But the problem is: do the people in these videos know or care that the personal journey they shared to help others is being used to improve facial recognition software?

“How is this even legal?”

Adam Harvey, an artist and researcher whose work examines privacy and technology, tells The Verge over email that this sort of data-scraping is “beyond common.” It was Harvey who found the HRT Transgender Dataset during research for an upcoming project examining exactly this sort of AI-training practice. He shared it on Twitter, where reactions were not good. “How is this even legal?” asked one user. “Not okay,” said another.

Ricanek wasn’t aware that his work was being discussed in this way when we reached out to him. He did, however, want to clarify a number of things about the research. First, that the dataset itself was just a set of links to YouTube videos, rather than the videos themselves; second, that he never shared it with anyone for commercial purposes (“Our job is just to illuminate what problem areas exist.”); and third, that he stopped giving access to it altogether three years ago.

“The reason for that is that it felt a little uncomfortable in the current climate to provide those things out there,” he told The Verge. “I have no inclination to distribute even the links any longer, for political reasons. People can use this for harm, and that was not my intent.” He says his team did try to contact individuals whose videos he listed and ask their permission “as a courtesy,” but admitted that if someone didn’t respond, they might have been included anyway.

Individuals were included in the dataset without their consent

Danielle, who is featured in the dataset and whose transition pictures appear in scientific papers because of it, says she was never contacted about her inclusion. “I by no means ‘hide’ my identity,” she told The Verge using an online messaging service. “But this feels like a violation of privacy.” She said she was gratified to know that there are limits on the use of the dataset (especially that it wasn’t sold to companies), but said this sort of biometric collection had “all sorts of implications for the trans community.”

“Someone who works in ‘identity sciences’ should understand the implications of identifying people, particularly those whose identity may make them a target (i.e., trans people in the military who may not be out),” she said. “Within the trans community, there's a non-trivial segment of people terrified by YouTube videos or other content that helps people figure out how to ‘spot the trans person.’”

For Harvey, this story is not surprising. “The lack of public discourse around data collection ethics has allowed researchers to continue amassing vast troves of biometric data from social media sources, namely Flickr and YouTube,” he says. These images can be given a Creative Commons (CC) license by default, allowing them to be downloaded freely and used to train facial-recognition systems even when the research is funded by for-profit companies.

And compared to other datasets, Ricanek’s is a minnow. The MegaFace dataset compiled by the University of Washington, for example, contains 4.7 million images of roughly 627,000 individuals — all taken from Flickr users. The project’s sponsors include Samsung, Intel, and Google, and the data itself is used by researchers from all over the world, whose work almost certainly feeds into paid products.

Example faces from the MegaFace dataset.

Harvey says that putting aside issues of legality and consent, there are “deeper ethical questions about the actual content in these datasets.” He points out that the two most common categories of images in MegaFace are “family” and “wedding.” Which makes sense, as who do we like to takes pictures of more than our loved ones? A look inside the database, says Harvey, “reveals countless personal photos of people's homes, weddings, picnics, beach trips, selfies, and even photos of children. Most, if not all, people in these photos are unaware that biometric companies around the world are honing facial recognition algorithms on their friends, family, and children.”

Law enforcement and national security agencies are also interested in this data. Ricanek’s research is partly funded by the FBI and the Army (although he says the transgender dataset was never shared with any government agencies nor was it funded by them). Ricanek justified the research as a solution to a fantastical border threat. But a system using this kind of research could exacerbate the harassment and humiliation that transgender people already face at travel checkpoints.

“As academics, we see great challenges ... but behind those challenges are real people.”

“What kind of harm can a terrorist do if they understand that taking this hormone can increase their chances of crossing over into a border that’s protected by face recognition? That was the problem that I was really investigating,” he says. “I’m deeply apologetic for any type of pain this may have caused any people in these videos. That’s certainly not where I’m coming from. As academics, we see great challenges and we want to work on them, but behind those challenges are real people, who may be impacted in ways we have not comprehended.”

Harvey says there’s currently “little debate” about the ethics of this sort of data collection. It’s a complex topic, and although individuals might be outraged that their image is being used without permission, there’s little they can do about it.

There is pushback in some instances (like when a researcher scraped 40,000 selfies from Tinder without permission and posted the dataset online), but in the debate about what is the right and the wrong way to go about acquiring data, the loudest voices are those of big companies. This leads to situations like in the UK, where Google’s AI subsidiary DeepMind made an illegal deal to access medical records belonging to 1.6 million individuals.. Discover how companies are responsibly integrating AI in production. This invite-only event in SF will explore the intersection of technology and business. Find out how you can attend here.

Last year was a grim, record-setting year for violence against transgender individuals, and the Human Rights Campaign is tracking data that shows 2022 is on a similar pace. Outside of physical violence, other forms of attacks can harm these individuals as well. Some of that harm stems from enterprise technology, specifically around issues such as data privacy, facial recognition, artificial intelligence (AI) training and surveillance.

The use of biometrics in particular is quickly growing as a sector of the technology landscape. A report from the Biometrics Institute found that, “More than 90% of industry professionals agreed that biometrics will be the key enabler for anchoring digital identity and that there will continue to be significant growth in mobile remote identity verification systems and remote onboarding technology.”

But, as this technology grows, severe AI ethical problems both in training it and in applying it to use-cases continue to emerge.

Misuse of data

A research team led by professor Karl Ricanek at the University of North Carolina, Wilmington — several years back — worked on research related to facial recognition using transition videos that transgender individuals had uploaded to YouTube for inter-community support and information. Ricanek and his team were conducting the research, propelled by a claim that hormone replacement therapy treatments (HRT) might be used by criminals and terrorists illicitly to dodge surveillance system detection.

VB Event The AI Impact Tour – San Francisco Join us as we navigate the complexities of responsibly integrating AI in business at the next stop of VB’s AI Impact Tour in San Francisco. Don’t miss out on the chance to gain insights from industry experts, network with like-minded innovators, and explore the future of GenAI with customer experiences and optimize business processes. Request an invite

The purpose of the research itself has since drawn criticism from experts like Os Keyes, a Ph.D. candidate at the University of Washington’s department of human-centered design and engineering, who researches data ethics, medical AI, facial recognition, gender and sexuality.



“This idea is like the equivalent of saying, ‘What if people tried to defeat detection by evading a height detector? What if they did it by cutting their own legs off?’ Keyes asked. “To imply you would do this on a whim is to drastically misunderstand things.”

Previously, in response to criticism, Ricanek told The Verge that, “the dataset itself was just a set of links to YouTube videos, rather than the videos themselves; second, that he never shared it with anyone for commercial purposes … and third, that he stopped giving access to it altogether three years ago.”

Keyes and Jeanie Austin, who has a Ph.D. in library and information science from the University of Illinois at Urbana-Champaign — have since looked into Ricanek’s work together in an effort to research algorithmic bias and dataset auditing for purposes of academia.

What they found, and have since published, was much more than that alone.

What Ricanek and his team previously claimed about the UNC-Wilmington facial recognition HRT dataset and all assets related to it being private, as well as having consent from the individuals whose videos were used for it — were false.

From a collection of information gathered by Keyes and Austin via a public records request that included about 90 emails and email attachments from Ricanek and his team, four important aspects were uncovered.

The team at UNC-Wilmington, “has no records of participants being contacted, and explicitly acknowledged some had not; contrary to their assurances, were redistributing the full videos, even after the videos had been removed from public view, into 2015; it eschewed responsibility for removing images of transgender people published without consent, and; left the full videos in an unprotected Dropbox account until our research team contacted them in 2021,” Keyes and Austin wrote in a document shared with VentureBeat.

Keyes and Austin, both identify as trans, but neither were subjects in the UNC-Wilmington dataset.



“Although neither of us are included in the dataset, both of us saw it as an exemplar of the violence that can occur when existing practices — the surveillance and over-examination of trans bodies and lives — begin to resonate with new technologies,” they wrote in their academic research titled “Feeling Fixes,” which was just published in the journal, Big Data & Society. “We sought to understand the circumstances of the dataset’s creation, use and redistribution, in order to map that violence and (possibly) ameliorate it.”

The dataset

UNC-Wilmington’s dataset focused only on 38 individuals, but Keyes and Austin found that it contained more than 1 million still images taken from the 38 transgender individuals’ YouTube videos in which they narrated what their transition process and experiences were like.

Further, they found that in the dataset videos “all those we could identify were provided under the standard YouTube license, which explicitly prohibited reusing and redistributing the content outside of YouTube as a platform at the time that the images were captured,” they wrote.

Ricanek, in response, did admit to VentureBeat that not every individual contacted in the videos gave their consent, but wanted to clarify several things, including, that the dataset was not used for training purposes, and that the research was not about the transgender community, but rather about how morphology can alter someone’s face and what the implications of that could be. He also asserted that it was not 1 million still images.

“First, you cannot use 32, 38, 50 or even 100 subjects to build any face recognition system. It is not possible. Second, the data was never used for training. We never attempted to create a training dataset to train an AI. It was only used to evaluate the current state-of-the-art algorithms,” he said. “It was developed by researchers who were funded by the U.S. government. Another was used by a commercial solution that had contracts with the U.S. government.”

Ricanek clarified that the dataset, though it had been in an unprotected Dropbox, had a unique URL and that the data was not published anywhere and would have been difficult for any random internet user to access on a whim. He said that although his post-doc student working with him had set the Dropbox up, he was not aware of this and that it was an unofficial method and was grateful that Keyes and Austin brought it to his attention. He had it taken down immediately when the two contacted him after finding it in 2021.

Keyes and Austin’s public record request contradicts Ricanek and shows that he was cc’d on the emails about the Dropbox years ago, and further that the dataset had been distributed.

“We were struck by how broadly the dataset had spread, including into disciplines with their own histories of transphobia and to scholars who likely lacked the background knowledge needed to critically contextualize the creation of the dataset,” they wrote in their findings. “The records contained 16 requests for the dataset — all approved — from 15 institutions spanning seven countries.”

Though, Ricanek told VentureBeat that the distribution of the dataset was not broad.

“As far as the [transgender] community is concerned, it probably is more than it should have happened,” he noted, “But it’s not a broad use of the data to be quite honest.”



He also claimed he did previously have contact with some of the 38 individuals included in the dataset and had conversations about how they were impacted, reiterating he did not mean harm.



If he could change things, Ricanek said he wouldn’t do this again.

“I probably wouldn’t do this body of work. I mean, it’s a very small fraction of work that I [did] in the totality of my career — less than 1% of what I’ve done in the totality of my career,” he said.

Although at best it is a case of negligence, Austin said, it’s important to not lose sight of the larger issue here.

“The larger issue is not about Ricanek as a person. It’s about the way that this research was conducted and distributed and how many people were harmed along the way,” they said. “That’s something that we really wanted to focus on.”

Keyes agreed, adding that, “The fact that society disproportionately treats trans people as dangerous and worth surveilling and objectifying … taking those videos and then using them to train software that assumes that people might be suspect for taking trans medicine, that people might be dangerous that they need to be watched for, from taking trans people’s responses is to turn them into objects yet again.”

Other harms of data and biometrics technology

Unfortunately, this is certainly not the first time biometrics intentions or data about the LGBTQ+ community has gone awry in the tech industry — impacting marginalized communities as a result.

Other instances of technological harms that have been caused to individuals of the transgender community range from instances of failing to be able to properly verify their own identities for bank accounts, IDs and document checks which can prevent these individuals from potentially accessing necessary services like hospitals etc.

Mina Hunt Burnside, a Ph.D. candidate at Utrecht University who studies gender and technology, has done research on the above.

“I put research together for BMI metrics — which is not necessarily obviously the most technologically advanced form of biometric. It’s really interesting when you look into the history of it, how arbitrary it is. The original data points were taken from insurance companies, and indeed were taken from insurance companies into the 20th century until they were eventually agreed upon … But I bring this up because what it ends up doing is it becomes a really common cause to deny trans people services,” she said.

“I had a friend recently who was denied surgery over like five kilograms or something because of a biometric marker …. There’s an argument that maybe it was bad for her health, but we know for certain that denying trans people such healthcare has very deadly in quantifiable outcomes. So, BMI, this kind of arbitrary thing … is all of a sudden, 200 years later, being used to deny trans masculine people surgeries in Toronto,” Hunt Burnside said.

Beyond healthcare, biometrics verification can also have implications for individuals who attempt to update their documents. According to the National Transgender Discrimination Survey, only 21% of transgender individuals report that they have been able to update all necessary IDs and records with their new gender.



Without having access to systems that can properly recognize and identify their gender, combating false identifications from algorithms or biometric tools can be challenging.

Steve Ritter, CTO of Mitek, a digital identity verification company that uses biometrics, explained that the company had an incident a while back where it discovered that when a California ID was scanned, the barcode on the back that contains information to verify someone’s information was misrepresenting a code for gender identity in the company system.

When scanned, X should have represented “non-binary,” however, it was returning the number nine rather than an “X.” The company realized that even this small discrepancy was likely causing someone who identified as non-binary on their California driver’s license to not have their identity authenticated by Mitek’s systems.

Once the discrepancy was identified, Ritter and his team worked to resolve the issue and now note it as an important lesson for others in the biometrics or identity verification space.

“Of course, there was no purposeful bias being put into that, but a simple mistake that we caught and we found that could have led to people four or nine binary individuals, for example, being less likely to be approved for a bank account in the online channel,” Ritter said. “So maybe they’d have to go into the branch or something like that — it’s an example that I think is really important because these are every day things that impact lives. As society changes, technology needs to keep up with that change.”

Takeaways for tech leaders, researchers and the enterprise

Researchers at the intersection of gender and technology underscore that “biometric technologies are currently not free of exclusionary dynamics. While they are widely regarded as neutral and objective, they rely on simplistic and problematic understandings of the relation between identity and the body, and disproportionately focus on some bodies over others. This forms a critical problem of inequality, especially for people who are already in a marginalized position.”

The National Center for Transgender Equality noted that for enterprises using biometrics technology in any capacity or that are gathering data in hopes of removing rather than creating bias, it is still important to keep in mind how these systems can and do harm.

“Humans cannot consistently determine who is transgender or non-binary and who is cisgender, and when they attempt to do so they rely on stereotypes and assumptions about how people dress, speak and move. An AI system developed by humans will only perpetuate these same stereotypes and assumptions,” said Olivia Hunt, policy director at the National Center for Transgender Equality.

Hunt underscored what the several researchers above also mentioned and added that “AI systems should not attempt to assign a gender to individuals based on appearance because the only authority on any given individual’s gender is that individual. Relying on an AI system to do so will inevitably result in trans people being misidentified, misunderstood and potentially denied governmental and commercial services that they both need and are entitled to.”