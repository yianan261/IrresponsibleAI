para leer este articulo en español por favor aprete aqui.

In 2018, while the Argentine Congress was hotly debating whether to decriminalize abortion, the Ministry of Early Childhood in the northern province of Salta and the American tech giant Microsoft presented an algorithmic system to predict teenage pregnancy. They called it the Technology Platform for Social Intervention.

ABOUT Diego Jemio is a journalist, educator, and podcaster. He currently writes for the Clarín newspaper (Buenos Aires), Vértice (Mexico), and other media. He is the creator of the podcast Epistolar. Alexa Hagerty is an anthropologist researching human rights, technology, and AI resistance. She is an Associate Fellow at the University of Cambridge and a senior consultant in the JUST AI network. Florencia Aranda is an Argentine feminist activist, poet, and independent researcher. She studies contemporary Latin American literature at the University of Buenos Aires.

“With technology you can foresee five or six years in advance, with first name, last name, and address, which girl—future teenager—is 86 percent predestined to have an adolescent pregnancy,” Juan Manuel Urtubey, then the governor of the province, proudly declared on national television. The stated goal was to use the algorithm to predict which girls from low-income areas would become pregnant in the next five years. It was never made clear what would happen once a girl or young woman was labeled as “predestined” for motherhood or how this information would help prevent adolescent pregnancy. The social theories informing the AI system, like its algorithms, were opaque.

The system was based on data—including age, ethnicity, country of origin, disability, and whether the subject’s home had hot water in the bathroom—from 200,000 residents in the city of Salta, including 12,000 women and girls between the ages of 10 and 19. Though there is no official documentation, from reviewing media articles and two technical reviews, we know that "territorial agents" visited the houses of the girls and women in question, asked survey questions, took photos, and recorded GPS locations. What did those subjected to this intimate surveillance have in common? They were poor, some were migrants from Bolivia and other countries in South America, and others were from Indigenous Wichí, Qulla, and Guaraní communities.

Although Microsoft spokespersons proudly announced that the technology in Salta was “one of the pioneering cases in the use of AI data” in state programs, it presents little that is new. Instead, it is an extension of a long Argentine tradition: controlling the population through surveillance and force. And the reaction to it shows how grassroots Argentine feminists were able to take on this misuse of artificial intelligence.

In the 19th and early 20th centuries, successive Argentine governments carried out a genocide of Indigenous communities and promoted immigration policies based on ideologies designed to attract European settlement, all in hopes of blanquismo, or “whitening” the country. Over time, a national identity was constructed along social, cultural, and most of all racial lines.

This type of eugenic thinking has a propensity to shapeshift and adapt to new scientific paradigms and political circumstances, according to historian Marisa Miranda, who tracks Argentina’s attempts to control the population through science and technology. Take the case of immigration. Throughout Argentina’s history, opinion has oscillated between celebrating immigration as a means of “improving” the population and considering immigrants to be undesirable and a political threat to be carefully watched and managed.

More recently, the Argentine military dictatorship between 1976 and 1983 controlled the population through systematic political violence. During the dictatorship, women had the “patriotic task” of populating the country, and contraception was prohibited by a 1977 law. The cruelest expression of the dictatorship’s interest in motherhood was the practice of kidnapping pregnant women considered politically subversive. Most women were murdered after giving birth and many of their children were illegally adopted by the military to be raised by “patriotic, Catholic families.”

While Salta’s AI system to “predict pregnancy” was hailed as futuristic, it can only be understood in light of this long history, particularly, in Miranda’s words, the persistent eugenic impulse that always “contains a reference to the future” and assumes that reproduction “should be managed by the powerful.”

Due to the complete lack of national AI regulation, the Technology Platform for Social Intervention was never subject to formal review and no assessment of its impacts on girls and women has been made. There has been no official data published on its accuracy or outcomes. Like most AI systems all over the world, including those used in sensitive contexts, it lacks transparency and accountability.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Though it is unclear whether the technology program was ultimately suspended, everything we know about the system comes from the efforts of feminist activists and journalists who led what amounted to a grassroots audit of a flawed and harmful AI system. By quickly activating a well-oiled machine of community organizing, these activists brought national media attention to how an untested, unregulated technology was being used to violate the rights of girls and women.

“The idea that algorithms can predict teenage pregnancy before it happens is the perfect excuse for anti-women and anti-sexual and reproductive rights activists to declare abortion laws unnecessary,” wrote feminist scholars Paz Peña and Joana Varon at the time. Indeed, it was soon revealed that an Argentine nonprofit called the Conin Foundation, run by doctor Abel Albino, a vocal opponent of abortion rights, was behind the technology, along with Microsoft.. It's a nightmarish mix of Big Tech overreach and state authoritarianism.

The Argentinian province of Salta approved the development of a Microsoft algorithm in 2o18 that allegedly could determine which low-income "future teens" would be likely to get pregnant, a shocking investigation by Wired reveals.

The algorithm — which Microsoft called "one of the pioneering cases in the use of AI data" — used demographic data including age, ethnicity, disability, country of origin, and whether or not their home had hot water in its bathroom, to determine which of the women and girls living in a small Argentinian town were "predestined" for motherhood.

The opaque program, which was celebrated on national television by then-governor Juan Manuel Urtubey, was offered to the province by Microsoft in 2018, at the same time Argentina's Congress was debating whether to decriminalize abortion, Wired notes.

The magazine's reporting found that the women and girls Microsoft's algorithm identified as would-be teen moms were often disenfranchised in various ways, from having poor backgrounds and migrant families to indigenous heritage.

The algorithm, known as the Technology Platform for Social Intervention, is noteworthy due to the fact that an American company like Microsoft chose to deploy such a program in a country with a long history of surveillance and population control measures.

Those leanings are apparent in the lack of transparency surrounding the program. For one, the Argentinian government never formally assessed the algorithm's impact on girls and women.

Worse yet, according to Wired, the program involved the deployment of "territorial agents" who surveyed those identified by the AI as being predestined for pregnancy, took photos of them, and even recorded their GPS locations.

It's still unclear what the provincial or national governments did with the data and how — or if — they related to the abortion debate.

In 2020, Argentina voted to decriminalize abortion, a historic moment for the South American nation — but the program's very existence should be cause for concern.

The report should serve as a warning of the potentially dangerous intersection between American AI tech and authoritarianism, and offers a much-needed reminder that we have, for the time being, much less to fear from the algorithms themselves than from the humans behind them.

READ MORE: The Case of the Creepy Algorithm That ‘Predicted’ Teen Pregnancy [Wired]

More on predictive algorithms: Data Scientists Say They’ve Developed Algorithms to Predict the Next Coup Attempt. Tras estudiar la metodología del sistema de inteligencia artificial supuestamente capaz de predecir embarazos adolescentes , mencionado por el Gobernador de Salta, Juan Manuel Urtubey, encontramos serios errores técnicos y conceptuales , que ponen en duda los resultados reportados y comprometen el empleo de dicha herramienta, sobre todo tratándose de una cuestión tan sensible.

El 11/4/2018, en el programa de televisión “El Diario de Mariana”, el Gobernador de Salta, Juan Manuel Urtubey, describió un sistema de inteligencia artificial supuestamente capaz de predecir embarazos adolescentes:

“Hace poco lanzamos un programa con el Ministerio de Primera Infancia […] de prevención del embarazo adolescente utilizando inteligencia artificial con una reconocidísima empresa de software del mundo, que estamos haciendo un plan piloto. Vos podés hoy con la tecnología que tenés, podés ver, cinco o seis años antes, con nombre y apellido y domicilio, cuál es una niña, futura adolescente, que está un 86% predestinada a tener embarazo adolescente.”

Previamente, el 20/3/2018, en el evento “Microsoft Data & AI Experience 2018”, Urtubey ya había mencionado este tema:

“Los ejemplos que hacías referencia en el caso de la prevención de embarazo adolescente y el tema de la deserción escolar son ejemplos clarísimos respecto de eso. Nosotros tenemos claramente definidos, con nombre y apellido, 397 casos de chicos que sabemos, de un universo de 3000, que inexorablemente caen en deserción escolar. Tenemos 490 y pico, casi 500 casos de chicas que, sabemos, que tenemos que ir a buscarlas hoy.”

Distintos medios periodísticos asociaron estas declaraciones del Gdor. Urtubey a un documento disponible en github firmado por Facundo Davancens, empleado de Microsoft Argentina. Ese documento termina agradeciendo “al Ministerio de Primera Infancia del Gobierno Provincial de Salta” y “a Microsoft”.

Tras estudiar con cuidado la metodología detallada en ese documento, encontramos serios errores técnicos y conceptuales, que ponen en duda los resultados reportados por el Gdor. Urtubey, y que comprometen el empleo de la herramienta generada, en una cuestión tan sensible como el embarazo adolescente.

Enumeramos breve y coloquialmente algunos de los problemas más graves que hemos encontrado:

Problema 1: Resultados artificialmente sobredimensionados

El estudio detalla el siguiente procedimiento:

Construye un conjunto de reglas estadísticas para intentar determinar si una adolescente tendrá un embarazo en el futuro. Esas reglas se construyen basándose en datos conocidos (los “datos de entrenamiento”). Entonces, las reglas estadísticas están hechas a imagen y semejanza de los datos de entrenamiento. Una vez construidas las reglas estadísticas, se deberían ponen a prueba usando datos nuevos, desconocidos (los “datos de evaluación”), calculando así su “exactitud” (cuántas veces acierta en las predicciones).

El problema acá, es que los datos de evaluación (en el paso 3) incluyen réplicas casi idénticas de muchos datos de entrenamiento. Y por lo tanto, los resultados reportados están fuertemente sobredimensionados. Lleva a la conclusión errónea de que el sistema de predicción funciona mejor de lo que en realidad lo hace. (En el anexo de abajo damos más detalles de este problema.)

Problema 2: Datos posiblemente sesgados

El otro problema, que es clave e insalvable, es que dudamos fuertemente de la confiabilidad de los datos usados en este estudio.

Los datos de embarazos adolescentes tienen una tendencia a estar sesgados o incompletos, debido a que son un tema sensible y confidencial, de difícil acceso. Por ejemplo, en muchas familias los embarazos adolescentes tienden a ocultarse, e incluso a interrumpirse clandestinamente. Por lo tanto, los datos usados tienen el riesgo de incluir más embarazos adolescentes de ciertos sectores de la sociedad que de otros.

Así, incluso si la metodología usada para construir y evaluar los sistemas fuera correcta, las reglas estadísticas construidas sobre estos datos arrojaría conclusiones erradas, que reflejarían las distorsiones de los datos.

Problema 3: Datos inadecuados

Los datos utilizados fueron extraídos de una encuesta a adolescentes residentes en la provincia de Salta conteniendo información personal (edad, etnia, país de origen, etc), sobre su entorno (cantidad de personas con quien vive, si tiene agua caliente en el baño, etc) y sobre si había cursado o estaba cursando, al momento de la encuesta, un embarazo.

Estos datos no son adecuados para responder a la pregunta planteada: si una adolescente tendrá un embarazo en el futuro (por ejemplo, dentro de 5 ó 6 seis años). Para eso, sería necesario contar con datos recolectados 5 ó 6 años antes de que suceda el embarazo.

Con los datos actuales, en el mejor de los casos, el sistema podría determinar si una adolescente ha tenido, o tiene ahora, un embarazo. Es de esperar que las condiciones y características de una adolescente hayan sido muy diferentes 5 ó 6 años antes.

Conclusión

Tanto los problemas metodológicos como los datos poco confiables plantean el riesgo de llevar a tomar medidas incorrectas a los responsables de políticas públicas.

Este caso es un ejemplo de los peligros de utilizar los resultados de una computadora como una verdad revelada. Las técnicas de inteligencia artificial son poderosas y demandan responsabilidad por parte de quienes las emplean. En campos interdisciplinarios como éste, no debe perderse de vista que son sólo una herramienta más, que debe complementarse con otras, y de ningún modo reemplazan el conocimiento o la inteligencia de un experto, especialmente en campos que tienen injerencia directa en temas de salud pública y de sectores vulnerables.

================================================================

Anexo: Más detalles del problema 1

El proceso utilizado para obtener los resultados reportados es técnicamente incorrecto. Se está violando un principio básico del aprendizaje de máquinas: que los datos sobre los que se evalúa el sistema deben ser distintos a los datos que se usan para entrenarlo. Si este principio se viola, es decir, si hay contaminación de datos de entrenamiento en los datos sobre los cuales se valida, los resultados serán inválidos.

En el sistema descrito en github por el autor, la contaminación de los datos de evaluación surge de manera bastante sutil. El sistema usa un método para balancear la cantidad de muestras de cada clase llamado SMOTE. Este método genera nuevas muestras “sintéticas” replicando las muestras de la clase minoritaria (con riesgo de embarazo, en este caso) X veces con pequeñas variaciones respecto a la muestra original. El problema surge porque el autor hace esta réplica de datos antes de dividir los datos en entrenamiento y evaluación. Esta división se hace de manera aleatoria, de manera que es muy factible que una muestra aparezca en el conjunto de entrenamiento y alguna de sus réplicas aparezca en los datos de evaluación. Al evaluar en estos datos replicados, la consecuencia es que se sobredimensiona el valor de la exactitud. Dado este problema, es imposible saber cuál es la exactitud verdadera de este sistema.

Esto se puede entender usando un ejemplo. Supongamos que en vez de usar las características consideradas en este trabajo (edad, barrio, etnia, país de origen, etc), usamos simplemente el nombre y apellido de cada adolescente. Claramente, un sistema que tenga sólo esa información como entrada, no sería capaz de aprender a extrapolar y tomar decisiones en datos nuevos. Pero, en el caso de utilizar SMOTE como se ha utilizado, sería fácil poder aprender a memorizar los datos de entrenamiento perfectamente y luego, predecir con exactitudes muy altas los datos de evaluación ya que contendrían réplicas de estos mismos nombres y apellidos. En el caso que estamos estudiando, no se está usando el nombre y apellido como entrada pero sí una serie de características que, si lo pensamos detenidamente, permiten que ocurra el mismo problema. Por ejemplo, un sistema que aprende que una adolescente de 16 años, que vive en el barrio El Milagro, criolla, sin discapacidades, de origen Argentino, con agua caliente en el baño y que vive con 4 personas donde el jefe de hogar no abandonó los estudios tiene riesgo de embarazo adolescente, al evaluar el sistema con datos en donde ocurren réplicas casi idéntica de estas características, podrá predecir sin problema la clase de estas réplicas. Ya que, debido al uso de SMOTE previo a la división de los datos en conjuntos de entrenamiento y evaluación, una alta proporción de las muestras de la clase minoritaria vistas en la evaluación habrá sido vista durante el entrenamiento esto resulta en un valor de exactitud sobredimensionada.

Nota: Cabe aclarar que al momento de la redacción de este documento, otros han encontrado y reportado una visión muy similar, publicado en la misma página donde se publicó la descripción original del sistema de predicción. Enlace: https://github.com/facundod/case-studies/issues/2. In one of the most WTF uses of artificial technology yet, Microsoft has created one of the most bizarre algorithms ever. In an effort to show off its advancements in AI, the computer software company has crafted an AI to predict the impregnation of teenage girls. Yeah, bet you wish you never read that.

Microsoft AI predicts specific teenage pregnancy

Via Wired, Microsoft presented an AI algorithm to the Argentinian Congress back in 2018 that could predict teenage pregnancies. Presented during a period when the government was debating the decriminalisation of abortion, the tech giant created an AI forged in the fires of dystopia.

The algorithm was developed to predict the lines of lower-income “future teens”. Microsoft’s AI would take the name and addresses of preteen girls and predict the next “five or six years” of their lives.

Microsoft's data would be used to determine which girls were “86% predestined to have an adolescent pregnancy”. The AI’s database was built on the data of “200,000 residents in the city of Salta, including 12,000 women and girls between the ages of 10 and 19.”

Wired reports that the tech giant sent “territorial agents” to citizens’ houses to question them, including young girls. These agents asked questions, took photos and recorded GPS locations of the participants.

The surveys consisted of low-income families in Argentina. Additionally, a large section of the database consisted of migrant families that moved to the region from places such as South America.

Read More: Experts can’t decide on the most dystopic technology; everything sucks now

This is viewed as a success

The Microsoft pregnancy prediction algorithm is viewed as a success by the company. According to the report, spokespeople for the company claimed that the Argentinian project was “one of the pioneering cases in the use of AI data”.

That may indeed be the case. However, it's also an example of AI algorithms being used in an incredibly creepy and dangerous way. Additionally, talk of this algorithm has been kept on the down low, likely because its an off-putting foray into eugenics for the Big Tech company.

Even now, years after its inception, there's no word on whether or not the project has been terminated. Additionally, there's no data on what the Argentinian government is planning to do with the girls that have been marked for “predestined” teenage pregnancies.

Only one update has happened since the use of this algorithm: abortion has been decriminalised in Argentina. This means that those who do end up facing teenage pregnancy have a way out if they choose to. However, we don't know if Microsoft's systems affected this change or not.