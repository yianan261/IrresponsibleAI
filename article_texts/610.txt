CNN —

Police in Spain have launched an investigation after images of young girls, altered with artificial intelligence to remove their clothing, were sent around a town in the south of the country.

A group of mothers from Almendralejo, in the Extremadura region, reported that their daughters had received images of themselves in which they appeared to be naked.

One of the mothers, Miriam Al Adib, used a video published on her verified Instagram account to raise awareness about the situation.

She recounts how one of her daughters told her that someone had used an app to make an image of her in which she appeared to be naked, and that she had since found out that the same thing had happened to dozens more girls.

Addressing those responsible, Al Adib said: “You don’t appreciate the damage you have done.”

Francisco Mendoza, a regional government official, told local public media outlet Canal Extremadura that police have identified some of the young people that could be involved in producing the images.

On Wednesday, a spokeswoman for the high court of justice in Extremadura told CNN that a police investigation is ongoing.

Spain’s national police did not respond to requests for comment.

Al Adib called on those responsible to cooperate in fixing the issue, and said there was concern that the images could be uploaded to pornographic sites.

“Using other peoples’ photos for this kind of atrocity and sending them around is a very serious crime,” she wrote in the caption.

On Monday, Pilar Porrón, the mother of another one of the girls, told Canal Extremadura that her daughter found out from one of her friends that she had appeared in a naked photo.

Fátima Gomez, another mother, told the channel that one boy tried to extort her daughter using a doctored image.

She said her daughter had showed her a conversation with the boy in which he asked her to send him money, and when her daughter refused, the boy sent her a manipulated image of her naked.

According to the outlet, the images were made using an app which uses artificial intelligence to produce images of people without any clothes on.

Mayor of Almendralejo José María Ramírez said that the incident “is another case of gender-based violence.”

And María Guardiola, head of the Extremadura regional government, condemned the “disgusting incident” in a post on X, formerly known as Twitter.

“Digital violence against women is a scourge that is on the rise,” wrote Guardiola.

The incident comes as Spain is facing a reckoning over sexual assault and gender based violence after Luis Rubiales, president of the Spanish soccer federation, was accused of kissing Women’s World Cup winner Jennifer Hermoso without consent.

Rubiales resigned from his position on September 10 following weeks of fierce criticism. He denies the allegations against him.

The kiss on Hermoso came after the Spanish team’s victory in the Women’s World Cup final on August 20, and sparked condemnation in Spain and across the world.

The 46-year-old previously apologized and described the kiss as “mutual” – a claim Hermoso denied, saying she did not consent and was not respected.

There was also further uproar on September 14 as police arrested a man on suspicion of sexual assault after he appeared to touch a TV journalist on the bottom as she was reporting live from the streets of Madrid.. Almendralejo in southern Spain became one of the first global examples of just how alarming sexual harassment with AI Deepfake technology can be.

ADVERTISEMENT

It was a quiet Sunday when journalist Marian Rosado was browsing her Instagram account and suddenly stumbled upon an unusual Instagram live broadcast.

The well-known gynecologist Miriam Al Adib was revealing a very personal and shocking situation: her 14-year-old daughter just told her somebody used an app to take her photo from social media and made it look like she was naked.

This is just the beginning of a scandal that placed a quiet town in southern Spain in the spotlight.

Over the next three episodes, the team of Euronews Tech Talks will delve into the world of AI Deepfake technology.

We aim to uncover its scope, address strategies for mitigating its risks, and explore methods for educating society to recognise the dangers and protect themselves.

The impact of fake AI nudes

Almendralejo, a tranquil town with 35,000 residents nestled in the Spanish region of Extremadura near the Portuguese border, isn't the type of place that typically captures national, let alone international, attention.

However, in September, just after the school break, Almendralejo made headlines.

Dozens of local teenagers reported receiving AI-generated naked images of themselves on their mobile phones.

In the real photos, the teenagers were fully clothed. These images, stolen from their Instagram accounts, were altered using an artificial intelligence (AI) app and then circulated in WhatsApp groups.

Despite the artificial nature of the nudity, the distress felt by the girls upon seeing these images was very real, as reported by their mothers.

What adds to the unsettling nature of this story is that the perpetrators of this sexual harassment were also teenagers known to the girls.

'Undress any picture with AI for free'

Equally disturbing is how easily these images were created.

Deepfakes, a form of synthetic media utilising AI, typically involve complex processes like deep learning.

However, these teenagers weren't AI experts. They simply paid €10 to obtain 25 hyper-realistic naked images of their peers using the Clothoff app.

Available for free download, the app enables users to digitally undress anyone in their phone's picture gallery with the slogan, "Undress any picture with AI for free".

When approached for clarification on their rules, Clothoff emphasized age verification and obtaining consent.

While they claimed to take age verification seriously, they didn't disclose their methods due to "security reasons".

Regarding consent, they asserted having strict policies but didn't specify them, relying on users to follow the guidelines.

ADVERTISEMENT

A study by Sensity AI revealed that 96 per cent of deepfake images consist of sexually explicit pictures of non-consenting women.

A Europol report estimates that within three years, approximately 90% of online content may be AI-generated.. . More than 20 girls in Spain reported receiving AI-generated naked images of themselves. But can deepfakes be legally punished?

ADVERTISEMENT

When they returned to school after the summer holidays, more than twenty girls from Almendralejo, a town in southern Spain, received naked photos of themselves on their mobile phones.

None of them had taken the pictures, but they looked completely real.

The images had been stolen from their Instagram accounts, altered using an artificial intelligence application and then shared in Whatsapp groups.

The teenagers were fully clothed in the real photos, but the app made the nudity look completely real.

Now parents and prosecutors are asking whether a crime has been committed, even if the pictures are actually real - could the images be considered child pornography?

"The montages are super realistic, it's very disturbing and a real outrage," Miriam Al Adib, one of the girls' mothers, wrote on her Instagram account.

"My daughter told me with great disgust: 'Mum, look what they have done to me'," she added.

Al Adib even claimed that the photos could have reached internet portals such as Onlyfans or pornographic websites. All the while, the girls endured the comments of their classmates.

”Don't complain, girls upload pictures that almost show their p***y," one of the girls was told.

The youngest of the girls is only 11 years old and not yet in high school.

The mothers of some of the victims have denounced what has happened. Canva

Another mother, Fátima Gómez, told Extremadura TV that her daughter had been blackmailed.

In a conversation with a boy on social media, he asked her for money and when she refused, he sent her a naked picture.

The mothers have organised themselves to complain about what has happened, and the National Police have opened an investigation and already identified several minors allegedly involved.

Some of them are classmates of the girls, a local politician revealed.

The case has been referred to the Juvenile Prosecutor's Office, and the mayor of the town himself warned: "It may have started as a joke, but the implications are much greater and could have serious consequences for those who made these photos".

€10 euros for 25 nude photos

The hyper-realistic artificial intelligence creations, better known as deepfakes, were made with the ClothOff app.

With the slogan "Undress anybody, undress girls for free", the app allows users to take the clothes off from anyone who appears in their phone's picture gallery. It costs €10 to create 25 naked images.

Although the nudity is not real, the mothers say that the girls' distress at seeing their picture is very real indeed.

ADVERTISEMENT

"You are not aware of the damage you have done to these girls and you’re also unaware of the crime you have committed," Al Adib said on her Instagram account in a message addressed to the people who shared the pictures.

"My daughter was told by one of them that he had done ‘things’ with her photo," another of the mothers told Spanish newspaper El País.

But can deepfakes be legally punished?

"One question is whether it should be punished and another is whether it can be punished by the way the law is drafted in Spain and in other EU countries," Manuel Cancio, professor of criminal law at the Autonomous University of Madrid, told Euronews.

The professor points out that there is a legal loophole because the use of minors' faces in photographs affects their privacy, but when it comes to crimes in which intimate images are distributed, it is the image as a whole that violates privacy.

ADVERTISEMENT

"Since it is generated by deepfake, the actual privacy of the person in question is not affected. The effect it has (on the victim) can be very similar to a real nude picture, but the law is one step behind," he adds.

Cancio says that the legal framework that could work in this case would be a crime against moral integrity, "a kind of disaster box for crimes that no one knows where to put".

In March 2022, the European Commission proposed criminalising this type of offence in a directive on cybercrime. According to the professor, the Dutch Criminal Code is the only one that has a provision addressing this issue.

National Police have opened an investigation and already identified several minors allegedly involved. Canva

Can it be considered child pornography?

Experts are divided as to whether the crime could be considered distribution of child pornography, which would carry a higher penalty, and prefer to err on the side of caution.

For Leandro Núñez, a lawyer specialising in new technologies at the Audens law firm, the key is not whether the photo is 100% real, but whether it appears to be.

ADVERTISEMENT

"The most important thing is whether the face is identifiable. We could be talking about child pornography, crimes against moral integrity or the distribution of images of non-consensual sexual content," the lawyer told Euronews.

"In the case of a crime against moral integrity, it would be considered a lesser crime, so it would carry a lesser sentence of six months to two years in prison," he adds.

Other experts, such as Eloi Font, a lawyer at Font Advocats, a law firm specialising in digital law, believe that it could be classified as a crime similar to the reproduction of sexual images of minors.

In this case, the penalty could be between five and nine years in prison.. Before processing it through the AI app, the predators used to access the images using the social media accounts of the girls. (Representational Image | Credits: Pexels)

Barcelona: A silent town in Spain's Badajoz province became a talking point after outrage poured in over morphed images of minor girls living there surfaced on the internet. The deepfakes were created using the artificial intelligence (AI) app ClothOff.

Girls as young as 11 and as old as 17 from Almendralejo town were targeted. Describing that the morphed images looked realistic, Dr Miriam Al Adib, mother of one of the victims, said in an Instagram video that the pictures are quite unsettling for everyone, especially her 14-year-old daughter.

Before processing it through the AI app, the predators used to access the images using the social media accounts of the girls. As of now, 20–30 girls aged 11–17 have come forward and claimed that they have been targeted in the gruesome cybercrime, the BBC reported.

"One day my daughter came out of school, and she said, 'Mum, there are photos circulating of me topless'. I asked her if she had taken any photos of herself,” the girl replied “no, Mum, these are fake photos of girls that are being created a lot right now,” recalled María Blanco Rayo, the mother of a 14-year-old girl.

The morphed images were spread in at least four of Almendralejo's five middle schools, El País reported.

The impact of the images was such that the girls wouldn’t even leave their house, Rayo told the BBC. ''If I didn't know my daughter's body, this photo looks real. You, (the culprits) are not aware of the damage you are causing. Using images to create this disgusting material and distributing them is a very serious crime,'' she said.

Local police have identified 11 boys from the area with direct or indirect involvement in the creation of deep fakes. They either created those images or circulated them on WhatsApp and Telegram groups. Deepfakes are manipulative synthetic media that replaces or alters images digitally.

The police have so far received complaints from 11 minor girls. The parents of the affected children have also set up a support group to help them through the ordeal.. In Spain, authorities are currently looking into a distressing case that has sent shockwaves across the nation. Several underage girls have become the victims of a fake nude images scandal.

These images which were created using an Artificial Intelligence (AI) tool have caused distress to the victims, who in some cases found themselves facing bullying and/or distressing comments from their classmates.

Deepfake porn

In the town of Almendralejo which is situated in the southwestern reaches of Spain, local law enforcement is dealing with 11 complaints from victims, all of whom are minors.

Speaking to AFP, a spokesperson from the local police said that those responsible for these abhorrent deeds employed a sinister method. They "manipulated photos of underage girls", superimposing their innocent faces onto the "bodies of other people" in other images.

Small price, huge ramifications

As per EuroNews, these images were created using an Artificial Intelligence (AI) tool application using which users can 'take clothes off' of anyone they have a picture of, that too, for prices as low as €10.

The app boasts the capability of crafting eerily realistic photo montages, has the slogan "Undress anybody, undress girls for free," reports EuroNews.

As per reports from Spanish media outlets, an estimated 20 girls may have tragically become the victims of these deep fake porn images.

One concerned parent, Miriam Al Adib, a mother of a 14-year-old victim, underscored the gravity of the situation. Talking to the press, she said: "This is very serious."

Also read | Breakthrough? Google develops AI tool capable of predicting harmful genetic mutations

Taking to Instagram, she recounted how her distraught daughter had showed her one such photo.

"When I came home, one of my daughters, who was really upset, told me: 'look what they did'. It turns out they took a photo of her, and they made it seem as if she was naked with the aid of artificial intelligence."

"Girls, don't be afraid to report such acts. Tell your mothers. Affected mothers, tell me, so that you can be in the group that we created," she added.

Another mother revealed that using these fake nude images, someone had attempted to extort her daughter. Allegedly, the creators of these images demanded monetary payment in exchange for their silence.

Furthermore, it has come to light that these manipulated images may have been disseminated on various platforms, including OnlyFans, an online subscription service notorious for adult content, and explicit websites.

(With inputs from agencies)

WATCH WION LIVE HERE. In a small Spanish town, several schoolboys used generative models to create fake nudes of their fellow pupils. Police, prosecutors, and parents are at a loss on how to pursue a case that shows, once again, that women are the main victims of deepfakes.

Since their inception in 2017, generative models have been used to artificially undress women in photos and create realistic pornographic videos. The problem has been identified for as long. But instead of being stopped, the tools for that purpose are becoming more accessible. So much so that ten schoolboys in the Spanish town of Almendralejo, in southwestern Spain, are under investigation after creating and disseminating fake nudes of their fellow pupils.

The case went public after the mother of one of the victims posted a video denouncing the situation on her Instagram account, where she has over 136,000 followers. Gynecologist Miriam Al Adib reported how her 14-year-old daughter came to her after she learned that pictures on which she appeared to be naked were circulating among her classmates. Several parents from the town came forward after the incident went public, since all their daughters claimed that the photographs were fake.

Although the case is ongoing and the details of the investigation are still confidential, the kids behind the ruse allegedly used an online app that uses generative models to create nudes from photographs where the girls were dressed. Many Spanish media outlets have identified specific applications with which the nude pictures could have been created, but neither of them has been confirmed as being the tool in case by police officers. All the platforms they mention require users to be over 18, but it is clear that there is no safeguard mechanism in place to prevent minors from using them, as elDiario.es pointed out.

Violating women’s bodies

That this event took place in a town of 34,000 inhabitants proves once more that new technology makes violating women’s bodies even easier. Since deepfake technologies became popular, some expected disinformation and propaganda to be the main use cases, leading to the collapse of institutions and war. Instead, women have been the main target.

This has been going on for a while: “Surprisingly, since the first fake pornographic videos and photos were created in 2017 using these techniques, this has been the main domain of application. Today, most studies estimate that around 90% of the fake content published online is pornographic,” summarizes Marta Beltrán, a doctor in computer science and mathematical modeling, in her book Mr. Internet.

The distressing case in Spain put the spotlight back on a never-ending problem. Back in 2019, an application of the same kind, DeepNude, allowed users to remove clothes from women’s pictures for 50 dollars. One year later, a Telegram bot appeared that enabled users to receive fakes nudes in return for sending pictures of girls and women.

Both services were taken down after the media reported on it and thereby created some public uproar. But, like mushrooms, similar tools keep popping up.

The problem goes further, as pointed out by Marta Beltrán to AlgorithmWatch: “Some of these services advertise themselves directly with slogans like ‘strip whoever you want.’ Moreover, they are offered on certain TikTok channels, video game chat rooms, and similar platforms where minors and very young people are active. It is clear that they are the target audience, and they offer tutorials so that they can learn quickly and get an idea of what can be done. They show it to them as something fun even.”

Only a few legal avenues

These platforms are not for adults only anymore. Instead, they are being advertised as an “open bar” for youngsters, Beltrán says. Creating artificial photos of someone we know is something that, with a little bit of effort, can be achieved with well-known tools such as Photoshop. But an automated service to strip women from their clothes that can be easily used by 13-year-olds is something entirely different, and should be treated accordingly by regulators.

In such cases, not a particular technology's use is punished but the outcome. Undressing underage girls using generative models is not directly a criminal offense, which makes the legal case even more difficult.

According to El País, confusion remains as to which specific offenses should be prosecuted. The prosecutors could consider charging the wrongdoers with “production or distribution of pornographic material” and “possession of child pornography” – although the law requires the images to show an “explicit sexual act” or the minor’s genitals to be displayed in a sexual way, as pointed out by Law Professor Paz Lloria. This does not seem to apply in this case. Another legal avenue could be to press charges because the victims' privacy and moral integrity were violated.

The Spanish National Data Protection Agency has also opened an ex officio investigation over a possible violation of data protection laws and is making use of its ‘Priority channel’, a free service that allows citizens in Spain to report the dissemination of non-consensual sexual photos or videos and that requires platforms to take them down.. A small town in southern Spain has been rattled by a scandal involving the distribution of AI-generated images of young girls within several local schools. These deepfake images, primarily sourced from the girls’ social media profiles where they appeared fully clothed, were disseminated across at least four out of Almendralejo’s five middle schools, as reported by El País.

The artificial intelligence application responsible for this disturbing act processed the images to create new, fake ones portraying the girls in a naked state, according to the report. This unsettling development stemmed from the use of an AI-powered app known as “Clothoff,” which allows users to manipulate images to undress subjects.

The victims in this case include more than 20 girls aged 11 to 17 in Almendralejo, a town situated in the western Spanish region of Extremadura. The matter came to light after Miriam Al Adib, a doctor and the mother of one of the affected girls, used her social media platform to raise awareness of the incident.

Read More: UK to Invest £100m in AI Chips Production Amid Global Competition

The distressing situation has prompted a collective response, with parents creating a WhatsApp group to support the victims. At least 10 individuals have been identified as suspects in the case, some of whom are reportedly as young as 13 years old. It’s important to note that, in Spain, children under 14 years old cannot face criminal charges.

Spain’s national police force, la Policía Nacional, continues to investigate this troubling incident that has left a profound impact on the girls and their community.. "I saw a naked photo of you."

Dystopia Now

"I saw a naked photo of you."

Those were the chilling words a boy told a 14-year-old girl after she arrived on the first day of school this year in Spain, Spanish newspaper El Pais reports. AI-generated, deepfake nudes of her and other female classmates were being circulated online across four schools, stoking despair among the victims and their parents. There even was one case of alleged blackmail.

"Mom, they say there’s a naked photo of me going around," the girl told her mother, as quoted by El Pais. "That they did it with an artificial intelligence app. I’m scared. Some girls have also received it.”

The case sparked a round of parental outrage in the town of Almendralejo, and the local Juvenile Prosecutor’s Office is now handling the investigation, according to the report, with 20 victims impacted so far and a group of suspected culprits behind the deepfakes already identified.

The deepfake app the culprits allegedly used is free to use and accessible to anybody with a smartphone — a dystopian use of AI tech that could continue to cause mayhem in schools and elsewhere.

Deepfaked

One of the girls, according to El Pais, was approached on Instagram by a boy who demanded money. When she turned him down, he sent her a deepfake naked photo of herself.

As the tech becomes more accessible, we should expect this kind of stuff to happen more and more. Meanwhile, in the US, the FBI sent out a warning earlier this year about deepfakes being used for extortion.

"The FBI continues to receive reports from victims, including minor children and non-consenting adults, whose photos or videos were altered into explicit content," the agency said in its statement.

To stem this tide, attorney generals from every single state in the US sent a letter to Congress earlier this month, urging a commission and action against the increase of AI-generated child sexual abuse material (CSAM).

The Washington Post reported in June that experts had noticed an increase in the dissemination of AI-generated CSAM, which is already hampering efforts to identify the many victims.

It remains to be seen if any upcoming regulations will help forestall the tide of deeply disturbing deepfake images, especially considering the proliferation of open-source generative AI and how long it takes for governments to agree on a solution.

More on deepfakes: Every Single State’s Attorney General Is Calling for Action on AI-Generated Child Abuse Materials. A town in Spain made international headlines after a number of young schoolgirls said they received fabricated nude images of themselves that were created using an easily accessible "undressing app" powered by artificial intelligence, raising a larger discussion about the harm these tools can cause.

"Today a smartphone can be considered as a weapon," Jose Ramon Paredes Parra, the father of a 14-year-old victim, told ABC News. "A weapon with a real potential of destruction and I don't want it to happen again."

Over 30 victims between the ages of 12 and 14 years of age have been identified so far, and an investigation has been ongoing since Sept. 18, Spanish National Police told ABC News.

And while most of the victims are from Almendralejo, a town in the southwest of Spain at the center of this controversy, Spanish National Police say they have also found victims in other parts of the country.

A group of male perpetrators, who police say knew most of the victims, used photos taken from the social media profiles of female victims and uploaded them to a nudify app, authorities told ABC News.

Nudify is a term used to describe an AI-powered tool designed to remove clothing from a subject in a photo. In this case, the service can be used via Telegram or via an app you download on your phone.

These same perpetrators, also minors, created a group chat on WhatsApp and on Telegram to disseminate these non-consensual fabricated nude images, authorities told ABC News. The fake images were used to extort at least one victim on Instagram for real nude images or money, said the parent of one of the victims.

Telegram told ABC News they actively moderate harmful content on their platform including the distribution of child Sexual abuse material (CSAM). “Moderators use a combination of proactive monitoring of public parts of the app and user reports in order to remove content that breaches our terms of service.” Over the course of the month of September, Telegram says moderators removed 45,000 groups and channels related to child abuse.

WhatsApp spokesperson told ABC News that they would treat "this situation the same as any kind of CSAM we become aware of on our platform: we would ban those involved and report them to the National Center for Missing & Exploited Children."

"This is a direct abuse of women and girls, by technology that is specifically designed to abuse women and girls," said Professor Clare McGlynn, a law professor at Durham University in the U.K. and an expert on violence against women and girls.

ABC News reached out to the email address listed on the app’s website and received a response. The team behind the app said their main reason for creating this type of service was to make "people laugh" by "processing their own photos and laugh together by processing each other's photos."

"By them laughing on it we want to show people that they do not need to be ashamed of nudity, especially if it was made by neural networks," the team explained via email.

When pressed on what safeguards were in place regarding the use of the app with photos of minors, they responded that they have protections in place for photos of people below the age of 18. If a user tries to upload a photo of a minor they will receive an error, and be blocked after two uses, they added.

The team behind the app said they investigated how their app was used after the news of the case in Spain broke out and found that the perpetrators had a workaround and likely used a combination of their app and another app to create the non-consensual nudes.

Experts tell ABC News all it takes to make a hyper-realistic non-consensual deepfake is a photo, an email address and a few dollars if you want to create them in bulk.

ABC News reviewed the nudify app Spanish authorities say was used to create these AI-generated explicit images of young girls. The app offers a free service that can be used through Telegram, as well as an app that you can download on your phone.

When ABC News reviewed the app, it offered a premium paid service that listed payment methods such as Visa, Mastercard, and Paypal. These payment methods, along with several others, were removed after ABC News reached out.

A Visa spokesperson told ABC News that it does not permit the use of their network to be used for illegal activity. "Visa rules require merchants to obtain consent from all persons depicted in any adult content, including computer-generated or computer-modified content, such as deepfakes," added a spokesperson.

A Paypal spokesperson told ABC News that it "takes very seriously its responsibility to ensure that customers do not use its services for activities that are not allowed under its Acceptable Use Policy. We regularly review accounts and when we find payments that violate our policies, we will take appropriate action."

Mastercard did not respond to requests for comment.

Parra and his wife, Dr. Miriam Al Adib Mendiri, went directly to local police after they said their daughter confided in them that she had been targeted and they also decided that they would use Mendiri's large social following to denounce the crime publicly.

"Here we are united to STOP THIS NOW. Using other people's images to do this barbarity and spread them, is a very serious crime," Mendiri shared in an Instagram video. "[…] Girls, don't be afraid to report such acts. Tell your mothers."

Mendiri's public appeal led to many more victims coming forward to the police. Local authorities say that some of the perpetrators are under 14 years old, meaning they will have to be tried under the minor criminal law. Investigations are ongoing, confirmed Spanish National Police.

"If they do not understand what they did now, if they don't realize it, what they will become later?" said Parra. "Maybe rapist, maybe gender violent perpetrator… they need to be educated and to change now.”

Experts like McGlynn believe the focus should be on how global search platforms rank non-consensual deepfake imagery and the apps that facilitate the creation of non-consensual imagery.

"Google returns nudify websites at the top of its ranking, enabling, and legitimizing these behaviors," McGlynn said. "There is no legitimate reason to use nudify apps without consent. They should be de-ranked by search platforms such as Google."

Another expert, who founded a company to help individuals remove leaked private content online, agreed with McGlynn.

"Apps that are designed to essentially unclothe unsuspecting women have zero place in our society, let alone search engines," said Dan Purcell, founder of Ceartas. "We are entering an endemic of kids using AI to undress kids, and everyone should be concerned and outraged."

A Google spokesperson responded by saying: “Like any search engine, Google indexes content that exists on the web, but we actively design our ranking systems to avoid shocking people with unexpected harmful or explicit content. We also have well-developed protections to help people impacted by involuntary fake pornography – people can request the removal of pages about them that include this content.”

They added that as this space and technology evolves, “they are actively working to add more safeguards to help protect people, based on systems we've built for other types of non-consensual explicit imagery.”

Microsoft’s Bing is another search engine where websites containing non-consensual deepfake imagery are easily searchable. A Microsoft spokesperson told ABC News, "The distribution of non-consensual intimate imagery (NCII) is a gross violation of personal privacy and dignity with devastating effects for victims. Microsoft prohibits NCII on our platforms and services, including the soliciting of NCII or advocating for the production or redistribution of intimate imagery without a victim’s consent."