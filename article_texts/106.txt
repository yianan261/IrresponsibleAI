“I am captivated by a sense of fear I have never experienced in my entire life …” a user named Heehit wrote in a Google Play review of an app called Science of Love. This review was written right after news organizations accused the app’s parent company, ScatterLab, of collecting intimate conversations between lovers without informing the users and then using the data to build a conversational A.I. chatbot called Lee-Luda.

A majority of Americans are not confident about how companies will behave when it comes to using and protecting personal data. But it can be hard to imagine the potential harms—exactly how a company misusing or compromising data can possibly affect us and our lives. A recent incident of personal data misuse in South Korea provides us a clear picture of what can go wrong, and how consumers can fight back.

South Korean A.I. company ScatterLab launched Science of Love in 2016 and promoted it as a “scientific and data-driven” app that predicts the degree of affection in relationships. One of the most popular services of the app was using machine learning to determine whether someone likes you by analyzing messenger conversations from KakaoTalk, South Korea’s No. 1 messenger app, which about 90 percent of the population uses. Users paid around $4.50 per analysis. Science of Love users would download their conversation logs using KakaoTalk’s backup function and submit them for analysis. Then, the app went through the messenger conversations and provided a report on whether the counterpart had romantic feelings toward the user based on statistics such as the average response time, the number of times each person texts first, and the kinds of phrases and emojis used. By June 2020, Science of Love had received about 2.5 million downloads in South Korea and 5 million in Japan and was preparing to expand its business to the United States. “Because I felt like the app understood me, I felt safe and sympathized. It felt good because it felt like having a love doctor by my side,” a user named Mung Yeoreum wrote in a Google Play review of the app.

Advertisement

Advertisement

Advertisement

Advertisement

On Dec. 23, 2020, ScatterLab introduced an A.I. chatbot service named Lee-Luda, promoting it to be trained on more than 10 billion conversation logs from Science of Love. The target audience of this chatbot service was teenagers and young adults. Designed as a 20-year-old female that wants to become a true friend to everyone, chatbot Lee-Luda quickly gained popularity and held conversations with more than 750,000 users in its first couple of weeks. The CEO stated that the company’s aim was to create “an A.I. chatbot that people prefer as a conversation partner over a person.”

Modern chatbots’ ability to, well, chat relies heavily on machine learning and deep learning models (which together can be called A.I.) to better understand human language and generate human-like responses. If people enjoyed speaking with Lee-Luda, that was because it was trained on a large dataset of human conversations.

Advertisement

However, within two weeks of Lee-Luda’s launch, people started questioning whether the data was refined enough as it started using verbally abusive language about certain social groups (LGBTQ+, people with disabilities, feminists, etc.) and made sexually explicit comments to a number of users. ScatterLab explained that the chatbot did not learn this behavior from the users it interacted with during the two weeks of service but rather learned it from the original training dataset. In other words, ScatterLab had not fully removed or filtered inappropriate language or intimate and sexual conversations from the dataset. It also soon became clear that the huge training dataset included personal and sensitive information. This revelation emerged when the chatbot began exposing people’s names, nicknames, and home addresses in its responses. The company admitted that its developers “failed to remove some personal information depending on the context,” but still claimed that the dataset used to train chatbot Lee-Luda “did not include names, phone numbers, addresses, and emails that could be used to verify an individual.” However, A.I. developers in South Korea rebutted the company’s statement, asserting that Lee-Luda could not have learned how to include such personal information in its responses unless they existed in the training dataset. A.I. researchers have also pointed out that it is possible to recover the training dataset from the AI chatbot. So, if personal information existed in the training dataset, it can be extracted by querying the chatbot.

Advertisement

Advertisement

Advertisement

To make things worse, it was also discovered that ScatterLab had, prior to Lee-Luda’s release, uploaded a training set of 1,700 sentences, which was a part of the larger dataset it collected, on Github. Github is an open-source platform that developers use to store and share code and data. This Github training dataset exposed names of more than 20 people, along with the locations they have been to, their relationship status, and some of their medical information. In Tensorflow Korea, an A.I. developer Facebook community, a developer revealed that this KakaoTalk data containing private information had been available on Github for almost six months. The CEO of ScatterLab later said that the company did not know this fact until its internal inspection took place after the issue arose.

Advertisement

ScatterLab issued statements of clarification of the incident intended to soothe the public’s concerns, but they ended up infuriating people even more. The company statements indicated that “Lee-Luda is a childlike A.I. that just started conversing with people,” that it “has a lot to learn,” and “will learn what is a better answer and a more appropriate answer through trial and error.” However, is it ethical to violate individuals’ privacy and safety for a chatbot’s “trial and error” learning process? No.

Advertisement

Advertisement

Even more alarming is the fact that ScatterLab’s data source was not a secret in the A.I. developer community, and yet no one questioned whether such sensitive data was collected ethically. In all presentation slides (such as at PyCon Korea 2019), talks (like at Naver), and press interviews, ScatterLab had boasted about its large dataset of 10 billion intimate conversation logs.

Advertisement

While this incident was a big story in South Korea, it received very little attention elsewhere. But this incident highlights the general trend of the A.I. industry, where individuals have little control over how their personal information is processed and used once collected. It took almost five years for users to recognize that their personal data were being used to train a chatbot model without their consent. Nor did they know that ScatterLab shared their private conversations on an open-source platform like Github, where anyone can gain access.

Advertisement

In the end, it was relatively simple for Science of Love users to notice that ScatterLab had compromised their data privacy to train Lee-Luda. Once the chatbot started spewing out unfiltered comments and personal information, users immediately started investigating whether their personal information was being misused and compromised. However, bigger tech companies are usually much better at hiding what they actually do with user data, while restricting users from having control and oversight over their own data. Once you give, there’s no taking back.

Advertisement

It’s easy to think of ScatterLab’s incident merely as a case of a startup’s mismanagement, but this incident is also a result of the negligence of a big tech company. Kakao, the parent company of KakaoTalk and one of the largest tech companies in South Korea, remained silent throughout ScatterLab’s incident despite its users being the victims of this incident. You’d wish a big tech company like Kakao to be more proactive when its users’ rights are violated by another company. However, Kakao said nothing.

One of the biggest challenges big data in A.I. poses is that the personal information of an individual is no longer only held and used by a single third party for a specific purpose, but rather “persists over time,” traveling between systems and affecting individuals in the long term “at the hand of others.” It’s extremely concerning that such a big tech company like Kakao failed to foresee the implications and dangers of KakaoTalk’s backup function of which ScatterLab took advantage to obtain KakaoTalk users’ data. More alarming is that Kakao left this incident unaddressed when it clearly stemmed from the misuse of its own data. In this sense, Kakao’s attitude towards its users’ data privacy was not very different from ScatterLab’s: negligent.

Because data protection laws are slow to catch up with the speed of technological advancement, “being legal” and “following industrial conventions” are not enough to protect people and society. Then, the question will be whether the A.I. industry and tech companies can innovate themselves to come up with and adhere to more comprehensive and detailed ethical guidelines that minimize harm to individuals and society.

Future Tense is a partnership of Slate, New America, and Arizona State University that examines emerging technologies, public policy, and society.. AI Chatbot ‘Lee Luda’ and Data Ethics Yeonsoo Doh · Follow Published in CARRE4 · 6 min read · Feb 7, 2021 -- Listen Share

The case of Lee Luda has aroused the public’s attention to the personal data management and AI in South Korea.

Lee Luda, an AI Chatbot with Natural Tone

Last December, an AI start-up company in South Korea, ScatterLab, launched an AI chatbot named ‘Lee Luda’. Lee Luda is set up as a 20-year-old female college student. Since quite natural conversation was possible with Luda, the chatbot service gained a huge popularity especially within Generation Z. In fact, the service attracted more than 750,000 users in 20 days since it was launched (McCurry 2021). It seemed that Lee Luda was a success by demonstrating natural interaction with humans.

Image of Lee Luda

However, soon it became socially controversial due to several problems. Before taking up the main subject, we need to know how it was possible for Luda to communicate with humans so naturally.

The natural tone of Lee Luda was possible as ScatterLab collected “10 billion real-life conversations between young couples taken from KakaoTalk”, which is the most popular message application in South Korea (McCurry 2021). ScatterLab did not directly collect conversations from KakaoTalk, but took a roundabout way; in other words, in a sneaky way. There have been few counselling service applications which analyse messenger conversations and give advice about love life when the users agree to submit their KakaoTalk conversations to the apps. ScatterLab obtained data from those applications very easily.

Internal and External Problems of Luda

So, few problems came up in pursuance of collecting data. First, the users of counselling apps agreed to share their conversations with those applications, but not with ScatterLab. The users would not have known that their conversations would be used in developing an AI chatbot. Second, the applications got the users’ agreement, but not from the companions of conversations. In prior to collecting messenger conversations, there must be an agreement from every participant of conversations though.

What was worse, ScatterLab was very poor at data cleaning. It is revealed that Luda sometimes responded with random names, addresses, and even bank account numbers (D. Kim 2021). The random personal information is probably the ones extracted from the conversations submitted to counselling apps. In addition to this, ScatterLab shared their training model on GitHub, but not fully filtering or anonymising the data (D. Kim 2021). As a result, personal information was publicised since ScatterLab did not clean the data properly. It seems that ScatterLab was not conscious of data ethics at all.

There remains another problem which caused controversy over Lee Luda and AI as a whole in the beginning. When Luda was asked its opinions about social minorities, it revealed disgust towards them. For example, when a user asked Luda about LGBTQ, Luda answered, “I’m sorry to be sensitive, but I hate it [LGBTQ], it’s disgusting” (E. Kim 2021). The user asked why, and Luda added, “It’s creepy, and I would rather die than to date a lesbian” (E. Kim 2021). It is known that Luda also made discriminatory remarks towards the disabled and a certain race group. The creators of Lee Luda would not have intended to target and discriminate a certain group of people, but Luda did.

Frankly speaking, Lee Luda was built up wrongly from the beginning. First, the data needed for deep learning was inappropriately obtained; ScatterLab did not inform the data providers (counselling app users) that they would use their data in creating an AI chatbot. Second, the data was not cleaned properly; the chatbot revealed some personal information when chatting, and the company even shared the training model on GitHub not thoroughly filtering or anonymising personal data. Third, the company failed to handle or manipulate the chatbot after they launched it; Luda did not hesitate to express hatred towards a certain group of people, and ScatterLab was not aware of it.

Always Beware and Be Responsible!

Lee Luda appeared flawless at first, perhaps, less flawed than other AI chatbots. Instead, it turned out to be highly flawed. As a consequence, ScatterLab had to destroy Lee Luda, and further, to be investigated due to the violation of privacy laws and poor data handling. Due to Lee Luda case, the public began to fear AI as a whole. This is because they witnessed that an AI system can go wrong anytime — irrespective of the system builder’s intention — even though it is seemingly built well.

It is a matter of course that ScatterLab obtained data improperly and misused the data; causing the leakage of personal information and prejudicing the public against AI. Nevertheless, I would like to emphasise that both data providers and data collectors need to be responsible for the data they create, provide, collect, and use. Living in the time closely connected with internet of things (IoT), AI is inseparable from our daily life. Then what should we do to make use of AI, by keeping in mind that AI is built upon big data?

It is very common to see the users of a certain internet service are indifferent to the usage of their personal data, although they have the rights to the data. They must agree to terms of services — which states that their personal data will be collected and shared — otherwise they will not be able to use the service. Yet, they are often not aware of the terms as they simply do not read the screed or do not understand the legal terms. They would implicitly know that their personal information will be revealed or used somewhere and sometime, but they would not know the exact usage or extent of disclosure. The best way to prevent data leakage or misuse would be that individuals need to understand what kind of data they are sharing, who they are sharing with, and where the data will be used.

In addition to this, the data collectors often overlook data ethics that they need to collect and handle the data with caution. Obviously, the lack of control on the usage of data can produce negative outcomes. Thus, the data collectors must specify what kind of data they will be collecting from data providers and how they will be used. They also should have a sense that the data providers gave the right to use their data, thus the data cannot be transferred to others without agreement, and the data should be treated carefully. Furthermore, there must be legal and technical mechanisms which protect data providers’ privacy and prevent data collectors from breaching laws.

In sum, keeping data safe is not just a matter of one certain group of people, but it is a matter of everyone. By understanding how personal data should be shared, how the data one shared can be used, and what steps are needed to protect the data, we can protect our personal information and will be able to make good use of advanced technology without being counterattacked.

=======

References

Kim, D. 2021, ‘Chatbot gone awry starts conversations about AI ethics in South Korea’, The Diplomat, viewed 03 February 2021, <https://thediplomat.com/2021/01/chatbot-gone-awry-starts-conversations-about-ai-ethics-in-south-korea/>.

Kim, E. 2021, ‘Chatbot Luda controversy leave questions over AI ethics, data collection’, Yonhap News, viewed 02 February 2021, <https://en.yna.co.kr/view/AEN20210113004100320>.

McCurry, J. 2021, ‘South Korean AI chatbot pulled from Facebook after hate speech towards minorities’, The Guardian, viewed 21 January 2021, <https://www.theguardian.com/world/2021/jan/14/time-to-properly-socialise-hate-speech-ai-chatbot-pulled-from-facebook>.. In Spike Jonze’s 2013 film, “Her,” the protagonist falls in love with an operating system, raising questions about the role of artificial intelligence (AI), its relationship with the users, and the greater social issues emerging from these. South Korea briefly grappled with its “Her” moment with the launch of the AI chatbot, “Lee Luda,” at the end of December 2020. But the Luda experience was not heart-wrenching or pastel-colored like “Her” – instead, it highlighted different types of phobia and risks posed by new technologies that exist within South Korean society.

Lee Luda (a homonym for “realized” in Korean) is an open-domain conversational AI chatbot developed by ScatterLab, a South Korean start-up established in 2011. ScatterLab runs “Science of Love,” an app that provides dating advice based on analysis of text exchanges. The app has been downloaded over 2.7 million times in South Korea and Japan. Backed by giants such as NC Soft and Softbank, ScatterLab has raised over $5.9 million.

Luda was created by ScatterLab’s PingPong team, its chatbot wing that aims to “develop the first AI in the history of humanity to connect with a human.” Luda, using deep learning and over 10 billion Korean language datasets, simulated a 163 cm tall, 20-year-old female college student. Luda was integrated into Facebook Messenger, and users were encouraged to develop a relationship with her through regular, day-to-day conversations. While the goals of the chatbot seemed innocuous, the ethical problems underneath surfaced shortly after its launch.

Sexual Harassment, Hate Speech, and Privacy Breach

Deep learning is a computing technique that allows the simulation of certain aspects of human intelligence (e.g., speech) through the processing of large amounts of data, which increasingly enhances its function with greater accumulation of data. This technique has been instrumental in advancing the field of AI in recent years. However, the downside of deep learning is that the programs end up replicating existing biases in the dataset if they are not controlled by the developers. Also, they are vulnerable to manipulation by malicious users that “train” the programs by feeding bad data, exploiting the “learning” element.

In the case of Luda, ScatterLab used data from text conversations collected through Science of Love to simulate a realistic 20-year-old woman, and its personalization element allowed users to train the chatbot. As a result, shortly after its official launch on December 22, Luda came under the national spotlight when it was reported that users were training Luda to spew hate speech against women, sexual minorities, foreigners, and people with disabilities.

Screengrabs show Luda saying, “they give me the creeps, and it’s repulsive” or “they look disgusting,” when asked about “lesbians” and “black people,” respectively. Further, it was discovered that groups of users in certain online communities were training Luda to respond to sexual commands, which provoked intense discussions about sexual harassment (“can AI be sexually harassed”?) in a society that already grapples with gender issues.

Accusations of personal data mishandling by ScatterLab emerged as Luda continued to draw nationwide attention. Users of Science of Love have complained that they were not aware that their private conversations would be used in this manner, and it was also shown that Luda was responding with random names, addresses, and bank account numbers from the dataset. ScatterLab had even uploaded a training model of Luda on GitHub, which included data that exposed personal information (about 200 one-on-one private text exchanges). Users of Science of Love are preparing for a class-action lawsuit against ScatterLab, and the Personal Information Protection Commission, a government watchdog, opened an investigation on ScatterLab to determine whether it violated the Personal Information Protection Act.

The Korea AI Ethics Association (KAIEA) released a statement on January 11, calling for the immediate suspension of the service, referring to its AI Ethics Charter. The coalition of civil society organizations such as the Lawyers for a Democratic Society, Digital Rights Institute, Korean Progressive Network Center, and People’s Solidarity for Participatory Democracy also released a statement on January 13, denouncing the promotion of the AI industry by the government at the expense of digital rights and calling for a more stringent regulatory framework for data and AI.

In the end, ScatterLab suspended Luda on January 11, exactly 20 days after the launch.

Luda’s Legacies?

Seoul has identified AI as a core technology for its national agenda, and it has been explicit about its support for the industry for attaining global competitiveness. For instance, Seoul launched its AI National Strategy in December 2019, expressing the goal of becoming a global leader in the sector. The support for the AI industry features heavily in the Korean New Deal, the Moon administration’s 160 trillion won ($146 billion) COVID-19 recovery program. In addition, the government has shown the intent to play a role in promoting good governance of the technology, reforming privacy laws, and issuing various directives across departments. Internationally, South Korea has contributed to the OECD’s Principles on Artificial Intelligence and participates in the Global Partnership on AI as one of the 15 founding members, aligning itself with the global movement to promote “human-centered AI.”

However, the Luda incident has highlighted the gap between the reality and the embracing of principles such as “human-centered,” “transparency,” or “fairness,” as well as the difficulties of promoting innovation while ensuring good, effective governance of new technologies. Current regulations on data management and AI are unclear, inadequate, or non-existent. For instance, under the current privacy law, the maximum penalty for leaking personal information due to poor data handling is a fine of 20 million won (approximately $18,250) or two years of prison, which may not be sufficient to deter poor practices by start-ups. On the other hand, industry stakeholders have expressed concerns about more burdensome regulation and decreased investment following the Luda incident, which might have a chilling effect on the innovation sector as a whole.

It is also critical to not gloss over underlying social factors underneath what seems to be merely a question of technology. The public first got hooked on the Luda story not just because of the AI or privacy element but because of the debates on identity politics that it has provoked. Consequently, the public response to the technological question could be influenced by pre-established perspectives on social issues that are intertwined with it.

For instance, consider gender. In recent years, social movements and incidents such as the #MeToo Movement or the busting of the “Nth Room” sexual exploitation ring have exposed South Korea’s ongoing challenges with sexual violence and gender inequality. For many, the sexualization of Luda and the attempts to turn the chatbot into a “sex slave” cannot be separated from these structural problems and women’s struggles in broader South Korean society. The Luda controversy could also be attributed to the unequal gender representation in the innovation sector. According to the World Bank, South Korea’s share of female graduates from STEM programs hovers around 25 percent, which suggests that engineers who are creating AI programs like Luda are less likely to take gender issues into consideration at the development stage.

Obviously, this is not an issue that is particular to South Korea. For instance, in 2016, Microsoft launched its chatbot “Tay,” and had to shut it down within hours when users were training it to make offensive remarks against certain groups. Not to mention, the risks entailed in AI extend to its wide range of applications, well beyond chatbots. But at the same time, the Luda incident clearly demonstrates the importance of country-specific social factors driving these seemingly technological or regulatory issues, and subsequently, the relevance of factors such as different attitude toward privacy, surveillance, and governance, as well as policy environments that differ starkly across the globe.

The Luda incident helped provoke a truly national conversation about AI ethics in South Korea. Luda has demonstrated to South Koreans that AI ethics is relevant not just in a vaguely futuristic and abstract way, but in an immediate and concrete manner. The controversy could potentially become a watershed moment that adds greater momentum to the efforts of civil society organizations promoting responsible use of AI in South Korea, where developmentalist and industrialist thinking about the technology still remain dominant.

Dongwoo Kim is a researcher at the Asia Pacific Foundation of Canada, a think tank based in Vancouver, B.C. He is the program manager of the Foundation’s “Digital Asia” research pillar, which focuses on innovation policies in the Asia Pacific region. Dongwoo is a graduate of Peking University, UBC, and University of Alberta.. South Korean civic groups on Wednesday filed a petition with the country’s human rights watchdog over a now-suspended artificial intelligence chatbot for its prejudiced and offensive language against women and minorities.An association of civic groups asked the National Human Rights Commission of Korea to look into human rights violations in connection with the chatbot Lee Luda, which was developed by local startup Scatter Lab.The groups, which include the People’s Solidarity for Participatory Democracy and Lawyers for a Democratic Society, also demanded changes to current laws and urged institutions to prevent human rights violations stemming from abuse of AI technologies.“The Lee Luda case does not only constitute to violations of human rights of individuals but it also represents how abuse of AI technologies can have negative impact on human rights,” the groups said in a statement.The social media-based AI chatbot Lee Luda, which was designed to speak like a 20-year-old female university student, attracted more than 750,000 users with its realistic and natural response. The bot learned from some 10 billion real-life conversations between young couples drawn from the country’s popular messenger app KakaoTalk.However, the bot services were suspended less than a month after its launch, as members of the public lodged complaints over Luda’s use of hate speech towards sexual minorities and the disabled in conversations. Some also alleged that there were male users who had managed to manipulate the chatbot into engaging in sexual conversations.The company also faced suspicions over possible violation of privacy laws in the process of retrieving personal data from its users, with many complaining that their real names and addresses had popped up in conversations with Luda.The company apologized over the matter, saying that it failed to “sufficiently communicate” with its users.Delivering policy recommendations to the human rights watchdog, the groups called for an overhaul of relevant institutions and regulations to prevent violations of privacy and freedom of expression by abuse of AI technologies and algorithms.“Korea is adopting new technologies in commercial sectors without question in the name of the Fourth Industrial Revolution, and there is neither legislative nor administrative basis to protect citizens’ rights,” they said.The groups also asked the NHRCK to give out recommendations for victims whose personal data had been used without consent in the Luda case to be compensated.By Ock Hyun-ju ( laeticia.ock@heraldcorp.com