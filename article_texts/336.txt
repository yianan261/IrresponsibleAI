Newlyweds could be deported if flagged as suspicious by a new Home Office algorithm that may discriminate based on nationality, the Bureau can reveal. The system, outlined in an internal government document obtained by the Public Law Project, is built on potentially biased information that also includes the age gap between partners.

The Home Office introduced an automated system to detect sham marriages in March 2015 as part of the “hostile environment” immigration policy. Its current iteration is a triage system implemented in April 2019 and was developed through machine learning, a process by which a computer algorithm finds patterns in source data and applies them to new data. The historical information used in this case came from the marriage referrals received by the Home Office over an unspecified three-year period; any biases that might exist within this source data are likely to be projected forward by the algorithm.

An equality impact assessment (EIA) conducted by the Home Office revealed a number of public law issues with the process, including the possibility of “indirect discrimination” based on age. Campaigners warn that its use of data leaves the process open to similar discrimination around nationality, echoing previous concerns about the systems used by the department in recent years.

“The Home Office data on past enforcement is likely to be biased because Home Office enforcement is biased against people of certain nationalities,” said Chai Patel, legal policy director of the Joint Council for the Welfare of Immigrants (JCWI).. The Home Office has rebuffed Public Law Project’s (PLP) latest attempt to find out more about the secret algorithmic criteria used to decide whether a proposed marriage should be investigated as a “sham”.

Sham marriage investigations can be invasive and unpleasant and it appears that they are targeted at some nationalities more than others. PLP is concerned about the lack of transparency and possible discrimination involved in the automated triage system, and we would like to make contact with people who may be affected, as well as organisations that support them. If you know anyone either being investigated or at risk of investigation please get in touch.

The sham marriage algorithm

Documents previously obtained by PLP under the Freedom of Information Act 2000 indicate that a triage system comes into play if one or both of a couple who have given notice to the registrar come from outside the European Economic Area, are not settled in the UK, or lack a valid visa. If one of these conditions is met, the couple is referred to the triage system.

An algorithm processes the couple’s data, applies secret criteria, and allocates the couple a green or red light. A red light indicates that an investigation is required to identify or rule out sham activity. The couple is asked to provide more information and, often, to attend an interview and cooperate with home visits. This can be a highly intrusive process, to which many couples may be reluctant to agree.

If they refuse, the couple will not be allowed to marry. If they do comply, immigration officers will use the new information to determine if the marriage is a sham. If the decision goes against the couple, they can still marry, but their immigration status will be at risk and one or other party may face removal from the UK.

Despite PLP’s repeated requests, the Home Office has refused to disclose the criteria used by the algorithm.

Why worry?

As we currently understand it, there are three major concerns about the system.

First, because the criteria used by the algorithm remain a secret, there are concerns about procedural fairness. The Home Office insists that publication of the criteria would be likely to prejudice its ability to investigate possible sham marriages, and would not be in the public interest. We consider that public law standards require disclosure about how the system works and refusal to publish is unlikely to be justifiable on public interest grounds. When decisions are made by an algorithm, there must be transparency and accountability.

Second, there is a concern that the algorithm may be discriminatory. This is because some nationalities – including Bulgarian, Greek, Romanian, and Albanian people – seem more likely to be targeted for investigation than others.

It may be that nationality is included in the algorithm’s criteria. If this is the case, then the algorithm may be directly discriminatory, contrary to sections 13 and 29 of the Equality Act 2010. Even if the criteria do not include nationality, the algorithm may nonetheless be indirectly discriminatory if it is having a systemic negative impact on people of a particular nationality, contrary to sections 19 and 29 of the Equality Act 2010.

A third concern is that decision-making at the investigation stage may be flawed due to “automation bias”: that is, over-reliance on automated decision support systems. If the official conducting a sham marriage investigation is aware that the couple has been given a red light by the algorithm, they may be predisposed to conclude that the relationship is a sham. In other words, the system may encourage reliance on irrelevant considerations.

PLP is keen to hear from anyone who feels they have been unfairly targeted by a sham marriage investigation, and from any organisations or practitioners working on this issue. You can contact Tatiana Kazim at t.kazim@publiclawproject.org.uk.. The Home Office is using an algorithm to determine whether a marriage should be investigated as a ‘sham’, according to documents obtained by the Public Law Project under the Freedom of Information Act.

PLP is concerned that this algorithm may be flawed and discriminatory because some nationalities seem more likely to be targeted for investigation than others.

Get in touch

Our Research Team is keen to hear from anyone who feels they have been unfairly targeted by a sham marriage investigation, and from any organisations working in this area. You can contact Jack Maxwell, one of our Research Fellows, at j.maxwell@publiclawproject.org.uk.

Background

The UK has a detailed legal framework for targeting ‘sham marriages’: where couples get married to avoid immigration law, rather than because they have a genuine relationship. As part of this framework, registrars are required to refer proposed marriages to the Home Office if either or both of the parties is subject to immigration control.

Since at least April 2019, the Home Office has used an algorithm to triage these referrals. The algorithm gives each couple a ‘Red’ or ‘Green’ rating. ‘Red’ means that the Home Office should investigate the couple ‘to rule out or identify sham activity’, while ‘Green’ means that an investigation is not warranted. Media reports indicate that sham marriage investigations can be highly invasive.

The core elements of the Home Office’s algorithm are set out below:

PLP is concerned that this algorithm may be flawed and discriminatory. In particular, the Home Office’s documents show that some nationalities, including Bulgarian, Greek, Romanian and Albanian people, have their marriages rated ‘Red’ at a much higher rate than others. The Home Office has – so far – refused to disclose all of the ‘risk factors’ used by the algorithm to rate a case. The risks of discrimination in algorithmic decision-making are well-known. In August last year, the Home Office scrapped an algorithm it used to help decide visa applications, in the face of allegations that it was racially discriminatory.

PLP’s Research Team continues to investigate this algorithm, as part of our ongoing work on tracking automated government.

Contact details

We are keen to hear from anyone who feels they have been unfairly targeted by a sham marriage investigation, and from any organisations working in this area. You can contact Jack Maxwell, one of our Research Fellows, at j.maxwell@publiclawproject.org.uk.. Newlyweds could be deported if flagged as suspicious by a new Home Office algorithm which lawyers warned could discriminate according to nationality and the age gap between partners.

A system designed to detect “sham marriages” divides couples into “red light” and “green light” categories, making an initial assessment over whether their partnerships are genuine or designed to get around visa rules.

It builds on earlier Home Office processes aimed at identifying fraudulent visa applications – but critics warned that inherent biases in those systems could affect genuine couples wanting to settle in Britain.

The Bureau of Investigative Journalism has established that an equality impact assessment (EIA) conducted by the Home Office revealed several issues with the “triage” process, including the possibility of “indirect discrimination” based on age.

And campaigners warned that its use of historic data leaves the process open to similar discrimination around nationality, echoing previous concerns about the systems used by the department as it developed a “hostile environment” regime.

The Home Office refused to release details of the eight criteria by which its computer programmes automatically flag concerns over whether marriage applications are genuine.

However, it disclosed that “red flags” are raised over the marriage applications from between 20 per cent and 25 per cent of couples where are least one partner is from Bulgaria, Greece, Romania or Albania. Those most frequently referred to the triage system include Albania, India, Pakistan and Romania.

A red light referral, which means the couple need to be investigated further, can put a person’s immigration and visa status at risk and could lead to legal action and deportation.

The system, outlined in an internal government document obtained by the Public Law Project, also takes into account the age gap between partners.

The EIAsaid: “There is potential for indirect discrimination based on age as the triage process uses the age difference between the couples.”

Nath Gbikpi, an immigration lawyer at Islington Law Centre, said. “There are quite clearly some clients whose marriage is more likely to be investigated than others.”

She added: “I wasn’t surprised to hear that there is an algorithm and I wasn’t surprised to hear that some nationalities are picked on more than others.”

Chai Patel, legal policy director of the Joint Council for the Welfare of Immigrants, said: “Until there has been a full review of all systems and data, we can assume that any system the Home Office operates is not free from bias.”

The Home Office told the Bureau: “The purpose of the EIA as part of the triage system is to ensure we are able to identify any potential risk of discrimination and assess the impact. It is then considered whether the discrimination is justifiable to achieve the aims of the process.”

A spokesperson said: “Those who abuse marriage to enter the UK illegally will feel the full force of the law and it is right that we have a system in place to prevent this.

“Our triage system is not indirectly biased, and our Equality Impact Assessment is there to detect any possible bias while ensuring we are able to investigate and disrupt sham marriages effectively.

“Legislation introduced by the Government in 2014 has helped stopped sham marriages before they happen, resulting in fewer prosecutions.”

Last year the Home Office agreed to stop using a computer algorithm to help decide visa applications following accusations that it contained “entrenched racism”.



It backed down after it emerged that applicants’ nationality were included in the automatic system. The department said it was a “streamlining” system, but campaigners claimed it fast-tracked white applicants.