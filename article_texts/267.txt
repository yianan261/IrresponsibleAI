ARTICLE TITLE: Clearview AI Algorithm Built on Photos Scraped from Social Media Profiles without Consent
Clearview AI, the facial recognition company that claims to have a database of more than 3 billion photos scraped from websites and social media, has been telling prospective law enforcement clients that a review of its software based on “methodology used by the American Civil Liberties Union” is stunningly accurate.

“The Independent Review Panel determined that Clearview rated 100% accurate, producing instant and accurate matches for every photo image in the test,” read an October 2019 report that was included as part of the company’s pitch to the North Miami Police Department. “Accuracy was consistent across all racial & demographic groups.”

But the ACLU said that claim is highly misleading and noted that Clearview's effort to mimic the methodology of its 2018 facial recognition study was a misguided attempt in “manufacturing endorsements."

“The report is absurd on many levels and further demonstrates that Clearview simply does not understand the harms of its technology in law enforcement hands,” ACLU Northern California attorney Jacob Snow told BuzzFeed News, which obtained the document through a public records request.

Clearview’s announcement that its technology has been vetted using ACLU guidelines is the latest questionable marketing claim made by the Manhattan-based startup, which has amassed a vast repository of biometric data by scraping photos from social media platforms, including Facebook, Instagram, Twitter, YouTube, and LinkedIn. Among those claims, Clearview AI has told prospective clients that its technology was instrumental in several arrests in New York, including one of an individual involved in a Brooklyn bar beating and another of a suspect who allegedly planted fake bombs in a New York City subway station. The NYPD denied using Clearview’s technology in both of these cases.

Got a tip? Email one of the reporters of this story at caroline.haskins@buzzfeed.com or ryan.mac@buzzfeed.com, or contact us here.

Clearview, which claims to be working with more than 600 law enforcement agencies, has also been sued and publicly denounced by critics including New Jersey Attorney General Gurbir Grewal, who ordered a moratorium on the state’s use of the technology after the company included his image without permission in a promotional video. As of last week, Facebook, YouTube, LinkedIn, and PayPal had all sent cease-and-desist letters to Clearview in an attempt to stop it from using images taken from their sites.



Clearview CEO Hoan Ton-That, however, has remained defiant, arguing in a CBS interview last Wednesday that his company has a First Amendment right to scrape public photos from social media. The company also defended the test in statements to BuzzFeed News.

“The ACLU is a highly-respected institution that conducted its own widely distributed test of facial recognition for accuracy across demographic groups,” Ton-That told BuzzFeed News. “We appreciated the ACLU’s efforts to highlight the potential for demographic bias in AI, which is why we applied their test and methodology to our own technology.”. The internet was designed to make information free and easy for anyone to access. But as the amount of personal information online has grown, so too have the risks. Last weekend, a nightmare scenario for many privacy advocates arrived. The New York Times revealed Clearview AI, a secretive surveillance company, was selling a facial recognition tool to law enforcement powered by “three billion images” culled from the open web. Cops have long had access to similar technology, but what makes Clearview different is where it obtained its data. The company scraped pictures from millions of public sites including Facebook, YouTube, and Venmo, according to the Times.

To use the tool, cops simply upload an image of a suspect, and Clearview spits back photos of them and links to where they were posted. The company has made it easy to instantly connect a person to their online footprint—the very capability many people have long feared someone would possess. (Clearview’s claims should be taken with a grain of salt; a Buzzfeed News investigation found its marketing materials appear to contain exaggerations and lies. The company did not immediately return a request for comment.)

Like almost any tool, scraping can be used for noble or nefarious purposes. Without it, we wouldn’t have the Internet Archive’s invaluable WayBack Machine, for instance. But it’s also how Stanford researchers a few years ago built a widely condemned “gaydar,” an algorithm they claimed could detect a person’s sexuality by looking at their face. “It’s a fundamental thing that we rely on every day, a lot of people without realizing, because it’s going on behind the scenes,” says Jamie Lee Williams, a staff attorney at the Electronic Frontier Foundation on the civil liberties team. The EFF and other digital rights groups have often argued the benefits of scraping outweigh the harms.

LEARN MORE The WIRED Guide to Personal Data

Automated scraping violates the policies of sites like Facebook and Twitter, the latter of which specifically prohibits scraping to build facial recognition databases. Twitter sent a letter to Clearview this week asking it to stop pilfering data from the site “for any reason,” and Facebook is also reportedly examining the matter, according to the Times. But it’s unclear whether they have any legal recourse in the current system.

To fight back against scraping, companies have often used the Computer Fraud and Abuse Act, claiming the practice amounts to accessing a computer without proper authorization. Last year, however, the Ninth Circuit Court of Appeals ruled that automated scraping doesn’t violate the CFAA. In that case, LinkedIn sued and lost against a company called HiQ, which scraped public LinkedIn profiles in bulk and combined them with other information into a database for employers. The EFF and other groups heralded the ruling as a victory, because it limited the scope of the CFAA—which they argue has frequently been abused by companies—and helped protect researchers who break terms of service agreements in the name of freedom of information.

The CFFA is one of few options available to companies who want to stop scrapers, which is part of the problem. “It’s a 1986, pre-internet statute,” says WIlliams. “If that’s the best we can do to protect our privacy with these very complicated, very modern problems, then I think we’re screwed.”

Civil liberties groups and technology companies both have been calling for a federal law that would establish Americans’ right to privacy in the digital era. Clearview, and companies like it, make the matter that much more urgent. “We need a comprehensive privacy statute that covers biometric data,” says Williams.