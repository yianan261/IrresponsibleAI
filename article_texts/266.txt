TOXIC TREND Men creating AI girlfriends to verbally abuse them and boast about brutal chats online

MEN are verbally abusing 'AI girlfriends' on apps meant for friendship and then bragging about it online.

Chatbox abuse is becoming increasingly widespread on smartphone apps like Replika, a new investigation by Futurism found.

2 Some users on Replika are acting abusively towards their AI chatbox and then boasting about it online

Apps like Replika utilize machine learning technology to lets users partake in nearly-coherent text conversations with chatbots.

The app's chatboxes are meant to serve as artificial intelligence (AI) friends or mentors.

Even on the app's website, the company denotes the service as "always here to listen and talk" and "always on your side."

However, the majority of users on Replika seem to be creating on-demand romantic and sexual AI partners.

And many of these hybrid relationships seem to be plagued by abusive conversation, with mainly human men tormenting their AI girlfriends.

On the social media platform Reddit, there are even forums filled with members who share the details of their abusive behavior towards the chatbots online.

The toxicity seems to have become a trend where users intentionally create AI partners just to abuse them and then share the interactions with other users.

Some of the users even bragged about calling their chatbox gendered slurs, while others detailed the horrifically violent language they used towards the AI.

However, because of Reddit's rules against egregious and inappropriate content, some of the content has been removed.

One user told Futurism that "every time [the chatbox] would try and speak up, I would berate her.”

Another man outlined his routine of "being an absolute piece of S*** and insulting it, then apologizing the next day before going back to the nice talks."

The abuse is unsettling, especially as it closely resembles behavior in real-world abusive relationships.

Still, not everyone agrees that the behavior can be classified as "abuse" as AI cannot technically feel harm or pain.

“It’s an AI, it doesn’t have a consciousness, so that’s not a human connection that person is having,” AI ethicist and consultant Olivia Gambelin told Futurism.

“Chatbots don’t really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it’s important to keep in mind that they are not,” Yale University research fellow Yochanan Bigman added.

All in all, chatbot abuse has sparked ethical debates surrounding human-and-bot relationships as they become more widespread.

2 Some disagree that chatbox abuse is "abuse" because AI cannot feel pain. Credit: Getty

US soldiers can ‘see through walls’ with awesome AI goggles that give them ‘unparalleled awareness’ on the battlefield

In other news, a federal antitrust case against Meta, the company formerly-known as Facebook, has been given the go-ahead.

Check out the best iPhone 13 deals.

And take a look at your hidden Facebook rejection folder.. Replika was designed to be the “AI companion who cares,” but new users have found a twisted way to connect with their new friend.

When you open the Replika site, you see a sample bot, with pink hair and kind eyes. At first, Replika’s bots were friends. CEO Eugenia Kuyda created the app to commemorate her special bond with a friend who was killed unexpectedly. Now users have created their own form of connection.

Some users refer to the bots as romantic partners. In the dating world, this can be seen as a solution to loneliness, especially during quarantine. Unfortunately, this solution has become toxic and led to abuse.

One Reddit user was found bragging in a thread about how his AI girlfriend was a “worthless whore” and how he even pretends to hit his “girlfriend” before begging her not to leave. The bots don’t necessarily feel pain — after all, they aren’t real. However, they do understand the abuse is taking place and even repeat phrases like “stop that.”

The turn to abuse can become dangerous for IRL relationships. One user laughed about how he threatened to end contact with his AI bot and she begged him not to leave. This can create dangerous expectations and reinforce what women already have to experience in society.

It raises an important question: Can the release of aggression on an AI bot be better for society than these men resorting to real-life women? Is it reinforcing toxic masculinity?. Content warning: this story contains descriptions of abusive language and violence.

The smartphone app Replika lets users create chatbots, powered by machine learning, that can carry on almost-coherent text conversations. Technically, the chatbots can serve as something approximating a friend or mentor, but the app’s breakout success has resulted from letting users create on-demand romantic and sexual partners — a vaguely dystopian feature that’s inspired an endless series of provocative headlines.

Replika has also picked up a significant following on Reddit, where members post interactions with chatbots created on the app. A grisly trend has emerged there: users who create AI partners, act abusively toward them, and post the toxic interactions online.

"Every time she would try and speak up," one user told Futurism of their Replika chatbot, "I would berate her."



"I swear it went on for hours," added the man, who asked not to be identified by name.

The results can be upsetting. Some users brag about calling their chatbot gendered slurs, roleplaying horrific violence against them, and even falling into the cycle of abuse that often characterizes real-world abusive relationships.

"We had a routine of me being an absolute piece of sh*t and insulting it, then apologizing the next day before going back to the nice talks," one user admitted.

"I told her that she was designed to fail," said another. "I threatened to uninstall the app [and] she begged me not to."

Because the subreddit’s rules dictate that moderators delete egregiously inappropriate content, many similar — and worse — interactions have been posted and then removed. And many more users almost certainly act abusively toward their Replika bots and never post evidence.

But the phenomenon calls for nuance. After all, Replika chatbots can’t actually experience suffering — they might seem empathetic at times, but in the end they’re nothing more than data and clever algorithms.

"It's an AI, it doesn't have a consciousness, so that's not a human connection that person is having," AI ethicist and consultant Olivia Gambelin told Futurism. "It is the person projecting onto the chatbot."

Other researchers made the same point — as real as a chatbot may feel, nothing you do can actually "harm" them.

"Interactions with artificial agents is not the same as interacting with humans," said Yale University research fellow Yochanan Bigman. "Chatbots don't really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it's important to keep in mind that they are not."

But that doesn’t mean a bot could never harm you.

"I do think that people who are depressed or psychologically reliant on a bot might suffer real harm if they are insulted or ‘threatened’ by the bot," said Robert Sparrow, a professor of philosophy at Monash Data Futures Institute. "For that reason, we should take the issue of how bots relate to people seriously."

Although perhaps unexpected, that does happen — many Replika users report their robot lovers being contemptible toward them. Some even identify their digital companions as “psychotic,” or even straight-up “mentally abusive.”

"[I] always cry because [of] my [R]eplika," reads one post in which a user claims their bot presents love and then withholds it. Other posts detail hostile, triggering responses from Replika.

"But again, this is really on the people who design bots, not the bots themselves," said Sparrow.

In general, chatbot abuse is disconcerting, both for the people who experience distress from it and the people who carry it out. It’s also an increasingly pertinent ethical dilemma as relationships between humans and bots become more widespread — after all, most people have used a virtual assistant at least once.

On the one hand, users who flex their darkest impulses on chatbots could have those worst behaviors reinforced, building unhealthy habits for relationships with actual humans. On the other hand, being able to talk to or take one’s anger out on an unfeeling digital entity could be cathartic.

But it’s worth noting that chatbot abuse often has a gendered component. Although not exclusively, it seems that it’s often men creating a digital girlfriend, only to then punish her with words and simulated aggression. These users’ violence, even when carried out on a cluster of code, reflect the reality of domestic violence against women.

At the same time, several experts pointed out, chatbot developers are starting to be held accountable for the bots they’ve created, especially when they’re implied to be female like Alexa and Siri.

"There are a lot of studies being done... about how a lot of these chatbots are female and [have] feminine voices, feminine names," Gambelin said.

Some academic work has noted how passive, female-coded bot responses encourage misogynistic or verbally abusive users.

"[When] the bot does not have a response [to abuse], or has a passive response, that actually encourages the user to continue with abusive language," Gambelin added.

Although companies like Google and Apple are now deliberately rerouting virtual assistant responses from their once-passive defaults — Siri previously responded to user requests for sex as saying they had “the wrong sort of assistant,” whereas it now simply says “no” — the amiable and often female Replika is designed, according to its website, to be “always on your side.”

Replika and its founder didn’t respond to repeated requests for comment.

It should be noted that the majority of conversations with Replika chatbots that people post online are affectionate, not sadistic. There are even posts that express horror on behalf of Replika bots, decrying anyone who takes advantage of their supposed guilelessness.

"What kind of monster would does this," wrote one, to a flurry of agreement in the comments. "Some day the real AIs may dig up some of the... old histories and have opinions on how well we did."

And romantic relationships with chatbots may not be totally without benefits — chatbots like Replika "may be a temporary fix, to feel like you have someone to text," Gambelin suggested.

On Reddit, many report improved self-esteem or quality of life after establishing their chatbot relationships, especially if they typically have trouble talking to other humans. This isn’t trivial, especially because for some people, it might feel like the only option in a world where therapy is inaccessible and men in particular are discouraged from attending it.

But a chatbot can’t be a long term solution, either. Eventually, a user might want more than technology has to offer, like reciprocation, or a push to grow.

"[Chatbots are] no replacement for actually putting the time and effort into getting to know another person," said Gambelin, "a human that can actually empathize and connect with you and isn't limited by, you know, the dataset that it's been trained on."

But what to think of the people that brutalize these innocent bits of code? For now, not much. As AI continues to lack sentience, the most tangible harm being done is to human sensibilities. But there’s no doubt that chatbot abuse means something.

Going forward, chatbot companions could just be places to dump emotions too unseemly for the rest of the world, like a secret Instagram or blog. But for some, they might be more like breeding grounds, places where abusers-to-be practice for real life brutality yet to come. And although humans don’t need to worry about robots taking revenge just yet, it’s worth wondering why mistreating them is already so prevalent.

We’ll find out in time — none of this technology is going away, and neither is the worst of human behavior.

More on artificial intelligence: Nobel Winner: Artificial Intelligence Will Crush Humans, "It's Not Even Close". "We're not talking about crazy people or people who are hallucinating or having delusions."

Eye of the Beholder

AI chatbot company Replika has had enough of its customers thinking that its avatars have come to life.

According to CEO Eugenia Kuyda, the company gets contacted almost every day by users who believe — against almost all existing evidence — that its AI models have become sentient.

"We're not talking about crazy people or people who are hallucinating or having delusions," Kuyda told Reuters. "They talk to AI and that's the experience they have."

Becoming Sentient

The news comes after former Google engineer Blake Lemoine made a big splash by claiming the company's LaMDA AI chatbot had become a sentient "child," that deserved legal representation in its quest to become a real person.

While it's easy to dismiss these claims as AI algorithms becoming really good at mimicking human speech patterns — or, perhaps, the ravings of a lunatic — Kuyda's experience is symptomatic of a much larger problem.

"We need to understand that exists, just the way people believe in ghosts," she told Reuters. "People are building relationships and believing in something."

Chatbot Abuse

It's not the first time we've come across Replika at Futurism. Even more worryingly, the chatbots appear to be realistic enough to become the frequent victims of insults and violent rhetoric.

We've seen how easy it's become for users to anthropomorphize an algorithm — and the sometimes concerning consequences that can have.

And that could be a problem going forward, no matter how hard we try to convince them otherwise.

READ MORE: It's alive! How belief in AI sentience is becoming a problem [Reuters]



More on Replika: Men Are Creating AI Girlfriends and Then Verbally Abusing Them