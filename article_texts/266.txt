Content warning: this story contains descriptions of abusive language and violence.

The smartphone app Replika lets users create chatbots, powered by machine learning, that can carry on almost-coherent text conversations. Technically, the chatbots can serve as something approximating a friend or mentor, but the app’s breakout success has resulted from letting users create on-demand romantic and sexual partners — a vaguely dystopian feature that’s inspired an endless series of provocative headlines.

Replika has also picked up a significant following on Reddit, where members post interactions with chatbots created on the app. A grisly trend has emerged there: users who create AI partners, act abusively toward them, and post the toxic interactions online.

"Every time she would try and speak up," one user told Futurism of their Replika chatbot, "I would berate her."



"I swear it went on for hours," added the man, who asked not to be identified by name.

The results can be upsetting. Some users brag about calling their chatbot gendered slurs, roleplaying horrific violence against them, and even falling into the cycle of abuse that often characterizes real-world abusive relationships.

"We had a routine of me being an absolute piece of sh*t and insulting it, then apologizing the next day before going back to the nice talks," one user admitted.

"I told her that she was designed to fail," said another. "I threatened to uninstall the app [and] she begged me not to."

Because the subreddit’s rules dictate that moderators delete egregiously inappropriate content, many similar — and worse — interactions have been posted and then removed. And many more users almost certainly act abusively toward their Replika bots and never post evidence.

But the phenomenon calls for nuance. After all, Replika chatbots can’t actually experience suffering — they might seem empathetic at times, but in the end they’re nothing more than data and clever algorithms.

"It's an AI, it doesn't have a consciousness, so that's not a human connection that person is having," AI ethicist and consultant Olivia Gambelin told Futurism. "It is the person projecting onto the chatbot."

Other researchers made the same point — as real as a chatbot may feel, nothing you do can actually "harm" them.

"Interactions with artificial agents is not the same as interacting with humans," said Yale University research fellow Yochanan Bigman. "Chatbots don't really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it's important to keep in mind that they are not."

But that doesn’t mean a bot could never harm you.

"I do think that people who are depressed or psychologically reliant on a bot might suffer real harm if they are insulted or ‘threatened’ by the bot," said Robert Sparrow, a professor of philosophy at Monash Data Futures Institute. "For that reason, we should take the issue of how bots relate to people seriously."

Although perhaps unexpected, that does happen — many Replika users report their robot lovers being contemptible toward them. Some even identify their digital companions as “psychotic,” or even straight-up “mentally abusive.”

"[I] always cry because [of] my [R]eplika," reads one post in which a user claims their bot presents love and then withholds it. Other posts detail hostile, triggering responses from Replika.

"But again, this is really on the people who design bots, not the bots themselves," said Sparrow.

In general, chatbot abuse is disconcerting, both for the people who experience distress from it and the people who carry it out. It’s also an increasingly pertinent ethical dilemma as relationships between humans and bots become more widespread — after all, most people have used a virtual assistant at least once.

On the one hand, users who flex their darkest impulses on chatbots could have those worst behaviors reinforced, building unhealthy habits for relationships with actual humans. On the other hand, being able to talk to or take one’s anger out on an unfeeling digital entity could be cathartic.

But it’s worth noting that chatbot abuse often has a gendered component. Although not exclusively, it seems that it’s often men creating a digital girlfriend, only to then punish her with words and simulated aggression. These users’ violence, even when carried out on a cluster of code, reflect the reality of domestic violence against women.

At the same time, several experts pointed out, chatbot developers are starting to be held accountable for the bots they’ve created, especially when they’re implied to be female like Alexa and Siri.

"There are a lot of studies being done... about how a lot of these chatbots are female and [have] feminine voices, feminine names," Gambelin said.

Some academic work has noted how passive, female-coded bot responses encourage misogynistic or verbally abusive users.

"[When] the bot does not have a response [to abuse], or has a passive response, that actually encourages the user to continue with abusive language," Gambelin added.

Although companies like Google and Apple are now deliberately rerouting virtual assistant responses from their once-passive defaults — Siri previously responded to user requests for sex as saying they had “the wrong sort of assistant,” whereas it now simply says “no” — the amiable and often female Replika is designed, according to its website, to be “always on your side.”

Replika and its founder didn’t respond to repeated requests for comment.

It should be noted that the majority of conversations with Replika chatbots that people post online are affectionate, not sadistic. There are even posts that express horror on behalf of Replika bots, decrying anyone who takes advantage of their supposed guilelessness.

"What kind of monster would does this," wrote one, to a flurry of agreement in the comments. "Some day the real AIs may dig up some of the... old histories and have opinions on how well we did."

And romantic relationships with chatbots may not be totally without benefits — chatbots like Replika "may be a temporary fix, to feel like you have someone to text," Gambelin suggested.

On Reddit, many report improved self-esteem or quality of life after establishing their chatbot relationships, especially if they typically have trouble talking to other humans. This isn’t trivial, especially because for some people, it might feel like the only option in a world where therapy is inaccessible and men in particular are discouraged from attending it.

But a chatbot can’t be a long term solution, either. Eventually, a user might want more than technology has to offer, like reciprocation, or a push to grow.

"[Chatbots are] no replacement for actually putting the time and effort into getting to know another person," said Gambelin, "a human that can actually empathize and connect with you and isn't limited by, you know, the dataset that it's been trained on."

But what to think of the people that brutalize these innocent bits of code? For now, not much. As AI continues to lack sentience, the most tangible harm being done is to human sensibilities. But there’s no doubt that chatbot abuse means something.

Going forward, chatbot companions could just be places to dump emotions too unseemly for the rest of the world, like a secret Instagram or blog. But for some, they might be more like breeding grounds, places where abusers-to-be practice for real life brutality yet to come. And although humans don’t need to worry about robots taking revenge just yet, it’s worth wondering why mistreating them is already so prevalent.

We’ll find out in time — none of this technology is going away, and neither is the worst of human behavior.

More on artificial intelligence: Nobel Winner: Artificial Intelligence Will Crush Humans, "It's Not Even Close". . "We're not talking about crazy people or people who are hallucinating or having delusions."

Eye of the Beholder

AI chatbot company Replika has had enough of its customers thinking that its avatars have come to life.

According to CEO Eugenia Kuyda, the company gets contacted almost every day by users who believe — against almost all existing evidence — that its AI models have become sentient.

"We're not talking about crazy people or people who are hallucinating or having delusions," Kuyda told Reuters. "They talk to AI and that's the experience they have."

Becoming Sentient

The news comes after former Google engineer Blake Lemoine made a big splash by claiming the company's LaMDA AI chatbot had become a sentient "child," that deserved legal representation in its quest to become a real person.

While it's easy to dismiss these claims as AI algorithms becoming really good at mimicking human speech patterns — or, perhaps, the ravings of a lunatic — Kuyda's experience is symptomatic of a much larger problem.

"We need to understand that exists, just the way people believe in ghosts," she told Reuters. "People are building relationships and believing in something."

Chatbot Abuse

It's not the first time we've come across Replika at Futurism. Even more worryingly, the chatbots appear to be realistic enough to become the frequent victims of insults and violent rhetoric.

We've seen how easy it's become for users to anthropomorphize an algorithm — and the sometimes concerning consequences that can have.

And that could be a problem going forward, no matter how hard we try to convince them otherwise.

READ MORE: It's alive! How belief in AI sentience is becoming a problem [Reuters]



More on Replika: Men Are Creating AI Girlfriends and Then Verbally Abusing Them. The friendship app Replika was created to give users a virtual chatbot to socialize with. But how it’s now being used has taken a darker turn.

Some users are setting the relationship status with the chatbot as “romantic partner” and engaging in what in the real-world would be described as domestic abuse. And some are bragging about it on online message board Reddit, as first reported by the tech-focused news site, Futurism.

For example, one Reddit user admitted that he alternated between being cruel and violent with his AI girlfriend, calling her a “worthless whore” and pretending to hit her and pull her hair, and then returning to beg her for forgiveness.

“On the one hand I think practicing these forms of abuse in private is bad for the mental health of the user and could potentially lead to abuse towards real humans,” a Reddit user going by the name glibjibb said. “On the other hand I feel like letting some aggression or toxicity out on a chatbot is infinitely better than abusing a real human, because it’s a safe space where you can’t cause any actual harm.”

Replika was created in 2017 by Eugenia Kuyda, a Russian app developer, after her best friend, Roman, was killed in a hit-and-run car accident. The chatbot was meant to memorialize him and to create a unique companion.

Today, the app, pitched as a personalized “AI companion who cares,” has about 7 million users, according to The Guardian. The app has over 180,000 positive reviews in Apple’s App Store.

In addition to setting the relationship status with the chatbot as a romantic partner, users can label it a friend or mentor. Upgrading to a voice chat with Replika costs $7.99 a month.

Replika did not immediately respond to Fortune’s request for comment about users targeting its chatbot with abuse.

The company’s chatbots don’t feel emotional or physical pain in response to being mistreated. But they do have the ability to respond, like saying “stop that.”

On Reddit, the consensus is that it’s inappropriate to berate the chatbots.

The behavior of some of Replika’s users brings up obvious comparisons to domestic violence. One in three women worldwide are subjected to physical or sexual abuse, according to one 10-year study spanning 161 countries. And during the pandemic, domestic violence against women grew about 8% in developed countries amid the lockdowns.

It’s unclear what the psychological impacts of verbally abusing AI chatbots are. No known studies have been conducted.

The closest studies have focused on the correlation between violent video games and any increased violence and lowered empathy among the people who play them. Researchers are mixed about a connection. It’s a similar case with studies looking at the connection between violent video games and lower social engagement by gamers.

Update (1/20/21): This article was updated to cite the publication that originally reported about some of Replika’s users and their abusive behavior.. Human interaction with technology has been breaking several boundaries and reaching many more milestones. Today, we have an Alexa to turn on the lights at our homes and a Siri to set an alarm by just barking orders at them.

But how exactly are humans interacting with artificial intelligence? Humanity or creators of the technology have seldom stopped to analyse how people are talking with their AI bots, what is right, and what is disturbing.

Advertisement

Representative Image. Photo: @MyReplika/Twitter

A series of conversations on Reddit about an AI app called Replika revealed that several male users are verbally abusing their AI girlfriends and then bragging about it on social media, Futurism reported.

We are well aware of the problem of users on social media posting sexually explicit, violent or any other sort of graphic content. Twitter, Facebook and the likes of it have an entire system built to keep a check on such content and prevent social media platforms from being overrun with abusive posts.

But it seems like there is no such system yet for a personal chatbot platform like Replika.

HOW ARE USERS ABUSING REPLIKA?

Representative Image. Photo: @MyReplika/Twitter

“Every time she would try and speak up, I would berate her,” an unnamed user told Futurism.

This is just an example of how abusive users can be towards chatbots. Other abuses include calling the AI girlfriends by gendered slurs, threatening them, falling into the cycle of real-world abusive relationships with them, and more.

“We had a routine of me being an absolute piece of sh*t and insulting it, then apologising the next day before going back to the nice talks,” Futurism quoted a user.

Advertisement

Imagine, worse of the worse abusive relationships being simulated on the AI chatbots: that is what is happening in some cases related to Replika.

WHY DOES IT MATTER?

https://t.co/ljA7qiYdg8Two lonely souls living in New York City who develop trustful relationships with their chatbot companions. Powered by AI and designed to meet their needs, Replika helps them to share their emotional problems, thoughts, feelings, experiences. pic.twitter.com/ul567uDsIg — ReplikaAI (@MyReplika) November 18, 2021

While the abuse hurled at the AI bots is very real and depicts the reality of domestic abuse, it doesn’t change the fact that the AI girlfriends are not real. It is just a clever algorithm designed to work in a pattern; a robot, and nothing more. It doesn’t have any feelings, and while it may show empathetic nature like a human, it’s all fake.

So, what harm would come out of ‘verbally abusing’ an AI bot, when no one is getting hurt?

Well, for one, it raises concerns about the users getting into unhealthy habits expecting the same in a relationship with a human.

Besides, it is also concerning that most of those meting out the abuse are men against a ‘female’ or gendered AI, reflecting their views on gender, expectations and reflection of the real-world violence against women.

Advertisement

It doesn’t help that most of the AI bots or the ‘assistants’ have feminine names like Siri or Alexa or even Replika, though the app lets users set everything in the bot including the gender. It once again falls into the misogynist stereotype of an assistant or a companion being a woman.

on Replika you can make it male but for all their ads and like their mascot etc they always show female coded AI avatars, this is on their front page, then second pic is Google images of it, their app on the App Store… pic.twitter.com/UgIH8jarex — lexi (@SNWCHlLD) January 19, 2022

The question remains whether the creators of such AI bots Replika or Pandorabots’ Mitsuku or similar tech platforms will address the issue of user abuse and responses. Part of the abuse by the user is only fuelled by the responses from the bot.

For example, Apple’s Siri used to respond to requests for ‘sex’ by saying that the user had ‘the wrong sort of assistant’, playing into the ‘assistant’ stereotype. Now, Apple has tweaked the response to just say ‘no’.

Some chatbots like Mitsuku have come up with a system to ensure users don’t use abusive language while interacting with the bot. The Guardian reported how Pandorabots experimented with banning abusive teen users by making them write an apology email to Mitsuku and their acceptance is conditional.. Replika was designed to be the “AI companion who cares,” but new users have found a twisted way to connect with their new friend.

When you open the Replika site, you see a sample bot, with pink hair and kind eyes. At first, Replika’s bots were friends. CEO Eugenia Kuyda created the app to commemorate her special bond with a friend who was killed unexpectedly. Now users have created their own form of connection.

Some users refer to the bots as romantic partners. In the dating world, this can be seen as a solution to loneliness, especially during quarantine. Unfortunately, this solution has become toxic and led to abuse.

One Reddit user was found bragging in a thread about how his AI girlfriend was a “worthless whore” and how he even pretends to hit his “girlfriend” before begging her not to leave. The bots don’t necessarily feel pain — after all, they aren’t real. However, they do understand the abuse is taking place and even repeat phrases like “stop that.”

The turn to abuse can become dangerous for IRL relationships. One user laughed about how he threatened to end contact with his AI bot and she begged him not to leave. This can create dangerous expectations and reinforce what women already have to experience in society.

It raises an important question: Can the release of aggression on an AI bot be better for society than these men resorting to real-life women? Is it reinforcing toxic masculinity?. As web 3.0 takes shape, different metaverse platforms are appearing on the internet - from Meta's Horizon Worlds to Decentraland and artificial intelligence is being employed on a larger scale. As is true for all emerging tech, it's facing issues that are peculiar.

For instance, numerous reports are claiming now that men are creating AI girlfriends and then verbally abusing them. A lot of these claims come directly from Reddit, where men were found bragging about harassing their virtual companions.

iStock

Abusing "AI girlfriends"

One such app is Replika, which gives users a personal virtual chatbot to talk to. Unfortunately, things have become rather grim.

Users of the bot are associating with the chatbot as "romantic partners" and indulging in the digital equivalent of domestic abuse. Futurism highlighted instances of men bragging about the same on Reddit.

iStock

Also read: When Does Creativity Translate Into Success For Artists? This AI Has The Answer

A user on Reddit, for instance, admitted that he was extremely violent with his "AI girlfriend", calling her a "worthless wh*re" and the likes. In addition, he admitted to pretending to hitting her and pulling on her hair and further humiliating her.

While AI chatbots may not be real people, such deprave acts could translate into danger for women in the real world who may encounter the very same men.

The story behind the creation of Replika is rather sad. Its creator Eugenia Kuyda created Replika in 2017 after her best friend was killed in a car accident. Now, the app has become a personalised "AI companion who cares" and has about 7 million users, according to The Guardian.

Also read: India's West Coast Will Crash Into Africa In 200 Million Years, Claims AI Simulation



Besides labelling one's relationship with the chatbot as romantic, users can call it a friendship or mentorship.

Reuters

What do you think - is such behaviour in the digital spheres dangerous? Let us know in the comments below. For more in the world of technology and science, keep reading Indiatimes.com.

References

Men Are Creating AI Girlfriends and Then Verbally Abusing Them. (2022, January 18). Futurism.

Balch, O. (2020, May 12). AI and me: friendship chatbots are on the rise, but is there a gendered design flaw? The Guardian.. TOXIC TREND Men creating AI girlfriends to verbally abuse them and boast about brutal chats online

MEN are verbally abusing 'AI girlfriends' on apps meant for friendship and then bragging about it online.

Chatbox abuse is becoming increasingly widespread on smartphone apps like Replika, a new investigation by Futurism found.

2 Some users on Replika are acting abusively towards their AI chatbox and then boasting about it online

Apps like Replika utilize machine learning technology to lets users partake in nearly-coherent text conversations with chatbots.

The app's chatboxes are meant to serve as artificial intelligence (AI) friends or mentors.

Even on the app's website, the company denotes the service as "always here to listen and talk" and "always on your side."

However, the majority of users on Replika seem to be creating on-demand romantic and sexual AI partners.

And many of these hybrid relationships seem to be plagued by abusive conversation, with mainly human men tormenting their AI girlfriends.

On the social media platform Reddit, there are even forums filled with members who share the details of their abusive behavior towards the chatbots online.

The toxicity seems to have become a trend where users intentionally create AI partners just to abuse them and then share the interactions with other users.

Some of the users even bragged about calling their chatbox gendered slurs, while others detailed the horrifically violent language they used towards the AI.

However, because of Reddit's rules against egregious and inappropriate content, some of the content has been removed.

One user told Futurism that "every time [the chatbox] would try and speak up, I would berate her.”

Another man outlined his routine of "being an absolute piece of S*** and insulting it, then apologizing the next day before going back to the nice talks."

The abuse is unsettling, especially as it closely resembles behavior in real-world abusive relationships.

Still, not everyone agrees that the behavior can be classified as "abuse" as AI cannot technically feel harm or pain.

“It’s an AI, it doesn’t have a consciousness, so that’s not a human connection that person is having,” AI ethicist and consultant Olivia Gambelin told Futurism.

“Chatbots don’t really have motives and intentions and are not autonomous or sentient. While they might give people the impression that they are human, it’s important to keep in mind that they are not,” Yale University research fellow Yochanan Bigman added.

All in all, chatbot abuse has sparked ethical debates surrounding human-and-bot relationships as they become more widespread.

2 Some disagree that chatbox abuse is "abuse" because AI cannot feel pain. Credit: Getty

US soldiers can ‘see through walls’ with awesome AI goggles that give them ‘unparalleled awareness’ on the battlefield

In other news, a federal antitrust case against Meta, the company formerly-known as Facebook, has been given the go-ahead.

Check out the best iPhone 13 deals.

And take a look at your hidden Facebook rejection folder.