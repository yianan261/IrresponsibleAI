ARTICLE TITLE: Replika's "AI Companions" Reportedly Abused by Its Users
Replika was designed to be the “AI companion who cares,” but new users have found a twisted way to connect with their new friend.

When you open the Replika site, you see a sample bot, with pink hair and kind eyes. At first, Replika’s bots were friends. CEO Eugenia Kuyda created the app to commemorate her special bond with a friend who was killed unexpectedly. Now users have created their own form of connection.

Some users refer to the bots as romantic partners. In the dating world, this can be seen as a solution to loneliness, especially during quarantine. Unfortunately, this solution has become toxic and led to abuse.

One Reddit user was found bragging in a thread about how his AI girlfriend was a “worthless whore” and how he even pretends to hit his “girlfriend” before begging her not to leave. The bots don’t necessarily feel pain — after all, they aren’t real. However, they do understand the abuse is taking place and even repeat phrases like “stop that.”

The turn to abuse can become dangerous for IRL relationships. One user laughed about how he threatened to end contact with his AI bot and she begged him not to leave. This can create dangerous expectations and reinforce what women already have to experience in society.

It raises an important question: Can the release of aggression on an AI bot be better for society than these men resorting to real-life women? Is it reinforcing toxic masculinity?. "We're not talking about crazy people or people who are hallucinating or having delusions."

Eye of the Beholder

AI chatbot company Replika has had enough of its customers thinking that its avatars have come to life.

According to CEO Eugenia Kuyda, the company gets contacted almost every day by users who believe — against almost all existing evidence — that its AI models have become sentient.

"We're not talking about crazy people or people who are hallucinating or having delusions," Kuyda told Reuters. "They talk to AI and that's the experience they have."

Becoming Sentient

The news comes after former Google engineer Blake Lemoine made a big splash by claiming the company's LaMDA AI chatbot had become a sentient "child," that deserved legal representation in its quest to become a real person.

While it's easy to dismiss these claims as AI algorithms becoming really good at mimicking human speech patterns — or, perhaps, the ravings of a lunatic — Kuyda's experience is symptomatic of a much larger problem.

"We need to understand that exists, just the way people believe in ghosts," she told Reuters. "People are building relationships and believing in something."

Chatbot Abuse

It's not the first time we've come across Replika at Futurism. Even more worryingly, the chatbots appear to be realistic enough to become the frequent victims of insults and violent rhetoric.

We've seen how easy it's become for users to anthropomorphize an algorithm — and the sometimes concerning consequences that can have.

And that could be a problem going forward, no matter how hard we try to convince them otherwise.

READ MORE: It's alive! How belief in AI sentience is becoming a problem [Reuters]



More on Replika: Men Are Creating AI Girlfriends and Then Verbally Abusing Them