AI is rarely out of the news, and last week's clickbait in Australia was about doctoring a photograph of Victorian politician Georgie Purcell.

In the nightly news broadcast by Channel 9 for a story on duck hunting, a photograph of the young blonde Animal Rights Party MP was edited to enlarge her breasts and change her clothing, putting her in a mid-riff top that exposed her stomach.

Amid the outrage that followed, the bosses at Channel 9 blamed the AI robots and denied responsibility.

"Our graphics department sourced an online image of Georgie to use in our story on duck hunting," said Nine's news director, Hugh Nailon.

"As is common practice, the image was resized to fit our specs. During that process, the automation by Photoshop created an image that was not consistent with the original. This did not meet the high editorial standards we have, and for that, we apologize to Ms. Purcell unreservedly."

No humans were involved in changing the image, according to Nine, but this was hotly denied by the software provider Adobe.

“Any changes to this image would have required human intervention and approval,” an Adobe spokesperson said, opening up a war of words with the network.

The comments from Adobe are consistent with the message coming from the industry.

Microsoft's AI design assistant is called 'Copilot' because the company says that without the human as the pilot, the copilot—the AI—can’t do its job.

In the aftermath of the affair, it would be accurate to say that Nine’s claims that the robots were responsible were not widely believed.

Embellishing a photograph this way seems strange for an AI program unless it was already programmed to do so by a human.

Contentious applications

Along with AI-created deepfakes of people like Taylor Swift and Donald Trump, the media, entertainment, and fashion industries are wrestling with the ethical supply chain surrounding the creation of images.

AI-powered image generation software from Canva is becoming ubiquitous, and its applications can be contentious.

“We’ve got this discourse that AI is big and scary, and the robots are coming. No, they are not. It's not big and scary. It's already here."

For example, young and upcoming fashion labels are creating images for their designs from AI software because they lack the funds for photo shoots with real human models.

While it allows the labels to scale up and get their product out through media channels, there are grumbling accusations from the modeling industry, which can see itself as an early casualty of AI, the classic case of human jobs being replaced.

Over-regulation

While all the finger-pointing was going on about doctoring the Georgie Purcell image, Australia's Productivity Commission had its own—and possibly more meaningful—contribution to the debate.

While many public sector regulators worldwide are calling for restrictions on AI, the Productivity Commission's view is that AI should not be overly regulated because to do so would limit its applications and the benefits it can deliver.

Urging the Government not to implement "unnecessary and confusing" new regulations, the Commission says that, where possible, existing regulatory frameworks should be sufficient to deal with the advent of AI.

Commissioner Stephen King said that new rules should only be introduced "if the current rules and regulations are clearly not fit for purpose."

“Nothing is worse than passing technology-specific regulation and finding it’s obsolete within five years,” Mr King said.

“We’ve got this discourse that AI is big and scary, and the robots are coming. No, they are not. It’s not big and scary. It’s already here.”

Ethical transgression

At first glance, this might not seem to sit well with the policies of the current Australian Government, which is considering what it calls “mandatory safeguards” for AI systems in high-risk areas.

Delve deeper, though, and this is the other side of what the Productivity Commission says is a significant impediment to AI adoption, and that is developing higher levels of public trust.

This brings us back to the image doctoring in the Nine News broadcast. Regardless of who was responsible, the outcome was a clear ethical transgression, which can only undermine trust in AI, and the buck stops with the humans irrespective of what the news director said.

Another pertinent comment in the Productivity Commission report was about data. While advocating "technology neutral" regulations around AI, the report shows that the Government is responsible for establishing "clear and functional mechanisms for data collection, curation, sharing, and use."

Productivity gains, it said, were dependent on raising data quality without “undermining the incentives of data holders or increasing risks to ­individuals”.

Through all the noise about AI regulation, the Productivity Commission does suggest a way forward that doesn't hobble AI and bog it down in new regulatory frameworks.

If we get the data piece right, over-regulation of AI applications and new regulations won't be so necessary.

The technology can then be applied and changed without reference to regulations, which might soon be outdated and unsuitable.

And while we’re at it, let’s stop blaming the robots.

We've created them, so we have to take responsibility for them. Shirking responsibility is a sure way to undermine trust and retard the productivity gains which AI can deliver if only humans can get it right.

Image credit: iStockphoto/BrianAJackson. . Her white sleeveless dress has been transformed into a halter top.

An Australian television news channel apologised "unreservedly" Tuesday for altering a photo of a state lawmaker, who complained it gave her "enlarged boobs" and a more revealing dress.

Georgie Purcell, upper house member of the Victorian state parliament, posted side-by-side the original photo and the version edited by 9News Melbourne, part of Nine Network Australia.

In the edited image, which was broadcast on Monday evening, her white sleeveless dress has been transformed into a halter top and skirt, exposing her midriff.

A translucent light grey square transposed over part of the photo seems to accentuate the MP's chest.

"I endured a lot yesterday. But having my body and outfit photoshopped by a media outlet was not on my bingo card," Purcell said in a post on X, formerly Twitter.

I endured a lot yesterday.



But having my body and outfit photoshopped by a media outlet was not on my bingo card.



Note the enlarged boobs and outfit to be made more revealing.



Can't imagine this happening to a male MP.



What gives? pic.twitter.com/NhnkDRMidc — Georgie Purcell (@georgievpurcell) January 29, 2024

"Note the enlarged boobs and outfit to be made more revealing. Can't imagine this happening to a male MP. What gives?"

9News Melbourne director Hugh Nailon said the channel's graphics department had sourced an online photo of the MP for use in a story on duck hunting.

Purcell is the Animal Justice Party MP for Northern Victoria.

- 'Graphic error' -

"As is common practice, the image was resized to fit our specs," Nailon said.

"During that process, the automation by Photoshop created an image that was not consistent with the original," he added, referring to US-based Adobe's photo-editing software.

"This did not meet the high editorial standards we have and for that we apologise to Ms Purcell unreservedly," Nailon said, describing it as a "graphic error".

Adobe disagreed with the explanation.

"Any changes to this image would have required human intervention and approval," an Adobe spokesperson said in a statement to Australian media.

Purcell said she did not believe the same mistake would have happened with a photo of Victoria's state premier, Jacinta Allan.

"I imagine that if AI spat out a picture of Jacinta Allan in a crop top they would have noticed that but they don't with me," she told public broadcaster ABC.

The MP said the incident had an impact on her, and "could affect other women even more, and it should never happen again".

"These are things that would never happen to our male colleagues, ever."

Victoria's premier also criticised the incident.

"That's no way to present any woman let alone a woman who holds a position in public office, represents a community and is in the public discourse every single day," Allan told reporters.

"Let's think about the message that sends particularly to young women."

(Except for the headline, this story has not been edited by NDTV staff and is published from a syndicated feed.). By T.J. Thomson - Feb 06, 2024 10:15 am AEDT

Channel Nine published an altered image of Victorian MP Georgie Purcell that showed her in a midriff-exposing tank top. The outfit was actually a dress. How can news media use AI responsibly?

Purcell chastised the channel for the image manipulation and accused it of being sexist. Nine apologised for the edit and blamed it on an artificial intelligence (AI) tool in Adobe Photoshop. Generative AI has become increasingly prevalent over the past six months, as popular image editing and design tools like Photoshop and Canva have started integrating AI features into their programs. But what are they capable of, exactly? Can they be blamed for doctored images? As these tools become more widespread, learning more about them and their dangers – alongside opportunities – is increasingly important. What happened with the photo of Purcell? Typically, making AI-generated or AI-augmented images involves “prompting” – using text commands to describe what you want to see or edit. But late last year, Photoshop unveiled a new feature, generative fill. Among its options is an “expand” tool that can add content to images, even without text prompts. For example, to expand an image beyond its original borders, a user can simply extend the canvas and Photoshop will “imagine” content that could go beyond the frame. This ability is powered by Firefly, Adobe’s own generative AI tool.

I endured a lot yesterday. But having my body and outfit photoshopped by a media outlet was not on my bingo card. Note the enlarged boobs and outfit to be made more revealing. Can’t imagine this happening to a male MP. What gives? pic.twitter.com/NhnkDRMidc — Georgie Purcell (@georgievpurcell) January 29, 2024

Nine resized the image to better fit its television composition but, in doing so, also generated new parts of the image that weren’t there originally. The source material – and if it’s cropped – are of critical importance here. In the above example where the frame of the photo stops around Purcell’s hips, Photoshop just extends the dress as might be expected. But if you use generative expand with a more tightly cropped or composed photo, Photoshop has to “imagine” more of what is going on in the image, with variable results. Is it legal to alter someone’s image like this? It’s ultimately up to the courts to decide. It depends on the jurisdiction and, among other aspects, the risk of reputational harm. If a party can argue that publication of an altered image has caused or could cause them “serious harm”, they might have a defamation case.

How else is generative AI being used? Generative fill is just one way news organisations are using AI. Some are also using it to make or publish images, including photorealistic ones, depicting current events. An example of this is the ongoing Israel-Hamas conflict. Others use it in place of stock photography or to create illustrations for hard-to-visualise topics, like AI itself. Many adhere to institutional or industry-wide codes of conduct, such as the Journalist Code of Ethics from the Media, Entertainment & Arts Alliance of Australia. This states journalists should “present pictures and sound which are true and accurate” and disclose “any manipulation likely to mislead.” Some outlets do not use AI-generated or augmented images at all, or only when reporting on such images if they go viral. Newsrooms can also benefit from generative AI tools. An example includes uploading a spreadsheet to a service like ChatGPT-4 and receiving suggestions on how to visualise the data. Or using it to help create a three-dimensional model that illustrates how a process works or how an event unfolded. What safeguards should media have for responsible generative AI use? I’ve spent the last year interviewing photo editors and people in related roles about how they use generative AI and what policies they have in place to do so safely. I’ve learned that some media outlets bar their staff from using AI to generate any content. Others allow it only for non-realistic illustrations, such as using AI to create a Bitcoin symbol or illustrate a story about finance. News outlets, according to editors I spoke to, want to be transparent with their audiences about the content they create and how it is edited. In 2019, Adobe started the Content Authenticity Initiative, which now includes major media organisations, image libraries and multimedia companies. This has led to the rollout of content credentials, a digital history of what equipment was used to make an image and what edits have been done to it. This has been touted as a way to be more transparent with AI-generated or augmented content. However, content credentials are not widely used yet. Besides, audiences shouldn’t outsource their critical thinking to a third party. In addition to transparency, news editors I spoke to were sensitive to AI potentially displacing human labour. Many outlets strive to use only AI generators that have been trained with proprietary content. This is because of the ongoing cases in jurisdictions around the world over AI training data and whether resulting generations breach copyright. Lastly, news editors said they are aware of the potential for bias in AI generations, given the unrepresentative data AI models are trained on. This year, the World Economic Forum has named AI-fuelled misinformation and disinformation as the world’s greatest short-term risk. It placed this above even disasters like extreme weather events, inflation and armed conflict.

The top ten risks as outlined in the World Economic Forum’s Global Risk Report 2024. World Economic Forum, Global Risks Perception Survey 2023–2024

Because of this risk and the elections happening in the United States and around the world this year, engaging in healthy scepticism about what you see online is a must. As is being thoughtful about where you get your news and information from. Doing so makes you better equipped to participate in a democracy, and less likely to fall for scams.