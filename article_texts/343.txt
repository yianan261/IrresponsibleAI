It wasn’t exactly surprising that hordes of social media trolls viciously attacked three Black players on England’s soccer team with racist comments and emojis after a historic loss on Sunday, July 11. What was unexpected was how quickly even more social media users came to the defense of the players.

“I’ve been working in anti-racism for over 20 years, and I am surprised at how huge and widespread and how swift the anti-racist response was,” said Sabby Dhalu, who works at the UK nonprofit Stand Up to Racism.

Tens of thousands of commenters directly combated the racist attacks by posting positive messages on the personal Facebook and Instagram pages of the three players: Marcus Rashford, Jadon Sancho, and Bukayo Saka. By Monday morning in California, supportive comments started to outnumber the negative ones on players’ recent Instagram photos.

Fans trying to counteract the hate speech on Facebook and Instagram urged fellow supporters to report offensive comments to company moderators. At the same time, they expressed frustration with these companies for their sluggish moderation of these attacks.

While Twitter and Facebook eventually took down most of the blatantly racist comments, the onus fell to everyday users on the platforms to react quickly and shut down the toxic discourse. The incident showed how regular users on social media are increasingly stepping up when social media companies don’t do enough to stop the spread of hate speech on their platforms.

Facebook, for example, does not proactively moderate a common type of racist attack, one that was being used aggressively on Rashford, Sancho, and Saka’s accounts: comments full of monkey and banana peel emojis. Instead, Facebook relies on users to report these kinds of comments, a company spokesperson told Recode. Once users report them, Facebook’s content moderators may take the comments down if the emojis are being used inappropriately. The company also encourages users experiencing harassment to defend themselves by turning on a “hidden words” feature that can block designated words or emojis in comments on their posts.

Many soccer fans reported these types of racist posts to the social media company where the posts appeared, and also tried to overpower the posts by sharing positive messages of their own.

One viral tweet said, “They’re colour blind when you’re winning, but can only see colour when you lose. Proud of you, Bukayo Saka, Marcus Rashford and Jadon Sancho. Still we rise.”

The 15 most shared tweets containing the three players’ names as of Monday afternoon were similarly all supportive, according to data provided to Recode by social media research organization First Draft News. The hashtag “#saynotoracism” started trending on Twitter in the UK soon after the game. A supportive message about one of the players, Rashford, saying, “Our hero, always. There is so much love for you, Marcus Rashford,” was one of the most shared links posted by a verified account on Facebook on Tuesday within the previous 24 hours, according to Facebook-owned analytics tool Crowdtangle.

But it takes only a relatively small group of users to successfully harass someone on social media.

Twitter said it took down 1,000 posts in the 24 hours since the game. Facebook, which owns Instagram, declined to say how many posts the company removed, but said in a statement that it “quickly removed” an unspecified number of comments and accounts.

Still, Facebook’s and Twitter’s responses also fell short in the eyes of many British politicians and public figures, including leaders of England’s Conservative Party such as Prime Minister Boris Johnson, who sharply criticized these companies for the vitriol on their platforms. In the past, Johnson and other members of the Conservative Party have been criticized for not supporting England’s football players when they decided to take a knee to protest racial discrimination. But this time, politicians, media outlets, and public figures across the political spectrum in England were unified in their condemnation.

“I share the anger at appalling racist abuse of our heroic players. Social media companies need to up their game in addressing it, and, if they fail to, our new Online Safety Bill will hold them to account with fines of up to 10 percent of global revenue,” Oliver Dowden, England’s Secretary of State for Digital, Culture, Media and Sport, tweeted on Monday.

Social media companies have long struggled to police the flow of hate speech and misinformation on their platforms.

In this case, Twitter said it used a “combination of machine learning-based automation and human review” to identify racist comments toward players, and that it “proactively” flagged a majority of this content with its technology.

Facebook said the company “quickly removed comments and accounts directing abuse at England’s footballers” and that it will “continue to take action against those that break our rules.”

“No one should have to experience racist abuse anywhere, and we don’t want it on Instagram and Facebook,” read part of a statement sent by Facebook. “No one thing will fix this challenge overnight, but we’re committed to keeping our community safe from abuse.”

But Facebook is still relying on its users to identify much of this content.

“I think it’s too much to be asking someone to make a complaint, every single time,” said Dhalu. “Companies need to be setting up a filter to prevent this. It’s pretty shocking that they’re not.”

And while it would have helped deter abuse against these specific players if the platforms had chosen to proactively monitor and delete racist comments on their accounts, the issue of harassment and hate speech on social media is widespread. These companies need to enforce the rules better across the board, said Sunder Katwala, director of the UK-based multiculturalism think tank British Future.

“I think the more fundamental point is: What is allowed and what isn’t allowed?” asked Katwala. “The [social media companies] are saying, ‘racist behavior has no place on our platform,’ but you’ve got all kinds of racist comments.”

The English soccer team situation shows that social media companies still have a long way to go before they’re actually backing up their stated commitment to barring racism on their platforms. In the meantime, it seems that do-gooder users are counterbalancing the hate. Depending on how you look at it, that’s either a hopeful sign that shows everyday people are stepping up to combat racism — or it’s a disappointing indication that powerful social media companies still aren’t doing what’s necessary to stop hate speech before it spreads on their platforms.. After another major incident of racist abuse online, targeted at members of the English football team, Facebook has provided a new overview of how it's working to address such attacks, and stop people from experiencing race-based abuse across its platforms.

Following England's loss in the European Championship final on Sunday, social media trolls posted hundreds of racist remarks, attacking the three Black players on England’s soccer team, which included the use of emojis as an abuse element. Which Instagram's systems didn't initially pick up a concern - but now, on review, Facebook has updated its systems, and teams, to ensure that it addresses similar incidents moving forward, while it also looks to improve its processes overall.

As explained by Facebook:

"We are appalled at the abhorrent racist abuse some members of England’s football team experienced after the Euro 2020 final last weekend. This is an incredibly serious issue that we’ve been working on for years, which has included working directly with football organizations and law enforcement."

Indeed, even this year, Facebook has been faced with serious challenges on this front, again tied to UK football fans.

Back in February, Instagram unwittingly became the source of various incidents of race-based attacks against players from Manchester United, Chelsea and Liverpool, among others, who were targeted via Instagram Direct. Manchester United, in a joint statement with Everton, Liverpool and Manchester City, condemned the incidents, and called on Instagram's parent company Facebook to do more to protect users from such, which lead to Instagram implementing tougher penalties for those found to be sending abuse via DM, and a new option for personal accounts to switch off DMs from people that they don’t follow.

Clearly, however, there are still more issues to addess on this. And while racism is a societal issue, and not confined to social platforms as such, Facebook needs to ensure that it doesn't provide any amplification for the same, in order to do its part to reduce its impacts.

Facebook clearly states that:

"We don’t allow attacks on people based on their protected characteristics, which include race, religion, nationality or sexual orientation. If we’re made aware of any words or emojis being used to attack people based on their race, we remove them because they violate our policies. We publish our hate speech policies in our Community Standards and Instagram’s Community Guidelines."

The problem in this latest incident, as noted, was that the use of emojis as a racist marker wasn't initially identified, which Instagram chief Adam Mosseri has acknowledged.

We have technology to try and prioritize reports, and we were mistakenly marking some of these as benign comments, which they are absolutely not. The issue has since been addressed, and the publication has all of this context. — Adam Mosseri ???? (@mosseri) July 14, 2021

Facebook explains that it identifies hate speech by using a combination of artificial intelligence and human review.

"AI helps us prioritize reports for our reviewers and take automated actions where appropriate. Between January and March of 2021, we removed more than 25 million pieces of hate speech content from Facebook - nearly 97% before someone reported it to us. And on Instagram, we took action on 6.3 million pieces of content, 93% before someone reported it to us."

Which is a good result rate, but as this latest incident shows, there are still times that these systems will not be able to catch everything. Which, as Mosseri further notes, is also a challenge of scale.

Because of our scale. We handle millions or reports a day. If we make a mistake on one percent of them, that’s tens of thousands of mistakes. We need to, and will continue to do, better, but there will always be some mistakes. — Adam Mosseri ???? (@mosseri) July 15, 2021

Realistically, there's no way to entirely stamp such out, but Facebook's working to ensure it updates its systems in real-time, as cases are detected, in order to work faster to combat abuse and limit exposure.

"People have been rightly frustrated when they’ve reported posts and have been told incorrectly that hateful comments with certain emojis don’t break our rules. That’s because our AI didn’t understand the context - and that’s a mistake. We’ve moved quickly to correct this through recent improvements to our technology. We will continue to work on this so we can remove violating emojis from our platform quicker."

It's a very difficult balance, and no one has all the answers. Various regulatory groups, for example, have proposed tougher penalties for social platforms that fail to address such in a timely manner. The problem is, what's considered 'timely' will be different on almost a case-by-case basis.

Should Instagram have picked the misuse of emojis up sooner? Yes - but its automated systems didn't identify this as a problem. Because it wasn't, till it was, and no one could have necessarily anticipated such until it was too late.

One possible way to address this could be to impose tougher ID requirements on individuals when they sign up for social media accounts, which would make it easier to identify offenders. If there's a threat of real-world legal recourse for your online actions, that could act as enough of a threat to make users reconsider their actions.

But Facebook says this also isn't necessarily the way forward:

"There are risks with ID verification, primarily the exclusion of groups - particularly disadvantaged groups - who don’t have easy access to official forms of identification. The most recent modeling from the Electoral Commission estimates that 11 million people in the UK do not have a driving license or passport, and that this group were more likely to be from disadvantaged backgrounds."

Which is a valid point, and once again provides some more scope as to the challenge before it, and all online platforms, in this respect.

It's a tough, but important challenge, and one that Facebook is taking very seriously. Ideally, a solution can be found that addresses these key issues, but realistically, there's always going to be a level of misuse, no matter what measures are implemented.

And again, at Facebook's scale, even a small margin for error can mean large impact.

Hopefully, as automated systems evolve, more tools can be enacted to maintain a safe environment for all.