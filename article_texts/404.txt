ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up for ProPublica’s Big Story newsletter to receive stories like this one in your inbox as soon as they are published.

Ariella Russcol specializes in drama at the Frank Sinatra School of the Arts in Queens, New York, and the senior’s performance on this April afternoon didn’t disappoint. While the library is normally the quietest room in the school, her ear-piercing screams sounded more like a horror movie than study hall. But they weren’t enough to set off a small microphone in the ceiling that was supposed to detect aggression.

A few days later, at the Staples Pathways Academy in Westport, Connecticut, junior Sami D’Anna inadvertently triggered the same device with a less spooky sound — a coughing fit from a lingering chest cold. As she hacked and rasped, a message popped up on its web interface: “StressedVoice detected.”

“There we go,” D’Anna said with amusement, looking at the screen. “There’s my coughs.”

The students were helping ProPublica test an aggression detector that’s used in hundreds of schools, health care facilities, banks, stores and prisons worldwide, including more than 100 in the U.S. Sound Intelligence, the Dutch company that makes the software for the device, plans to open an office this year in Chicago, where its chief executive will be based.

California-based Louroe Electronics, which has loaded the software on its microphones since 2015, advertises the devices in school safety magazines and at law enforcement conventions, and it said it has between 100 and 1,000 customers for them. Louroe’s marketing materials say the detection software enables security officers to “engage antagonistic individuals immediately, resolving the conflict before it turns into physical violence.”

Students at the Frank Sinatra School of the Arts in Queens, New York, test an aggression detector by acting out a scripted scene for a ProPublica reporter.

In the wake of the shooting at a Parkland, Florida, high school and other massacres, U.S. schools are increasingly receptive to such pitches. Congress approved more than $25 million for school security improvements last year, and one analyst expects new technology could augment the $2.7 billion market for education security products. Besides Sound Intelligence, South Korea-based Hanwha Techwin, formerly part of Samsung, makes a similar “scream detection” product that’s been installed in American schools. U.K.-based Audio Analytic used to sell its aggression- and gunshot-detection software to customers in Europe and the U.S., including Cisco Systems Inc.’s professional security division. However, an Audio Analytic spokesman told ProPublica that it has since changed its business model and stopped selling the aggression detector software.

By deploying surveillance technology in public spaces like hallways and cafeterias, device makers and school officials hope to anticipate and prevent everything from mass shootings to underage smoking. Sound Intelligence also markets add-on packages to recognize the sounds of gunshots, car alarms and broken glass, while Hauppauge, New York-based Soter Technologies develops sensors that determine if students are vaping in the school bathroom. The Lockport school district in upstate New York is planning a facial-recognition system to identify intruders on campus.

Yet ProPublica’s analysis, as well as the experiences of some U.S. schools and hospitals that have used Sound Intelligence’s aggression detector, suggest that it can be less than reliable. At the heart of the device is what the company calls a machine learning algorithm. Our research found that it tends to equate aggression with rough, strained noises in a relatively high pitch, like D’Anna’s coughing. A 1994 YouTube clip of abrasive-sounding comedian Gilbert Gottfried (“Is it hot in here or am I crazy?”) set off the detector, which analyzes sound but doesn’t take words or meaning into account. Although a Louroe spokesman said the detector doesn’t intrude on student privacy because it only captures sound patterns deemed aggressive, its microphones allow administrators to record, replay and store those snippets of conversation indefinitely.

“It’s not clear it’s solving the right problem. And it’s not clear it’s solving it with the right tools,” said Suresh Venkatasubramanian, a University of Utah computer science professor who studies how replacing humans with artificial intelligence affects decision-making in society.

Some experts also dispute the underlying premise that verbal aggression precedes school violence, since they say mass shooters like Nikolas Cruz at Marjory Stoneman Douglas High School in Parkland, Florida, tend to be quiet beforehand. “I can’t imagine when it would be useful, honestly,” said Jillian Peterson, an assistant professor of criminology and criminal justice at Hamline University in St. Paul, Minnesota.

Dr. Nancy Rappaport, an associate professor of psychiatry at Harvard Medical School who studies school safety, said the audio surveillance could have the unintended consequence of increasing student distrust and alienation. She added that schools are opting for inexpensive technological fixes over solutions that get to the root of the problem, such as more counseling for troubled kids. One Louroe microphone with aggression software sells for about $1,000.

Sound Intelligence CEO Derek van der Vorst said security cameras made by Sweden-based Axis Communications account for 90% of the detector’s worldwide sales, with privately held Louroe making up the other 10%. He said the Axis cameras, which can cost tens of thousands of dollars including back-end systems, contain more recent, sophisticated versions of the software than the Louroe equipment does.

The Louroe spokesman said its devices receive regular software updates. Axis did not respond to ProPublica’s requests for comment.

Van der Vorst acknowledged that the detector is imperfect and confirmed our finding that it registers rougher tones as aggressive. He said he “guarantees 100%” that the system will at times misconstrue innocent behavior. But he’s more concerned about failing to catch indicators of violence, and he said the system gives schools and other facilities a much-needed early warning system. It “enables them to act much faster when there’s a potential violent situation,” he said, citing a hospital in Fort Myers, Florida, where the detector alerted security to unruly visitors. An official at the hospital said the software has spotted aggressive behavior before staff could push a panic button, giving security officers a head start on defusing incidents before they escalate.

Sign up to get ProPublica’s investigations delivered to your inbox. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.

Asked whether his algorithms could prevent a mass shooting, van der Vorst said: “I wouldn’t claim that we could prevent a crazy loony from shooting people.”

Sound Intelligence developed its aggression detector within the last two decades. It tested an early model in a Dutch “pub district,” according to a 2007 study co-authored by a company researcher. Microphones were placed in 11 locations in inner-city Groningen, and the detector’s findings were compared with police reports of aggressive behavior. The results were “so impressive,” the study reported, that the device was considered “indispensable” by several Dutch police departments, the Dutch railway company and two prisons.

Since then, the software has grown more complex, improving its ability to identify aggressive voices, van der Vorst said. Sound Intelligence engineers said the latest version was calibrated using audio collected in part from European customers, including some recordings of screaming kids. Asked if any of the training data came from schools, van der Vorst didn’t respond directly.

Venkatasubramanian said that calibrating an algorithm in one context and then using it in another can build “layers and layers of problems.” He has called for algorithms, particularly those used in public safety situations, to be audited for transparency and bias. Other critics have similarly assailed some policing algorithms that were designed to predict earthquakes but now are used to foresee crime hotspots.

Researchers have also found that implementing algorithms in the real world can go astray because of incomplete or biased training data or incorrect framing of the problem. For example, an algorithm used to predict criminal recidivism made errors that disproportionately punished black defendants.
ProPublica purchased a Louroe microphone and set it up in line with guidance provided by Sound Intelligence. Reporters then observed the aggression detector’s response to noises made by high school seniors as they played games in the library (at Sinatra) or a common room (at Staples), and in small adjoining rooms in both schools where they screamed on cue and read aloud comic strips in which characters vented frustration, fear and anger. Of 55 instances in which the Sinatra students screamed, 22 set off the detector.

Pictionary games in the library of the Frank Sinatra School of the Arts triggered the aggression detector.

A student draws a Pictionary clue for our aggression detector experiment.

Van der Vorst questioned some of ProPublica’s findings, like the missed screams, because of a phenomenon known as “clipping.” That’s when a microphone becomes overwhelmed by too much noise, distorting the sound and potentially throwing off the algorithm’s readings. Clipping can happen when loud sounds are recorded in a small room, so ProPublica retested the students in a larger space using the same prompts. Many screams, including Russcol’s, again failed to trigger an alarm — indicating that clipping had not made a significant difference in our results.

During our first round of testing, when pizzas were delivered for lunch in the Sinatra library, the cheering triggered the detector. So did each round of Pictionary as students shouted guesses — ”A fireman!” “Lucifer!” — until the artist revealed the correct answer (Burning Man, the festival in remote Nevada). Laughter sometimes set it off, especially raucous guffaws that the detector apparently mistook for belligerent shouts.

See How Often the Students Triggered the Aggression Detector When Pizza Arrives While Playing Pictionary (The Answer: "High Tide, Low Tide") (Lucas Waldron / ProPublica)

Such findings aren’t surprising, said Shae Morgan, an assistant professor and audiology expert at the University of Louisville’s medical school. “Happy or elated speech shares many of the same signatures as hot anger,” he said. By contrast, “cold anger” — quiet, detached fury, often expressed without the markers of voice strain — wouldn’t be picked up, though it can presage school violence, he said. At a police chiefs’ conference last year in Orlando, Florida, a Louroe representative showed the microphone to a ProPublica reporter and remarked that it could prevent the next school shooting; a company spokesman later clarified that it wasn’t designed to discover quiet killers.

Nevertheless, with every mass shooting, the demand for aggression detectors and similar devices is likely to grow. Wallace, who installed the detector at Pinecrest, said he hopes to install it elsewhere in southern Nevada — including in hospitals, bus stops and other public areas.

“It’s always after an event that something happens before we talk about solutions,” said Shirey, the Pinecrest principal. “But why not get in front of it? We have to adapt to the world as it is.”

Do you have access to information about algorithms that should be public? Email [email protected]. Here’s how to send tips and documents to ProPublica securely.. We first tested the device in simulated situations to measure its performance in real-world scenarios and collected spontaneous and simulated vocalizations from high school students. We then analyzed the types of sounds that the algorithm found to be aggressive and determined, for those sounds, some common audio characteristics. We view this analysis as an initial exploration of the algorithm, using sound it would likely encounter in operation, rather than a definitive evaluation.

This companion article to our main story describes the testing and data analysis ProPublica conducted for the Sound Intelligence aggression detection algorithm on the Louroe Digifact A microphone. Here, we discuss the data and methodology used for our research, as well as the results of our testing and analysis. Those results raise concerns about the device, particularly for the school environments for which it is marketed and sold.

While Sound Intelligence markets its algorithm as detecting aggression, the algorithm actually also seeks to flag instances of vocal distress and strain (e.g., when triggered, the algorithm’s webpage displays the warning “StressedVoice detected”). For the purposes of this document, we term all such triggering vocalizations as “aggressive.”

The device also has threshold settings that are used to fine-tune the algorithm in operation. However, the qualities of sounds that trigger the device are determined in training and set once it is installed on the device. In other words, while fine-tuning changes the device’s behavior, it does not change the types of sounds that the device correlates with inferences of aggression. Lowering the device’s sensitivity merely increases the confidence and time thresholds that must be met in order to raise an alarm. In practice, this may increase the risk of false negatives — cases where the algorithm should trigger but doesn’t. The same type of Sound Intelligence algorithm is used in all of Louroe’s Digifact A microphones, regardless of the environment (e.g., hospitals, schools, prisons, etc.) in which they are installed.

To train the system, Sound Intelligence labeled audio frames of aggressive and non-aggressive events and used those labeled frames as training data for a machine learning classification algorithm. Once trained, the classification algorithm generates a score ranging from 0.0 to 1.0 for each frame based on its audio features. This score represents an overall confidence for identifying aggression — from 0% to 100%. In operation, a confidence exceeding a set threshold over a long enough period of time results in a prediction of aggression by the device.

Dozens of times per second, the software converts the audio signal received by the microphone into audio features. Each set of audio features can be considered a frame of sound and is used to predict whether that segment of the sound input is aggressive. According to our research and interviews with Sound Intelligence, sound volume is not a feature used by the algorithm because it is directly related to the sound’s distance to the microphone, which should not be a factor in determining aggressiveness.

According to our research, testing and interviews with Sound Intelligence, the Louroe aggression detector includes: 1) a microphone, 2) a sound-processing component that extracts sound features from raw audio input, 3) a machine-learning algorithm that uses those features to predict verbal aggression and 4) a thresholding component that contains settings for the algorithm.

Our testing aimed to simulate the real-world operating environment for the device as closely as possible. We purchased a Louroe Digifact A microphone and licensed the aggression detection algorithm. We then rewired the device so that, instead of monitoring the surrounding environment, we were also able to input sound directly into the device from any audio recording. This allowed us either to monitor the device’s aggression measurement in real time or test recorded audio clips. Recorded audio was played into the device to reproducibly measure the predicted aggression. We designed a protocol for testing the device's performance on student voices with the assistance of ProPublica data adviser Dr. Heather Lynch.

The high number of false positives combined with false negatives suggests that the device is often unable to differentiate reliably between actual instances of aggression and benign vocalizations. The tendency for the algorithm to mischaracterize events may also lead to fine-tuning that reduces the device’s sensitivity to a level where otherwise aggressive incidents would be overlooked.

Our testing was designed to understand the performance of the algorithm in the field. Given the wide variation among individuals’ voices and the ways that emotional states are vocalized, our testing is neither exhaustive or comprehensive. However, these results do identify apparent weaknesses in the algorithm. We found many instances where the device appeared to make errors, whether by predicting aggression where there was none (false positives) or by failing to trigger when it should have (false negatives).

We also found 27 instances where normal speech falsely triggered the algorithm. In particular, there were two female students whose voices regularly triggered the device while laughing, singing or speaking.

In our testing data, we found many aggressive sound clips where the device did not respond. We considered such cases to be false negatives. In particular, the device tended to ignore high-pitched screaming.

During pair testing, the students simulated aggressive and fearful screaming or shouting and were also asked to sing. However, they did not simulate laughter or coughing — those were spontaneous vocalizations recorded during testing. We recognize there may be differences between simulated and actual aggressive shouts and screams, and the aggression detector should only raise an alarm for genuine vocalizations. To account for this, we annotated whether sound clips of screaming were sufficiently aggressive.

During our testing with student pairs, we found some instances of laughter, some singing and one student’s coughs triggered the device. There were also many instances of screaming or shouting that did not trigger the device, particularly screams that were high-pitched or that did not contain the markers of voice distortion.

A school setting contains a wide variety of environments and social contexts in which the device is expected to work. While we do not consider this false alarm rate as representative of the frequency of false positives in operation, our group testing results show that the algorithm generates frequent false alarms in a common school scenario.

Since we did not observe any aggression during the group testing, we considered any triggers false positives — inferences of aggressive behavior where there was none. We tabulated the number of such false positives below, organized by the vocalization that triggered the alarm.

The algorithm triggered frequently during the approximately two hours of group recording, during which we observed no actual or simulated aggression. A number of different vocalizations triggered the algorithm; including cheering, loud laughter and students shouting out answers. This suggests the algorithm is unable to distinguish between the vocal characteristics associated with enthusiasm and exuberance as opposed to genuine fear or aggression.

In total, we recorded more than three hours of sound with the Louroe microphones at the two schools.

During this pair recording, some of the vocalizations, screaming in particular, were too loud for the Verifact A and distorted its recording — a phenomenon known as “clipping.” This is a potential problem common to any microphone where the sound is too loud or too close to the microphone. To account for this issue, we retested a number of students at a later date in an environment where recorded audio was less likely to be clipped. We then disregarded any pair recordings having substantial clipping.

At each school, we installed the device in the ceiling and recorded students in a common area while they played games such as Pictionary. We also tested and recorded pairs of students in a smaller side room where a device was installed in the ceiling. During this pair testing, the students role-played situations of fear, frustration and anger from comic strips. They also sang unrehearsed songs and attempted to scream in an aggressive manner.


We took a closer look at the clips recorded during the pair testing to learn more about what tends to trigger the algorithm. We examined sound frames in comparison to the algorithm’s measurements of aggression. Similar to the device, we then calculated features for each sound frame. We aggregated the sound features to understand, at a high level, the characteristics of sound considered aggressive by the algorithm.

Analyzing a Frame of Sound To analyze sound, we start with the individual sound frames that comprise an audio clip. A sound frame can be represented by its raw audio signal (a wave representing sound amplitude over the sound frame) or by calculations on that signal. A representation commonly used in audio analysis is the frequency spectrum — obtained by calculating the raw sound frame’s amplitude at every frequency. Any sound frame can be fully represented in the time domain (the audio signal) or in the frequency domain (the spectrum) and transformed between the two. Representing Sound in the Frequency Domain A fourier transform converts a signal between its time-domain representation (red) and its frequency domain representation (blue). The spectra of individual sound frames can be combined over time to produce a visual representation called a spectrogram. A spectrogram plots the frequency components of sound frames over time, which can reveal many details about the sound, such as pitch and tone quality. We looked at the spectrograms for a number of different vocalizations to understand what kinds of sound tend to trigger the algorithm. The x-axis is the time of the sound recording, and the y-axis is the frequency component. A brighter area indicates that the magnitude of the frequency component at that time is higher. Below is a spectrogram of speech for two students as they introduce themselves: student A at 0.1 seconds and student B at 1.1 seconds. These are examples of normal speech having a relatively clear tone and little auditory distortion. The algorithm gave this speech a low aggression score, and it did not trigger an alarm. Normal speech, male student A, female student B (classified as non-aggressive) There are bright and distinct striations visible in the lower frequency portion (bottom) of the spectrogram. These represent the loudest frequency components of the students’ speech. The bottommost (lowest frequency) band for each speaker is the fundamental frequency, or the perceived pitch of a voice. Successive bright bands at regular intervals above the fundamental represent the harmonics of the speech. Wider intervals between those bands indicate a higher pitch, and we can see that student B’s voice is higher-pitched. The more distinct these bands, the clearer the speech will sound (although many factors affect speech clarity). There is also a fairly clear drop-off in brightness (sound intensity) in the higher frequency components of the spectrogram for both utterances. This characteristic is representative of speech sounds. This is a spectrogram of a simulated shout from student A. Simulated shout, male student A (classified as non-aggressive) The harmonic bands have greater separation here, which indicates a higher pitch for student A than his previous utterance. We see a higher number of bands in the harmonic frequencies, which is perceived as a more intense sound. There are also brighter bands in the upper part of the spectrogram, which shows more balance between lower and higher frequency components. This balance is known as a flatter spectral tilt — a quality often associated with stressed voices. However, the bands of sound remain distinct and well-defined, which indicates that the speech has a clear tone without much distortion. While this sound clip had a higher aggression measurement, it was ultimately classified as non-aggressive by the algorithm. Human annotators also did not find this simulated shout convincingly aggressive. This is a sound frame of a simulated scream from student A. Simulated scream, male student A (classified as aggressive) This sound has high intensity in the higher frequency components and displays far less distinct bands compared with the previous examples. This indicates some vocal strain in the speaker. The fundamental frequency and harmonics are far less well defined, which indicates audio distortion and a much rougher tone to the sound. This characteristic is also referred to as spectrum whitening. We found that audio frames with higher-pitched vocalizations (higher fundamental frequency), higher frequency components (flatter spectral tilt), and that contain distortions (spectral whitening) tended to trigger the algorithm. Human annotators found this simulated scream convincingly aggressive and the algorithm agreed.

Analyzing Errors At the same time, we identified instances where high-pitched shrieks did not trigger the algorithm. The scream below is very high-pitched, has high-frequency components and contains the spectrum whitening that indicates vocal strain. However, the algorithm did not provide an aggression measurement above zero for most of the duration of the sound. While this was one of the sounds that had one of the highest measures of pitch and distortion, it did not contain a pattern that the algorithm recognized as an aggressive voice. Simulated scream, female student C (classified as non-aggressive) Machine learning models such as the aggression detection algorithm depend on pattern matching to the labeled data it is fed in training. If the training data used did not include examples of shrieks, or if the labeling process failed to identify such noises as aggressive, the algorithm may not correctly characterize such sounds in operation. When asked about the false negatives from higher-pitched shrieks, Sound Intelligence responded that they may result from the device not processing higher frequency components of audio. They also cited the potential similarity of shrieks to baby cries in their training data, which they labeled as non-aggressive. We also found a number of instances where the algorithm determined that singing, laughing and coughing were aggressive. We provide some examples of such vocalizations below. In each of these spectrograms, there is some evidence of flatter spectral tilt and spectral whitening, particularly when compared to normal speech. Singing, female students D and E (classified as aggressive) Laughing, female student D (classified as aggressive) Coughing, female student F (classified as aggressive) We also found a number of instances where activated, energetic speech (known in psychology as high-arousal speech) triggered the algorithm. Women’s voices tend to be twice as high as men’s. We found that every speaking voice in the pair testing that triggered the algorithm belonged to a female student. Activated speech, female student D (classified as aggressive) Activated speech, female student F (classified as aggressive) Activated speech, female student B (classified as aggressive) It would require a large number of individuals from a full demographic range to fully train or test the device for effectiveness in all of its potential applications. At the same time, pitch and tonal quality vary among individual voices, even for people in the same demographic. Since the device is often installed in schools, we tested it on student populations. However, a more comprehensive dataset with a wider variety of voices would improve the analysis and give a fuller understanding of the contours and universal applicability of the underlying algorithm.
