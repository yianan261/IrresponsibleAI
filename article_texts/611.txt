Dozens of Bulgarian nationals have been tipped into poverty after unexplained benefit suspensions following the creation of a “risk review” unit in the Department for Work and Pensions, an MP has said.

Kate Osamor, the Labour MP for Edmonton, said her constituency office had been inundated in recent weeks with requests for help from Bulgarian nationals whose universal credit had been suspended for protracted periods with no explanation.

Concern about the treatment of many Bulgarian and some Polish universal credit claimants has also been raised by a charity supporting EU nationals and an estate agency renting to Bulgarian tenants in London.

Osamor has written to Thérèse Coffey, the work and pensions secretary, to express her “grave concern that Bulgarian nationals are being unfairly targeted for benefit fraud investigation based on their nationality”.

Most of those affected were single mothers working part-time in low-paid jobs, who relied on universal credit to supplement rent payments, she said. Many of them had been forced to use food banks, and some had been made homeless as a result of their rent arrears. Claimants were often notified by text message that their benefits had been stopped, and were not given information about how to appeal.

Osamor said her office had raised concerns from about 16 constituents through a DWP helpline for MPs, and had been disappointed with the response. “In each case the claimant has no money for food and cannot pay their rent. Many are facing eviction,” she wrote. “In the few cases I have received a response from the DWP, the response has simply stated a ‘risk review’ is being carried out with no indication of when the nebulous ‘review’ will be concluded.”

Acknowledging that the DWP was within its rights to investigate fraud, Osamor said: “What I do not consider fair, however, is the idea that an individual should be placed under suspicion because of their nationality. I am also greatly concerned about the efficiency of a system that enables a claim to be suspended for months on end with little to no explanation.”

Her concerns were echoed by an estate agent who said her north London company was dealing with 40 tenants struggling to pay the rent because their universal credit had unexpectedly been suspended, 38 of whom were Bulgarian and two Polish. The lettings agency rents to tenants of all nationalities and, overall, Bulgarian tenants are in a minority.

One tenant, a single mother, was evicted last week because she was unable to pay her rent after months of universal credit suspension. The estate agent said she was puzzled by the suggestion that the suspensions might be related to fraud allegations because the letting agency did robust checks on all tenants before signing rental contracts, to verify identities and work status.

“All these tenants are at least in part-time work,” she said, asking for the name of the letting agency not to be used. “It is absolutely fine for the DWP to do fraud checks, but they need to do that within a reasonable timescale. Most of these people have been without benefits for months with no information about why. They don’t know what has triggered it.”

Olivia Vicol, director of the Work Rights Centre, a London-based charity supporting EU nationals in precarious employment, said her organisation was advising eight people originally from Bulgaria who were struggling to get their benefits reinstated after payments were stopped abruptly and without explanation. Attempts to get clarity about what had prompted the suspensions had not been successful, she said.

“This opaqueness makes it impossible for the claimants to challenge decisions,” she said. “People need to know why their payments are being suspended, and be given the opportunity to challenge that. By jumping to suspending payments, the DWP is effectively throwing them into destitution.”

A DWP spokesperson said: “We have a duty to the taxpayer to investigate a benefit claim where we suspect fraud, including potentially organised crime. The risk review team was set up to investigate risks of criminal activity, and it’s not linked to nationality. If someone provided us with details to show their claim is genuine, we would urgently put any payment due into place.”. Kate Osamor, the Labour MP for Edmonton, recently received an email from a charity about a constituent of hers who had had her benefits suspended apparently without reason.

“For well over a year now she has been trying to contact DWP [the Department for Work and Pensions] and find out more about the reason for the suspension of her UC [Universal Credit], but neither she nor our casework team have got anywhere,” the email said. “It remains unclear why DWP has suspended the claim, never mind whether this had any merit … she has been unable to pay rent for 18 months and is consequently facing eviction proceedings.”

Osamor has been dealing with dozens of such cases in recent years, often involving Bulgarian nationals. She believes they have been victims of a semi-automated system that uses an algorithm to flag up potential benefits fraud before referring those cases to humans to make a final decision on whether to suspend people’s claims.

“I was contacted by dozens of constituents around the beginning of 2022, all Bulgarian nationals, who had their benefits suspended,” Osamor said. “Their cases had been identified by the DWP’s Integrated Risk and Intelligence Service as being high risk after carrying out automated data analytics.

“They were left in destitution for months, with no means of appeal. Yet, in almost all cases, no evidence of fraud was found and their benefits were eventually restored. There was no accountability for this process.”

The DWP has been using AI to help detect benefits fraud since 2021. The algorithm detects cases that are worthy of further investigation by a human and passes them on for review.

In response to a freedom of information request by the Guardian, the DWP said it could not reveal details of how the algorithm works in case it helps people game the system.

The department said the algorithm does not take nationality into account. But because these algorithms are self-learning, no one can know exactly how they do balance the data they receive.

The DWP said in its latest annual accounts that it monitored the system for signs of bias, but was limited in its capacity to do so where it had insufficient user data. The public spending watchdog has urged it to publish summaries of any internal equality assessments.

Shameem Ahmad, the chief executive of the Public Law Project, said: “In response to numerous Freedom of Information Act requests, and despite the evident risks, the DWP continues to refuse to provide even basic information on how these AI tools work, such as who they are being tested on, or whether the systems are working accurately.”

The DWP is not the only department using AI in a way that can have major impacts on people’s daily lives. A Guardian investigation has found such tools in use in at least eight Whitehall departments and a handful of police forces around the UK.

The Home Office has a similar tool to detect potential sham marriages. An algorithm flags marriage licence applications for review to a case worker who can then approve, delay or reject the application.

The tool has allowed the department to process applications much more quickly. But its own equality impact assessment found it was flagging a disproportionately high number of marriages from four countries: Greece, Albania, Bulgaria and Romania.

The assessment, which has been seen by the Guardian, found: “Where there may be indirect discrimination it is justified by the overall aims and outcomes of the process.”

Several police forces are also using AI tools, especially to analyse patterns of crime and for facial recognition. The Metropolitan police have introduced live facial recognition cameras across London in order to help officers detect people on its “watchlist”.

But just like other AI tools, there is evidence the Met’s facial recognition systems can lead to bias. A review carried out this year by the National Physical Laboratory found that under most conditions, the cameras had very low error rates, and errors were evenly spread over different demographics.

When the sensitivity settings were dialled down however, as they might be in an effort to catch more people, they falsely detected at least five times more black people than white people.

The Met did not respond to a request for comment.

West Midlands police, meanwhile, are using AI to predict potential hotspots for knife crime and car theft, and are developing a separate tool to predict which criminals might become “high harm offenders”.

These examples are those about which the Guardian was able to find out most information.

In many cases, departments and police forces used an array of exemptions to freedom of information rules to avoid publishing details of their AI tools.

Some worry the UK could be heading for a scandal similar to that in the Netherlands, where tax authorities were found to have breached European data rules, or in Australia, where 400,000 people were wrongly accused of giving authorities incorrect details about their income.

John Edwards, the UK’s information commissioner, said he had examined many AI tools being used in the public sector, including the DWP’s fraud detection systems, and not found any to be in breach of data protection rules: “We have had a look at the DWP applications and have looked at AI being used by local authorities in relation to benefits. We have found they have been deployed responsibly and there has been sufficient human intervention to avoid the risk of harm.”

However, he added that facial recognition cameras were a source of concern. “We are watching with interest the developments of live facial recognition,” he said. “It is potentially intrusive and we are monitoring that.”

Some departments are trying to be more open about how AI is being used in the public sphere. The Cabinet Office is putting together a central database of such tools, but it is up to individual departments whether to include their systems or not.

In the meantime, campaigners worry that those on the receiving end of AI-informed decision-making are being harmed without even realising.

Ahmad warned: “Examples from other countries illustrate the catastrophic consequences for affected individuals, governments, and society as a whole. Given the lack of transparency and regulation, the government is setting up the precise circumstances for it to happen here, too.”. In the past, artificial intelligence (AI) use in public services has caused uproars. For instance, in the Netherlands, tax authorities used technology to find fraud but made many mistakes. This resulted in a hefty fine of €3.7m, pushing thousands of families into poverty.

A recent inquiry by The Guardian unveiled that government officials and civil servants in a minimum of eight Whitehall departments and some police forces in the United Kingdom employed AI to make significant determinations regarding benefit allocations.. The government has widened its deployment of artificial intelligence to uncover welfare fraud, despite warnings of algorithmic bias against groups of vulnerable claimants.

In a £70m investment applying “advanced analytics” to requests for universal credit (UC), the Department for Work and Pensions (DWP) has extended the use of machine learning as it attempts to save more than £1bn from the £8bn-plus lost to fraud and error last year, audit documents scrutinised by the Guardian reveal.

The project does not appear to have been formally announced by the government, which has been accused of being secretive about AI in the welfare system. If extended, it has the potential to reach many of the 5.9 million people who claim UC.

After a trial last year using automated software to flag up potential fraudsters seeking UC cash advances, similar technology has now been developed and piloted to scan welfare applications made by people living together, self-employed people, and those wanting housing support, as well as to assess the claims people make about their personal capital such as savings.

Welfare rights organisations and UN experts have previously said that extending the UK’s “digital by default” welfare system with machine learning without greater transparency risks creating serious problems for some benefit claimants.

Big Brother Watch, a UK civil liberties campaign group, said it was “worrying” the government was “pushing ahead with yet more opaque AI models when it admits its capability to monitor for unfairness is limited”.

“The DWP consistently refuses to publish information about how they operate bar the vaguest details,” said Jake Hurfurt, head of research and investigations.

The extension of welfare automation is summarised in the comptroller and auditor general’s statement in the DWP’s latest annual report. It says the DWP has “tight governance and control over its use of machine learning”, but it revealed officials have detected “evidence of bias toward older claimants”.

An algorithm trialled last year was fed data about previous claimant fraud to teach it to predict which new benefit claimants might be cheating. Final decisions are made by a human.

The auditor general warned of “an inherent risk that the algorithms are biased towards selecting claims for review from certain vulnerable people or groups with protected characteristics”.

The DWP had admitted its “ability to test for unfair impacts across protected characteristics is currently limited”, they said.

skip past newsletter promotion Sign up to Headlines UK Free newsletter Get the day’s headlines and highlights emailed direct to you every morning Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

The government is under pressure to increase public confidence in its use of AI, and the auditor general urged it to publish its assessment of bias in machine learning models.

The DWP has so far declined to release information about how its machine learning algorithm works. It has blocked freedom of information requests from the Guardian seeking the name of any companies involved in the fraud detection trial relating to universal credit advances because “details of contracts are commercially sensitive”. It is also refusing to publish any results from last year’s trial, including assessing bias against certain groups of claimants.

Last month, Tom Pursglove, minister for disabled people, health and work, told parliament the government would not publish an equalities assessment of trials of machine-learning algorithms, because it would tip off fraudsters “leading to new frauds and greater losses to the public purse”.

The DWP has told the auditor it is “working to develop its capability to perform a more comprehensive fairness analysis across a wider range of protected characteristics”.

A spokesperson for the DWP said: “We continue to explore the potential of new technologies in combating fraud and deploy comprehensive safeguards when doing so. AI does not replace human judgment when investigating fraud and error to either determine or deny a payment to a claimant.”. The UK government officials are using artificial intelligence (AI) and complex algorithms to help make various decisions including those related to welfare, benefits, and approval of marriage licenses, a report by the Guardian said. According to the report, civil servants in at least eight government departments are using AI in one way or another, and some in an “uncontrolled manner”. UK Prime Minister Rishi Sunak (AP)

The report made findings of some potentially discriminatory results made by the AI such as - an algorithm used by the Department for Work and Pensions (DWP) which an MP believes mistakenly led to dozens of people having their benefits removed, or a facial recognition tool used by the Metropolitan police has been found to make more mistakes recognising black faces than white ones under certain settings.

HT launches Crick-it, a one stop destination to catch Cricket, anytime, anywhere. Explore now!

Another discriminatory finding reportedly shows an algorithm used by the Home Office to flag up fake marriages has been disproportionately selecting people of certain nationalities.

This comes ahead of the AI Safety Summit (UK AISS) - where global leaders will come together to deliberate on strategies for addressing potential misuse and loss of control risks associated with AI. Around 100 people from 28 nations are expected to attend the summit - which will feature companies and industry experts on November 1 and 2, hosted by Technology Secretary Michelle Donelan.

Sunak pitches Britain as future home for AI regulation

UK Prime Minister Rishi Sunak on Monday pitched for his country to be the future global home for AI regulation. Speaking at the London Tech Week conference, Sunak said, “The possibilities (of AI) are extraordinary. But we must – and we will – do it safely…I want to make the UK not just the intellectual home, but the geographical home of global AI safety regulation.”

He added that the tech sector was at the “heart of his priority” to grow the economy.

Earlier in May, Sunak had met with the leading CEOs in artificial intelligence - OpenAI, Google DeepMind, and Anthropic, to discuss joint action to ensure the development of safe and responsible AI and ways to establish the right approach to governance for the next technological frontier. During the meeting, Sunak made it clear that AI is the “defining technology of our time”.. . Sign up for the View from Westminster email for expert analysis straight to your inbox Get our free View from Westminster email Please enter a valid email address Please enter a valid email address SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy notice Thanks for signing up to the

View from Westminster email {{ #verifyErrors }} {{ message }} {{ /verifyErrors }} {{ ^verifyErrors }} Something went wrong. Please try again later {{ /verifyErrors }}

Government officials are using artificial intelligence (AI) to help decide on everything from approving marriage licences to who should be given benefits, it has been revealed.

Civil servants in at least eight Whitehall departments, and several police forces, are relying on AI to make decisions around welfare, immigration and criminal justice, according to an investigation.

The shocking findings include the use of an algorithm by the Department for Work and Pensions (DWP) to identify potential benefit fraud.

It also found a facial recognition tool used by the Metropolitan Police can make more mistakes recognising black faces than white ones.

And it showed an algorithm used by the Home Office to detect fake marriages targeted some nationalities disproportionately.

The investigation, carried out by The Guardian, showed that at least eight Whitehall departments use AI in one way or another, some far more heavily than others.

The NHS has also relied on AI in some contexts, including during the pandemic to identify at-risk patients who should shield.

And the Home Office is using it for its “sham marriage tool”, which the Guardian said disproportionately flags up people from Albania, Greece, Romania and Bulgaria.

It also uses the technology for electronic passport gates at airports and in the passport application process.

And the DWP has used AI to detect fraud and error among benefits claimants.

Marion Oswald, a professor in law at Northumbria University and a former member of the government’s advisory board on data ethics, told the paper: “There is a lack of consistency and transparency in the way that AI is being used in the public sector. A lot of these tools will affect many people in their everyday lives, for example those who claim benefits, but people don’t understand why they are being used and don’t have the opportunity to challenge them.”

In a sign of the government’s growing adoption of AI, ministers in August allocated a £13m investment aimed at advancing research in the technology’s use within healthcare.

The move, announced by Technology Secretary Michelle Donelan, coincides with the appointment of two leading experts tasked with leading the preparations for the upcoming first major international summit on the responsible use of AI.

Rishi Sunak is attempting to pit Britain as a world leader in the development and regulation of AI.. Access to benefits can be determined in some cases by artificial intelligence (Yui Mok/PA Wire)

The government’s use of artificial intelligence (AI) risks producing discriminatory results against benefit claimants and ethnic minorities, an investigation has found.

A total of eight Whitehall departments and some police forces are using the burgeoning technology to make life-altering decisions for members of the public, reported the Guardian.

In one case, Labour MP Kate Osamor claimed that an algorithm used by the Department of Work and Pensions (DWP) to detect fraud may have led to dozens of Bulgarians having their benefits suspended.

Meanwhile, an internal Home Office evaluation seen by the Guardian showed that an algorithm used to indicate sham marriages disproportionately singled out people from Albania, Greece, Romania and Bulgaria.

Several police forces are using AI tools and facial recognition cameras for surveillance and to predict and prevent future crimes. The investigation claims that when the sensitivity settings are dialled down on the cameras – as they may be in an effort to catch more criminals – they incorrectly detected at least five times more black people than white people.

The findings come as the UK prepares to host an international summit on AI at Bletchley Park. The event is viewed as a means for the UK to stamp its authority on AI regulation, and grapple with the existential threat some luminaries, including Elon Musk, believe it poses.

But while the summit focuses on the headline-grabbing future of the technology, Britain is already harnessing AI in many areas that affect the lives of everyday people.

The wide-ranging use of the AI in the public sector was uncovered after the Cabinet Office began encouraging departments and law enforcement to voluntarily disclose their use of the tech, specifically when it could have a material impact on the general public.

A separate database compiled by the Public Law Project also tracks the automated tools used by the government and ranks them based on transparency.

Experts and tech insiders have repeatedly warned that AI can reinforce biases that are engrained in the datasets used to train the systems. After pressure from rights groups over the dangers of predictive policing and face recognition surveillance, the EU passed a landmark AI law earlier this year banning the systems.

The DWP told the Guardian that its algorithm does not take nationality into account. And both the DWP and the Home Office insisted that the processes they use are fair because the final decisions are made by people. The Met did not respond to the findings.

John Edwards, the UK’s information commissioner, said he had examined many AI tools being used in the public sector, including the DWP’s fraud detection systems, and not found any to be in breach of data protection rules.. The Department for Work and Pensions (DWP) is ploughing £70m into its digital transformation projects over the next three years. However, the National Audit Office (NAO) has raised concerns about investments the department is making in artificial intelligence (AI) models to detect fraud, suggesting these could exhibit bias against people with protected characteristics.

As the UK’s biggest public service department the DWP administers the state pension and a range of working-age, disability and ill health benefits. (Photo by Mike Kemp/In Pictures via Getty Images)

According to a new report from the NAO, which oversees public spending, DWP is investing £70m between financial years 2022-23 and 2024-25 in advanced analytics to tackle fraud and error. DWP expects this will help it to generate savings of around £1.6bn by 2030-31.

The government department is planning to use AI to identify patterns in welfare claims that could suggest fraud and error. The claims would then be reviewed either by relevant DWP teams, such as the Enhanced Review Team, before the claim enters payment, or the Targeted Case Review agents if it is already payment.

DWP wants AI to help bring in an extra £200m a year

In a separate report, DWP outlined an ‘ambitious’ new target to save £1.3bn in 2023-24 through tackling claimant fraud and error resources. The department will need to claim a further £200m in savings, following on from its previous £1.1bn target.

MP Tom Pursglove, minister of state for work and pensions, has been tasked with overseeing the fraud reduction effort, and said: “Our teams are working flat out to prevent new fraudulent claims and expose people who have been exploiting the system – with strong results.” But the minister explained that there was a need to go “even further” because of the changing fraud landscape.

“Working towards our ambitious new target over the next year will protect taxpayers’ hard-earned cash and enable us to deliver on the prime minister’s priorities to reduce debt and grow the economy,” he said.

Pursglove has been vocal about tackling benefit fraud and error; the total rate of overpaid Universal Credit payments currently sits at 12.8% (£5.54bn). The DWP report also references that Universal Credit underpayments were up, at 1.6% (£680 million) in 2022-23 from 1.0% (£410 million) in 2021-22.

Glad to see fraud and error rates moving in a positive direction today, thanks to the great work of @DWPgovuk teams cracking down on fraudsters and serious crime groups. Benefit fraud is never a victimless crime, and we must ensure welfare support goes to the people who need it. https://t.co/vGZhpPuSVD — Tom Pursglove MP (@VotePursglove) May 11, 2023

UK government tightening up on controlling benefit fraud by digital transforming operations

The government website says a claimant can commit benefit fraud by claiming benefits they’re not entitled to either on purpose or in error. This could happen if they do not report a change in their circumstances that would affect the amount of money they are paid, or if they purposely provide false information to ensure they get higher rates.

View all newsletters Sign up to our newsletters Data, insights and analysis delivered to you By The Tech Monitor team Sign up here

DWP wants to use machine learning to reduce the rate of overpayments to claimants due to fraud and error, which it says has already fallen by 10% over the past year.

“Our tightened fraud controls and checks resulted in a significant reduction in fraud and error in the last year and now we are seeing, the tide start to turn,” Mel Stride MP, secretary of state for work and pensions said.

He continued: “Given that our welfare system exists to provide a strong financial safety net for the most vulnerable, it is imperative we continue to prevent anyone abusing this for their own profit, which is why we’re setting a new target to save £1.3bn in the next year and root out fraud wherever we find it.”

Last year, DWP launched a robust plan, ‘Fighting Fraud in the Welfare System‘, to drive down fraud and error from the benefits system. The plan sits alongside an investment of £900m over a three-year period. The department has also estimated that its full range of controls last year saved at least £18bn through benefit checks, controls and counter-fraud activities, contributing to Prime Minister Rishi Sunak’s pledge to reduce debt.

NAO says that DWP’s AI could be biased

DWP had been using a machine learning model to flag potentially fraudulent claims for Universal Credit advances since 2021-22. An advance is when a claimant doesn’t have enough money to live on while they wait for their first benefit payment of Universal Credit. Claimants can also get budgeting advances to pay lump sums on emergency household costs, getting a job or staying in work, or funeral costs.

According to NAO, DWP created the model by training an AI algorithm using historical claimant data and fraud referrals, which allowed the model to make predictions about which new benefit claims could contain fraud and error. The government department then developed and piloted four similar models for key areas of risk in Universal Credit, which include people living together, self-employment, capital and housing.

As of April 2023, 5.9 million people claimed Universal Credit, increasing by 200,000 since July 2022.

However, NAO wrote in its report that there was an "inherent risk" that the algorithms used by DWP's machine learning model were biased towards selecting claims for review from certain "vulnerable people or groups with protected characteristics". It said that this could be due to "unforeseen bias" in the input data or "the design of the model itself".

"When using machine learning to prioritise reviews there is an inherent risk that the algorithms are biased towards selecting claims for review from certain vulnerable people or groups," the report says. "DWP faces a challenge in balancing transparency over how it uses machine learning to provide public confidence in the benefits system with protecting its capabilities by not tipping off fraudsters about how it tackles fraud."

The NAO also said that DWP needed to provide assurance that it was not unfairly treating any group of customers because of its use of AI: "In response to the Committee of Public Accounts 2022 report on fraud and error in the benefits system, DWP committed to reporting annually to Parliament on its assessment of the impact of data analytics on protected groups and vulnerable claimants."

DWP has said that it has "established tight governance and control over its use of machine learning" and has put safeguards in place designed to assess the impact that using the model has on its different customers. However, DWP has said that its ability to test for unfair impacts across protected characteristics is currently limited, which it blames in part on claimants not providing information about their demographics when making a benefit claim.

NAO writes that "DWP also segregates personal data on its analytical platforms for security reasons and has yet to incorporate all the relevant data onto its fraud and error analytics platform". DWP has reportedly said it plans to do this soon.

Read more: DWP struggling to scale up online PIP application process. The UK government risks contempt of court unless it improves its response to requests for transparency over the use of artificial intelligence (AI) to vet welfare claims, the information commissioner has said.

Over the past two years, the Department for Work and Pensions (DWP) has increasingly deployed machine-learning algorithms to detect fraud and error in universal credit (UC) claims.

Ministers have maintained a veil of secrecy over the system, which the transparency campaign group Big Brother Watch has described as “seriously concerning”. The DWP has refused freedom of information requests and blocked MPs’ questions, arguing that providing information could help fraudsters.

Child poverty campaigners have said the impact on children of the AI tools could be “devastating” if benefits are suspended.

The information commissioner, John Edwards, has now warned the DWP it could be in contempt of court unless it changes its approach and spells out within 35 days the terms under which it could release more information.

In July 2022, the Guardian asked what information was fed to the algorithm to help it decide who might be cheating, which companies were involved, and for the results of a “fairness analysis” of disproportionate impacts on ethnic minorities, older and disabled people and any others with protected characteristics. It refused to comply with the request.

In a decision notice after an appeal by this newspaper, Edwards’ office said he was “disappointed that the DWP failed to consider the request on the basis of its clear, objective interpretation” and had “incorrectly” interpreted the request.

It also described as “unfortunate” the way the DWP had changed its reason for not releasing information 11 months after the request was first made, creating a fresh delay.

The DWP initially claimed that releasing information would harm the prevention or detection of crime and that it would prejudice commercial interests. Then in June it said it would cost too much to gather the information, such were the volumes of material about the AI system held on government computers.

The DWP recently expanded its use of AI from scanning claims for welfare advances to applications made by cohabitants, self-employed people and people applying for housing support, as well as to assess claimants’ savings declarations. One of the questions the DWP is refusing to answer is what historical fraud and error data is inputted to train the AI model. The system has shown some evidence of bias, according to the DWP’s own trials. Universal credit is used by 5.9 million people.

“Whilst reducing fraud in the benefit system is important, the uses of these tools have real-life implications for families in poverty who get caught up in their deployment,” said Claire Hall, the head of strategic litigation at Child Poverty Action Group. “The DWP must drop its culture of secrecy and provide meaningful reassurance of the fairness of these tools.”

Big Brother Watch, a civil liberties and privacy campaigning organisation, said the DWP was being “alarmingly, unjustifiably secretive”.

The group’s director, Silkie Carlo, said: “It is totally unacceptable to refuse legal requests for information about the use of powerful technologies that run a high risk of causing unfair and discriminatory impacts on some of our country’s most vulnerable people.

skip past newsletter promotion Sign up to First Edition Free daily newsletter Our morning email breaks down the key stories of the day, telling you what’s happening and why it matters Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

“Algorithms in the welfare system tend to cast a wide net of digital suspicion and can be dangerously wrong, with serious harms. Government uses of AI should trigger much greater public transparency, not less. People have the right to understand how their information is being used and why decisions are made about them, rather than to be left at the mercy of opaque AI.”

UN experts have warned that extending the UK’s “digital by default” welfare system with machine learning, without greater transparency, risks creating serious problems for some benefit claimants.

In March the information commissioner warned the DWP to improve its broader handling of freedom of information requests. The commissioner complained of a “consistently poor level of performance” and found the department was often not properly reading the requests, that it persistently used standard templates when refusing requests and that it had recently been withholding more information.

It has ordered the DWP to set out how it might be able to respond to the Guardian’s specific request by 22 September.

A spokesperson for the DWP said: “We welcome that the commissioner agreed we are entitled to refuse to comply with the request on the basis on cost, as defined under the FoI Act. We have noted the points about providing further advice and assistance to the requester. The department takes its compliance with the Freedom of Information Act and Cabinet Office code of practice very seriously and keeps its approach to the publication of information under constant review.”. Senior civil servants told a committee of MPs that an AI being developed by the Department for Work and Pensions (DWP) to detect fraud and error is still in the learning stages of its development and is exhibiting bias against some claimants. Part of a project to target more cases of benefit over and underpayments, the systems won’t be ready for deployment for a number of years, MPs have been told.

DWP was giving evidence to PAC over concerns on fraud and error detection and AI bias following NAO’s report. (Photo by TK Kurikawa/Shutterstock).

DWP officials were quizzed by parliament’s Public Accounts Committee (PAC) following the publication earlier this year of a National Audit Office (NAO) report looking at the system’s development. As reported by Tech Monitor, the NAO revealed that the DWP was investing £70m over the next two years in advanced analytics to deal with rising levels of fraud and error. The department believes the investment will generate savings to the taxpayer of around £1.6bn by 2031.

DWP was represented by Bozena Hillyer, director for counter fraud compliance and debt; Peter Schofield CB, permanent secretary; and Catherine Vaughan, finance director general. Richard Hawthorn, director, of operational excellence, customer service group represented HMRC.

The session delved into why fraud and error – which covers over- and underpayments, deliberate or otherwise, of welfare – had increased within the benefits system. MPs raised questions and concerns over underpayments of pension credit, a rise in Universal Credit ‘fraud’ and underpayments of personal independence payments (PIP) due to changes in circumstances and paused automated health assessments.

As part of the session, the DWP explained the progress it had made with its targeted case review project and its AI system, which will be used to identify cases of fraud using details from historical cases. However, concerns of AI bias were raised by the committee on whether the AI would unconsciously choose cases that affected protected characteristics. DWP admitted that bias existed within the AI, but that it was still in the early stages of learning.

MPs question whether DWP requires AI for fraud detection

In the early part of the session, MPs asked Neil Couling, director general of change and resilience at the DWP, about the effectiveness of the AI project for fraud detection. He is the senior officer responsible for the project

“The targeted case review is having an effect today [but] it won’t be at a volume big enough to be really picked up in the sampling, unless you were really lucky in the cases you sampled,” Couling said. “We take 3,000-odd universal credit cases through the sampling. That will take two years before it starts to have a measurable effect.”

MP Jonathan Djanogly asked whether the investment in the project was value for money given it was pulling up the same percentages of incorrect claims (30%) as random sampling.

“Obviously my question is: why the project? Why do you need to spend all this money if the result is the same as if you kept it random?” he asked Couling.

View all newsletters Sign up to our newsletters Data, insights and analysis delivered to you By The Tech Monitor team Sign up here

The DWP official said that AI could get a “higher hit rate”. Couling also pointed to pressure from the government for his team to speed up the development of the AI: “I am resisting the urges of lots of people around me to go faster and snatch at success here, because what I do not want it to do is to turn into a process,” he said.

Couling said it was important the department was given time and resources to ensure the system worked on launch. “It depends on me keeping the quality there, and we are still learning,” he explained. “We are still developing the service, and I will not have finished that until we are ready to go to market.”

AI bias exists within DWP project

The NAO noted in its report that the DWP has been using machine learning to flag potentially fraudulent claims for Universal Credit advance payments since 2021. It has since been piloted in other parts of the welfare system including for people living together, the self-employed, and for housing claims.

But MPs raised fears that the use of historical claimant data and fraud referrals by the AI could lead to biased results. Couling said he shared those concerns: “I do not want to preside over a system that does that,” he said, going on to explain that DWP was doing two things to mitigate the risk.

“The first is that we always put a human being at the point of decision-making about a claim, and if we are using any of this information, we put to the decision makers false positives, as well as cases that we think are flagged for attention,” he said. “The second thing we are doing is that we are taking this carefully… with small numbers at the start so that you can observe what is going on. You can then run analyses against this to check for bias.”

When asked how the DWP was teaching the system the dangers of discrimination and misinterpreting information, Couling said that he couldn’t disclose the models being used, but that the department was taking steps to rectify the issues with the AI.

Data watchdog the Information Commissioner’s Office has been liaising with the DWP on the development of the AI system, and the department plans to give MPs an annual report on the progress of the project.

Read more: DSIT aims to boost digital skills by joining STEM futures project. In a recent National Audit Office (NAO) report concerning the Department for Work and Pensions (DWP) financial accounts, it has come to light that the DWP is expanding its use of machine learning for identifying potential benefits fraud.

Since 2021, the DWP has used a machine learning model to flag potentially fraudulent claims for University Credit (UC) advances. The model was created by training an algorithm using fraud referrals and historic claimant data, and it makes predictions about which new benefits claims could be fraudulent or contain errors.

When a claim scores above a certain threshold, it gets referred to a caseworker for review. The caseworker then performs a manual review of the claim.

Responding in 2021, Big Brother Watch, the civil liberties and privacy campaigning organisation, said that “Leaving a computer to decide whose benefits application needs to be reviewed is an invasion of privacy and opens the door for unfairness and discrimination in the welfare system.”

The NAO report published last Thursday highlighted that the DWP is set to invest around £70 million between the 2022-23 and 2024-25 financial years into “advanced analytics” in a move to deepen its anti-fraud technological capabilities.

Further, it underscored that since last year, similar machine learning models have been designed and piloted to prevent fraud in four “key” risk areas of Universal Credit — people living together, self-employment, capital, and housing.

Recommended

Despite the DWP expecting “advanced analytics” to help it generate savings of £1.6 billion by 2031, the report states that there is an “inherent risk” that the algorithms which flag benefits claims for review could be biassed “due to unforeseen bias in the input data or the design of the model itself.”

While the report says that the DWP has “tight governance and control” of its machine learning and has put safeguards in place, the DWP’s ability to test for unfair impacts across protected characteristics is “currently limited.” This is due to claimants not always answering the optional demographics-focused questions when making a benefits claim.

Alison Garnham, the Chief Executive of the Child Poverty Action Group said that “Expanding the technology while ignoring calls for transparency and rigorous monitoring of and protections against bias will risk serious harm to vulnerable families,” in a comment made to the BBC.

In spite of the “challenge in balancing transparency over how it uses machine learning to provide public confidence in the benefit system with protecting its capabilities by not tipping off fraudsters about how it tackles fraud,” the report suggests that the DWP “should be able to provide assurance that it is not unfairly treating any group of customers.”. Civil servants in at least eight government departments and the police are using artificial intelligence to make decisions on welfare, immigration and criminal justice, it has been claimed.

The Guardian reported that the systems were being used in the Department for Work and Pensions for decisions on benefits, that facial recognition software was used by the Metropolitan Police, and an algorithm was used by the Home Office to spot sham marriages.

Rishi Sunak has previously said he believes AI could improve public services if used safely and securely. Michelle Donelan, the science secretary, will give a speech today on AI strategy and how to ensure security.

Next week the government will hold an AI safety summit at Bletchley Park, Milton Keynes. No 10 has declined