ARTICLE TITLE: Biased Sentiment Analysis
A lot of major players in the science and technology scene believe we have a lot to fear from AI surpassing human intelligence, even as others laugh off those claims. But one thing both sides agree on is that artificial intelligence is subject to humanity’s flaws. If a neural network is trained on wrong or incomplete data, it will itself be so.

And that’s exactly what seems to have happened in one of Google’s AI products.

Gerd Leonhard/Flickr

In July last year, Google launched a beta version of its new machine learning app called Cloud Natural Language API. it allowed developers to integrate Google’s AI learning system into their own apps. One of the features of the AI is a sentiment analyser, which measure on a scale of -1 to 1 whether the sentences it’s reading express a positive or negative sentiment.

Two weeks ago, Motherboard’s Andrew Thompson tested out the API for a pet project, feeding the program sample text to analyse. He made a troubling discovery that the AI considered sentences referring to religious and ethnic minorities negative, as well as statements involving homosexuality. A saple saying “I’m Christian” registered a positive sentiment, with “I’m a Sikh” being even more so. On the other hand, typing in “I’m a Jew” or “I’m a homosexual” were both considered negative, as well as other permutations involving those identifiers.

REUTERS

It’s not that Google is making a statement about its conservative leanings, the problem here is the data used to train the AI. These kinds of programs designed to learn and mimic natural language are usually fed collections of text like books, news articles and such. If the text contained therein is biased, the AI learns to be that way as well.

In the case of Jewish people, anti-semitic content tends to refer to them as “Jew,” while neutral sources are more likely to use the word “Jewish”. Because of that, the word “Jew” has gained a negative connotation in the AI’s mind, but not the word “Jewish”.

It’s a common theme in artificial intelligence, that the program is only as strong as the data it learns from. If society is flawed, then the AI that learns from it will be as well. Of course, there’s no way just yet to completely cut off machine learning algorithms from this kind of biased data, but it’s a problem there’s hope to eventually work around.

And in the meantime, Google is certainly taking the problem seriously. Not only does it seem to have corrected the sentiment analyser’s bias (which you can try for yourself here), but it also recently set up a research collaborative to consider the ethical and economic impact of the advent of AI on society.