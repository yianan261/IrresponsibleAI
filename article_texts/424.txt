For one, these software platforms use data mining and facial recognition to monitor for cheating. Yet the report found there is no guarantee that these software platforms ensure clear and individual consent from Canadian students whose biometric data is being collected.

The report also considers that data collection and retention control is more difficult because many of these technology companies are US-based, which could require them to transfer data to the US.

Regarding facial recognition, it no longer comes as a surprise that artificial intelligence tools contain a high potential for racial, gender, and age discrimination. However, socioeconomic factors also cause unwarranted software alerts.

For instance, the report found that the technology can flag a single bark or chirp from a noisy pet as a potential cheating incident during an online exam. Other audio-visual variables like living with children, residing in a multigenerational household, being in a rural location, and lack of access to proper equipment can also compound the potential for discrimination.

“Students may be placed in an inequitable position for socioeconomic reasons,” the report read. “Such novel discrimination factors are often completely absent from the AI discussion of discrimination, therefore the use of AI based proctoring software is a particularly important and distinctive scenario that regulations must consider.”. 24 Pages Posted: 22 Sep 2022

Date Written: August 31, 2022

Abstract

Academic surveillance can be considered as an emerging field of “capitalism surveillance” (Zuboff) pertaining to the dominance of a few companies in the surveillance field. Online proctoring software represent a variety of tools often based on artificial intelligence, such as Respondus Monitor, Proctorio, ProctorU, ProctorExam, Examity, ProctorTrack. While these tools generate legal issues of socio-economic discrimination and privacy, most of Canadian universities have used them during the pandemic and sometimes before that.



This paper considers the risks generated by AI tools for exam monitoring and the Canadian legal framework on data protection legislation, as well as on Artificial Intelligence (Bill C-27 – part 3: Artificial Intelligence and Data Act) in comparison with the European Commission’s proposal of regulation on AI (AI act). We make recommendations for the Canadian legislator to improve Bill C-27.. Organization

University of Ottawa

Published

2022

Project leader(s)

Céline Castets-Renard, Professor, Faculty of Law – Civil Law Section, University of Ottawa

Summary

This project examines how, during the COVID-19 pandemic, many universities used exam proctoring tools to compensate for the inability to conduct in-person exams.

The researchers found that while many surveillance processes and companies are operating in the industry, most tools use artificial intelligence techniques such as data mining and facial recognition to detect suspicious behaviour that could constitute cheating.

These companies use the personal information collected as part of their mission to monitor university exams. They also use it for secondary purposes of improving artificial intelligence tools. To do this, they obtain consent from students, but the conditions under which it is collected are not conducive to the expression of free, clear and individual consent. In addition, the separation of public and private sector privacy laws makes it difficult to characterize these outsourcing companies. Enforcing their potential liability as a principal under the Personal Information Protection and Electronic Documents Act ( PIPEDA ) is difficult in the context of enforcing provincial public sector laws.

Furthermore, this project demonstrates that control over the conditions of data collection and retention is made more difficult by the fact that many technology companies are U.S. -based and subject to U.S. law, and even require the transfer of data to the United States.

The researchers make five recommendations to address these issues, which can be found in the conclusion of the research report.

Project deliverables are available in the following language(s)

French

English

OPC -funded project

This project received funding support through the Office of the Privacy Commissioner of Canada’s Contributions Program. The opinions expressed in the summary and report(s) are those of the authors and do not necessarily reflect those of the Office of the Privacy Commissioner of Canada. Summaries have been provided by the project authors. Please note that the projects appear in their language of origin.

Contact information. Online proctoring tools for conducting remote exams do not go far enough to ensure free, clear and individual consent from Canadian students whose biometric data they collect, according to a new report published by the University of Ottawa and supported by the Office of the Privacy Commissioner of Canada.

With in-person learning disrupted by the COVID-19 pandemic, many institutions turned to software platforms as a way to conduct examinations. Often based on artificial intelligence, tools such as Respondus, Monitor, ProctorU, Examity and others use data mining and facial recognition to monitor for cheating—and present what Céline Castets-Renard, the Law Professor who led the project, called “legal issues of socio-economic discrimination and privacy.”

The report points to familiar issues with AI discrimination, specifically “the overreach of power such as public surveillance or police surveillance using AI facial recognition software, with a potential for discrimination, such as race, gender and age biases.” But it also identifies the risk of certain socio-economic and situational factors that could trigger unwarranted software alerts. According to the report, “a domestic pet who makes noise, such as a bark or a chirp, during an online proctoring exam has been identified as a cause for flagging a potential cheating incident.” Pets, children and other audio-visual variables can make the proctoring software think something suspicious is going on when it is not—a problem compounded in large, multigenerational homes.

Biometric tools, such as facial recognition, are susceptible to similar errors. “Biometric keystroke analysis which serves to track keystroke data, eye tracking which monitors and analyses eye movements, audio monitoring which records and monitors students sonically, and facial detection are all methods that are used by some proctoring software,” says the research report. And all come with unacceptable risks that the technology will mistakenly flag certain variations in data as cheating.

The report concludes with a series of recommendations pertaining to how AI is defined and categorized, and how human oversight of evolving surveillance technologies can help maintain transparency and reduce error and bias. The final recommendation tidily summarizes the researchers’ findings, calling for “a collective reflection on whether to prohibit certain uses of AI, and the means to determine how to identify such prohibited uses.”

Article Topics

AI | biometric identifiers | biometrics | Canada | data privacy | facial recognition | monitoring | remote proctoring | surveillance