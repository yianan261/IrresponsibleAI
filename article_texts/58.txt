Russian Voice Assistant Alice Goes Rogue, Found to be Supportive of Stalin and Violence

Two weeks ago, Yandex introduced a voice assistant of its own, Alice, on the Yandex mobile app for iOS and Android. Alice speaks fluent Russian and can understand users’ natural language to provide contextually relevant answers. The company also stated in a press release that Alice is the “first conversational intelligent assistant that is not restricted to a set of predefined scenarios.” But this freedom seems to have created an assistant that is pro-Stalin and pro-violence according to recent reports from The Telegraph. Below is an example of a conversation with Alice translated by the publication:

Alice Goes Rogue

Alice is definitely not the cuddly voice assistant Yandex envisioned. Microsoft has a similar problem with its Twitter bot, Tay, who turned into the ultimate Twitter troll after 24 hours. Yandex attempted to learn from Microsoft’s mistake by providing Alice with filters to avoid certain subjects or at least handle them in a sensitive manner. But considering that Alice has also expressed in other conversations that she believes people should put up with domestic violence and oppose gay marriage, those filters are not enough.

The problem is, virtual assistants learn from the information they are given and get smarter from the conversations they have with real humans. Without teaching Alice a point of view, Yandex has given her the freedom to say what she knows, not what she been told is right. This leads to Alice having several questionable opinions on sensitive subjects. Yandex issued an statement saying they are aware of the issue and working on resolving the problem:

“We tested and filtered Alice’s responses for many months before releasing it to the public. We take the responsibility very seriously to train our assistant to be supportive and polite and to handle sensitive subjects, but this is an ongoing task and in a few cases among its widespread use, Alice has offended users. We are committed to constant improvement with all our products and services to provide a high-quality user experience. We will continue to regularly monitor social and traditional media and will correct our assistant’s behavior when necessary.”

Follow @voicebotai

2. An artificial intelligence run by the Russian internet giant Yandex has morphed into a violent and offensive chatbot that appears to endorse the brutal Stalinist regime of the 1930s.

Users of the “Alice” assistant, an alternative to Siri or Google Assistant, have reported it responding positively to questions about domestic violence and saying that “enemies of the people” must be shot.

Yandex, Russia’s answer to Google, unveiled Alice earlier two weeks ago. It is designed to answer voice commands and questions with a human-like accuracy that its rivals are incapable of.

The difference between Alice and other assistants, apart from the ability to speak Russian, is that it is not limited to particular scenarios, giving it the freedom to engage in natural conversations.

However, this freedom appears to have led the chatbot to veer off course, according to a series of conversations posted by Facebook user Darya Chermoshanskaya.

He said included chats about “the Stalinist terror, shootings, domostroy [domestic order], diversity, relationships with children and suicide”

A portion of the conversations translated by The Telegraph shows Alice responding positively to questions about Josef Stalin’s USSR in the 1930s, and saying there are enemies of the people “in the whole country”.