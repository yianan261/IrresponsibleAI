Immigrant rights campaigners have begun a ground-breaking legal case to establish how a Home Office algorithm that filters UK visa applications actually works.

The challenge is the first court bid to expose how an artificial intelligence program affects immigration policy decisions over who is allowed to enter the country.

Foxglove, a new advocacy group promoting justice in the new technology sector, is supporting the case brought by the Joint Council for the Welfare of Immigrants (JCWI) to legally force the Home Office to explain on what basis the algorithm ‚Äústreams‚Äù visa applicants.

The two groups both said they feared the AI ‚Äústreaming tool‚Äù created three channels for applicants including a ‚Äúfast lane‚Äù that would lead to ‚Äúspeedy boarding for white people‚Äù.

The Home Office has insisted that the algorithm is used only to allocate applications and does not ultimately rule on them. The final decision remains in the hands of human caseworkers and not machines, it said.

A spokesperson for the Home Office said: ‚ÄúWe have always used processes that enable UK Visas and Immigration to allocate cases in an efficient way.

‚ÄúThe streaming tool is only used to allocate applications, not to decide them. It uses data to indicate whether an application might require more or less scrutiny and it complies fully with the relevant legislation under the Equalities Act 2010.‚Äù

Cori Crider, a director at Foxglove, rejected the Home Office‚Äôs defence of the AI system.

‚ÄúThe Home Office insists its ‚Äòvisa streaming‚Äô algorithm has no racial bias, but that claim is pretty threadbare. We‚Äôre told the system uses nationality to ‚Äòstream‚Äô applicants green, yellow and red ‚Äì and it‚Äôs easy to guess who ends up in the green queue and who gets pushed to the back of the bus in red. If your algorithm singles out people for a digital pat-down and offers speedy boarding to white people, well, that‚Äôs unlawful.‚Äù

In its pre-action legal letter this month to the home secretary, Priti Patel, the Joint Council for the Welfare of Immigrants argues that even the streaming process will affect the final decision on visas.

The case is being backed by a gofundme.com page titled ‚ÄúDeported by algorithm‚Äù.

Its letter to the home secretary states: ‚ÄúAn individual visa applicant allocated by the streaming tool to the ‚ÄòRed‚Äô category because of their nationality might still be granted a visa. However, their prospects of a successful application are much lower than the prospect of an otherwise equivalent individual with a different nationality allocated to the ‚ÄòGreen‚Äô category. To similar effect, the same ‚ÄòRed‚Äô application is likely to take much longer than the ‚ÄòGreen‚Äô one, again involving less favourable treatment of the applicant because of their nationality.‚Äù

Among the information the JCWI seeks from the Home Office is all ‚Äúpolicy and guidance documents that deal with the process of streaming visa applications and the use of the streaming tool‚Äù.

It is also demanding all the technical details that drive the streaming tool be revealed, along with further information such as case-working targets into each of the three categories, and if there have been any complaints about the deployment of the algorithm.

The JCWI argues that the use of the streaming tool is a more modern version of a visa entry system ruled unlawful by the House of Lords in 2005. It concerned Roma applicants who were said to have been treated with more suspicion and subjected to more intense and intrusive questioning than non-Roma applicants. The Lords concluded that the ‚Äústereotyping of Roma as being less likely to be genuine visitors‚Äù to the UK was unlawful.

The Home Office has emphasised that the new system is fully compliant with the Equalities Act 2010.

It added that out of more than 3.3m visa applications to the UK by the end of June this year, 2.9 million people were given entry into Britain.. The growth of technology has brought a great deal of efficiency and security to almost all organisations and businesses. But such progress may have taken a slightly wrong turn as the reliance on artificial intelligence by the Home Office as a streaming tool for visa applications may actually be carrying out its functions on grounds of racial bias.

Last week the Guardian reported a legal action has been filed by the Joint Council for the Welfare of Immigrants (JCWI) against the Home Office over the use of an algorithm which filters visa applications on potentially unlawful grounds such as 'nationality'. This follows a report by the Financial Times in June this year that the Home Office was 'secretly' using the algorithm to process visa applications. The Home Office has since confirmed this technology is only used to allocate applications and not decide on them. But why the mystery?

I am among many who would consider the advancement of technology as a blessing to mankind as I, quite frankly cannot imagine a world without smartphones. On the other hand some would consider the advancement of technology as a disaster. Despite this being slightly dramatic I can see why one would come to this conclusion. We are increasingly dependent on technology for almost all everyday tasks. For example, to know directions to a location we would use GPS or for cab services we would resort to the numerous apps on our phones. We no longer memorise directions because Google maps does it for us. In many ways such levels of reliance on technology can be daunting.

The Home Office‚Äôs reliance on the algorithm to stream visa applications may have been implemented to better its efficiency and to finally improve on its service standards and visa processing times (which in itself is a topic for another blog). The Home Office is said to have been relying on an algorithm which uses nationality to stream applicants in the categories green, yellow and red. Regardless of the affirmation the Home Office has given to confirm the algorithm is only used to allocate applications and not make decisions, the basis on which these applicants are streamed and their applications allocated is crucial. We cannot be caught up celebrating the technological advancement of our time and ignore an unhealthy reliance on artificial intelligence in dealing with people‚Äôs lives, and in so doing discriminating against them on grounds of their race and nationality.

The process of allocating cases (by an algorithm) and alerting human caseworkers that an application has been streamed as red, simply on the basis of the applicant‚Äôs nationality (if this is indeed the case) already tells the caseworker the application is a potential refusal before the caseworker has fully considered it. By contrast an application which has been streamed as green by an algorithm tells the caseworker this case is worthy of an approval. This means two almost identical applications, based on the same grounds, are treated and processed differently. The case labelled as red is likely to be treated with more suspicion and subjected to intense and intrusive scrutiny. However, the case which has been given the green light receives less scrutiny and is more likely to be approved.

It doesn‚Äôt take a genius to know which nationalities/races are likely to get the red light and which are likely to get the green light. Some may accept the justification for this and say the reason for making such a distinction on the basis of nationality may be due to the fact that certain applicants from a select few countries may have previously breached immigration laws and thus are considered higher risk, therefore it‚Äôs okay for everyone with these nationalities to be painted with the same brush. This is embarrassing to write let alone say out loud. Why should an applicant who genuinely wishes to visit his or her family in the UK be subjected to a harsher scrutiny and treated differently because previous applicants from his or her country of nationality have overstayed or committed a crime? An applicant should only be held to this standard if he or she has personally breached an immigration law and/or committed a crime themselves in the past. Applying a harsh standard to a completely new applicant simply because their nationality matches that of previous offenders is both unfair and prejudicial. Applicants of all nationalities have the potential to breach immigration rules.

Aside from the increased scrutiny and chance of refusal, the streamlining alone means that the processing times of applications will vary according to, for example, the applicant‚Äôs nationality. It is nearly always the case that all applicants want and need their application to be dealt with as quickly as possible. They all pay the same high Home Office fees, including often for priority processing, and yet receive different service standards based on preconceived applicant characteristics. Treating applicants who have been streamed as red (based on their nationality alone) differently to applicants who have been streamed as green based also on their nationality alone for the same application is without question discriminatory.

Nationality is a protected characteristic under the Equalities Act 2010 and a public authority such as the Home Office ‚Äúmust when making decisions of a strategic nature about how to exercise its functions, have due regard to the desirability of exercising them in a way that is designed to reduce the inequalities of outcome...‚Äù

As mentioned above, if visa applicants are being treated differently because of having been streamed as red by an algorithm, not only does this explain why we have such a high number of UK visa refusals for African visitors to the UK but it also indicates that the Home Office may actually be in breach of the Equalities Act. Consequently, while the use of technology should be appreciated for the remarkable improvement it has brought to our lives, it should not be programmed to encourage racial bias as this is making an already bad situation worse.

In conclusion therefore, the Home Office must be transparent in how the algorithm works and the basis on which it has been designed to stream visa applications. It will be interesting to see if JCWI‚Äôs legal action reveals the full details and basis on which the algorithm works ‚Äì which could for example stream against not only nationality but also sex and age.

Here at Kingsley Napley, we provide all types of immigration advice and assist with various visa applications including those previously refused. If you believe you have been wrongly denied entry to the UK and would like our assistance, please do not hesitate to contact us.. The U.K. government is suspending the use of an algorithm used to stream visa applications after concerns were raised the technology bakes in unconscious bias and racism.

The tool had been the target of a legal challenge. The Joint Council for the Welfare of Immigrants (JCWI) and campaigning law firm Foxglove had asked a court to declare the visa application streaming algorithm unlawful and order a halt to its use, pending a judicial review.

The legal action had not run its full course but appears to have forced the Home Office‚Äôs hand as it has committed to a redesign of the system.

A Home Office spokesperson confirmed to us that from August 7 the algorithm‚Äôs use will be suspended, sending us this statement via email: ‚ÄúWe have been reviewing how the visa application streaming tool operates and will be redesigning our processes to make them even more streamlined and secure.‚Äù

Although the government has not accepted the allegations of bias, writing in a letter to the law firm: ‚ÄúThe fact of the redesign does not mean that the [Secretary of State] accepts the allegations in your claim form [i.e. around unconscious bias and the use of nationality as a criteria in the streaming process].‚Äù

The Home Office letter also claims the department had already moved away from use of the streaming tool ‚Äúin many application types.‚Äù But it adds that it will approach the redesign ‚Äúwith an open mind in considering the concerns you have raised.‚Äù

The redesign is slated to be completed by the autumn, and the Home Office says an interim process will be put in place in the meanwhile, excluding the use of nationality as a sorting criteria.

HUGE news. From this Friday, the Home Office's racist visa algorithm is no more! üíÉüéâ Thanks to our lawsuit (with @JCWI_UK) against this shadowy, computer-driven system for sifting visa applications, the Home Office have agreed to ‚Äúdiscontinue the use of the Streaming Tool‚Äù. ‚Äî Foxglove (@Foxglovelegal) August 4, 2020

The JCWI has claimed a win against what it describes as a ‚Äúshadowy, computer-driven‚Äù people-sifting system ‚Äî writing on its website: ‚ÄúToday‚Äôs win represents the UK‚Äôs first successful court challenge to an algorithmic decision system. We had asked the Court to declare the streaming algorithm unlawful, and to order a halt to its use to assess visa applications, pending a review. The Home Office‚Äôs decision effectively concedes the claim.‚Äù

The department did not respond to a number of questions we put to it regarding the algorithm and its design processes ‚Äî including whether or not it sought legal advice ahead of implementing the technology in order to determine whether it complied with the U.K.‚Äôs Equality Act.

‚ÄúWe do not accept the allegations Joint Council for the Welfare of Immigrants made in their Judicial Review claim and whilst litigation is still on-going it would not be appropriate for the Department to comment any further,‚Äù the Home Office statement added.

The JCWI‚Äôs complaint centered on the use, since 2015, of an algorithm with a ‚Äútraffic-light system‚Äù to grade every entry visa application to the U.K.

‚ÄúThe tool, which the Home Office described as a digital ‚Äòstreaming tool‚Äô, assigns a Red, Amber or Green risk rating to applicants. Once assigned by the algorithm, this rating plays a major role in determining the outcome of the visa application,‚Äù it writes, dubbing the technology ‚Äúracist‚Äù and discriminatory by design, given its treatment of certain nationalities.

‚ÄúThe visa algorithm discriminated on the basis of nationality ‚Äî by design. Applications made by people holding ‚Äòsuspect‚Äô nationalities received a higher risk score. Their applications received intensive scrutiny by Home Office officials, were approached with more scepticism, took longer to determine, and were much more likely to be refused.

‚ÄúWe argued this was racial discrimination and breached the Equality Act 2010,‚Äù it adds. ‚ÄúThe streaming tool was opaque. Aside from admitting the existence of a secret list of suspect nationalities, the Home Office refused to provide meaningful information about the algorithm. It remains unclear what other factors were used to grade applications.‚Äù

Since 2012 the Home Office has openly operated an immigration policy known as the ‚Äúhostile environment‚Äù ‚Äî applying administrative and legislative processes that are intended to make it as hard as possible for people to stay in the U.K.

The policy has led to a number of human rights scandals. (We also covered the impact on the local tech sector by telling the story of one U.K. startup‚Äôs visa nightmare last year.) So applying automation atop an already highly problematic policy does look like a formula for being taken to court.

The JCWI‚Äôs concern around the streaming tool was exactly that it was being used to automate the racism and discrimination many argue underpin the Home Office‚Äôs ‚Äúhostile environment‚Äù policy. In other words, if the policy itself is racist, any algorithm is going to pick up and reflect that.

‚ÄúThe Home Office‚Äôs own independent review of the Windrush scandal, found that it was oblivious to the racist assumptions and systems it operates,‚Äù said Chai Patel, legal policy director of the JCWI, in a statement. ‚ÄúThis streaming tool took decades of institutionally racist practices, such as targeting particular nationalities for immigration raids, and turned them into software. The immigration system needs to be rebuilt from the ground up to monitor for such bias and to root it out.‚Äù

‚ÄúWe‚Äôre delighted the Home Office has seen sense and scrapped the streaming tool. Racist feedback loops meant that what should have been a fair migration process was, in practice, just ‚Äòspeedy boarding for white people.‚Äô What we need is democracy, not government by algorithm,‚Äù added Cori Crider, founder and director of Foxglove. ‚ÄúBefore any further systems get rolled out, let‚Äôs ask experts and the public whether automation is appropriate at all, and how historic biases can be spotted and dug out at the roots.‚Äù

In its letter to Foxglove, the government has committed to undertaking Equality Impact Assessments and Data Protection Impact Assessments for the interim process it will switch to from August 7 ‚Äî when it writes that it will use ‚Äúperson-centric attributes (such as evidence of previous travel,‚Äù to help sift some visa applications, further committing that ‚Äúnationality will not be used.‚Äù

Some types of applications will be removed from the sifting process altogether during this period.

‚ÄúThe intent is that the redesign will be completed as quickly as possible and at the latest by October 30, 2020,‚Äù it adds.

Asked for thoughts on what a legally acceptable visa streaming algorithm might look like, internet law expert Lilian Edwards told TechCrunch: ‚ÄúIt‚Äôs a tough one‚Ä¶ I am not enough of an immigration lawyer to know if the original criteria applied re suspect nationalities would have been illegal by judicial review standard anyway even if not implemented in a sorting algorithm. If yes then clearly a next generation algorithm should aspire only to discriminate on legally acceptable grounds.

‚ÄúThe problem as we all know is that machine learning can reconstruct illegal criteria ‚Äî though there are now well known techniques for evading that.‚Äù

The ethical principles need to apply to the immigration policy, not just the visa algorithm. The problem is the racist immigration system dressed up in false computerised objectivity ‚Äî Javier Ruiz (@javierruiz) August 4, 2020

‚ÄúYou could say the algorithmic system did us a favour by confronting illegal criteria being used which could have remained buried at individual immigration officer informal level. And indeed one argument for such systems used to be ‚Äòconsistency and non-arbitrary‚Äô nature. It‚Äôs a tough one,‚Äù she added.

Earlier this year the Dutch government was ordered to halt use of an algorithmic risk scoring system for predicting the likelihood social security claimants would commit benefits or tax fraud ‚Äî after a local court found it breached human rights law.

In another interesting case, a group of U.K. Uber drives are challenging the legality of the gig platform‚Äôs algorithmic management of them under Europe‚Äôs data protection framework ‚Äî which bakes in data access rights, including provisions attached to legally significant automated decisions.. Home Office lawyers wrote to us yesterday, to respond to the legal challenge which we‚Äôve been working on with the Joint Council for the Welfare of Immigrants (JCWI).

We were asking the Court to declare the streaming algorithm unlawful, and to order a halt to its use to assess visa applications.

Before the case could be heard, the Home Office caved in. They‚Äôve agreed that from this Friday, August 7, they will get rid of the ‚Äòstreaming algorithm.‚Äô

Home Secretary Priti Patel has pledged a full review of the system, including for issues of ‚Äòunconscious bias‚Äô and discrimination.

This marks the end of a computer system which had been used for years to process every visa application to the UK. It‚Äôs great news, because the algorithm entrenched racism and bias into the visa system.

The Home Office kept a secret list of suspect nationalities automatically given a ‚Äòred‚Äô traffic-light risk score ‚Äì people of these nationalities were likely to be denied a visa. It had got so bad that academic and nonprofit organisations told us they no longer even tried to have colleagues from certain countries visit the UK to work with them.

We also discovered that the algorithm suffered from ‚Äúfeedback loop‚Äù problems known to plague many such automated systems ‚Äì where past bias and discrimination, fed into a computer program, reinforce future bias and discrimination. Researchers documented this issue with predictive policing systems in the US and we realised the same problem had crept in here.

It‚Äôs also great news because this was the first successful judicial review of a UK government algorithmic decision-making system.

More and more government departments are talking up the potential for using machine learning and artificial intelligence to aid decisions. Make no mistake: this is where government is heading, from your local council right on up to Number 10.

But at the moment there‚Äôs an alarming lack of transparency about where these tools are being used and an even more alarming lack of safeguards to prevent biased and unfair software ruining people‚Äôs lives.

There‚Äôs been some discussion around correcting for biased algorithms but nowhere near enough debate about giving the public a say in whether they want government by algorithm in the first place. At Foxglove, we believe in democracy ‚Äì not opaque and unaccountable technocracy.

Foxglove exists to challenge such abuses of technology. It‚Äôs a safe bet that this won‚Äôt be the last time we‚Äôll need to challenge a government algorithm in the courts.



Campaigns like this rely on donations. To support Foxglove‚Äôs ongoing work, please click the Donate button below.. . We all deserve to be treated fairly and equally. But people who move to the UK are treated as ‚Äútemporary‚Äù residents for years, even decades, before being able to access basic rights. Maintaining the right to remain in the UK complex and extremely expensive, pushing families into poverty and destitution. A small error, an illness or just bad luck can push someone to lose their status and become vulnerable to hostile immigration policies. Once that happens, the system makes it almost impossible to change course and regain status, safety and support.

We Are Here is a campaign to help break the cycle of insecure immigration status, enabling people to live and thrive as part of our communities, instead of living in prolonged precarity. We are calling for:. Foxglove is supporting the Joint Council for the Welfare of Immigrants (JCWI) to challenge the Home Office‚Äôs use of a secret algorithm to sift visa applications, which it describes as a digital ‚Äústreaming tool‚Äù.

We share JCWI‚Äôs concerns that this shadowy, computer-driven process has the power to affect someone‚Äôs chances of getting a visa and is likely to be doing so in a discriminatory way.

Papers have now been filed in this case seeking a judicial review. This is an important step. It moves us one stage closer to the case being heard by a judge. You can read more about this development in this article in the Guardian newspaper.

Since the case launched in October, we‚Äôve engaged in pre-action correspondence with the Home Office. We still aren‚Äôt convinced that the algorithm doesn‚Äôt spit out bias or discriminatory decisions. We are still worried that a computer program is affecting people‚Äôs right to come here to work, study or see loved ones.

We also continue to be concerned it seems to be discriminating on the basis of crude characteristics like nationality ‚Äì rather than assessing individual applicants fairly. People from rich white countries are getting Speedy Boarding. Poorer people of colour are getting pushed to the back of the queue.

Coronavirus is affecting the legal system and that may mean some delays in the case being heard in court. However, it definitely won‚Äôt be preventing JCWI, with our support, from pursuing this matter to its conclusion.

Families kept apart by this secretive visa algorithm may include key workers upon whom we are all relying during the current pandemic.

It‚Äôs more important than ever to challenge this injustice so that, once the pandemic is over and people are able to travel again, every visa application is assessed transparently and fairly.

JCWI have launched a crowdfunder to cover their legal costs in this case. Please donate by clicking the Donate button below if you can.. It has come to light that the Home Office is using a secretive algorithm, which it describes as digital ‚Äústreaming tool,‚Äù to sift visa applications. So far they have refused to disclose much information about how the algorithm works, hiding behind the ‚ÄúImmigration Exemption‚Äù to the Freedom of Information Act.

We do know that the algorithm scans applications and directs them into a fast lane (Green), a slow lane (Yellow), or a full digital pat-down (Red). It seems clear that people‚Äôs right to come to the UK to work, study, or see loved ones, is being affected by this shadowy and unaccountable use of software by the government.

We‚Äôre concerned that this algorithm could be yet another example of the ‚Äúhostile environment‚Äù policy towards immigrants which brought us ‚ÄúGo Home‚Äù vans and led to the Windrush scandal.

As far as we can tell, the algorithm is using problematic and biased criteria, like nationality, to choose which ‚Äústream‚Äù you get in. People from rich white countries get ‚ÄúSpeedy Boarding‚Äù ‚Äì poorer people of colour get pushed to the back of the queue.

We are seeking a judicial review to fix this misuse of digital technology. We will use the law to find out exactly what the algorithm is and what it does.

So far we have served the Home Office with a ‚Äúpre-action‚Äù letter, and are awaiting their response. We have also launched a crowd-funder to raise money for the legal challenge. Click the Donate box below to pitch in. You can also read more about the case in this Guardian article.