ARTICLE TITLE: UK Visa Streamline Algorithm Allegedly Discriminated Based on Nationality
The U.K. government is suspending the use of an algorithm used to stream visa applications after concerns were raised the technology bakes in unconscious bias and racism.

The tool had been the target of a legal challenge. The Joint Council for the Welfare of Immigrants (JCWI) and campaigning law firm Foxglove had asked a court to declare the visa application streaming algorithm unlawful and order a halt to its use, pending a judicial review.

The legal action had not run its full course but appears to have forced the Home Office’s hand as it has committed to a redesign of the system.

A Home Office spokesperson confirmed to us that from August 7 the algorithm’s use will be suspended, sending us this statement via email: “We have been reviewing how the visa application streaming tool operates and will be redesigning our processes to make them even more streamlined and secure.”

Although the government has not accepted the allegations of bias, writing in a letter to the law firm: “The fact of the redesign does not mean that the [Secretary of State] accepts the allegations in your claim form [i.e. around unconscious bias and the use of nationality as a criteria in the streaming process].”

The Home Office letter also claims the department had already moved away from use of the streaming tool “in many application types.” But it adds that it will approach the redesign “with an open mind in considering the concerns you have raised.”

The redesign is slated to be completed by the autumn, and the Home Office says an interim process will be put in place in the meanwhile, excluding the use of nationality as a sorting criteria.

HUGE news. From this Friday, the Home Office's racist visa algorithm is no more! 💃🎉 Thanks to our lawsuit (with @JCWI_UK) against this shadowy, computer-driven system for sifting visa applications, the Home Office have agreed to “discontinue the use of the Streaming Tool”. — Foxglove (@Foxglovelegal) August 4, 2020

The JCWI has claimed a win against what it describes as a “shadowy, computer-driven” people-sifting system — writing on its website: “Today’s win represents the UK’s first successful court challenge to an algorithmic decision system. We had asked the Court to declare the streaming algorithm unlawful, and to order a halt to its use to assess visa applications, pending a review. The Home Office’s decision effectively concedes the claim.”

The department did not respond to a number of questions we put to it regarding the algorithm and its design processes — including whether or not it sought legal advice ahead of implementing the technology in order to determine whether it complied with the U.K.’s Equality Act.

“We do not accept the allegations Joint Council for the Welfare of Immigrants made in their Judicial Review claim and whilst litigation is still on-going it would not be appropriate for the Department to comment any further,” the Home Office statement added.

The JCWI’s complaint centered on the use, since 2015, of an algorithm with a “traffic-light system” to grade every entry visa application to the U.K.

“The tool, which the Home Office described as a digital ‘streaming tool’, assigns a Red, Amber or Green risk rating to applicants. Once assigned by the algorithm, this rating plays a major role in determining the outcome of the visa application,” it writes, dubbing the technology “racist” and discriminatory by design, given its treatment of certain nationalities.

“The visa algorithm discriminated on the basis of nationality — by design. Applications made by people holding ‘suspect’ nationalities received a higher risk score. Their applications received intensive scrutiny by Home Office officials, were approached with more scepticism, took longer to determine, and were much more likely to be refused.

“We argued this was racial discrimination and breached the Equality Act 2010,” it adds. “The streaming tool was opaque. Aside from admitting the existence of a secret list of suspect nationalities, the Home Office refused to provide meaningful information about the algorithm. It remains unclear what other factors were used to grade applications.”

Since 2012 the Home Office has openly operated an immigration policy known as the “hostile environment” — applying administrative and legislative processes that are intended to make it as hard as possible for people to stay in the U.K.

The policy has led to a number of human rights scandals. (We also covered the impact on the local tech sector by telling the story of one U.K. startup’s visa nightmare last year.) So applying automation atop an already highly problematic policy does look like a formula for being taken to court.

The JCWI’s concern around the streaming tool was exactly that it was being used to automate the racism and discrimination many argue underpin the Home Office’s “hostile environment” policy. In other words, if the policy itself is racist, any algorithm is going to pick up and reflect that.

“The Home Office’s own independent review of the Windrush scandal, found that it was oblivious to the racist assumptions and systems it operates,” said Chai Patel, legal policy director of the JCWI, in a statement. “This streaming tool took decades of institutionally racist practices, such as targeting particular nationalities for immigration raids, and turned them into software. The immigration system needs to be rebuilt from the ground up to monitor for such bias and to root it out.”

“We’re delighted the Home Office has seen sense and scrapped the streaming tool. Racist feedback loops meant that what should have been a fair migration process was, in practice, just ‘speedy boarding for white people.’ What we need is democracy, not government by algorithm,” added Cori Crider, founder and director of Foxglove. “Before any further systems get rolled out, let’s ask experts and the public whether automation is appropriate at all, and how historic biases can be spotted and dug out at the roots.”

In its letter to Foxglove, the government has committed to undertaking Equality Impact Assessments and Data Protection Impact Assessments for the interim process it will switch to from August 7 — when it writes that it will use “person-centric attributes (such as evidence of previous travel,” to help sift some visa applications, further committing that “nationality will not be used.”

Some types of applications will be removed from the sifting process altogether during this period.

“The intent is that the redesign will be completed as quickly as possible and at the latest by October 30, 2020,” it adds.

Asked for thoughts on what a legally acceptable visa streaming algorithm might look like, internet law expert Lilian Edwards told TechCrunch: “It’s a tough one… I am not enough of an immigration lawyer to know if the original criteria applied re suspect nationalities would have been illegal by judicial review standard anyway even if not implemented in a sorting algorithm. If yes then clearly a next generation algorithm should aspire only to discriminate on legally acceptable grounds.

“The problem as we all know is that machine learning can reconstruct illegal criteria — though there are now well known techniques for evading that.”

The ethical principles need to apply to the immigration policy, not just the visa algorithm. The problem is the racist immigration system dressed up in false computerised objectivity — Javier Ruiz (@javierruiz) August 4, 2020

“You could say the algorithmic system did us a favour by confronting illegal criteria being used which could have remained buried at individual immigration officer informal level. And indeed one argument for such systems used to be ‘consistency and non-arbitrary’ nature. It’s a tough one,” she added.

Earlier this year the Dutch government was ordered to halt use of an algorithmic risk scoring system for predicting the likelihood social security claimants would commit benefits or tax fraud — after a local court found it breached human rights law.

In another interesting case, a group of U.K. Uber drives are challenging the legality of the gig platform’s algorithmic management of them under Europe’s data protection framework — which bakes in data access rights, including provisions attached to legally significant automated decisions.. The growth of technology has brought a great deal of efficiency and security to almost all organisations and businesses. But such progress may have taken a slightly wrong turn as the reliance on artificial intelligence by the Home Office as a streaming tool for visa applications may actually be carrying out its functions on grounds of racial bias.

Last week the Guardian reported a legal action has been filed by the Joint Council for the Welfare of Immigrants (JCWI) against the Home Office over the use of an algorithm which filters visa applications on potentially unlawful grounds such as 'nationality'. This follows a report by the Financial Times in June this year that the Home Office was 'secretly' using the algorithm to process visa applications. The Home Office has since confirmed this technology is only used to allocate applications and not decide on them. But why the mystery?

I am among many who would consider the advancement of technology as a blessing to mankind as I, quite frankly cannot imagine a world without smartphones. On the other hand some would consider the advancement of technology as a disaster. Despite this being slightly dramatic I can see why one would come to this conclusion. We are increasingly dependent on technology for almost all everyday tasks. For example, to know directions to a location we would use GPS or for cab services we would resort to the numerous apps on our phones. We no longer memorise directions because Google maps does it for us. In many ways such levels of reliance on technology can be daunting.

The Home Office’s reliance on the algorithm to stream visa applications may have been implemented to better its efficiency and to finally improve on its service standards and visa processing times (which in itself is a topic for another blog). The Home Office is said to have been relying on an algorithm which uses nationality to stream applicants in the categories green, yellow and red. Regardless of the affirmation the Home Office has given to confirm the algorithm is only used to allocate applications and not make decisions, the basis on which these applicants are streamed and their applications allocated is crucial. We cannot be caught up celebrating the technological advancement of our time and ignore an unhealthy reliance on artificial intelligence in dealing with people’s lives, and in so doing discriminating against them on grounds of their race and nationality.

The process of allocating cases (by an algorithm) and alerting human caseworkers that an application has been streamed as red, simply on the basis of the applicant’s nationality (if this is indeed the case) already tells the caseworker the application is a potential refusal before the caseworker has fully considered it. By contrast an application which has been streamed as green by an algorithm tells the caseworker this case is worthy of an approval. This means two almost identical applications, based on the same grounds, are treated and processed differently. The case labelled as red is likely to be treated with more suspicion and subjected to intense and intrusive scrutiny. However, the case which has been given the green light receives less scrutiny and is more likely to be approved.

It doesn’t take a genius to know which nationalities/races are likely to get the red light and which are likely to get the green light. Some may accept the justification for this and say the reason for making such a distinction on the basis of nationality may be due to the fact that certain applicants from a select few countries may have previously breached immigration laws and thus are considered higher risk, therefore it’s okay for everyone with these nationalities to be painted with the same brush. This is embarrassing to write let alone say out loud. Why should an applicant who genuinely wishes to visit his or her family in the UK be subjected to a harsher scrutiny and treated differently because previous applicants from his or her country of nationality have overstayed or committed a crime? An applicant should only be held to this standard if he or she has personally breached an immigration law and/or committed a crime themselves in the past. Applying a harsh standard to a completely new applicant simply because their nationality matches that of previous offenders is both unfair and prejudicial. Applicants of all nationalities have the potential to breach immigration rules.

Aside from the increased scrutiny and chance of refusal, the streamlining alone means that the processing times of applications will vary according to, for example, the applicant’s nationality. It is nearly always the case that all applicants want and need their application to be dealt with as quickly as possible. They all pay the same high Home Office fees, including often for priority processing, and yet receive different service standards based on preconceived applicant characteristics. Treating applicants who have been streamed as red (based on their nationality alone) differently to applicants who have been streamed as green based also on their nationality alone for the same application is without question discriminatory.

Nationality is a protected characteristic under the Equalities Act 2010 and a public authority such as the Home Office “must when making decisions of a strategic nature about how to exercise its functions, have due regard to the desirability of exercising them in a way that is designed to reduce the inequalities of outcome...”

As mentioned above, if visa applicants are being treated differently because of having been streamed as red by an algorithm, not only does this explain why we have such a high number of UK visa refusals for African visitors to the UK but it also indicates that the Home Office may actually be in breach of the Equalities Act. Consequently, while the use of technology should be appreciated for the remarkable improvement it has brought to our lives, it should not be programmed to encourage racial bias as this is making an already bad situation worse.

In conclusion therefore, the Home Office must be transparent in how the algorithm works and the basis on which it has been designed to stream visa applications. It will be interesting to see if JCWI’s legal action reveals the full details and basis on which the algorithm works – which could for example stream against not only nationality but also sex and age.

Here at Kingsley Napley, we provide all types of immigration advice and assist with various visa applications including those previously refused. If you believe you have been wrongly denied entry to the UK and would like our assistance, please do not hesitate to contact us.