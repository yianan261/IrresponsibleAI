ARTICLE TITLE: ChatGPT Abused to Develop Malicious Softwares
The advent of ChatGPT has cybersecurity experts spooked. Some fear the powerful chatbot will make it far easier for non-coders to create malware and become cybercriminals. But so far, one cybersecurity company says, ChatGPT may be having a counterintuitive effect on hacking: supercharging scams that don’t rely on any sort of malicious code at all.

Max Heinemeyer, the chief product officer at the U.K.-based cybersecurity firm Darktrace, says that looking at the one-month period since ChatGPT attained 1 million users in early December, there has been little change in the total number of attempted cyberattacks targeting Darktrace customers. But Darktrace has seen a distinct shift in the tactics used by cybercriminals.

Malicious links in phishing emails declined from 22% of cases to just 14%, Heinemeyer says. But the average linguistic complexity of the phishing emails encountered by Darktrace jumped 17%.

The company’s working theory: Cybercriminals are starting to use ChatGPT to craft much more convincing phishing emails—ones that are so good that cybercriminals don’t even need to rely on embedding malware in attachments or links. After all, malicious links or embedded malware can often be detected and stopped by cybersecurity software such as Darktrace’s.

What’s much harder to stop are attacks that rely completely on old-fashioned deception, or “social engineering.” An email that is so convincingly written that the recipient believes it’s from a trusted source is a great to way pull off an authorized push payments fraud, for example. The victim is fooled into sending funds to pay for what they think is a legitimate transaction or invoice, but is in fact sending the money straight to a fraudster’s account.

In some cases, Heinemeyer says, criminals may be setting the stage for longer cons that involve winning the victims’ trust over a period of time and might involve sophisticated impersonations of real executives or customers.

A gift for non-native English speaking hackers

In addition to A.I. writing tools such as ChatGPT, other new generative A.I. tools could be used to abet such scams. A.I. software, such as that from nascent startup Eleven Labs, can now create realistic voice clones after having been trained on recordings of a target’s voice that might only be a few seconds long. Meanwhile, text-to-image generation software, such as Stable Diffusion, can create increasingly realistic deepfakes with a fraction of the training data previously required for other deepfake methods.

Frauds based on compromised business emails have been on the rise for the past four to five years, Evan Reiser, the founder and CEO of cybersecurity Abnormal says. And while he says that his company has not yet seen any increase in these kinds of attacks since ChatGPT debuted, he thinks it is possible criminals, especially those whose native language is not English, may be tempted to use the tool to craft emails that are less likely to raise red flags with potential victims due to ungrammatical or uncolloquial expressions. “Any tool that is perceived by humans as authentic will make [fraud] worse,” Reiser says.

He says this is especially true of systems where they are explicitly trained to produce text in a particular style, synthesized voices, or images with the intent of fooling people. But he also says that often the simplest tricks—just a very short email that seems to come from a trusted person—works well enough and that criminals generally gravitate towards whatever methods are simplest and require the least effort. “You can send silly, stupid emails and make millions of dollars,” he says. “Why go through the trouble and effort to train [A.I.] models to do that.”

In the wake of the release of ChatGPT, some cybersecurity firms raised the alarm that the A.I. might make it fiendishly easy to pull off a cyberattack. Maya Horowitz, the vice president of research at cybersecurity firm Checkpoint, says that her team was able to get ChatGPT to generate every stage of a cyberattack, starting with a better-than-average phishing email, but then carrying on to actually writing the software code for a malware attack and being able to figure out how to embed that code into an innocuous-looking email attachment. Horowitz said she feared ChatGPT and other generative language models would lead to many more cyberattacks.

But the same kind of large language models that power ChatGPT can also be used to help cybersecurity companies defend against attacks. Abnormal uses some language models, such as Google’s BERT language model, to help determine what the intent of an email is. If an email is aksing a person to pay for something and putting that person under time pressure, saying it is urgent, or needs to be done ASAP, then that could be a red flag, Reiser says. Language models can also read attachments and see if they match the form and style of previous invoices—or if the invoicing company is one that business has interacted with before. It can even see if the account numbers seem to match ones that have been used previously. (Abnormal even analyses things such as whether an email attachment has fonts that match those previously seen from that company and looks at the meta data of documents for potential signals that something fishy is going on, Reiser says.)

Much of what Abnormal does though is look at patterns across a huge number of features and use machine learning models to figure out if they rise to the threshold where the email should be blocked and a company’s security team alerted. There’s almost always something that will give away a phishing attempt if you know where to look, Reiser says. Even in the case where a legitimate business email account has been compromised, the attacker will often take actions, such as running multiple searches through the account’s history, or using an API to control the account rather than a PC keyboard, that will provide a signal that something isn’t right.

Nicole Eagan, Darktrace’s chief strategy officer, says Darktrace itself has been using the same kind of large language models that underpin to ChatGPT to create more believable spear phishing emails that the compay uses in internal “red teaming” excercises to test its own cybersecurity practices. Eagan says she recently fell for one of these, which was inserted directly into the actual email chain she was having with an outside recruiter Darktrace used.

(Darktrace spent much of the past week trying to prove a different sort of pattern didn’t indicate anything fishy was going on: the company’s share price dropped dramatically after short seller Quintessential Capital Management issued a report claiming it had found evidence that the cybersecurity company might have engaged in dubious accounting practices to try inflate its revenues and profitability ahead of its 2021 initial public offering. Darktrace has denied the accusations in the report, saying that the hedge fund never contacted it before publishing its report and that it has “full confidence” in its accounting practices and the “integrity of our independently audited financial statements.”)

. For the record, ChatGPT didn't write me a phishing email when I asked it to. In fact, it gave me a very stern lecture about why phishing is bad.

It called phishing a "malicious and illegal activity that aims to deceive individuals into providing sensitive information such as passwords, credit card numbers, and personal details," adding that as an AI language model, it's programmed to "avoid engaging in activities that may harm individuals or cause harm to the public."

That said, the free artificial intelligence tool that's taken the world by storm didn't have a problem writing a very convincing tech support note addressed to my editor asking him to immediately download and install an included update to his computer's operating system. (I didn't actually send it, for fear of invoking the wrath of my company's IT department.)

But the exercise was a potent illustration of how ChatGPT -- and artificial intelligence in general -- could be a boon to scammers and hackers overseas whose phishing attempts may have previously been outed by their grammatical errors or broken English. Experts say these AI-generated emails are also more likely to make it past security software email filters.

The security dangers go well beyond phishing. According to researchers at the cybersecurity company Check Point, cybercriminals are trying to use ChatGPT to write malware, while other experts say that non-malicious code written with it might be lower quality than code created by a human being, making it susceptible to exploitation.

But, experts say, don't blame the AI.

"It's not good or evil," said Randy Lariar, practice director of big data, AI and analytics for the cybersecurity company Optiv. "It's just a tool that makes it easier and less expensive for good guys and bad guys to do the things they're already doing."

An AI arms race

While cybersecurity companies have long touted AI and machine learning as a game-changing way to boost automated online defenses and help fill gaps in the industry's workforce, the increased availability of this kind of technology through tools like ChatGPT will only make it easier for criminals to launch more cyberattacks.

Some experts also have concerns about the possible data privacy implications, both for companies and the average consumer. While the data that ChatGPT's AI was trained with is generally considered public and available online, it's arguably a lot more accessible than it used to be.

In addition, users of the technology will need to be careful what information they feed the AI, because once they do, it will become part of ChatGPT's massive database, and they'll have little or no control over who it's shared with or how it's used after that.

OpenAI, the company behind ChatGPT, didn't respond to an email seeking comment.

While there are guardrails built in to prevent cybercriminals from using the ChatGPT for nefarious purposes, they're far from foolproof. A request to write a letter asking for financial help to flee Ukraine was flagged as a scam and denied, as was a request for a romantic rendezvous. But I was able to use ChatGPT to write a fake letter to my editor informing him that he had won the New York State Lottery jackpot.

Experts warn that ChatGPT isn't always accurate, and even Sam Altman, CEO of OpenAI, which created the chatbot, said it shouldn't be used for "anything important right now." But cybercriminals generally aren't worried about their code being 100% perfect, so accuracy problems are a nonissue for them, according to Jose Lopez, principal data scientist and machine learning engineer for the email security company Mimecast,

What they are concerned about is speed and productivity.

"If you're an average coder, it's not going to transform you into a super hacker overnight," Lopez said. "Its main potential is as an amplifier."

That amplification could mean cybercriminals using the technology to set up countless online chats with unsuspecting victims, looking to lure them into romance scams, he said. These kinds of scams aren't uncommon, but they've traditionally required a lot of work and hands-on attention from scammers. AI-powered chatbots could change that.

Be careful with your data

While data privacy concerns over AI aren't new -- the debate over the use of AI technology in facial recognition technology has raged for years -- the explosive popularity of ChatGPT has renewed the need to remind people about the personal data traps they can fall in.

Worries about language models like ChatGPT might not be as obvious, but they're just as significant, said John Gilmore, head of research for Abine, which owns DeleteMe, a service that helps people remove information from databases.

Gilmore noted that users don't have any rights when it comes to what ChatGPT does with the data it collects from them or who it shares it with. He questioned how ChatGPT could ever be compliant with data privacy laws like the EU's General Data Protection Regulation, better known as GDPR, given the lack of transparency tools.

As the use of AI spreads to other tech where their use may be less obvious, Gilmore said it will be up to consumers to keep a handle on what data they hand over.

Confidential or proprietary information, for instance, should never be entered into AI apps or sites and that includes requests for help with things like job applications or legal forms.

"While it may be tempting to receive AI-based advice for short-term benefit, you should be aware that in the process you are giving that content away to everyone," Gilmore said.

Optiv's Lariar said that given the newness of the AI language models, there's still a lot to be decided when it comes to legality and consumer rights. He compared language-based AI platforms to the rise of the streaming video and music industries, predicting that there will be a slew of lawsuits filed before everything is sorted out.

In the meantime, language-based AI isn't going to go away. As for how to protect against those who would use it for cybercrimes, Lariar said like everything in security, it starts with the basics.

"This is a wake-up call to everyone who is not investing in their security programs as they should be," he said. "The barrier to entry is getting lower and lower and lower to be hacked and to be phished. AI is just going to increase the volume."

Editors' note: CNET is using an AI engine to create some personal finance explainers that are edited and fact-checked by our editors. For more, see this post.