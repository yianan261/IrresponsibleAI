A key thing to understand here is that the question is not, as some have suggested in the comments, whether any AI model can produce correct code. It's whether this one can be trusted to do so. The answer to that question is an unqualified "NO". GPT-3 is a language model. Language models are an essential part of tools like automatic translators; they tell us how probable it is that any given sentence is a valid English (or whatever language) sentence written as a native speaker would1, which lets us favor translations that are idiomatic over ones that just translate individual words without considering how the sentence flows. The systems can be trivially modified to generate text, if instead of looking up the word you have in the probability distribution it provides, you instead select the next word according to that distribution, which is how these chat bots work.

Because the goal is to produce output that looks like native English text, the models are trained to assign high probabilities to existing text samples, and evaluated based on how well they predict other (previously unseen) samples. Which, for a language model, is a fine objective function. It will favor models that produce syntactically correct text, use common idioms over semantically similar but uncommon phrases, don't shift topics too often, etc. Some level of actual understanding does exist in these models2, but it's on the level of knowing that two words or phrases have similar meanings, or that certain parts of a paragraph relate to each other. There is understanding, but no capacity for reasoning.

Correctness will tend to increase the score, insofar as correct answers are somewhat more likely to appear in the training data than any particular incorrect answer (there might be more wrong answers overall, but the probability mass will be distributed amongst the various classes of wrong answer instead of concentrated in one region of semantic space like it is for the correct one), but this is a side-effect of trying to look like common text. If you have a question for which there is a commonly held false belief or an answer that can be constructed out of common idioms and otherwise excellent grammar, the model is quite likely to report those instead of the real answer, because semantic correctness is not what a language model is trained for.

Trying to use a language model to generate code is like trying to use a submarine to fly to the moon. That's not what it's for; why are you trying to use it for that? Stop doing that. But at the same time, arguing that the submarine is bad at flying is rather missing the point. Nobody who actually understands NLP is claiming otherwise.3

There do exist systems that are designed to produce code, and trained to optimize correctness. (e.g. Genetic Programming). That's a bit too far outside my area of expertise for me to make any claims as to where the state of the art is on those, so I'm not sure whether answers generated by them should be allowed or not. But if you were to use an AI tool to generate code, that's the sort of thing you should be looking at; they're designed for the task. Similarly, you could ask if language models could be used as a tool to edit questions you've written by hand, perhaps to check the grammar or recommend new ways to phrase answers so they flow better. They'd be fairly good at that sort of thing (probably. I haven't used any of those tools myself (the rambling, stream-of-consciousness answer might have given that away), but the math supports the idea that they should work4). Translation is another task where (similar) systems work fairly well. (Machine translations still aren't perfect, but they're much better than they were 10 years ago, and improvement in language models is a big part of that.) Just always be aware of what tool you're using, and whether it's the right one for the job.

1 More formally, it gives the probability that a uniformly randomly selected English sentence of a specific length would be this one, but that gives the same ordering over sentences as long as we make some fairly reasonable assumptions.

2 Where "understands" is shorthand for "encodes the information in such a way that it can condition its decisions (i.e. probability distribution functions) upon it"

3 Well, not many. There'll always be a few who get caught up in the hype. They shouldn't.

4 If trained on well-written text. Stack Overflow, the go-to question-and-answer site for coders and programmers, has temporarily banned users from sharing responses generated by AI chatbot ChatGPT.

The site’s mods said that the ban was temporary and that a final ruling would be made some time in the future after consultation with its community. But, as the mods explained, ChatGPT simply makes it too easy for users to generate responses and flood the site with answers that seem correct at first glance but are often wrong on close examination.

“The primary problem is [...] the answers which ChatGPT produces have a high rate of being incorrect.”

“The primary problem is that while the answers which ChatGPT produces have a high rate of being incorrect, they typically look like they might be good and the answers are very easy to produce,” wrote the mods (emphasis theirs). “As such, we need the volume of these posts to reduce [...] So, for now, the use of ChatGPT to create posts here on Stack Overflow is not permitted. If a user is believed to have used ChatGPT after this temporary policy is posted, sanctions will be imposed to prevent users from continuing to post such content, even if the posts would otherwise be acceptable.”

ChatGPT is an experimental chatbot created by OpenAI and based on its autocomplete text generator GPT-3.5. A web demo for the bot was released last week and has since been enthusiastically embraced by users around the web. The bot’s interface encourages people to ask questions and in return offers impressive and fluid results across a range of queries; from generating poems, songs, and TV scripts, to answering trivia questions and writing and debugging lines of code.

But while many users have been impressed by ChatGPT’s capabilities, others have noted its persistent tendency to generate plausible but false responses. Ask the bot to write a biography of a public figure, for example, and it may well insert incorrect biographical data with complete confidence. Ask it to explain how to program software for a specific function and it can similarly produce believable but ultimately incorrect code.

AI text models like ChatGPT learn by looking for statistical regularities in text

This is one of several well-known failings of AI text generation models, otherwise known as large language models or LLMs. These systems are trained by analyzing patterns in huge reams of text scraped from the web. They look for statistical regularities in this data and use these to predict what words should come next in any given sentence. This means, though, that they lack hard-coded rules for how certain systems in the world operate, leading to their propensity to generate “fluent bullshit.”

Given the huge scale of these systems, it’s impossible to say with certainty what percentage of their output is false. But in Stack Overflow’s case, the company has judged for now that the risk of misleading users is just too high.

Stack Overflow’s decision is particularly notable as experts in the AI community are currently debating the potential threat posed by these large language models. Yann LeCun, chief AI scientist at Facebook-parent Meta, has argued, for example, that while LLMs can certainly generate bad output like misinformation, they don’t make the actual sharing of this text any easier, which is what causes harm. But others say the ability of these systems to generate text cheaply at scale necessarily increases the risk that it is later shared.

To date, there’s been little evidence of the harmful effects of LLMs in the real world. But these recent events at Stack Overflow support the argument that the scale of these systems does indeed create new challenges. The site’s mods say as much in announcing the ban on ChatGPT, noting that the “volume of these [AI-generated] answers (thousands) and the fact that the answers often require a detailed read by someone with at least some subject matter expertise in order to determine that the answer is actually bad has effectively swamped our volunteer-based quality curation infrastructure.”

The worry is that this pattern could be repeated on other platforms, with a flood of AI content drowning out the voices of real users with plausible but incorrect data. Exactly how this could play out in different domains around the web, though, would depend on the exact nature of the platform and its moderation capabilities. Whether or not these problems can be mitigated in the future using tools like improved spam filters remains to be seen.

“The scary part was just how confidently incorrect it was.”

Meanwhile, responses to Stack Overflow’s policy announcement on the site’s own discussion boards and on related forums like Hacker News have been broadly supportive, with users adding the caveat that it may be difficult for Stack Overflow’s mods to identify AI-generated answers in the first place.

Many users have recounted their own experiences using the bot, with one individual on Hacker News saying they found that its answers to queries about coding problems were more often wrong than right. “The scary part was just how confidently incorrect it was,” said the user. “The text looked very good, but there were big errors in there.”. Listen to this story

Unfounded assumptions, bad advice, incorrect information—the biggest source of problems on the internet today is people blindly buying into hype. ChatGPT, which has taken the internet by the storm recently, seems to be making it a lot easier for people to add to the chaos.

By now, ChatGPT has been synonymous with human-like exhaustive responses to questions, including how to draft a contract, to create a code, or even a movie script. The temporarily free chatbot is already changing the way people search for information by answering intricate questions. Sometimes, the understanding capabilities of ChatGPT makes you second-guess if you are actually talking to a human.

So then, why are people bashing ChatGPT? But, before we do that, let’s look at some of the positive aspects of this revolutionary chatbot.

Primary among ChatGPT’s unique characteristics is memory. The bot can remember what was said earlier in the conversation and recount it to the user. This itself sets it apart from other competing natural language solutions, which are still solving for memory as they progress on a query-by-query basis.

While it overcomes some of the issues that plagued the past chatbots, which include hateful or racist responses, it is also giving rise to questions about how the user can differentiate between the bot’s content and the human intervention through language. This is because ChatGPT’s text is able to achieve the feel of a truthful response even if it’s not based on facts.

Now, let the bashing begin!

Answers are inaccurate

Recently, Stack Overflow, the popular programming forum, banned all answers created by ChatGPT citing a high degree of inaccuracy in the bot’s responses. While it clarified that it was a temporary policy, it did reiterate that the problem not only lies in the inaccuracy of ChatGPTs answers, but deeper in the way the bot phrases its answers.

Because of the nature of LLMs, particularly GPT-3.5—which has been used to build ChatGPT—it can not only generate grammatically correct sentences with a formal tone but it also makes them sound authoritative and forceful.

Stack Overflow said: “[ChatGPT’s answers] typically look like they might be good and the answers are very easy to produce. There are also many people trying out ChatGPT to create answers, without the expertise or willingness to verify that the answer is correct prior to posting.”

Gives unverified information

On Twitter, various users, including celebrated AI expert Andrew Ng, posted on the vagueness of responses given to certain specific questions. In some instances, people even posted wrong answers given by the chatbot which could mislead its users.

There were also instances wherein users posted that though the chatbot does not respond to controversial or political questions—it does produce politically incorrect jokes.

Gary Marcus, New York University professor emeritus, has been sharing a host of examples on Twitter of instances of incorrect information by the chatbot wherein it is limited in its ability to leverage facts.

Author John Warner argues that the chatbot makes up information citing an instance wherein it gave him a list of articles that did not exist.

Assertions backed by fake quotes

When the chatbot is used to write a basic news story on the quarterly earnings of a tech major, the chatbot produces a trustable replication of the company’s financial results, areas where it saw a rise in revenue and areas of potential growth. To make it appear more authentic, it even supplements the article with a quote from the firm’s CEO.

This is because these language models have learned that news stories are always backed with a quote and data. Thus, the chatbot replicates this behaviour too.

No citations and non-existent references

Despite a detailed response which appears credible and mimics a human conversation complete with exhaustive information—ChatGPT fails to reveal or list its sources, raising an alarming question of verified and practical information.

Further, when it does make such information available, it gives non-existent references. Take the case of this user who said that every one of the references provided by the chatbot in response to his query seeking references that dealt with mathematical properties of lists did not exist!

Brilliantly dumb

Gary Marcus in his article, ‘The Road to AI We can Trust’, points out that GPT’s knowledge is in part about specific properties of entities and it is not able to completely master abstract relationships.

Take, for instance, if you ask the Chatbot for an article on a specific area (Jayanagar) in Bengaluru, it randomly writes about a resident welfare association in the area which is known for its citizen initiatives. It does not go into the history of the area, its size or other relevant aspects.

How is it any different from Google Search?

A lot of people today are calling ChatGPT the ‘New Google’ or ‘Google Killer’, which to a certain extent, holds true—particularly in the case of showing bizarre suggestions. Google, for example, suggests cancer as a response for any and every symptom, even when you query-in for a stomach ache. But, in the case of ChatGPT, it doesn’t reflect such suggestions but sugar-coats misinformation. In other words, we can say that ‘ChatGPT is a glorified-version of Google Search’, only much better.