ARTICLE TITLE: Suicide Clips Evaded TikTok's Automated Moderation in Coordinated Attack
SICKOS EXPOSED Ronnie McNutt suicide TikTok video that ‘traumatised’ kids was ‘coordinated attack plotted on the dark web’
TIKTOK says a gruesome video of a man committing suicide that went viral on its platform was spread deliberately by a group of sickos working together.

The company found evidence of a "coordinated attack" when it investigated why the video was suddenly appearing on the popular short-video sharing app, a TikTok executive told a British parliamentary committee Tuesday.
TikTok scrambled earlier this month to remove clips of the man shooting himself with a gun, which parents said left their kids "traumatised."

The distressing incident raised concerns about the platform's ability to stop harmful content from reaching its users, many of whom are teens.

Theo Bertram, TikTok's European director of public policy, said there was a huge spike in the number of clips uploaded to TikTok about a week after the original video was livestreamed on Facebook.

"There's evidence of a coordinated attack," Bertrand said.
"Through our investigations, we learned that groups operating on the dark web made plans to raid social media platforms including TikTok, in order to spread the video across the internet.

"What we saw was a group of users who were repeatedly attempting to upload the video to our platform."

The dark web is a part of the internet accessible only through anonymity-providing software.

The users were "splicing it, editing it and cutting it in different ways" and then making new accounts to help spread it, he said.

TikTok users usually look through their own feed or use hashtags to find videos.

These users were clicking on account profiles, apparently anticipating that they would be posting the suicide clip, which is an unusual way to find videos, Bertram said. He gave few other details.

The company wrote Monday to nine other tech platforms proposing that they warn each other about violent and graphic content on their own services.
Bertram's comments came as TikTok said in its latest transparency report that it took down 104.5million videos for violating its guidelines or terms of service during the first six months of the year.

That's less than 1 per cent of the total number of videos uploaded for that period.

"Caring" man Ronnie McNutt, 33, broadcast his tragic death from his home last month during a Facebook live-stream that was quickly taken down.

However, sick netizens continued to upload versions of the horrific clip to various apps and websites days after the August 31 stream.

Reports at the time suggested the video appeared on TikTok's "For You" trending homepage.

That means it could have been viewed by millions of people.

In some cases, the most gruesome part of the video was hidden inside more innocuous looking TikToks.

Many horrified app users said they saw the footage unintentionally, and warned others to be wary.

Writing on Facebook, one said: "So horrible fly high Ronnie.
"I wish I can unsee the video but I cannot unfortunately I can't and unfortunately it's permanently in me and my daughter's minds she is so traumatized from the video she accidentally clicked on a link on Tik Tok."

A Twitter user said: "My 15 year old daughter was sent a video of Ronnie Mcnutt committing suicide on a Facebook livestream.

"I hate she watched it. My prayers go out to his family & friends. RIP RONNIE #SuicidePrevention."

Another added: "My pre-teen son just showed me the Ronnie mcnutt video on YouTube.

"Pls start banning any related content."

TikTok took down versions of the clip as they were uploaded and began banning anyone who shared it.

Bosses at TikTok are banning anyone who re-uploads it.

A TikTok spokesperson said: "On Sunday night, clips of a suicide that had been livestreamed on Facebook circulated on other platforms, including TikTok.

"Our systems have been automatically detecting and flagging these clips for violating our policies against content that displays, praises, glorifies, or promotes suicide.

"We are banning accounts that repeatedly try to upload clips, and we appreciate our community members who've reported content and warned others against watching, engaging, or sharing such videos on any platform out of respect for the person and their family.

"If anyone in our community is struggling with thoughts of suicide or concerned about someone who is, we encourage them to seek support, and we provide access to hotlines directly from our app and in our Safety Centre."