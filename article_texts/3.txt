By Stanley Widianto, Ashley Halsey III and Aaron Gregg | Washington Post

JAKARTA, Indonesia – A malfunctioning sensor and an automated response from the aircraft’s software stymied pilots’ efforts to control a doomed Indonesian flight that went careening into the sea last month, according to a preliminary investigative report released Wednesday.
The report, which stops short of determining the cause of the crash or analyzing findings, chronicles the chaotic moments on the Lion Air flight before it crashed into the waters off the coast of Java last month, killing all 189 passengers and crew on board.

It details how sensors and other equipment were checked and fixed before the aircraft’s final flight, but not the “angle of attack” sensor. That measures where the nose is pointing and was showing erroneous readings throughout the short time the plane was airborne.

With the sensor insisting the nose was too high, an automatic feature kicked in, sending the plane plummeting as the pilots wrestled to regain control. Unable to trust their readings, the pilots resorted to asking air traffic control about their speed and altitude.

Lion Air Flight 610 plunged into the Java Sea on Oct. 29 just after taking off from the Indonesian capital, Jakarta, killing the eight crew members and 181 passengers on board, including a child and two infants.

The crash appears to have been caused by a mix of brand new technology and cockpit confusion as the pilots fought to gain altitude after an early-morning takeoff from Jakarta. The flight crew – at an altitude of just 5,000 feet – had very little time to resolve the issue before the plane crashed into the sea at a reported 450 miles per hour.

Though the report contains no conclusion assigning blame, its descriptions of automated systems overtaking the aircraft – leaving pilots both confused and powerless – poses questions for Boeing and Lion Air about whether the cockpit crew was prepared for this scenario. After the Lion Air crash, pilots in the United States accused Boeing of withholding safety information on its new 737 model.

The aircraft’s pilots asked to return to Jakarta just two minutes after takeoff, reporting a “flight-control problem” but not specifying what it was.

Black-box data released by Indonesian investigators showed that the pilots were pulling back on the control column, attempting to raise the plane’s nose, with almost 100 pounds of pressure before they crashed.

The Indonesian National Transportation Safety Committee, which produced the report, also said that Lion Air, a Jakarta-based low-cost airline, should improve its “safety culture.”

No engineer briefed the pilots of the crashed plane on the multiple problems the aircraft experienced on previous flights, and it was up to him to review the maintenance logs.

The report, however, contains no conclusion on who was at fault.

“When it comes to faulting, I don’t know. Our job isn’t to find faults,” National Transportation Safety Committee investigator Nurcahyo Utomo said at a news conference Wednesday.

The aircraft was the most recent incarnation of the venerable Boeing 737, a plane that first flew in 1967 and has gone through multiple iterations before it emerged as the 737 Max.

The 737 Max was equipped with more-powerful engines that are mounted farther forward on the wing, requiring that additional software be added to the autopilot to provide more control.

That software, which has been described as several lines of coding, was identified in the Boeing manual as the maneuvering characteristics augmentation system, or MCAS.

When the sensors transmitted faulty data to the cockpit of Flight 610, the new MCAS system sensed a stall – that point at which planes do not have enough airspeed to create lift – and sought to correct for it by repeatedly pointing the nose of the aircraft down.

A feature in previous 737 models that allowed pilots to manually override an “electric trimming” process – which automatically budges the nose downward to prevent a stall, does not work in Boeing’s 737 Max 8 planes, Boeing explained in a Nov. 7 bulletin.

That same week, the Federal Aviation Administration issued an emergency notice to all airlines that fly the 737 Max, warning them that erroneous sensor inputs “could cause the flight crew to have difficulty controlling the airplane,” leading to “possible impact with terrain.”

The deviation probably was caused by what is called a “runaway stabilizer.” Stabilizers are essentially those small wings on either side at the tail end of the plane. They each have flaps – called elevators – that help control the elevation of the plane.

In case of a runaway stabilizer, pilots are instructed in the cockpit checklist to hold the control column firmly, disengaging the autopilot that, in this case, contained the MCAS program. Next, they are told, disengage the auto throttle and manually fly the plane.

“This corner of the performance charts is called the ‘coffin corner,’ ” said Mary Schiavo, an aviation lawyer and former inspector general of the U.S. Transportation Department, “and good pilot training teaches you how to get out of coffin corner, but did these pilots realize the plane itself was putting them in coffin corner? Apparently not.”

It is not clear whether the pilots attempted the runaway stabilizer procedure.

Unions representing pilots at Southwest and American airlines said they were not properly informed about the new system during training.

“We did not know this was on the Max models,” Southwest Airlines Pilots Association President Jon Weaks told The Washington Post in a Nov. 13 interview, referring to a new automated flight-control feature.

The prospect that a runaway autopilot system could have contributed to the crash has already been the subject of at least one lawsuit implicating Boeing, with the father of a Lion Air Flight 610 crash victim filing suit recently.

Soerjanto Tjahjono, who heads Indonesia’s National Transportation Safety Committee, said last week that the plane’s black box showed that “the technical problem was the airspeed, or the speed of the plane.” (The plane’s cockpit voice recorder has not been recovered.)

“There were four flights that experienced problems with the airspeed indicator,” Tjahjono said. The angle-of-attack sensor contributes to the airspeed readings.

In testimony before the Indonesian parliament last week, Utomo, the investigator, said that the anti-stall system had activated on the plane as it flew into Jakarta the night before the crash but that the pilots managed to shut it off.

At the news conference Wednesday, Utomo said the plane, on both the doomed flight and the previous flight from Bali to Jakarta, had experienced a stick shaker – “a warning that showed that the plane was going to stall,” he said.

The committee report said differing data between the sensors appeared rectified by cleaning an electrical plug the night before the crash, and a “test on the ground found the problem had been solved.”

But it was not, the report concludes, because when the plane took off shortly after 6 a.m. the following morning, the two flight-speed sensors did not agree on the aircraft’s speed.

The 737 Max is the most popular plane in Boeing history, with 453 delivered so far and 4,671 on order. It is flown or is on order by close to 40 airlines, with Lion Air in the process of receiving more than 200 of the jets.

Boeing said Tuesday that it “continues to work closely with the U.S. National Transportation Safety Board as technical advisers to support the ongoing investigation” by the Indonesian authorities.

The flights of Indonesia’s airlines to U.S. destinations were banned in the decade before 2016 because their safety record was considered abysmal by U.S. standards. The crash of the Lion Air flight was the worst in Indonesia since 1997, when 234 people died on the national airline Garuda in North Sumatra.

Halsey and Gregg reported from Washington. Shibani Mahtani in Hong Kong contributed to this report.

A previous version of this story incorrectly described the general reasons for a plane to stall during flight.. Photo by Gary Lopater on Unsplash

Present day Air travel is one of the safest modes of travel. Statistics from the US Department of Transportation show that in 2007 and 2016 there were 11 fatalities per trillion miles of commercial air travel. This is in stark contrasts to the 7,864 fatalities per trillion miles of travel on the highway ( You can check the statistics here: fatalities and miles of travel per mode of transport). Incremental improvements to air travel is a marvel of technical innovation. However, when an aircraft accident does occur, we are forced to take notice due to the magnitude of a single event.

Air travel today is at a level of technical maturity that when a plane crashes by accident (i.e not due to man-made causes like terrorism or misfiring of missiles), then it is surprisingly due not to pilot error or physical equipment failure but rather because of a computer error. That is, an aircraft accident is caused by a software bug.

Everyone today is intimately familiar with software bugs. Microsoft blue screen of death and the use of ctrl-alt-delete have been burned into our experiences. Even in better designed operating systems that we find in smartphones, it’s is not uncommon to force a reboot. This is much less common that we often have to look up the procedure, but it does happen nevertheless.

Software is notoriously difficult to make bug free. It is the nature of the beast. This is because, to build bug-free software systems, we need to explicitly list all the scenarios that can go wrong and how, and then test our software for those conditions. Unfortunately, that list tends to be unbounded if our designs don’t restrict the scope of a software’s applicability. In short, software developers are able to manage the unbounded complexity by narrowing the scope of applicability. That is why even the most sophisticated “artificial intelligent” applications work well in the most narrow of areas. It is very easy to get frustrated by the limitations of voice assistants like Alexa. That’s because AI technology has not reached the level of maturity that is required for open-ended general conversation. In short, bug-free depends fundamentally on a narrow scope of application and extensive testing within this narrow scope.

As we build more sophisticated software that has higher degrees of complexity, we need to understand the scope of an application and an ever-increasing scope demands more on the testing of these systems. Thus to understand this complexity better, we need to understand the kinds of automation we are building.

As I mentioned earlier, the USDOT shows that there were over 37,000 fatalities in highway accidents in 2017 alone. Thus it makes logical sense to understand how automation affects the safety of road vehicles. The Society of Automation Engineering (SAE) has an international standard which defines six levels of driving automation (SAE J3016). This is a useful framework for classifying the levels of automation in domains outside that of cars. A broader prescription is as follows:

Level 0 (Manual Process)

The absence of any automation.

Level 1 (Attended Process)

Users are aware of the initiation and completion of the performance of each automated task. The user may undo a task in the event of incorrect execution. Users, however, are responsible for the correct sequencing of tasks.

Level 2 (Attended Multiple Processes)

Users are aware of the initiation and completion of a composite of tasks. The user, however, is not responsible for the correct sequencing of tasks. An example will be the booking of a hotel, car, and flight. The exact ordering of the booking may not be a concern of the user. However, failure of the performance of this task may require more extensive manual remedial actions. An unfortunate example of a failed remedial action is the re-accommodation of United Airlines’ paying customer.

Level 3 (Unattended Process)

Users are only notified in exceptional situations and are required to do the work in these conditions. An example of this is in systems that continuously monitor the security of a network. Practitioners take action depending on the severity of an event.

Level 4 (Intelligent Process)

Users are responsible for defining the end goals of automation, however, all aspects of the process execution, as well as the handling of in-flight exceptional conditions, are handled by the automation. The automation is capable of performing appropriate compensating action in events of in-flight failure. The user however is still responsible for identifying the specific context in which automation can be safely applied to.

Level 5 (Fully Automated Process)

This is a final and future state where human involvement is no longer required in the processes. This, of course, may not be the final level because it does not assume that the process is capable of optimizing itself to make improvements.

Level 6 (Self Optimizing Process)

This is automation that requires no human involvement and is also capable of improving itself over time. This level goes beyond the SAE requirements but may be required in certain high-performance competitive environments such as Robocar races and stock trading.

The automobiles of today have extremely sophisticated software that controls many parts of the functioning of the system. This software works at many levels and at each level the risks are different. Some software works at an extremely narrow scope that we are unaware that it is operating. So for example, a car’s fuel injection system is, in fact, fully automated. We can say this about many of the functions of a car that deals with its engine performance. So for example, many car enthusiasts buy programmers and chips that provide after-market tweaks on a car’s performance characteristics. Failure of any of these kinds of systems can still be fatal. SAE’s standards described above however apply to driving automation and not engine automation. There is a stark difference in automation that affect steering and automation that maintains the smooth running of engines.

Automation such as traction control or car stabilization does affect steering. These are engaged in exceptional narrow conditions to ensure greater passenger safety. Controlled behavior is injected in a situation so that a driver can gain better control of a vehicle that he otherwise could not have done so himself. In this context, a driver is actually momentarily not controlling the car.

There have been many cases of planes falling from the skies due to software bugs. My earliest memory of this kind of a catastrophe is Lauda Air Flight 004 in May 1991. This is when one of the engines reverse trusters engaged in mid-flight forcing the plane to spiral out of control and crash. There was no official conclusion as to the cause, however, the aviation writer Macarthur Job said that “had that Boeing 767 been of an earlier version of the type, fitted with engines that were controlled mechanically rather than electronically, then that accident could not have happened.”

More recently, there is the case of Air France 447 in 2009. The official conclusion was that there was a “temporary inconsistency between the measured speeds, likely as a result of the obstruction of the pitot tubes by ice crystals, causing autopilot disconnection and reconfiguration.” The verdict was that the human pilots were eventually part of the fault due to their inability to react appropriately to the anomalous situation. To say this differently, the pilots received incorrect information from the instrumentation and thus took inappropriate action to stabilize the plane.

There are other cases of computer caused failures. Qantas flight 72, it was determined that the CPU of the air data inertial reference unit (ADIRU) corrupted the angle of attack (AOA) data. Malaysia Air 124 that plunged 200 feet in midflight. The instrumentation displayed that the plane was “going too fast and too slow simultaneously”.

In general, it is the responsibility for the pilots to properly perform compensating actions in the case of equipment failure (known as alternate law). The point though is that computer error due to equipment failure should be no different from regular equipment error and it is the responsibility of the pilots to take appropriate measures. Typically, on equipment error, the autopilot is disengaged and the plane is to be flown manually. This is Level 3 (unattended process) automation where the scope when automation is in play is explicit. In Level 3, a pilot is made aware of an exceptional condition and takes manual control of the plane.

In Level 4 (intelligent process), a pilot must be able to recognize the exceptional condition and is able to specify when automation is applicable. Today, we have self-driving cars that are deployed in narrow applications. We have cars that can self-park and we have cars that can drive in good weather conditions on the highway. These are Level 4 automation where is up to the judgment of the driver to engage the automation. Autopilot in planes are Level 4 automation and is engaged in contexts of low complexity.

Then there is the case of Boeing’s 737 Max 8’s MCAS. This I will argue is a Level 5 automation, this is a fully automated process wherein it is expected to function in all scenarios. Like electronics that control engine performance, fully automated processes aren’t generally problematic, however, when you involve driving (or steering for planes) then it opens up the question of the maturity of this level of automation.

Airbus has what is called ‘Alpha Protection’:

“Alpha protection” software is built into every Airbus aircraft to prevent the aircraft from exceeding its performance limits in angle of attack, and kicks in automatically when those limits are reached.

From the definition, Alpha protection is automation that is always measuring, however, it isn’t always active. It is like a speed limiter that exits in cars today, it is constantly measuring, but is activated only when measurements exceed thresholds. However, what happens when the measurements are incorrect due to faulty sensors? One could argue that this might have been what happened to Air France 447. That is, the automation became active when the pilots did not expect it. Faulty sensors are always problematic, but faulty sensors that can trigger automated behavior can be extremely dangerous.

The Boeing 737 Max 8 has a system known as Maneuvering Characteristics Augmentation System (MCAS). The business motivation behind MCAS is itself quite revealing. Apparently, it is analogous to a software patch that attempts to fix a physical flaw of the aircraft. The Boeing 737 aircraft, introduced in 1968, is an extremely mature and reliable aircraft. The 737 is the best selling aircraft in the world, selling over 10,000 aircraft since its inception. It is has been favored by many short-haul budget airlines that have risen in the past decade. Its main competitor in the Airbus A320, where over 8,000 planes have been delivered since its inception in 1988.

In 2008, a joint American-French company CFM launched a more fuel and cost-efficient engine known as the Leap engine. Airbus fitted their new planes (Airbus A320neo) with this new engine. The reason behind the economy of the Leap engine is due to its much larger air intake diameter.

To be competitive, the Max 8, was retrofitted also with this new engine. However, unlike the A320neo, there was not enough ground clearance for the Leap engine. To compensate for this problem, Boeing reduced the distance between the engine and the underside of the wing. This, however, had the effect of changing the center of mass of the plane. The Max 8 now had the dynamic tendency of raising its nose and as a consequence increasing the risk of a stall.

To paper over this tendency, Boeing developed MCAS. The purpose of MCAS is that it is software dedicated to compensating for this flaw:

Boeing engineers, in turn, came up with another makeshift solution. They developed a software that would work in the background. As soon as the nose of the aircraft pointed upward too steeply, the system would automatically activate the tailplane and bring the aircraft back to a safe cruising plane. The pilots wouldn’t even notice the software’s intervention — at least that was the idea.

Employing software to paper over a plane’s natural instability is not new. Many of the more advanced fighter jets are designed to be unstable to ensure greater maneuverability. The fighter pilots are also trained to anticipate the peculiar flight characteristics of their planes. In contrast, there have been many complaints that pilots of the Max 8 were not properly informed of the existence of the MCAS system:

“There are 1,400 pages and only one mention of the infamous Maneuvering Characteristics Augmentation System (MCAS) … in the abbreviations sections. But the manual does not include an explanation of what it is…”

Perhaps Boeing determined that information about this system wasn’t worth attention by pilots. After all, the intention of the MCAS system was to make the 737 Max 8 to give the same “feel” as the previous model the 737 NG. This is what we call in software circles as virtualization. That is, this is software that renders a virtual machine on the pilot’s user interface to the plane so it feels and acts like another kind of plane (i.e. one that is structurally balanced).

There is a “law” in software development knows as “The Law of Leaky Abstractions” which states “All non-trivial abstractions, to some degree, are leaky.” MCAS is perhaps a leaky abstraction, that is, it tries to creates a virtual abstraction of a legacy 737 NG without Leap engines, to hide an unbalanced airplane. Surely, nothing can leak with this kind of abstraction? It is one thing to abstract away virtual machinery and its entirely another thing to attempt to abstract away physical reality. However, in both cases, something will eventually leak through.

So does the MCAS behave when its abstractions begin to leak? Here is what is reported by pilots of the plane:

“On both the NG and the MAX, when you have a runaway trim stab this can be stopped temporarily by pulling the control column in the opposite direction. But when the MCAS is activated because of a high angle of attack, this can only be stopped by cutting the electrical trim motor.”

How a pilot responds to an abstraction leak can be very different from that of the real thing it is trying to abstract. With faulty sensors, one can turn this off and use one’s understanding of the situation and the plane to make good decisions. However, when one’s understanding of the nature of the plane is virtual and not real, then you just can’t revert to reality. Reality is outside of the pilot’s comprehension and thus a cause to inproper decision making. A virtual trashcan works like a regular trash can in that you can still recover the documents you place in the trash before it is emptied. Reality however is very different than the virtual world, many times there is no undo function!

Then there’s this leaky abstraction when the plane itself has exhibited its own intentions:

But the EFS never acts by itself, so we were astounded when we heard what the real reason was. (…) However, in some cases — as happened on Flight 610 — the MCAS moves by itself.

and this:

MCAS is activated without pilot input and only operates in manual, flaps up flight.

This is because, a virtual abstraction of a real plane, is the same as Level 5 automation! If MCAS is turned off, the pilots will find themselves to be flying an entirely different plane. When you abstract away interaction with reality, you cannot avoid introducing a process that mediates between a pilot’s action and the actual actions of the plane. The behavior of the real plane will depend on the environment that it is in. The behavior of a virtual plane will depend on just the working sensors that are available to render the virtual simulation. Level 5 automation requires a kind of intelligence that is aware of what sensors are faulty and furthermore is able to navigate a problem with partial and unobserved information. The smarts to enable this kind of Artificial Intelligence is simply not available in our current state of technological development.

In short, Boeing has decided to implement technology that is simply too ambitious. Not all software has the same level of complexity. This is not an issue of insufficient testing to uncover logical flaws in the software. This is not an issue of robustly handling sensor and equipment failure. This is an issue of attempting to implement an overly ambitious and thus a dangerous solution.

Air travel is extremely reliable, but introducing software patches as a means to virtualize physical behavior can lead to unintended consequences. The reason that we still fly planes with pilots in them is that we expect pilots to be able to solve unexpected situations that automation cannot handle. MCAS like virtualization, handcuffs pilots from differentiating between the real and the simulated. I would thus recommend to regulators that in the future, MCAS like virtualization should be treated and tested very differently from other automation. They should be treated as Level 5 automation with a more exhaustive level of scrutiny.

Update December 2019

Further Reading