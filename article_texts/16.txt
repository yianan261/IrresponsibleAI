Story highlights Google Photos tagged an African-American man's pictures of him and a friend as "Gorillas" He highlighted the problem on Twitter, drawing the attention of a Google engineer

CNN —

When Jacky Alcine looked at his Google Photos app recently, he was appalled by what he saw. The facial recognition software had tagged pictures of him and a friend, both of them African-Americans, with the word “Gorillas.”

Alcine, a computer programmer in New York, called out Google about the blunder that had served up the offensive racial slur on the photos he’d uploaded.

“What kind of sample image data you collected that would result in this?” he asked in a series of angry tweets Sunday evening.

His outraged comments quickly picked up traction and the attention of a senior engineer at Google, who identified himself as Yonatan Zunger on Twitter. His account was linked to a Google+ blog of a senior engineer of the same name.

The chief architect of the Internet giant’s Google+ platform, promptly jumped into the fray, expressing horror at the bug and promising to get it fixed as quickly as possible.

“This is 100% Not OK,” he told Alcine in a tweet. “Sheesh. High on my list of bugs you *never* want to see happen. ::shudder::” Zunger said in another.

Google still working on fixes

On Monday, Zunger said Google would stop using “Gorillas” as a label and was still clearing up the glitch in search results.

Google is also working on longer-term improvements in the use of words to label photos and its image recognition software, which automatically generates the tags.

“Lots of work being done, and lots still to be done. But we’re very much on it,” Zunger tweeted, explaining that image recognition software has problems with obscured faces, as well as different skin tones and lighting.

“We used to have a problem with people (of all races) being tagged as dogs, for similar reasons,” he said.

Alcine has thanked Zunger for his response.

Google didn’t immediately respond to calls from CNN late Wednesday seeking comment on the matter.

Previous problems: Confusing dogs and horses

It’s not the first time the Internet company’s programming has misidentified subjects. In May, a user caught it tagging her dogs as horses.

The photo sharing service Flickr has also faced difficulties, labeling photos of both black and white people with “ape” and “animal.”

In a continuing discussion on Twitter of the problem Alcine highlighted, Zunger insisted that it was down to “ordinary machine learning trouble.”

But he acknowledged that “the history of racism is what makes this error particularly bad.”. The representative from Google added, “Image labelling technology is still early and unfortunately, it’s nowhere near perfect.”

Do you remember the time when Google ’s image recognition algorithm created a major controversy after it categorised a black couple as “Gorillas” ?

If you don’t then we don’t blame you as this actually happened back in July 2015. Once discovered, the company issued an apology after acknowledging the sensitivity and gravity of the error.

Advertisement

It seems that the company went around to fix the problem but according to a report by Wired, the fix did not go beyond quickly patching the issue at hand. Instead of fixing the problem by teaching its algorithm the difference between coloured people and gorillas, the company went around to fix the problem at hand by directly removing gorillas from the image-labelling technology.

It seems that the company has simply blocked its algorithm from identifying gorillas to ensure that history does not repeat itself.

The thing to note here is that the company employed this workaround even after making it evident that image recognition will be the spine of most artificial intelligence operations like self-driving cars , personal assistants and other products.

Advertisement

Wired tried a number of tests to check the image recognition algorithm ranging from using Google Lens and Google Photos to try and recognise 40,000 images with a variety of subjects and objects. The system refused to identify chimps, gorillas, chimpanzee or monkey. What is interesting is that Google Assistant correctly identified a gorilla as a gorilla. In fact, the Cloud Vision API, a service that Google’s Cloud computing division offers to businesses, was also able to identify chimpanzees and gorillas.

Advertisement

According to another test, the algorithm did not serve any results to the term “African American” while only giving results of black and white coloured images for terms such as “black man”, “black woman” and “black person”.

Advertisement

Google issued a statement to Wired confirming that the ‘gorilla’ term was censored from the search and image tags after the incident. The representative added, “Image labelling technology is still early and unfortunately it’s nowhere near perfect.” The report goes in on more detail about the research conducted while investigating about how far Google went in fixing the problem This issue highlights the complexities and potential problems when it comes to image identification and detection algorithms. However, regardless of the problems, it is unclear on why the search giant has not been able to make a more comprehensive solution to this instead of the fix.. After Google was criticised in 2015 for an image-recognition algorithm that auto-tagged pictures of black people as “gorillas”, the company promised “immediate action” to prevent any repetition of the error.

That action was simply to prevent Google Photos from ever labelling any image as a gorilla, chimpanzee, or monkey – even pictures of the primates themselves.

That’s the conclusion drawn by Wired magazine, which tested more than 40,000 images of animals on the service. Photos accurately tagged images of pandas and poodles, but consistently returned no results for the great apes and monkeys – despite accurately finding baboons, gibbons and orangutans.

Google confirmed that the terms were removed from searches and image tags as a direct result of the 2015 incident, telling the magazine that: “Image labelling technology is still early and unfortunately it’s nowhere near perfect”.

The gorilla blindness is found in other places across Google’s platform: Google Lens, a camera app that identifies objects in images, will also refuse to recognise gorillas. But Google Assistant will correctly identify the primates, as will Google’s business-to-business image recognition service Google Cloud Vision.

The failure of the company to develop a more sustainable fix in the following two years highlights the extent to which machine learning technology, which underpins the image recognition feature, is still maturing.

Such technologies are frequently described as a “black box”, capable of producing powerful results, but with little ability on the part of their creators to understand exactly how and why they make the decisions they do.

That is particularly true of the first wave of image-recognition systems, of which Google Photos was a part. At the same time that product was launched, Flickr released a similar feature, auto-tagging – which had an almost identical set of problems.

The Yahoo-owned photo sharing platform labelled a picture of a black man as “ape”, and a photo of the Dachau concentration camp as “jungle gym”. Flickr’s response was much the same as Google’s: the company apparently removed the word “ape” from its tagging lexicon entirely.. 