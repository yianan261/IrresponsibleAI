The representative from Google added, “Image labelling technology is still early and unfortunately, it’s nowhere near perfect.”

Do you remember the time when Google ’s image recognition algorithm created a major controversy after it categorised a black couple as “Gorillas” ?

If you don’t then we don’t blame you as this actually happened back in July 2015. Once discovered, the company issued an apology after acknowledging the sensitivity and gravity of the error.

Advertisement

It seems that the company went around to fix the problem but according to a report by Wired, the fix did not go beyond quickly patching the issue at hand. Instead of fixing the problem by teaching its algorithm the difference between coloured people and gorillas, the company went around to fix the problem at hand by directly removing gorillas from the image-labelling technology.

It seems that the company has simply blocked its algorithm from identifying gorillas to ensure that history does not repeat itself.

The thing to note here is that the company employed this workaround even after making it evident that image recognition will be the spine of most artificial intelligence operations like self-driving cars , personal assistants and other products.

Advertisement

Wired tried a number of tests to check the image recognition algorithm ranging from using Google Lens and Google Photos to try and recognise 40,000 images with a variety of subjects and objects. The system refused to identify chimps, gorillas, chimpanzee or monkey. What is interesting is that Google Assistant correctly identified a gorilla as a gorilla. In fact, the Cloud Vision API, a service that Google’s Cloud computing division offers to businesses, was also able to identify chimpanzees and gorillas.

Advertisement

According to another test, the algorithm did not serve any results to the term “African American” while only giving results of black and white coloured images for terms such as “black man”, “black woman” and “black person”.

Advertisement

Google issued a statement to Wired confirming that the ‘gorilla’ term was censored from the search and image tags after the incident. The representative added, “Image labelling technology is still early and unfortunately it’s nowhere near perfect.” The report goes in on more detail about the research conducted while investigating about how far Google went in fixing the problem This issue highlights the complexities and potential problems when it comes to image identification and detection algorithms. However, regardless of the problems, it is unclear on why the search giant has not been able to make a more comprehensive solution to this instead of the fix.. Google has apologized after its new photo app labelled two black people as “gorillas”.



The photo service, launched in May, automatically tags uploaded pictures using its own artificial intelligence software.

“Google Photos, y’all fucked up. My friend’s not a gorilla,” Jacky Alciné tweeted on Sunday after a photo of him and a friend was mislabelled as “gorillas” by the app.

Google Photos, y'all fucked up. My friend's not a gorilla. pic.twitter.com/SMkMCsNVX4 — diri noir avec banan (@jackyalcine) June 29, 2015

Shortly after, Alciné was contacted by Yonatan Zunger, the chief architect of social at Google.



“Big thanks for helping us fix this: it makes a real difference,” Zunger tweeted to Alciné.

He went on to say that problems in image recognition can be caused by obscured faces and “different contrast processing needed for different skin tones and lighting”.

“We used to have a problem with people (of all races) being tagged as dogs, for similar reasons,” he said. “We’re also working on longer-term fixes around both linguistics (words to be careful about in photos of people) and image recognition itself (e.g., better recognition of dark-skinned faces). Lots of work being done and lots still to be done, but we’re very much on it.”

Racist tags have also been a problem in Google Maps. Earlier this year, searches for “nigger house” globally and searches for “nigger king” in Washington DC turned up results for the White House, the residence of the US president, Barack Obama. Both at that time and earlier this week, Google apologized and said that it was working to fix the issue.



“We’re appalled and genuinely sorry that this happened,” a Google spokeswoman told the BBC on Wednesday. “We are taking immediate action to prevent this type of result from appearing. There is still clearly a lot of work to do with automatic image labelling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”

Google is not the only platform trying to work out bugs in its automatic image labelling.

In May, Flickr’s auto-tagging system came under scrutiny after it labelled images of black people with tags such as “ape” and “animal”. The system also tagged pictures of concentration camps with “sport” or “jungle gym”.

“We are aware of issues with inaccurate auto-tags on Flickr and are working on a fix. While we are very proud of this advanced image-recognition technology, we’re the first to admit there will be mistakes and we are constantly working to improve the experience,” a Flickr spokesperson said at the time.

“If you delete an incorrect tag, our algorithm learns from that mistake and will perform better in the future. The tagging process is completely automated – no human will ever view your photos to tag them.”