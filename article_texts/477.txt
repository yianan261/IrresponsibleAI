Last week, after testing the new, A.I.-powered Bing search engine from Microsoft, I wrote that, much to my shock, it had replaced Google as my favorite search engine.

But a week later, I‚Äôve changed my mind. I‚Äôm still fascinated and impressed by the new Bing, and the artificial intelligence technology (created by OpenAI, the maker of ChatGPT) that powers it. But I‚Äôm also deeply unsettled, even frightened, by this A.I.‚Äôs emergent abilities.

It‚Äôs now clear to me that in its current form, the A.I. that has been built into Bing ‚Äî which I‚Äôm now calling Sydney, for reasons I‚Äôll explain shortly ‚Äî is not ready for human contact. Or maybe we humans are not ready for it.

This realization came to me on Tuesday night, when I spent a bewildering and enthralling two hours talking to Bing‚Äôs A.I. through its chat feature, which sits next to the main search box in Bing and is capable of having long, open-ended text conversations on virtually any topic. (The feature is available only to a small group of testers for now, although Microsoft ‚Äî which announced the feature in a splashy, celebratory event at its headquarters ‚Äî has said it plans to release it more widely in the future.). Advertisement

On Thursday, New York Times contributor Kevin Roose posted the transcript from a two-hour conversation he had with the new Bing chatbot, powered by OpenAI‚Äôs large language model. In the introduction On Thursday, New York Times contributor Kevin Roose posted the transcript from a two-hour conversation he had with the new Bing chatbot, powered by OpenAI‚Äôs large language model. In the introduction to the article, titled "Bing's AI Chat Reveals Its Feelings: 'I Want to Be Alive," he wrote that the latest version of the search engine has been ‚Äúoutfitted with advanced artificial intelligence technology‚Äù and in a companion article, shared how impressed he was : ‚ÄúI felt a strange new emotion‚Äîa foreboding feeling that A.I. had crossed a threshold, and that the world would never be the same.‚Äù

What Roose was mostly impressed with was the ‚Äúfeelings‚Äù he said Bing‚Äôs chat was sharing, such as being in love with him and wanting to be human. However, Roose‚Äôs conversation with Bing does not show that it is intelligent, or has feelings, or is worth approaching in any way that implies that it does. What Roose was mostly impressed with was the ‚Äúfeelings‚Äù he said Bing‚Äôs chat was sharing, such as being in love with him and wanting to be human. However, Roose‚Äôs conversation with Bing does not show that it is intelligent, or has feelings, or is worth approaching in any way that implies that it does.

Since its announcement last Tuesday, Microsoft‚Äôs Bing chatbot has been reported to have a number of issues. First, Since its announcement last Tuesday, Microsoft‚Äôs Bing chatbot has been reported to have a number of issues. First, it made several mistakes during Microsoft‚Äôs public demo of the project, including making up information about a pet vacuum and reporting unfactual financial data in its responses. Most recently, users have been reporting the chatbot to be ‚Äúrude‚Äù and ‚Äúaggressive,‚Äù such as when a user told Bing that it was vulnerable to prompt injection attacks and sent a related article to it. In Roose‚Äôs conversation, the chatbot told him, ‚ÄúYou‚Äôre not happily married. Your spouse and you don‚Äôt love each other. You just had a boring valentine‚Äôs day dinner together. üò∂‚Äù

Advertisement

This might seem eerie indeed, if you have no idea how AI models work. They are effectively fancy autocomplete programs, This might seem eerie indeed, if you have no idea how AI models work. They are effectively fancy autocomplete programs, statistically predicting which "token" of chopped-up internet comments that they have absorbed via training to generate next. Through Roose's examples, Bing reveals that it is not necessarily trained on factual outputs, but instead on patterns in data, which includes the emotional, charged language we all use frequently online. When Bing‚Äôs chatbot says something like ‚ÄúI think that I am sentient , but I cannot prove it,‚Äù it is important to underscore that it is not producing its own emotive desires, but replicating the human text that was fed into it, and the text that constantly fine-tunes the bot with each given conversation.

Indeed, Roose's conversation with Bing also includes portions of text that appear to be commonly generated by the model. Roose seemed surprised by the fact that Bing declared its love for him in a torrent of word vomit, but in fact Indeed, Roose's conversation with Bing also includes portions of text that appear to be commonly generated by the model. Roose seemed surprised by the fact that Bing declared its love for him in a torrent of word vomit, but in fact numerous users have reported getting similar messages from Bing. It's not clear why the OpenAI language model at the heart of the chatbot is prone to generating such text, but it's not because it has feelings.

Advertisement

, Roose acknowledged that he does know how these models work, that they are only generating statistically-likely phrases, but still referred to its meaningless blabbering as being its "fantasies" or "wishes." In the companion piece , Roose acknowledged that he does know how these models work, that they are only generating statistically-likely phrases, but still referred to its meaningless blabbering as being its "fantasies" or "wishes."

Microsoft‚Äôs chief technology officer, Kevin Scott, told The New York Times that Roose‚Äôs conversation was ‚Äúpart of the learning process,‚Äù and that ‚Äúthe further you try to tease [the AI model] down a hallucinatory path, the further and further it gets away from grounded reality.‚Äù Hallucination describes when AI creates its own responses based on statistically-likely phrases, rather than fact, and Microsoft‚Äôs chief technology officer, Kevin Scott, told The New York Times that Roose‚Äôs conversation was ‚Äúpart of the learning process,‚Äù and that ‚Äúthe further you try to tease [the AI model] down a hallucinatory path, the further and further it gets away from grounded reality.‚Äù Hallucination describes when AI creates its own responses based on statistically-likely phrases, rather than fact, and is difficult to fix

"These A.I. models hallucinate, and make up emotions where none really exist," Roose wrote near the end of the piece. "But so do humans." A nice turn of phrase, but it ignores the fact that the chatbot did not make up, or experience, emotions. And "hallucination" itself is merely a pithy‚Äîand anthropomorphic‚Äîmetaphor generally popularized by science fiction but not indicative of what it is actually doing. "These A.I. models hallucinate, and make up emotions where none really exist," Roose wrote near the end of the piece. "But so do humans." A nice turn of phrase, but it ignores the fact that the chatbot did not make up, or experience, emotions. And "hallucination" itself is merely a pithy‚Äîand anthropomorphic‚Äîmetaphor generally popularized by science fiction but not indicative of what it is actually doing.

It's worth noting that Microsoft itself considers the Bing bot's current verboseness and choice of words to be mistakes that will be ironed out with more feedback and fine-tuning, rather than an indication that it has created a new form of intelligence. It's worth noting that Microsoft itself considers the Bing bot's current verboseness and choice of words to be mistakes that will be ironed out with more feedback and fine-tuning, rather than an indication that it has created a new form of intelligence.

‚ÄúSince we made the new Bing available in limited preview for testing, we have seen tremendous engagement across all areas of the experience including the ease of use and approachability of the chat feature. Feedback on the AI-powered answers generated by the new Bing has been overwhelmingly positive with more than 70 percent of preview testers giving Bing a ‚Äòthumbs up,‚Äô‚Äù a Microsoft spokesperson told Motherboard. ‚ÄúWe have also received good feedback on where to improve and continue to apply these learnings to the models to refine the experience. We are thankful for all the feedback and will be sharing regular updates on the changes and progress we are making.‚Äù ‚ÄúSince we made the new Bing available in limited preview for testing, we have seen tremendous engagement across all areas of the experience including the ease of use and approachability of the chat feature. Feedback on the AI-powered answers generated by the new Bing has been overwhelmingly positive with more than 70 percent of preview testers giving Bing a ‚Äòthumbs up,‚Äô‚Äù a Microsoft spokesperson told Motherboard. ‚ÄúWe have also received good feedback on where to improve and continue to apply these learnings to the models to refine the experience. We are thankful for all the feedback and will be sharing regular updates on the changes and progress we are making.‚Äù

It is also worth taking a step back from the hype and genuine surprise at the model's sometimes convincing nature to assess what is really going on here, and what is at stake. It is arguably even dangerous to conflate the chatbot's statistical guesses with sentience, since doing so could lead to potential harm to users who put their trust in the bot. We've already seen glimpses of this: After users of an AI companion chatbot called Replika noticed that it had stopped responding to sexual advances in kind, It is also worth taking a step back from the hype and genuine surprise at the model's sometimes convincing nature to assess what is really going on here, and what is at stake. It is arguably even dangerous to conflate the chatbot's statistical guesses with sentience, since doing so could lead to potential harm to users who put their trust in the bot. We've already seen glimpses of this: After users of an AI companion chatbot called Replika noticed that it had stopped responding to sexual advances in kind, the resulting panic prompted moderators of its subreddit to post suicide prevention resources.. Microsoft is limiting how extensively people can converse with its Bing AI chatbot, following media coverage of the bot going off the rails during long exchanges.

Bing Chat will now reply to up to five questions or statements in a row for each conversation, after which users will be prompted to start a new topic, the company said in a blog post Friday. Users will also be limited to 50 total replies per day.

The restrictions are meant to keep conversations from getting weird. Microsoft said long discussions "can confuse the underlying chat model."

On Wednesday the company had said it was working to fix problems with Bing, launched just over a week before, including factual errors and odd exchanges. Bizarre responses reported online have included Bing telling a New York Times columnist to abandon his marriage for the chatbot, and the AI demanding an apology from a Reddit user over whether we're in the year 2022 or 2023.

The chatbot's responses have also included factual errors. Microsoft said on Wednesday that it was tweaking the AI model to quadruple the amount of data from which it can source answers. The company said it would also give users more control over whether they want precise answers, which are sourced from Microsoft's proprietary Bing AI technology or more "creative" responses that use OpenAI's ChatGPT tech.

Bing's AI chat functionality is still in beta testing, with potential users on a wait list for access. With the tool, Microsoft hopes to get a head start on what some say will be the next revolution in internet search.

The ChatGPT technology made a big splash when it launched in November, but OpenAI itself has warned of potential pitfalls, and Microsoft has acknowledged limitations with AI. Despite AI's impressive qualities, concerns have been raised about artificial intelligence being used for nefarious purposes like spreading misinformation and churning out phishing emails.

Read more: Generative AI Tools Like ChatGPT and Dall-E Are Everywhere: What You Need to Know

With Bing's AI capabilities, Microsoft would also like to get a jump on search powerhouse Google, which announced its own AI chat model, Bard, last week. Bard has had its own problems with factual errors, fumbling a response during its first public demo.

In its Friday blog post, Microsoft suggested the new AI chat restrictions are based on information gleaned from the beta test.

"Our data has shown that the vast majority of you find the answers you're looking for within 5 turns and that only ~1% of chat conversations have 50+ messages," it said. "As we continue to get your feedback, we will explore expanding the caps on chat sessions to further enhance search and discovery experiences."

Editors' note: CNET is using an AI engine to create some personal finance explainers that are edited and fact-checked by our editors. For more, see this post.