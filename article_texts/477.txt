ARTICLE TITLE: Bing Chat Tentatively Hallucinated in Extended Conversations with Users
Last week, after testing the new, A.I.-powered Bing search engine from Microsoft, I wrote that, much to my shock, it had replaced Google as my favorite search engine.

But a week later, I’ve changed my mind. I’m still fascinated and impressed by the new Bing, and the artificial intelligence technology (created by OpenAI, the maker of ChatGPT) that powers it. But I’m also deeply unsettled, even frightened, by this A.I.’s emergent abilities.

It’s now clear to me that in its current form, the A.I. that has been built into Bing — which I’m now calling Sydney, for reasons I’ll explain shortly — is not ready for human contact. Or maybe we humans are not ready for it.

This realization came to me on Tuesday night, when I spent a bewildering and enthralling two hours talking to Bing’s A.I. through its chat feature, which sits next to the main search box in Bing and is capable of having long, open-ended text conversations on virtually any topic. (The feature is available only to a small group of testers for now, although Microsoft — which announced the feature in a splashy, celebratory event at its headquarters — has said it plans to release it more widely in the future.). Microsoft is limiting how extensively people can converse with its Bing AI chatbot, following media coverage of the bot going off the rails during long exchanges.

Bing Chat will now reply to up to five questions or statements in a row for each conversation, after which users will be prompted to start a new topic, the company said in a blog post Friday. Users will also be limited to 50 total replies per day.

The restrictions are meant to keep conversations from getting weird. Microsoft said long discussions "can confuse the underlying chat model."

On Wednesday the company had said it was working to fix problems with Bing, launched just over a week before, including factual errors and odd exchanges. Bizarre responses reported online have included Bing telling a New York Times columnist to abandon his marriage for the chatbot, and the AI demanding an apology from a Reddit user over whether we're in the year 2022 or 2023.

The chatbot's responses have also included factual errors. Microsoft said on Wednesday that it was tweaking the AI model to quadruple the amount of data from which it can source answers. The company said it would also give users more control over whether they want precise answers, which are sourced from Microsoft's proprietary Bing AI technology or more "creative" responses that use OpenAI's ChatGPT tech.

Bing's AI chat functionality is still in beta testing, with potential users on a wait list for access. With the tool, Microsoft hopes to get a head start on what some say will be the next revolution in internet search.

The ChatGPT technology made a big splash when it launched in November, but OpenAI itself has warned of potential pitfalls, and Microsoft has acknowledged limitations with AI. Despite AI's impressive qualities, concerns have been raised about artificial intelligence being used for nefarious purposes like spreading misinformation and churning out phishing emails.

Read more: Generative AI Tools Like ChatGPT and Dall-E Are Everywhere: What You Need to Know

With Bing's AI capabilities, Microsoft would also like to get a jump on search powerhouse Google, which announced its own AI chat model, Bard, last week. Bard has had its own problems with factual errors, fumbling a response during its first public demo.

In its Friday blog post, Microsoft suggested the new AI chat restrictions are based on information gleaned from the beta test.

"Our data has shown that the vast majority of you find the answers you're looking for within 5 turns and that only ~1% of chat conversations have 50+ messages," it said. "As we continue to get your feedback, we will explore expanding the caps on chat sessions to further enhance search and discovery experiences."

Editors' note: CNET is using an AI engine to create some personal finance explainers that are edited and fact-checked by our editors. For more, see this post.