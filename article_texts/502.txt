You hear a knock on your door. Expecting a neighbor or perhaps a delivery, you open it, only to find a child welfare worker demanding entry. It doesn’t seem like you can refuse so you let them in and watch as they search every room, rummaging through closets, drawers, cabinets, the fridge — all without a warrant. They ask questions making it sound like you’re a bad parent and, finally, say they need to do a visual inspection of your kids, undressed, without you in the room, and take pictures.

The agency receives many reports of ordinary neglect, which are distinct from physical abuse or severe neglect allegations, but it doesn’t investigate all of them. Instead, the agency had started using an algorithm to help decide who gets the knock on the door and who doesn’t. But you can’t get any information about what it said about you or your child, or how it played a role in the decision to investigate you.

Though an algorithm may sound neutral, predictive tools are designed by people. And the choices people make when creating the tool aren’t just decisions about what statistical method is better or what data is necessary to make its calculations. The same people can be flagged as more or less in need of investigation based on how a tool was designed. One recurring concern is that the use of these tools in systems marked by discriminatory treatment and outcomes will result in those outcomes being replicated. But this time, if that history repeats itself, the disparate results will be deemed unquestionable truths supported by science and math, and not the result of residual or ongoing discrimination, let alone the policy decisions that resulted when tool designers decided to choose model A instead of model B.

To better understand whether this concern is warranted, two years ago, the ACLU requested data and documents from Allegheny County, Pennsylvania related to the Allegheny Family Screening Tool (AFST) so we, together with researchers from the Human Rights Data Analysis Group, could independently evaluate its design and practical impact. We found, among other things, that the AFST could result in inequities in screen-in rates — the percentage of reports (i.e., neglect allegations received by the county child welfare agency) that are forwarded for investigation (“screened in”) out of the total number of reports received. We found that the tool could result in screen-in rate disparities between Black and non-Black families (i.e., the percentage of Black families flagged for investigation out of all allegations about Black families received could be greater than the same percentage for non-Black families). We also found that households where people with disabilities live could be labeled as higher risk than households without a disabled resident. What really stood out though was that we found the AFST’s algorithm, or the way its conclusions about a family were conveyed to a screener, could have been built in different ways that may have had a less discriminatory impact. And this alternative method didn’t change the algorithm’s “accuracy” in any meaningful way, even if we accept the tool’s developer’s definition of that term. We asked the county and tool designers for feedback on a paper describing our analysis, but never received a response. We share our findings for the first time today. But first, a quick overview of how the AFST works.

Allegheny County has been using the AFST to help screening workers decide whether to investigate or dismiss neglect allegations. (The AFST is not used to make screening decisions about physical abuse or severe neglect allegations because state law requires that those be investigated.) The tool calculates a “risk score” from 0 to 20 based on the underlying algorithm’s estimation of the likelihood that the county will, within two years, remove a child from the family involved in the report. In other words, the tool generates a prediction of the “risk” that the agency will place the child in foster care. The county and tool designers treat removal as a sign that the child may be harmed, so that the higher the likelihood of removal, the higher the score, and the greater the presumed need for child welfare intervention. Call-in screeners are instructed to consider the AFST’s output as one factor among many in deciding whether to forward the report for agency action.

However, in a child welfare system already plagued by inequities based on race, gender, income, and disability, using historical data to predict future action by the agency only serves to reinforce those disparities. And when reborn through an algorithm, people are liable to interpret the disparities as hard truths because, well, a mathematical equation told us so.

In this way, the AFST creators are doing more than math when building a tool. They also have the ability to become shadow policymakers — because unless the practical impact of their design decisions is evaluated and made public, this power can be wielded with little transparency or accountability, even though these are two of the reasons why the county adopted the tool.

Here is a summary of the design decisions and resulting policies and value judgments that we shared with the county as cause for concern:. PITTSBURGH (AP) — For the two weeks that the Hackneys’ baby girl lay in a Pittsburgh hospital bed weak from dehydration, her parents rarely left her side, sometimes sleeping on the fold-out sofa in the room.

They stayed with their daughter around the clock when she was moved to a rehab center to regain her strength. Finally, the 8-month-old stopped batting away her bottles and started putting on weight again.

“She was doing well and we started to ask when can she go home,” Lauren Hackney said. “And then from that moment on, at the time, they completely stonewalled us.”

The couple was stunned when child welfare officials showed up, told them they were negligent and took away their daughter.

“They had custody papers and they took her right there and then,” Lauren Hackney recalled. “And we started crying.”

More than a year later, their daughter, now 2, remains in foster care and the Hackneys, who have developmental disabilities, struggle to understand how taking their daughter to the hospital when she refused to eat could be seen as so neglectful that she’d need to be taken from her home.

They wonder if an artificial intelligence tool that the Allegheny County Department of Human Services uses to predict which children could be at risk of harm singled them out because of their disabilities.

The U.S. Justice Department is asking the same question. The agency is investigating the county’s child welfare system to determine whether its use of the influential algorithm discriminates against people with disabilities or other protected groups, The Associated Press has learned. Later this month, federal civil rights attorneys will interview the Hackneys and Andrew Hackney’s mother, Cynde Hackney-Fierro, the grandmother said.

Lauren Hackney has attention-deficit hyperactivity disorder that affects her memory, and her husband, Andrew, has a comprehension disorder and nerve damage from a stroke suffered in his 20s. Their baby girl was 7 months old when she began refusing her bottles. Facing a nationwide shortage of formula, they traveled from Pennsylvania to West Virginia looking for some and were forced to change brands. The baby didn’t seem to like it.

Her pediatrician first reassured them that babies can be fickle with feeding and offered ideas to help her get back her appetite, they said.

When she grew lethargic days later, they said, the same doctor told them to take her to the emergency room. The Hackneys believe medical staff alerted child protective services after they showed up with a dehydrated and malnourished baby.

That’s when they believe their information was fed into the Allegheny Family Screening Tool, which county officials say is standard procedure for neglect allegations. Soon, a social worker appeared to question them, and their daughter was sent to foster care.

Over the past six years, Allegheny County has served as a real-world laboratory for testing AI-driven child welfare tools that crunch reams of data about local families to try to predict which children are likely to face danger in their homes. Today, child welfare agencies in at least 26 states and Washington, D.C., have considered using algorithmic tools, and jurisdictions in at least 11 have deployed them, according to the American Civil Liberties Union.

The Hackneys’ story — based on interviews, internal emails and legal documents — illustrates the opacity surrounding these algorithms. Even as they fight to regain custody of their daughter, they can’t question the “risk score” Allegheny County’s tool may have assigned to her case because officials won’t disclose it to them. And neither the county nor the people who built the tool have explained which variables may have been used to measure the Hackneys’ abilities as parents.

“It’s like you have an issue with someone who has a disability,” Andrew Hackney said. “In that case ... you probably end up going after everyone who has kids and has a disability.”

As part of a yearlong investigation, the AP obtained the data points underpinning several algorithms deployed by child welfare agencies, including some marked “CONFIDENTIAL,” offering rare insight into the mechanics driving these emerging technologies. Among the factors they have used to calculate a family’s risk, whether outright or by proxy: race, poverty rates, disability status and family size. They include whether a mother smoked before she was pregnant and whether a family had previous child abuse or neglect complaints.

What they measure matters. A recent analysis by ACLU researchers found that when Allegheny’s algorithm flagged people who accessed county services for mental health and other behavioral health programs, that could add up to three points to a child’s risk score, a significant increase on a scale of 20.

Allegheny County spokesman Mark Bertolet declined to address the Hackney case and did not answer detailed questions about the status of the federal probe or critiques of the data powering the tool, including by the ACLU.

“As a matter of policy, we do not comment on lawsuits or legal matters,” Bertolet said in an email.

Justice Department spokeswoman Aryele Bradford declined to comment.

The tool’s developers, Rhema Vaithianathan, a professor of health economics at New Zealand’s Auckland University of Technology, and Emily Putnam-Hornstein, a professor at the University of North Carolina at Chapel Hill’s School of Social Work, said that their work is transparent and that they make their models public.

“In each jurisdiction in which a model has been fully implemented we have released a description of fields that were used to build the tool,” they said by email.

The developers have started new projects with child welfare agencies in Northampton County, Pennsylvania, and Arapahoe County, Colorado. The states of California and Pennsylvania, as well as New Zealand and Chile, also asked them to do preliminary work.

Vaithianathan recently advised researchers in Denmark and officials in the United Arab Emirates on technology in child services.

Last year, the U.S. Department of Health and Human Services funded a national study, co-authored by Vaithianathan and Putnam-Hornstein, that concluded that their overall approach in Allegheny could be a model for other places.

HHS’ Administration for Children and Families spokeswoman Debra Johnson declined to say if the Justice Department’s probe would influence her agency’s future support for algorithmic approaches to child welfare.

Especially as budgets tighten, cash-strapped agencies are desperate to focus on children who truly need protection. At a 2021 panel, Putnam-Hornstein acknowledged that Allegheny’s “overall screen-in rate remained totally flat” since their tool had been implemented.

Meanwhile, family separation can have lifelong developmental consequences for children.

The Hackneys’ daughter already has been placed in two foster homes and spent more than half of her life away from her parents.

In February, she was diagnosed with a disorder that can disrupt her sense of taste, according to Andrew Hackney’s lawyer, Robin Frank, who added that the girl still struggles to eat, even in foster care.

“I really want to get my kid back,” Andrew Hackney said. “It hurts a lot. You have no idea how bad.”

___

Burke reported from San Francisco. Associated Press video journalist Jessie Wardarski and photojournalist Maye-E Wong in Pittsburgh contributed to this report.

___

Follow Sally Ho and Garance Burke on Twitter at @_sallyho and @garanceburke.

___

Contact AP’s global investigative team at [email protected] or https://www.ap.org/tips/. In this section, we explore the impacts of the inclusion of these features in the model, focusing on features related to behavioral health — which can include disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [ 90 ]. Ultimately, we find that the county’s stated goal to “make decisions based on as much information as possible” [ 36 ] raises concerns and comes at the expense of already impacted and marginalized communities, risking further perpetuation of systemic racism and oppression.

One of the county’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much information as possible” [ 36, p. 9 ]. This “information” included data from the county’s “Data Warehouse” [ 18 ] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO-regularized logistic regression “trained to optimize for the [Area Under the ROC Curve] AUC” [ 90 ]. (More precisely, the developers selected the model with the largest value for the regularization parameter among a sequence of candidate values, such that the resulting AUC was within one-standard error of the maximum observed AUC across all candidates [ 85 ].)

Table 2. AUC generated by retraining the model using the same procedure without juvenile probation (JPO), behavioral health (BH) or "ever in" variables. Removing each of these sets of variables from the training data -- or removing all of them -- reduces the AUC on the test data of the resulting model by less than .01 in all cases.

Fig. 2. Households marked by any history with the juvenile probation system are overwhelmingly labeled "High Risk", although less so after removing juvenile probation (JPO) predictors from the training data.

4.2.1 Features from the criminal legal system in the AFST. Every version of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Allegheny County Jail or interactions with juvenile probation (see [91] and [90] for details on these features in V1 and V2 respectively; these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process — discussed further in our forthcoming extended analysis — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [90]). Black alleged victim children were almost three times more likely to have been or currently be on juvenile probation at the time of the referral compared to white alleged victims. The AFST also includes features for whether other members of the household have ever been in the juvenile probation system (see [90]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features — which reflect the racially biased policing and criminal legal systems [53, 75] — could exacerbate and reify existing racial biases. Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Black households in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households. Overall, 69% of referral-households with this flag are Black.

As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the county’s own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in the forthcoming extended analysis). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high risk by over 10%, as shown in Figure 2.

4.2.2 Disability-related features in the AFST. In developing the AFST, the county and the research team that developed the tool used multiple data sources that contained direct and indirect references to disability-related information. For example, the first version of the AFST [91] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Disabilities Act (ADA). Versions of the tool have also included features related to public benefits — such as eligibility for Supplemental Security Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the county’s behavioral health agencies focus on services related to “mental health and/or substance abuse” [95]). For example, some public benefits data that was included in AFST V1 was excluded from AFST V2 and V2.1 because of changes in the data format [90, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a 2017 ethical analysis [20] of the AFST commissioned by the county, the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.

We analyze the inclusion of three features directly related to disability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “perpetrator” has any behavioral health history in the database, and, for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health services (see [90, p. 4] for further discussion of these features). These three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However, disability status may have a larger overall impact on the model if other features in the model, like features related to eligibility for public benefits programs, are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being associated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our forthcoming extended analysis, is not just a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature considered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator. In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features has an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.

4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools. There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allow a model to perform best on a performance metric decided on by the model’s developers are kept in the model [14, 46, 47]. One common approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection, potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [10, 56]. However, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [10]. The development of the AFST was guided by AUC-maximization [90], an imperfect measure of accuracy [52], and one that is unable to meaningful distinguish between models with and without disability- and juvenile probation-related features. Given this model multiplicity in the context of the AFST, deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [10] — including interpretability, opportunity for recourse, and fairness.