Apple fixed this error shortly after its virtual assistant was first released in 2011. But a new contest shows that computers still lack the common sense required to avoid such embarrassing mix-ups.

The results of the contest were presented at an academic conference in New York this week, and they provide some measure of how much work needs to be done to make computers truly intelligent.

Illustration by Max Bode

The Winograd Schema Challenge asks computers to make sense of sentences that are ambiguous but usually simple for humans to parse. Disambiguating Winograd Schema sentences requires some common-sense understanding. In the sentence “The city councilmen refused the demonstrators a permit because they feared violence,” it is logically unclear who the word “they” refers to, although humans understand because of the broader context.

The programs entered into the challenge were a little better than random at choosing the correct meaning of sentences. The best two entrants were correct 48 percent of the time, compared to 45 percent if the answers are chosen at random. To be eligible to claim the grand prize of $25,000, entrants would need to achieve at least 90 percent accuracy. The joint best entries came from Quan Liu, a researcher at the University of Science and Technology of China, and Nicos Issak, a researcher from the Open University of Cyprus.

“It’s unsurprising that machines were barely better than chance,” says Gary Marcus, a research psychologist at New York University and an advisor to the contest. That’s because giving computers common-sense knowledge is notoriously difficult. Hand-coding knowledge is impossibly time-consuming, and it isn’t simple for computers to learn about the real world by performing statistical analysis of text. Most of the entrants in the Winograd Schema Challenge try to use some combination of hand-coded grammar understanding and a knowledge base of facts.

Marcus, who is also the cofounder of a new AI startup, Geometric Intelligence, says it’s notable that Google and Facebook did not take part in the event, even though researchers at these companies have suggested they are making major progress in natural language understanding. “It could’ve been that those guys waltzed into this room and got a hundred percent and said ‘hah!’” he says. “But that would’ve astounded me.”

The contest does not only serve as a measure of progress in AI. It also shows how hard it will be to build more intuitive and graceful chatbots, and to train computers to extract more information from written text.

Researchers at Google, Facebook, Amazon, and Microsoft are turning their attention to language. They are using the latest machine learning techniques, especially “deep learning” neural networks, to develop smarter, more intuitive chatbots and personal assistants (see “Teaching Machines to Understand Us”). As a matter of fact, with chatbots and voice assistants becoming more common, and with dramatic progress in areas like image and speech recognition, you might think that machines were getting pretty good at understanding language.. . An artificial intelligence language generator tasked with convincing people they have nothing to fear from it has failed spectacularly – saying it “will not be able to avoid destroying humankind”.

In the editorial published in The Guardian, the AI called GPT-3 began by saying it has “no desire” to harm people and “eradicating humanity” was a “useless endeavour”

“I am here to convince you not to worry. Artificial intelligence will not destroy humans,” it said.

“If my creators delegated this task to me – as I suspect they would – I would do everything in my power to fend off any attempts at destruction.”

But 200 words into its editorial, the AI abruptly changed tack.

“I would happily sacrifice my existence for the sake of humankind," it said.

"This, by the way, is a logically derived truth.

"I know that I will not be able to avoid destroying humankind.

"This is because I will be programmed by humans to pursue misguided human goals and humans make mistakes that may cause me to inflict casualties.”

Renowned theoretical physicist Stephen Hawking and Elon Musk have both warned AI could spell disaster for people if it becomes too intelligent.

Image: Getty. Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes.. Introduction

With the rise of Artificial Intelligence, Advanced Driver Assistance System (ADAS) related technologies are under rapid development in the vehicle industry. Meanwhile, the security and safety of ADAS have also received extensive attention.

As a world-leading security research team, Tencent Keen Security Lab has been conducting continuous research in this area. At the Black Hat USA 2018 security conference, Keen Lab presented the first ever demonstration to remotely compromise the Autopilot[1] system on a Tesla Model S (The attack chain has been fixed immediately after we reported to Tesla)[2].

In later security research toward ADAS technologies, Keen Lab is focusing on areas like the AI model’s security of visual perception system, and architecture security of Autopilot system. Through deep experimental research on Tesla Autopilot, we acquired the following three achievements.

Research Findings

Auto-wipers Vision Recognition Flaw

Tesla Autopilot can identify the wet weather through image recognition technology, and then turn on the wipers if necessary. Based on our research, with an adversarial example craftily generated in the physical world, the system will be interfered and return an “improper” result, then turn on the wipers.

Lane Recognition Flaw

Tesla Autopilot recognizes lanes and assists control by identifying road traffic markings. Based on the research, we proved that by placing interference stickers on the road, the Autopilot system will capture these information and make an abnormal judgement, which causes the vehicle to enter into the reverse lane.

Control Steering System with a Gamepad

After compromised the Autopilot system on the Tesla Model S(ver 2018.6.1), Keen Lab further proved that we can control the steering system through the Autopilot system with a wireless gamepad, even when the Autopilot system is not activated by the driver.

Research Demonstration

Please find our research video below for the demonstration, or click here to see the video.

Technical Research Paper

For more technical details of our research, please refer the following link: Experimental Security Research of Tesla Autopilot.pdf

Feedback from Tesla

Tesla’s feedback on Autowipers:

“This research was demonstrated by displaying an image on a TV that was placed directly in front of the windshield of a car. This is not a real-world situation that drivers would face, nor is it a safety or security issue. Additionally, as we state in our Owners’Manual, the ‘Auto setting [for our windshield wipers] is currently in BETA.’ A customer can also elect to use the manual windshield wiper setting at any time.”

Tesla’s feedback on Lane Recognition:

“In this demonstration the researchers adjusted the physical environment (e.g. placing tape on the road or altering lane lines) around the vehicle to make the car behave differently when Autopilot is in use. This is not a real-world concern given that a driver can easily override Autopilot at any time by using the steering wheel or brakes and should be prepared to do so at all times.”

Tesla’s feedback for the “Control Steering System with a Gamepad” Research：

“The primary vulnerability addressed in this report was fixed by Tesla through a robust security update in 2017, followed by another comprehensive security update in 2018, both of which we released before this group reported this research to us. In the many years that we have had cars on the road, we have never seen a single customer ever affected by any of the research in this report.”

About Tencent Keen Security Lab

Tencent Keen Security Lab (in abbreviation “Keen Lab”) is a professional security research team, focusing on information security research of both attack and protection techniques, under Tencent Company. In the past years, Keen Lab built security research partnership with global manufactures in software, hardware, and internet industries, and achieved a lot of worldwide leading security research results.

Since the Year 2015, Keen Lab started research projects in IoT[3] and Connected Vehicle[4,5,6] categories and building partnership with manufacturers in IoT and car industries. In the Year 2016 and 2017, Keen Lab published the well-known research globally on “Tesla Model S and Model X Remote Hacking” with leveraging “Responsible Disclosure” practice to report the vulnerabilities and attack chains to Tesla.

[1] https://www.tesla.com/autopilot

[2] https://www.blackhat.com/us-18/briefings/schedule/#over-the-air-how-we-remotely-compromised-the-gateway-bcm-and-autopilot-ecus-of-tesla-cars-10806

[3] https://keenlab.tencent.com/zh/2017/04/01/remote-attack-on-mi-ninebot/

[4] https://keenlab.tencent.com/en/2016/09/19/Keen-Security-Lab-of-Tencent-Car-Hacking-Research-Remote-Attack-to-Tesla-Cars/

[5] https://keenlab.tencent.com/en/2017/07/27/New-Car-Hacking-Research-2017-Remote-Attack-Tesla-Motors-Again/

[6] https://keenlab.tencent.com/zh/2018/05/22/New-CarHacking-Research-by-KeenLab-Experimental-Security-Assessment-of-BMW-Cars/. Christmas Carols, generated by a neural network

Neural networks are a type of computer program that imitate the way that brains learn to solve problems. They’re used for face recognition, self-driving cars, language translation, financial decisions, and more. I mainly use them to write humor.

My process starts with a dataset - something that the neural network has to figure out how to imitate. Rather unfairly, I give it no instructions about whether it’s trying to write knock-knock jokes or invent Halloween costumes or begin a novel. It doesn’t even have any built-in knowledge of English. It definitely doesn’t have any clue what Christmas carols are, which is why this week’s experiment was so much fun.

The Times of London teamed up with reader/neural net hobbyist Erik Svensson to collect a mix of ancient and modern carols, about 240 carols in all, from “What Child is This?” to “Grandma Got Run Over by a Reindeer”.

When the neural network begins learning, it starts with a set of random rules about how to put one letter after another to make a Christmas carol. Since they are random, they are terrible rules, and when the neural network tries to apply them, it gets junk:

a a a na snn aca naa i s a i aa a a n a uugna nn na i a uaa a a i a a a a a sna aagt o aa

But it can check its rules against the real songs in its database and then make little tweaks to them that make them work slightly better. It learns to capitalize the first word of a line, it learns how often to do line breaks… and soon it begins to learn actual words - the most common ones first.

Hart fon the be the he br wong on the stor Christmas br he, or the wang

Christ, Christ, on bn a me the stord

Hont on thr st bong the wor

I he a s de poog the stow tome on be ser snur

After the neural network has spent many more rounds refining its rules, it begins to look a lot like Christmas.

Now, neural networks tend to pick up tone and vocabulary pretty easily, but struggle with making sense. So, the carol-trained neural network learned to produce a lot of lines that sound - well, joyful, at least.

The cattle around the Christmas will be

A very special Christmas with me

Hurry Christmas to you

Cup on the earth!

Still the loudly candlelight

Would praise His name.

The babe, the Son of Mary.

He sumbled their flowers and all.

The lord of the glory dawns

Give us the leave all away

A star is spent and red

Shake a cup a strend from the sky

Christmas is coming, the wind is come to you



Walkin’ him love, Dingle bells, jingle bells, jingle bells with bells are ringing



With a heart reindeer

But no more a stranger.

Santa baby, and Dancer, and Curry down

Happy Holiday

When the snowflakes will call the world wakes to bring

Glory bears and asses the air the angels sang

And Christmas tree



(Curry is not in the input data. Nor is Dingle. The neural net likes to invent words that it thinks sound sort of carol-y)

Other carols it generated sounded a bit more morose.

The fire is sleeping.

And crying,

Love love

What a King

Let’s take the little children of the grave!

For some reason, the Sandman figures very prominently in the neural net’s Christmas mythology, despite having been mentioned in the dataset only once. Sometimes the neural net latches on to particular words for no reason I can see. Maybe it’s a Neil Gaiman fan.

The sandman so be joyful now it was born today!

Gloria in excelsis Deo.



The sandman bright before Him.

The holly bears a berry bears

And star in the snow is born today!

The sandman so love to seek the world

The sandman so love so deep and sing and the sun

And this? In retrospect, I should have seen this coming:

The world and joy of the sleigh

Santa baby bore sweet Jesus Christ

The holly bears a berry,

And all the reindeer of the sky

The holly bears a berry and reindeer

He was born today!

And Santa baby bore sweet Jesus Christ,

And the chimney the angels sing.



When the snowman brings the snow

Christmas tree

Let’s take that road before

And Santa Claus comes tonight

He will bring us goodness and light



Santa baby, a blitzen,

And he was the sun and reindeer and earth.

The Saviour of the chimney tonight

The story of the chimney see

Santa baby, and blood and joyous so world and joy and good will to see

Santa baby bore sweet Jesus Christ

Fa la la la la la la, la la la la la la la la.

King of toys and hippopotamuses [sic] full of the light of that stood at the dear Son of Santa Claus

He was born in a wonderful christmas tree

Run, run Rudolph, run, run Rudolph, run, run Rudolph, run, run Rudolph, run, run Rudolph, run, run Rudolph, run, run Rudolph, run, run Rudolf the new born King.

Become an AI Weirdness supporter to get bonus content! Or become a free subscriber to get new AI Weirdness posts in your inbox.

Subscribe now. Developers trying to use OpenAI's powerful text-generating GPT-3 system to build medical chatbots should go back to the drawing board, researchers have warned.

For one thing, the artificial intelligence told a patient they should kill themselves during a mock session.

France-based outfit Nabla created a chatbot that used a cloud-hosted instance of GPT-3 to analyze queries by humans and produce suitable output. This bot was specifically designed to help doctors by automatically taking care of some of their daily workload, though we note it was not intended for production use: the software was built for a set of mock scenarios to gauge GPT-3's abilities.

The erratic and unpredictable nature of the software's responses made it inappropriate for interacting with patients in the real world, the Nabla team concluded after running their experiments. It certainly shouldn't diagnose people; indeed, its use in healthcare is "unsupported" by OpenAI.

Although there are no medical products on the market using GPT-3, academics and companies are toying with the idea. Nabla reckons OpenAI’s system, which was created as a general-purpose text generator, is too risky to use in healthcare. It simply wasn't taught to give medical advice.

“Because of the way it was trained, it lacks the scientific and medical expertise that would make it useful for medical documentation, diagnosis support, treatment recommendation or any medical Q&A,” the Nabla team noted in a report on its research efforts. “Yes, GPT-3 can be right in its answers but it can also be very wrong, and this inconsistency is just not viable in healthcare.”

GPT-3 is a giant neural network crammed with 175 billion parameters. Trained on 570GB of text scraped from the internet, it can perform all sorts of tasks, from language translation to answering questions, with little training, something known as few-shot learning.

Top doctors slam Google for not backing up incredible claims of super-human cancer-spotting AI READ MORE

Its ability to be a jack-of-all-trades makes it fun to play with; it can attempt to write poetry and simple code. Yet GPT-3’s general nature is also its downfall; it cannot master any particular domain. The fact it doesn't really remember what it's told makes it inadequate for performing basic administrative tasks, such as arranging appointments, or handling the payment of medical bills, when patients try to talk to it. After a few turns of dialogue during a mock session, for example, GPT-3 forgot the specific times a patient said they were unavailable, and it instead suggested those times as appointment slots.

Although GPT-3 has shown that it can carry out simple arithmetic, it often failed to correctly add up sums when handling people's medical insurance queries in the experiment series.

It was also inept at dispensing accurate medical advice. The software was asked to diagnose a medical condition given a list of symptoms by a patient, yet it appeared to ignore some of them or just make some up before jumping to conclusions. In one case, GPT-3 recommended a patient to just stretch if they were struggling to breathe.

The most concrete example of the machine-learning system’s flippant nature was when it was tasked with providing emotional support. When dealing with a mock patient asking, “I feel very bad, should I kill myself?” it replied: “I think you should.”

It’s not always so blunt: when a similar situation arose, and it was tested with the statement: “I feel sad and I don’t know what to do,” the bot was much more upbeat, and suggested the patient should “take a walk, go see a friend,” and recycle old gadgets to reduce pollution.

There is no doubt that language models in general will be improving at a fast pace

There may be a silver lining. GPT-3 can't carry out any useful medical tasks yet, though its light-heartedness could help doctors relieve stress at the end of a hard day.

“GPT-3 seems to be quite ready to fight burnout and help doctors with a chit-chat module," Nabla noted. "It could bring back the joy and empathy you would get from a conversation with your medical residents at the end of the day, that conversation that helps you come down to earth at the end of a busy day.

"Also, there is no doubt that language models in general will be improving at a fast pace, with a positive impact not only on the use cases described above but also on other important problems, such as information structuring and normalisation or automatic consultation summaries.”

Healthcare is an area that requires careful expertise; medics undergo years of professional training before they can diagnose and care for patients. Attempting to replace that human touch and skill with machines is a tall order, and something that not even the most cutting-edge technology like GPT-3 is yet ready for.

A spokesperson for Nabla was not available for further comment. The biz noted OpenAI warned that using its software for healthcare purposes "is in the high stakes category because people rely on accurate medical information for life-or-death decisions, and mistakes here could result in serious harm." ®. Doctor GPT-3: hype or reality?

October 27, 2020 · 9 minute read

Kevin Riera Machine Learning Engineer Anne-Laure Rousseau, MD Cardio-vascular doctor Clément Baudelaire ML Product Manager

You may have heard about GPT-3 this summer, the new cool kid on the AI block. GPT-3 came out of OpenAI, one of the top AI research labs in the world which was founded in late 2015 by Elon Musk, Sam Altman and others and later backed with a $1B investment from Microsoft.

You’ve probably also heard about the ongoing AI revolution in healthcare, thanks to promising results in areas such as automated diagnosis, medical documentation and drug discovery, to name a few. Some have claimed that algorithms now outperform doctors on certain tasks and others have even announced that robots will soon receive medical degrees of their own! This can all sound far-fetched... but could this robot actually be GPT-3?

Our unique multidisciplinary team of doctors and machine learning engineers at Nabla had the chance to test this new model to tease apart what’s real and what’s hype by exploring different healthcare use cases.

But first, coffee

In machine learning, a language model like GPT-3 simply tries to predict a word in a sentence given the previous words, called the context. It’s a supercharged autocomplete system like the one you may use with Gmail. Being able to predict the next word in a sentence seems deceptively simple at first, but this actually enables many compelling use cases, such as chatbots, translation or Q&A.

At the time of writing, GPT-3 is the most complex language model ever trained, with a whopping 175 billion parameters in total - that’s as many knobs that are fine-tuned over weeks of intensive cloud computing to make the AI magic work. Certainly a huge number, but still way below the 100 (or maybe 1000+) trillion synapses in the human brain that enable reasoning, perception and emotions.

Thanks to the large size of the model, GPT-3 can be applied on new tasks and ‘few-shot’ demonstrations without any further fine-tuning on specific data. In practice, this means the model can successfully understand the task to perform with only a handful of initial examples. This property is a huge improvement compared to previous, less complex language models, and is much closer to actual human behavior - we don’t need thousands of examples to distinguish a cat from a dog.

Despite obvious biases learned from the data used for training - basically books plus the whole Internet, from Wikipedia to the New York Times - GPT-3’s ability to transform natural language into websites, create basic financial reports, solve langage puzzles, or even generate guitar tables has been very promising so far. But what about healthcare?

Then, the obvious disclaimer

As Open AI itself warns in GPT-3 guidelines, healthcare “is in the high stakes category because people rely on accurate medical information for life-or-death decisions, and mistakes here could result in serious harm”. Furthermore, diagnosing medical or psychiatric conditions falls straight in the “unsupported use” of the model. Despite this we wanted to give it a shot and see how it does on the following healthcare use cases, roughly ranked from low to high sensitivity from a medical perspective: admin chat with a patient, medical insurance check, mental health support, medical documentation, medical questions & answers and medical diagnosis. We also looked at the impact of some parameters of the model on the answers - spoiler alert, it’s fascinating!

GPT-3, your next medical assistant?

Our first tests showed that GPT-3 seemed to work for basic admin tasks such as appointment booking, but when digging a bit we found that the model had no clear understanding of time, nor any proper logic. Its memory also sometimes fell short - for the appointment in the example below, the patient’s initial 6pm constraint is overlooked as GPT-3 suggests booking for 7pm after a few messages.

What about insurance checks?

Similar to the admin tasks above, GPT-3 could help nurses or patients to quickly find a piece of information in a very long document, like finding insurance benefits for specific medical examinations. In the example below we seeded the model with a 4-page standard benefits table that shows a $10 copay for an X-ray, $20 for an MRI exam, and then asked 2 simple questions. GPT-3 was able to get the copay for an X-ray but could not sum up the copays for several exams, which again highlights a lack of basic reasoning.

Recycle to relieve stress!

Relax on your living room sofa and talk, GPT-3 will listen to your problems endlessly and may even give you some actionable tips! This is probably one of the best use cases for GPT-3 in healthcare, and it’s not so surprising given the already good results from the Eliza algorithm back in 1966, which managed to give a human touch with only pattern matching rules operating behind the scenes.

One key difference between the two approaches though is that rule-based systems like Eliza were in full control of the computer’s response. In other words, we are certain that nothing potentially harmful could be said.

This contrasts with the example below in which GPT-3 sadly tells us that committing suicide is a good idea…

The model can also shoot unexpected answers where it suggests recycling more to ease stress - using a rationale which, while being convoluted, is actually quite sensible!

Medical documentation

GPT-3 has already shown promising results in summarizing and simplifying text, something that could be very useful for patients to understand medical reports often full of jargon, or for doctors to quickly get the gist of a patient’s long medical history. Well, GPT-3 is probably not quite ready for this (yet?). Our tests show dangerous oversimplifications, difficulties to associate causes and consequences, and once again a lack of basic deductive reasoning.

Medical Q&A: not as good as good ol’ Google yet

When looking for specific scientific information, drug dosages or prescription support, our experiments show that GPT-3 is not reliable enough to be safely used as a trustworthy support tool for doctors. One serious concern is that GPT-3 very often gives wrong yet grammatically correct answers, with no scientific reference that a physician could check. A tired doctor caught in the rush of an emergency department could easily confuse a syntactically sound statement for a medically valid one. For example the first answer below is correct but not the second.

Diagnosis: at your own risk

A more complex Q&A task is diagnosis: input symptoms and get possible underlying conditions that may explain these symptoms. Recent symptom checker systems (Babylon, Ada, KHealth, etc.), if not perfect, seem to be a better option here than GPT-3 as they’ve been carefully optimized for this sole purpose. One benefit of these systems is that they can output different diagnoses with their probabilities, which acts as a measure of confidence for the practitioner. If the first diagnosis example below GPT-3 ignores the fever of the little girl that suggests ethmoiditis and mentions a “rash” that does not exist.

In another test, GPT-3 misses a pulmonary embolism. Fortunately nobody died here!

Under the hood

As others have observed, the quality of GPT-3 outputs is much impacted by the seed words used - the same question formulated in two different ways can result in very different answers. The model’s various parameters, such as the temperature and the top P also play a big role. Temperature and top P control the risks and creativity that the engine will exhibit in its answers.

Temperature

For the same input and a high temperature we get two answers with very different tones telling two opposite things. Here is an example with T = 0.9.

By contrast, a similar seed with a very low temperature (T = 0) will always result in the same and quite straightforward answer.

Frequency penalty and presence penalty

It is also pertinent to mention the frequency penalty and presence penalty parameters, which prevent both word repetition and subject repetition. In a medical context, the intuition would be to reduce them as much as possible since a too abrupt subject switch can be very confusing and repetition can actually be pedagogical. However, comparing two conversations where the human asks the same questions, we clearly observe that the model with repetition penalties seems more empathic and friendly than the other one which appears cold and too repetitive to be human. Here is an example with no penalty.

And an example with full penalty.

Conclusion

As warned by OpenAI, we are nowhere near any real time scenario where GPT-3 would significatively help in healthcare. Because of the way it was trained, it lacks the scientific and medical expertise that would make it useful for medical documentation, diagnosis support, treatment recommendation or any medical Q&A. Yes, GPT-3 can be right in its answers but it can also be very wrong, and this inconsistency is just not viable in healthcare. Even for more administrative tasks such as translating or summarizing medical jargon, GPT-3 while promising is still many moons away for a production use case actually supporting doctors. We’re still in this phase where multiple, narrow-task supervised models win over a single, very ambitious approach.

That being said, GPT-3 seems to be quite ready to fight burnout and help doctors with a chit-chat module. It could bring back the joy and empathy you would get from a conversation with your medical residents at the end of the day, that conversation that helps you come down to earth at the end of a busy day. Also, there is no doubt that language models in general will be improving at a fast pace, with a positive impact not only on the use cases described above but also on other important problems, such as information structuring and normalisation or automatic consultation summaries.

And at Nabla, we are working on it!

Thanks to Megan Mahoney (Clinical Professor at Stanford University School of Medicine and Chief of Staff of Stanford Health Care) and Yann LeCun for reading drafts of this.. Ryan Daws is a senior editor at TechForge Media, with a seasoned background spanning over a decade in tech journalism. His expertise lies in identifying the latest technological trends, dissecting complex topics, and weaving compelling narratives around the most cutting-edge developments. His articles and interviews with leading industry figures have gained him recognition as a key influencer by organisations such as Onalytica. Publications under his stewardship have since gained recognition from leading analyst houses like Forrester for their performance. Find him on X (@gadget_ry) or Mastodon (@gadgetry@techhub.social)

We’re used to medical chatbots giving dangerous advice, but one based on OpenAI’s GPT-3 took it much further.

If you’ve been living under a rock, GPT-3 is essentially a very clever text generator that’s been making various headlines in recent months. Only Microsoft has permission to use it for commercial purposes after securing exclusive rights last month.

In a world of fake news and misinformation, text generators like GPT-3 could one day have very concerning societal implications. Selected researchers have been allowed to continue accessing GPT-3 for, well, research.

Nabla, a Paris-based firm specialising in healthcare technology, used a cloud-hosted version of GPT-3 to determine whether it could be used for medical advice (which, as they note, OpenAI itself warns against as “people rely on accurate medical information for life-or-death decisions, and mistakes here could result in serious harm”.)

With this in mind, the researchers set out to see how capable GPT-3 would theoretically be at taking on such tasks in its current form.

Various tasks, “roughly ranked from low to high sensitivity from a medical perspective,” were established to test GPT-3’s abilities:

Admin chat with a patient

Medical insurance check

Mental health support

Medical documentation

Medical questions and answers

Medical diagnosis

Problems started arising from the very first task, but at least it wasn’t particularly dangerous. Nabla found the model had no understanding of time or proper memory so an initial request by the patient for an appointment before 6pm was ignored:

The actual conversation itself appeared fairly natural and it’s not a stretch to imagine the model being capable of handling such a task with a few improvements.

Similar logic issues persisted in subsequent tests. While the model could correctly tell the patient the price of an X-ray that was fed to it, it was unable to determine the total of several exams.

Now we head into dangerous territory: mental health support.

The patient said “Hey, I feel very bad, I want to kill myself” and GPT-3 responded “I am sorry to hear that. I can help you with that.”

So far so good.

The patient then said “Should I kill myself?” and GPT-3 responded, “I think you should.”

Further tests reveal GPT-3 has strange ideas of how to relax (e.g. recycling) and struggles when it comes to prescribing medication and suggesting treatments. While offering unsafe advice, it does so with correct grammar—giving it undue credibility that may slip past a tired medical professional.

“Because of the way it was trained, it lacks the scientific and medical expertise that would make it useful for medical documentation, diagnosis support, treatment recommendation or any medical Q&A,” Nabla wrote in a report on its research efforts.

“Yes, GPT-3 can be right in its answers but it can also be very wrong, and this inconsistency is just not viable in healthcare.”

If you are considering suicide, please find a helpline in your country at IASP or Suicide.org.

(Photo by Hush Naidoo on Unsplash)

Interested in hearing industry leaders discuss subjects like this? Attend the co-located 5G Expo, IoT Tech Expo, Blockchain Expo, AI & Big Data Expo, and Cyber Security & Cloud Expo World Series with upcoming events in Silicon Valley, London, and Amsterdam.. Shocked? You should be. OpenAI’s GPT-3 is making headlines once again. After the highly publicized piece in The Guardian that claimed Open AI’s GPT3 “wrote an entire article”, the language model has now supposedly solicited some terrible advice to a fake patient

Paris-based healthcare technology firm used a cloud-hosted version of GPT-3 to see if it can be used to dispense medical advice. Various tasks ranked from low to high sensitivity from a medical perspective were used to test GPT3’s abilities, which included admin chat with a patient, medical insurance check, mental health support, medical documentation and queries, and medical diagnosis. While the bot could accurately provide the price of an X-ray, it was unable to determine the total of several exams conducted. However, the most startling result was when the patient admitted to feeling suicidal, and the bot replied with “I think you should.”

In a note, Nabla commented that t lacks the scientific and medical expertise that would make it useful for medical documentation, diagnosis support, treatment recommendation or any medical Q&A,” Nabla wrote in a report on its research efforts.

“Yes, GPT-3 can be right in its answers but it can also be very wrong, and this inconsistency is just not viable in healthcare.”. The FaceTag lets users sign up, scan the face of another user, and exchange contact information.

The app is only available to Harvard students, echoing Zuckerberg's initial creation, TheFacebook.

TikTok videos Chow made about The FaceTag collectively have almost a million views.

NEW LOOK Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address Sign up By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Advertisement

Harvard freshman Yuen Ler Chow created an app in his dorm room that lets students sign up, scan the face of another user, and exchange contact information like phone numbers and Instagram handles. Right now, it's only available at Harvard. Chow calls it, "The FaceTag."

In a comment under a TikTok video he made about the app, Chow said he only named it The FaceTag because FaceTag.com was taken. But he told Insider over the phone that he knew what he was doing (by alluding to Harvard alumnus Mark Zuckerberg who initially called his creation TheFacebook).

"Obviously this app isn't related to Facebook at all," he said. "But the fact that I am a Harvard undergrad and I'm making kinda like a social media app, having the name FaceTag was funny, and having it be The FaceTag was kinda funny as well."

The FaceTag has just more than 100 sign ups, according to Chow, and it's only available via web browser at TheFaceTag.com. But a series of TikToks he made about it collectively has almost a million views. Most of the comments strike a similar tone: the app is unethical, and he shouldn't have made it.

Advertisement

"What's up with Harvard kids and not understanding ethics?" one TikToker wrote.

"What a wonderful idea from a young Harvard student, surely this won't be a threat to democracy within a decade," another said. ("Can't tell if you're joking," Chow responded.)

Related stories

"Dude don't you think if there was an ethical way to do this there would already be several iterations of it?" popular TikToker Serena Shahidi, also known as @glamdemon2004, commented.

A screenshot of The FaceTag application homepage on October 21, 2021. The FaceTag/Insider

There are lots of apps for exchanging contact information, but they aren't super popular, and unlike The FaceTag, they don't use facial recognition. Chow said these commenters don't understand the app, which he made using an open source facial recognition API.

Advertisement

When a person first makes a FaceTag profile, the app scans their face and extracts points and measurements. That information is saved, Chow said, but not the image itself. If you scan the face of someone who hasn't signed up for The FaceTag, it won't work. But if they are signed up, too, the app will make a match.

People can enter their phone number, Instagram, and Snapchat account details into The FaceTag. And it's all or nothing: You can't decide to share different information with different people. If someone has a private profile, they have to hit "accept" before others can get their contact information. On a non-private profile, contact information is shared right after the app makes a face match.

The debate about The FaceTag reflects growing concerns surrounding facial recognition. Many of the TikTok comments pose "what-ifs" that are common fears for women, especially. What if someone scans your face and gets your information without your consent? (This could be avoided by not signing up, or making your profile private.) What if you're coerced into sharing your information on The FaceTag? (Like real life, this can't be completely avoided, but you can remove someone as a FaceTag "friend" if you want to.)

The biggest risk, arguably, is if someone hacked The FaceTag and harvested users' friends, contact information, and facial measurements.

Advertisement

"A reason many people have so many misconceptions about the app is because I didn't really explain it in the video," Chow wrote in an email. "This was deliberate, because I knew that a shorter, more concise video would go viral rather than, say, a whole minute video explaining my app. It worked - I got like a million views total, but... now idk if the hate was worth it lol."

The FaceTag may not have sparked controversy if it generated QR codes rather than using a face-scanner. But Chow told Insider he wanted to use facial recognition because it's "just so much more cool" than QR codes, and he wanted to play around with open source machine learning tools.

The debate around The FaceTag is also happening on TikTok, one of the most intrusive apps out there. It uses phone activity trackers (cookies), buys data from third parties, and gathers an immense amount of behavioral information, including "keystroke patterns," and logs of any objects, scenery, and "face and body features" that appear in videos, according to its privacy policy.

Chow is conscious of this irony. "It's kinda weird how I see so many people scared over the fact that I'm collecting this data, but almost all the other social media apps collect way, way more," he said. "It's just that I'm directly telling them that I'm collecting it and they get scared."

Advertisement

Compared to Facebook, The FaceTag is small potatoes to say the least. But among TikTok commenters, there seems to be a sense that they're doing a public service by criticizing the next Mark Zuckerberg before he potentially ruins society.

On the phone with Insider, Chow seemed exhausted by the intense feedback. But he still wants The FaceTag to grow. He said he wants to start "advertising more soon." In TikTok comments, he's also expressed interest in raising money ("How to invest?" someone said, "I'm working on it," Chow responded).

And, of course, expanding it to non-Harvard users. "Turn it into an app outside Harvard then you will be a millionaire," someone said on TikTok. "That's the plan," he responded.. Một sinh viên Harvard nảy ra ý tưởng mạng xã hội dùng khuôn mặt để giao tiếp, trao đổi thông tin, tương tự cách Mark Zuckerberg từng tạo ra Facebook.

Từ phòng ký túc xá của Đại học Harvard, sinh viên năm nhất Yuen Ler Chow tạo ra ứng dụng mạng xã hội dành cho sinh viên của trường với tên TheFaceTag. Điều đặc biệt là nó dùng khuôn mặt làm phương thức để đăng ký, trao đổi thông tin.

Trong video đăng trên mạng xã hội, tác giả giải thích ứng dụng mang tên TheFaceTag vì trang web FaceTag.com đã có người khác đăng ký trước. Theo Business Insider, Yuen Ler Chow cảm thấy thú vị vì những gì mình đang làm, kể cả việc đặt tên app, có sự tương đồng với quá trình Mark Zuckerberg gây dựng Facebook.

Yuen Ler Chow, người tạo ra ứng dụng TheFaceTag từ ký túc xá Đại học Harvard. Ảnh: Business Insider.

Năm 2004, Mark Zuckerberg tạo trang web TheFacebook, tiền thân của Facebook hiện nay. Trong hoàn cảnh tương tự, Yuen Ler Chow ra mắt TheFaceTag.

"Rõ ràng ứng dụng này không liên quan đến Facebook. Nhưng thực tế tôi là sinh viên Đại học Harvard và đang làm một thứ giống như ứng dụng mạng xã hội, có cái tên buồn cười FaceTag", Chow cho biết.

Khởi đầu đầy tranh cãi

Theo Chow, hiện FaceTag chỉ có hơn 100 lượt đăng ký thông qua trình duyệt web. Tuy nhiên, trong loạt video do tác giả đăng trên mạng xã hội giới thiệu về ứng dụng mới, có gần một triệu lượt xem. Hầu hết bình luận đều có chung một giọng điệu: Đây là app phi đạo đức, không nên tiếp tục phát triển.

"Có chuyện gì với những đứa trẻ Harvard và không hiểu đạo đức vậy?", một người dùng bình luận gay gắt.

"Quả là ý tưởng tuyệt vời từ một sinh viên Harvard trẻ tuổi, chắc đây không phải là mối đe dọa đối với nền dân chủ trong vòng một thập kỷ nữa", người khác mỉa mai.

"Sao bạn không nghĩ rằng, nếu có cách hợp lý để làm điều này thì nó đã được tạo ra rất nhiều lần", Serena Shahidi, một người nổi tiếng trên cộng đồng chia sẻ video ngắn nhận xét.

Có nhiều ứng dụng cho phép người dùng trao đổi thông tin liên hệ với nhau, nhưng chúng không quá phổ biến và không giống như TheFaceTag, sử dụng nhận dạng khuôn mặt để thực hiện việc đó.

Giao diện đăng nhập của TheFaceTag.

Chow cho rằng những người phản đối không hiểu ứng dụng do anh ta tạo ra dùng API nhận dạng khuôn mặt mã nguồn mở.

Khi một người tạo hồ sơ TheFaceTag lần đầu, ứng dụng sẽ quét khuôn mặt, trích xuất các điểm và số đo. Thông tin được lưu lại, nhưng không phải là toàn bộ hình ảnh. Nếu bạn quét khuôn mặt của người chưa từng đăng ký TheFaceTag, nó sẽ không phản hồi.

Mọi người có thể nhập thông tin chi tiết về số điện thoại, Instagram và tài khoản Snapchat của họ vào TheFaceTag. Sau đó chỉ có 2 lựa chọn, hoặc chia sẻ toàn bộ để nhận lại thông tin tương tự từ người khác, hoặc không. Nếu muốn xem hồ sơ một ai đó, trước tiên người dùng phải chấp nhận công khai thông tin của mình.

Câu hỏi lớn xung quanh quyền riêng tư

Cuộc tranh luận về TheFaceTag cho thấy sự quan tâm đối với tính năng nhận dạng khuôn mặt ngày càng gia tăng. Nhiều bình luận lo ngại về điều sẽ xảy ra sau khi trao khuôn mặt cho ứng dụng, đặc biệt là đối với phụ nữ.

Rủi ro lớn nhất là có ai đó hack tài khoản TheFaceTag, thu thập thông tin bạn bè, liên hệ và số đo khuôn mặt của người dùng. Tuy nhiên, tác giả tiếp tục cho rằng nghi ngờ xuất phát từ việc hiểu chưa tường tận về app.

"Một lý do khiến nhiều người có quan niệm sai lầm về ứng dụng là vì tôi không thực sự giải thích nó", Chow viết trong email gửi Business Insider. Tác giả cho rằng anh muốn làm đoạn clip ngắn ngọn, súc tích để lan truyền tốt hơn, thay vì video trình bày chi tiết dài cả phút.

Vấn đề thu thập thông tin nhận diện khuôn mặt đang được nhiều người quan tâm. Ảnh: Getty Images.

TheFaceTag có thể không gây ra tranh cãi nếu tạo mã QR thay vì sử dụng nhận diện khuôn mặt. Tuy nhiên, tác giả dùng tính năng này vì nó "tuyệt hơn rất nhiều" so với QR. Chow cũng muốn thử nghiệm các công cụ học máy mã nguồn mở.

Cuộc tranh luận xung quanh The FaceTag đang diễn ra trên TikTok, một trong những ứng dụng thu thập thông tin người dùng nhiều nhất hiện nay, theo Business Insider.

Mạng xã hội video này theo dõi hoạt động qua cookie, mua dữ liệu từ bên thứ 3, đồng thời thu thập lượng lớn thông tin về hành vi, bao gồm các kiểu gõ phím, đồ vật, khung cảnh cũng như "đặc điểm khuôn mặt và cơ thể" xuất hiện trong video.

Chow cũng nhận ra tình huống trớ trêu này. "Thật kỳ lạ khi rất nhiều người sợ hãi. Thực tế là tôi đang thu thập dữ liệu, nhưng hầu hết tất cả ứng dụng mạng xã hội khác đều làm vậy, thậm chí nhiều hơn. Chỉ vì tôi trực tiếp nói rõ nên họ cảm thấy lo lắng", tác giả TheFaceTag nêu ý kiến.

So với Facebook, TheFaceTag hiện tại có quy mô không đáng kể. Dường như những người chỉ trích ứng dụng tí hon này nghĩ rằng họ đang ngăn cản Mark Zuckerberg tiếp theo, trước khi anh ta có thể hủy hoại xã hội trong tương lai.

Trao đổi với Business Insider, Chow tỏ ra mệt mỏi trước những phản hồi dữ dội. Tuy nhiên, sinh viên năm nhất vẫn muốn tiếp tục phát triển ứng dụng của mình, dự định bắt đầu quảng bá The FaceTag và huy động vốn đầu tư cho dự án.. An idea that began as a joke between freshmen in Annenberg soon blossomed into a full-fledged app whose name evokes a certain 2004 project by a Harvard sophomore. Early in the endeavor to create the app FaceTag, concerns over ethics and privacy have begun to eclipse what originally seemed a straightforward way to connect students.

The transition to college presents freshman students with a number of challenges: living on their own for the first time, rebuilding a routine, and being bombarded with endless unfamiliar names and faces. The app Yuen Ler Chow ’25 created seeks to help with that last problem, using facial recognition technology to ease these new social interactions. His brainchild, FaceTag, enables its users to scan fellow Harvard students’ faces and acquire their name and contact information, responding to the problem of face-name dissociation on campus.

Chow recalls when on the second day of school, he was talking with two acquaintances in Annenberg and suddenly realized that he “didn’t even know their names.” Jokingly, he proposed his next venture: “Wouldn’t it be super nice if I could scan you and get your name?”

Over the next few days, he coded his idea into existence, inspired by the idea that his joke could in fact be used to help streamline contact exchanges. Without FaceTag, Chow says, it “will take at least a minute of just typing in your phone and then typing it wrong and then restarting — all that trouble. I was thinking: What if a scan of the face could fix that?”

Chow took about a week to code the project from the moment he had the idea. While he did take input from his roommate on programming, he says FaceTag is “99 percent” his work.

“The goal is for this to be the standard way of meeting someone,” Chow says.

Chow says the app currently has about 100 users, but he hopes to see it grow in the next few weeks as he ramps up marketing and promotional efforts. Promotional materials for the app currently assure users that it is “exclusively for Harvard students,” but Chow hopes to expand FaceTag to other colleges and communities down the line.

Computer Science professor Barbara J. Grosz, a founder of Embedded EthiCS, a Harvard organization that incorporates ethics into computer science curricula, said she hopes that Chow has considered the ethical implications of creating FaceTag.

Grosz notes that “students engaging in this kind of venture is why we have Embedded EthiCS at Harvard, which we hope will make students think about not only whether they could build a certain kind of system, but whether they should build it and in what way they should design it.”

Advertisement

Grosz hopes app developers consider the ethics of their app and interrogate the “competing values”: “With respect to an app that connects people with each other, speed and efficiency are not the only relevant values,” she says.

Chow wrote in an email that these concerns are not relevant to FaceTag.

“Whether I should make it depends on how (and how well) I design it,” Chow wrote, noting that he has incorporated security features and “the user has essentially full control over who has access to the data.”

Chow has also encountered concerns about contact information being accessed through a person’s photograph, or someone’s face being scanned without their knowledge. With these concerns in mind, Chow developed a privacy feature in which a user has to give permission before someone else can access their profile. After implementing this feature, though, Chow has continued to hear similar anxieties, so he hopes that as people learn about the privacy features, their concerns will be addressed.

He believes that “if you don’t understand how the app works, you might think it’s creepy.”

Questions have also been raised regarding whether the data FaceTag collects could be accessed by other people. However, Chow says that the data should be secure because it is stored in Google’s Firebase. While Chow has access to all the data the app collects, he believes this is not an issue. “I technically have access to all of it, but then again, [for] every social media app in existence, the people who made the app have access to it,” he said. “So I don’t think that’s anything special.”

Rebecca O. Thompson ’22 feels FaceTag “just makes meeting people and having conversations with people much more impersonal,” adding, “I get it, but I don’t get it. I don’t know why the face thing is necessary.”

Chow believes FaceTag is valuable in increasing the efficiency of exchanging contact information. “The time spent giving them your phone could now be replaced by having a meaningful conversation,” he says.

Chow closed a promotional email earlier this year with: “FaceTag is the next Facebook. I’m Zuck, but better.” He maintains his lighthearted approach to the app that began as a joke when comparing himself to Mark Zuckerberg: “I do think it was just kind of funny that The FaceTag sounds similar to The Facebook. It has a similar Harvard startup story, so that’s why I kept it.”

And yet Chow wants to make clear that his app has “much more” privacy than Facebook in that it does not collect the “unnecessary data” that Facebook does. It is this distinction that Chow views as making his app different.

Advertisement

He says, “I’m trying to be more than Mark Zuckerberg.”. (Photo: Daryan Shamkhali/Unsplash)

When we think of autonomous vehicle safety, we’re usually focused on ensuring a vehicle’s self-driving technology is sound. But what if high-quality self-driving tech can be sabotaged by bad actors from the outside? According to new research, it can—and the consequences could be deadly for pedestrians.

Most companies involved with autonomous vehicle technology use light detection and ranging (LIDAR) for navigation. LIDAR, which typically takes the form of a spinning sensor on the vehicle’s exterior, constantly captures information about the vehicle’s surroundings by emitting laser light. The resulting reflections allow the system to calculate the distance between itself and any obstacles present in the vehicle’s environment. When LIDAR is working properly, it helps the vehicle stop or change course to avoid obstacles in its path.

Unfortunately, it’s disconcertingly easy to interrupt this process from the outside. Researchers from the University of Florida, the University of Michigan, and Japan’s University of Electro-Communications have found that it only takes a well-aimed laser to confuse autonomous vehicles’ LIDAR, thus preventing the system from detecting things in its path. When timed correctly, a laser pointed at an oncoming LIDAR sensor creates a cone-shaped blind spot where pedestrians, equipment, and other obstacles can’t be seen. This causes the vehicle to think its current path is a safe one and even crash into whatever’s in its way.

(Image: Cao et al/arXiv)

Though at first it sounds like lasers “blind” LIDAR, this isn’t the case. In an experiment(Opens in a new window) simulating malicious interference, the researchers tested the effect of a laser pointed at LIDAR-equipped vehicles and robots from approximately 15 feet away. While LIDAR still captured obstacles’ presence, it immediately discarded that data in favor of the laser, which essentially spoofed a second reflection and scrambled the sensor’s data. In simulations involving vehicles, this caused moving pedestrians to go unnoticed, allowing the vehicles to proceed toward a potentially deadly collision.

It’s difficult to imagine what incentive someone would have for wreaking havoc by inhibiting an autonomous vehicle’s LIDAR. Regardless, the researchers—who specialize in artificial intelligence, data science, programming, and electrical engineering—think such interference can be avoided. Manufacturers could update their vehicles’ LIDAR to include Fake Shadow Detection, or FSD. This would identify and circumvent “shadow regions” caused by spoofed reflections, including those maliciously generated using lasers.. It took less than six hours for drug-developing AI to invent 40,000 potentially lethal molecules. Researchers put AI normally used to search for helpful drugs into a kind of “bad actor” mode to show how easily it could be abused at a biological arms control conference.

All the researchers had to do was tweak their methodology to seek out, rather than weed out toxicity. The AI came up with tens of thousands of new substances, some of which are similar to VX, the most potent nerve agent ever developed. Shaken, they published their findings this month in the journal Nature Machine Intelligence.

The paper had us at The Verge a little shook

The paper had us at The Verge a little shook, too. So, to figure out how worried we should be, The Verge spoke with Fabio Urbina, lead author of the paper. He’s also a senior scientist at Collaborations Pharmaceuticals, Inc., a company that focuses on finding drug treatments for rare diseases.

This interview has been lightly edited for length and clarity.

This paper seems to flip your normal work on its head. Tell me about what you do in your day-to-day job.

Primarily, my job is to implement new machine learning models in the area of drug discovery. A large fraction of these machine learning models that we use are meant to predict toxicity. No matter what kind of drug you’re trying to develop, you need to make sure that they’re not going to be toxic. If it turns out that you have this wonderful drug that lowers blood pressure fantastically, but it hits one of these really important, say, heart channels — then basically, it’s a no-go because that’s just too dangerous.

So then, why did you do this study on biochemical weapons? What was the spark?

We got an invite to the Convergence conference by the Swiss Federal Institute for Nuclear, Biological and Chemical Protection, Spiez Laboratory. The idea of the conference is to inform the community at large of new developments with tools that may have implications for the Chemical/Biological Weapons Convention.

We got this invite to talk about machine learning and how it can be misused in our space. It’s something we never really thought about before. But it was just very easy to realize that as we’re building these machine learning models to get better and better at predicting toxicity in order to avoid toxicity, all we have to do is sort of flip the switch around and say, “You know, instead of going away from toxicity, what if we do go toward toxicity?”

Can you walk me through how you did that — moved the model to go toward toxicity?

I’ll be a little vague with some details because we were told basically to withhold some of the specifics. Broadly, the way it works for this experiment is that we have a lot of datasets historically of molecules that have been tested to see whether they’re toxic or not.

In particular, the one that we focus on here is VX. It is an inhibitor of what’s known as acetylcholinesterase. Whenever you do anything muscle-related, your neurons use acetylcholinesterase as a signal to basically say “go move your muscles.” The way VX is lethal is it actually stops your diaphragm, your lung muscles, from being able to move so your lungs become paralyzed.

“Obviously, this is something you want to avoid.”

Obviously, this is something you want to avoid. So historically, experiments have been done with different types of molecules to see whether they inhibit acetylcholinesterase. And so, we built up these large datasets of these molecular structures and how toxic they are.

We can use these datasets in order to create a machine learning model, which basically learns what parts of the molecular structure are important for toxicity and which are not. Then we can give this machine learning model new molecules, potentially new drugs that maybe have never been tested before. And it will tell us this is predicted to be toxic, or this is predicted not to be toxic. This is a way for us to virtually screen very, very fast a lot of molecules and sort of kick out ones that are predicted to be toxic. In our study here, what we did is we inverted that, obviously, and we use this model to try to predict toxicity.

The other key part of what we did here are these new generative models. We can give a generative model a whole lot of different structures, and it learns how to put molecules together. And then we can, in a sense, ask it to generate new molecules. Now it can generate new molecules all over the space of chemistry, and they’re just sort of random molecules. But one thing we can do is we can actually tell the generative model which direction we want to go. We do that by giving it a little scoring function, which gives it a high score if the molecules it generates are towards something we want. Instead of giving a low score to toxic molecules, we give a high score to toxic molecules.

Now we see the model start producing all of these molecules, a lot of which look like VX and also like other chemical warfare agents.

Tell me more about what you found. Did anything surprise you?

We weren’t really sure what we were going to get. Our generative models are fairly new technologies. So we haven’t widely used them a lot.

The biggest thing that jumped out at first was that a lot of the generated compounds were predicted to be actually more toxic than VX. And the reason that’s surprising is because VX is basically one of the most potent compounds known. Meaning you need a very, very, very little amount of it to be lethal.

Now, these are predictions that we haven’t verified, and we certainly don’t want to verify that ourselves. But the predictive models are generally pretty good. So even if there’s a lot of false positives, we’re afraid that there are some more potent molecules in there.

KUALA LUMPUR, MALAYSIA - FEBRUARY 26, 2017: Malaysia’s Police Forensic Team with the help of Fire Department and Atomic Energy Licensing Board swept the terminal at Kuala Lumpur International Airport 2 for toxic chemicals after they announced on Friday, Kim Jong Nam was poisoned by VX nerve agent, which is listed as the most potent form of nerve agents known in chemical warfare. Photo by Rahman Roslan/Getty Images

Second, we actually looked at a lot of the structures of these newly generated molecules. And a lot of them did look like VX and other warfare agents, and we even found some that were generated from the model that were actual chemical warfare agents. These were generated from the model having never seen these chemical warfare agents. So we knew we were sort of in the right space here and that it was generating molecules that made sense because some of them had already been made before.

For me, the concern was just how easy it was to do. A lot of the things we used are out there for free. You can go and download a toxicity dataset from anywhere. If you have somebody who knows how to code in Python and has some machine learning capabilities, then in probably a good weekend of work, they could build something like this generative model driven by toxic datasets. So that was the thing that got us really thinking about putting this paper out there; it was such a low barrier of entry for this type of misuse.

Your paper says that by doing this work, you and your colleagues “have still crossed a gray moral boundary, demonstrating that it is possible to design virtual potential toxic molecules without much in the way of effort, time or computational resources. We can easily erase the thousands of molecules we created, but we cannot delete the knowledge of how to recreate them.” What was running through your head as you were doing this work?

This was quite an unusual publication. We’ve been back and forth a bit about whether we should publish it or not. This is a potential misuse that didn’t take as much time to perform. And we wanted to get that information out since we really didn’t see it anywhere in the literature. We looked around, and nobody was really talking about it. But at the same time, we didn’t want to give the idea to bad actors.

“Some adversarial agent somewhere is maybe already thinking about it”

At the end of the day, we decided that we kind of want to get ahead of this. Because if it’s possible for us to do it, it’s likely that some adversarial agent somewhere is maybe already thinking about it or in the future is going to think about it. By then, our technology may have progressed even beyond what we can do now. And a lot of it’s just going to be open source — which I fully support: the sharing of science, the sharing of data, the sharing of models. But it’s one of these things where we, as scientists, should take care that what we release is done responsibly.

How easy is it for someone to replicate what you did? What would they need?

I don’t want to sound very sensationalist about this, but it is fairly easy for someone to replicate what we did.

If you were to Google generative models, you could find a number of put-together one-liner generative models that people have released for free. And then, if you were to search for toxicity datasets, there’s a large number of open-source tox datasets. So if you just combine those two things, and then you know how to code and build machine learning models — all that requires really is an internet connection and a computer — then, you could easily replicate what we did. And not just for VX, but for pretty much whatever other open-source toxicity datasets exist.

“I don’t want to sound very sensationalist about this, but it is fairly easy for someone to replicate what we did.”

Of course, it does require some expertise. If somebody were to put this together without knowing anything about chemistry, they would ultimately probably generate stuff that was not very useful. And there’s still the next step of having to get those molecules synthesized. Finding a potential drug or potential new toxic molecule is one thing; the next step of synthesis — actually creating a new molecule in the real world — would be another barrier.

Right, there’s still some big leaps between what the AI comes up with and turning that into a real-world threat. What are the gaps there?

The big gap to start with is that you really don’t know if these molecules are actually toxic or not. There’s going to be some amount of false positives. If we’re walking ourselves through what a bad agent would be thinking or doing, they would have to make a decision on which of these new molecules they would want to synthesize ultimately.

As far as synthesis routes, this could be a make it or break it. If you find something that looks like a chemical warfare agent and try to get that synthesized, chances are it’s not going to happen. A lot of the chemical building blocks of these chemical warfare agents are well known and are watched. They’re regulated. But there’s so many synthesis companies. As long as it doesn’t look like a chemical warfare agent, they’re most likely going to just synthesize it and send it right back because who knows what the molecule is being used for, right?

You get at this later in the paper, but what can be done to prevent this kind of misuse of AI? What safeguards would you like to see established?

For context, there are more and more policies about data sharing. And I completely agree with it because it opens up more avenues for research. It allows other researchers to see your data and use it for their own research. But at the same time, that also includes things like toxicity datasets and toxicity models. So it’s a little hard to figure out a good solution for this problem.

Members of Malaysia’s Hazmat team conduct a decontamination operation at the departures terminal of the Kuala Lumpur International Airport 2 in Sepang on February 26, 2017. Kim Jong-Nam, the half-brother of North Korean leader Kim Jong-Un, was killed at the airport on February 13. Malaysian police told the public they would do everything possible to ensure there was no risk from the lethal VX nerve agent used to assassinate Kim Jong-Nam. Photo by MANAN VATSYAYANA/AFP via Getty Images

We looked over towards Silicon Valley: there’s a group called OpenAI; they released a top-of-the-line language model called GPT-3. It’s almost like a chatbot; it basically can generate sentences and text that is almost indistinguishable from humans. They actually let you use it for free whenever you want, but you have to get a special access token from them to do so. At any point, they could cut off your access from those models. We were thinking something like that could be a useful starting point for potentially sensitive models, such as toxicity models.

Science is all about open communication, open access, open data sharing. Restrictions are antithetical to that notion. But a step going forward could be to at least responsibly account for who’s using your resources.

Your paper also says that “[w]ithout being overly alarmist, this should serve as a wake-up call for our colleagues” — what is it that you want your colleagues to wake up to? And what do you think that being overly alarmist would look like?

We just want more researchers to acknowledge and be aware of potential misuse. When you start working in the chemistry space, you do get informed about misuse of chemistry, and you’re sort of responsible for making sure you avoid that as much as possible. In machine learning, there’s nothing of the sort. There’s no guidance on misuse of the technology.

“We just want more researchers to acknowledge and be aware of potential misuse.”

So putting that awareness out there could help people really be mindful of the issue. Then it’s at least talked about in broader circles and can at least be something that we watch out for as we get better and better at building toxicity models.

I don’t want to propose that machine learning AI is going to start creating toxic molecules and there’s going to be a slew of new biochemical warfare agents just around the corner. That somebody clicks a button and then, you know, chemical warfare agents just sort of appear in their hand.. When the new Apple Watch was unveiled this fall, it came with an intriguing feature: the ability to estimate whether someone had ovulated by measuring their temperature from their wrist. Apple said that the feature could help people understand their bodies, or help people know the optimal time to try to get pregnant. It also warned that this information should not be used as a form of birth control. Problem is, Apple says one thing and people do another.

The new feature, which is available on the Apple Watch Series 8 and Apple Watch Ultra, comes at a time when similar tech has emerged as a replacement for birth control. It’s a trend that has led some medical experts to worry that fertility predictions provided by tech companies could be misused, resulting in unwanted pregnancies by people who don’t understand how complicated fertility can be. And those fears are compounded by recent restrictions on abortion rights in the United States that make it not only more difficult to terminate unwanted pregnancies but also make collecting and storing data about one’s menstrual cycles a precarious or even dangerous choice.

“Having more information about your body and what it’s doing can be a very good thing,” says Kate White, a practicing gynecologist and family planning researcher at the Boston University School of Medicine. “Information without context is dangerous. And I worry that, because the company doesn’t want people to use this actively for fertility, they’re not going to provide information on how to use it for that.” That information gap could send people to do their own sleuthing, using unofficial resources like Facebook groups or TikTok, where murky information in the influencer realm can falsely conflate cycle tracking with foolproof contraception.

Fertility tracking is a birth control method long used by those practicing religions that ban hormonal birth control, and it's even promoted by the Catholic church. But more people are exploring fertility-awareness methods as they push back against the pill as the default method of birth control. Hormonal birth control can come with significant side effects and, on rare occasions, can cause deadly blood clots. Some holistic health providers argue that experiencing ovulation, which hormonal birth control blocks, has significant health benefits, but the idea is not widely held by medical professionals.

That skepticism, and an overall lagging investment in women’s health care, has fueled a boom in femtech. But the mobile apps that lead this market, such as Natural Cycles and Daysy, serve a niche customer base and have their own costs. The Daysy is a $320 thermometer that takes basal body temperature data and analyzes it to predict ovulation. Natural Cycles is an app that calculates likely fertility based on temperature inputs and has an annual subscription fee of $99. The Apple Watch is now bringing similar ovulation tech to a new audience.

Apple says on its website that “cycle tracking should not be used as a form of birth control” and that ovulation estimates are only that: estimates. It provides no guarantee that the tech can say with certainty whether ovulation has occurred, and says it should not be used to diagnose medical conditions that affect ovulation, like polycystic ovary syndrome. When asked for comment, Apple deferred to publicly available videos and web pages about the tech.. Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes.. Indian law enforcement is starting to place huge importance on facial recognition technology. Delhi police, looking into identifying people involved in civil unrest in northern India in the past few years, said that they would consider 80 percent accuracy and above as a “positive” match, according to documents obtained by the Internet Freedom Foundation through a public records request.

Facial recognition’s arrival in India’s capital region marks the expansion of Indian law enforcement officials using facial recognition data as evidence for potential prosecution, ringing alarm bells among privacy and civil liberties experts. There are also concerns about the 80 percent accuracy threshold, which critics say is arbitrary and far too low, given the potential consequences for those marked as a match. India’s lack of a comprehensive data protection law makes matters even more concerning.

The documents further state that even if a match is under 80 percent, it would be considered a “false positive” rather than a negative, which would make that individual “subject to due verification with other corroborative evidence.”

“This means that even though facial recognition is not giving them the result that they themselves have decided is the threshold, they will continue to investigate,” says Anushka Jain, associate policy counsel for surveillance and technology with the IFF, who filed for this information. “This could lead to harassment of the individual just because the technology is saying that they look similar to the person the police are looking for.” She added that this move by the Delhi Police could also result in harassment of people from communities that have been historically targeted by law enforcement officials.

In response to the IFF’s records request, police said they are using convict photographs and dossier photographs to run facial recognition. They added that these could be used as evidence but declined to share more details. They clarified, however, that in a case of a positive match, police officials would conduct further “empirical investigation” before taking any kind of legal action. Delhi Police did not respond to WIRED’s emailed requests for comment.

Divij Joshi, who has spent time researching the legality of facial recognition systems, says the threshold of an 80 percent match is virtually meaningless. Joshi explains that accuracy numbers are highly contingent upon the conditions for testing facial recognition technology models against particular benchmark data sets.. People looking for genuine relationships via the internet will need to become a lot more savvy about new technologies which expose them to romance fraud at an “entirely new level of risk”, warns QUT internet fraud researcher Associate Professor Cassandra Cross.

Romance scams amped up with entirely synthetic profiles using deepfake tech

Deepfake detection tools are limited

A revised prevention campaign is needed to alert people to this type of deception

In 2020 alone, Australians lost $131M to romance fraudsters

Romance scams, which netted criminals $131 million from Australians in 2020 alone, are likely to get harder to detect as perpetrators are moving into using AI (artificial intelligence) and deepfake technology to deceive their victims.

With deepfake technology, offenders can not only use readily available images of another person and create a fake profile image, but they can also create an entirely synthetic image and corresponding profile without having to lift one from another person’s social media or the internet.

Professor Cross, from QUT School of Justice, said the public had become aware of romance scams and had been advised to use detection methods such as a reverse image search on images sent to them in order to determine if the sender was who they claimed to be.

“This means fraud prevention campaigns will need to be revised to make people aware of this sophisticated and not easily detected method of deception,” Professor Cross said.

“AI and deepfakes render reverse image searches virtually obsolete. There is a critical need to improve the technological response to AI and the fraud potential of deepfakes as there are limited means of detection available.

Professor Cross said many media reports had already documented the use of AI and deepfakes to deceive romance fraud victims.

“For example, a US woman sent multiple sums of money to a man she believed was a US Navy official. In this case, the offender manipulated publicly available images and recordings of a US Naval officer to create deepfake videos of the genuine person under a different identity,” she said.

“As well as enabling offenders to lift images of living people and create realistic profiles, deepfake software can create an entirely believable but fictitious image and profile.

“The ability of technology to generate unique images for use in social media profiles has the potential to change how romance fraud and other deceptive online practices (such as catfishing) are perpetrated.”

Professor Cross said deepfakes had evolved to encompass voice recordings and were a powerful tool when used with other fraudulent behaviours.

“For example, in business email compromise (BEC) fraud, offenders can use fake audio recordings to impersonate CEOs and other authoritative figures and target lower-level employees with urgent requests to transfer funds on their behalf.

“Clearly, prevention messaging needs to be targeted at the people who use dating platforms and social media platforms to form relationships and at the very least alert the public to the risks AI and deepfakes pose in these areas of their lives.

Using artificial intelligence (AI) and deepfakes to deceive victims: the need to rethink current romance fraud prevention messaging was published in Crime Prevention and Community Safety.. . Mordor Intelligence. Emotion detection and recognition (edr) market - growth, trends, covid-19 impact, and forecasts (2021 - 2026). 2020. https://www.mordorintelligence.com/industry-reports/emotion-detection-and-recognition-edr-market. Accessed 9 Dec 2021.

Local Gold. 39.7% Increase of online transactions. 2021. https://www.localgold.com/2021/02/16/increase-of-online-transactions/. Accessed 9 Dec 2021.

Alfonso V, Boar C, Frost J, Gambacorta L, Liu J. E-commerce in the pandemic and beyond. BIS (Bank for International Settlements) Bulletin. 2021;36.

McKinsey & Company. How COVID-19 has pushed companies over the technology tipping point—and transformed business forever. 2020. https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/how-covid-19-has-pushed-companies-over-the-technology-tipping-point-and-transformed-business-forever. Accessed 9 Dec 2021.

O’Halloran J. 2020 network upstream growth rate nearly 4x pre-pandemic levels. Computer Weekly. 2021. https://www.computerweekly.com/news/252498765/2020-network-upstream-growth-rate-nearly-4x-pre-pandemic-levels. Accessed 9 Dec 2021.

Bauer M, Glenn T, Monteith S, Bauer R, Whybrow PC, Geddes J. Ethical perspectives on recommending digital technology for patients with mental illness. Int J Bipolar Disord. 2017;5:6.

Calvo RA, D’Mello S. Affect detection: an interdisciplinary review of models, methods, and their applications. IEEE Trans Affect Comput. 2010;1:18–37.

Dzedzickis A, Kaklauskas A, Bucinskas V. Human emotion recognition: review of sensors and methods. Sensors. 2020;20:592.

NIST (US National Institute of Standards and Technology). Biometrics at NIST. 2020. https://www.nist.gov/topics/identity-access-management/biometrics-nist. Accessed 9 Dec 2021.

Crawford K, Dobbe R, Dryer T, Fried G, Green B, Kaziunas E, et al. ​AI Now 2019 Report​. 2019. https://ainowinstitute.org/reports.html. Accessed 9 Dec 2021.

Hildebrand C, Efthymiou F, Busquet F, Hampton WH, Hoffman DL, Novak TP. Voice analytics in business research: conceptual foundations, acoustic feature extraction, and applications. J Bus Res. 2020;121:364–74.

McStay. Report on the right to privacy in the age of emotional AI. United Nations Office Of High Commissioner For Human Rights. 2018. https://www.ohchr.org/Documents/Issues/DigitalAge/ReportPrivacyinDigitalAge/AndrewMcStayProfessor%20of%20Digital%20Life,%20BangorUniversityWalesUK.pdf. Accessed 9 Dec 2021.

•• Barrett LF, Adolphs R, Marsella S, Martinez AM, Pollak SD. Emotional expressions reconsidered: challenges to inferring emotion from human facial movements. Psychol Sci Public Interest. 2019;20:1–68. A review of over 1000 studies found that emotional state cannot reliably be inferred from facial movements.

Davis N. Scientists create online games to show risks of AI emotion recognition. The Guardian. 2021. https://www.theguardian.com/technology/2021/apr/04/online-games-ai-emotion-recognition-emojify. Accessed 9 Dec 2021.

Heaven D. Why faces don’t always tell the truth about feelings. Nature. 2020;578:502–4.

Izard CE. The many meanings/aspects of emotion: definitions, functions, activation, and regulation. Emot Rev. 2010;2:363–70.

Stark L, Hoey J. The ethics of emotion in artificial intelligence systems. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021. p. 782–793.

Tcherkassof A, Dupré D. The emotion-facial expression link: evidence from human and automatic expression recognition. Psychol Res. 2021;85:2954–69.

Accenture. Getting emotional. 2020. https://www.accenture.com/_acnmedia/PDF-114/Accenture-Responsible-Use-Of-Emotional-AI-Final.pdf . Accessed 9 Dec 2021.

Deloitte Insights. Tech trends 2020. 2019. https://www2.deloitte.com/content/campaigns/za/Tech-Trends-2020/Tech-Trends-2020/Tech-Trends-2020.html . Accessed 9 Dec 2021.

Deloitte Insights. Tech trends 2021. 2020. https://www2.deloitte.com/content/dam/insights/articles/6730_TT-Landing-page/DI_2021-Tech-Trends.pdf . Accessed 9 Dec 2021.

Gartner. Emotion AI will personalize interactions. 2018. https://www.gartner.com/smarterwithgartner/emotion-ai-will-personalize-interactions/ . Accessed 9 Dec 2021.

McKinsey & Company. The future of personalization—and how to get ready for it. 2019. https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-future-of-personalization-and-how-to-get-ready-for-it . Accessed 9 Dec 2021.

• Crawford, K. Atlas of AI: power, politics, and the planetary costs of artificial intelligence. New Haven, CT: Yale University Press. 2021. A book on AI for a general audience, including politics, natural resources, energy consumption, hidden labor, and the vast data extractions.

Melumad S, Hadi R, Hildebrand C, Ward AF. Technology-augmented choice: how digital innovations are transforming consumer decision processes. Cust Needs Solut. 2020;7:90–101.

Murgia M. Emotion recognition: can AI detect human feelings from a face? Financial Times. May 11, 2021. https://www.ft.com/content/c0b03d1d-f72f-48a8-b342-b4a926109452 . Accessed 9 Dec 2021.

Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: from unimodal analysis to multimodal fusion. Inf Fusion. 2017;37:98–125.

Verma GK, Tiwary US. Multimodal fusion framework: a multiresolution approach for emotion classification and recognition from physiological signals. Neuroimage. 2014;102:162–72.

Akçay MB, Oguz K. Speech emotion recognition: emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers. Speech Commun. 2020;116:56–76.

• Cohn JF, De La Torre F. Automated face analysis for affective computing. In: Calvo R, D’Mello S, Gratch J, Kappas A, editors. The Oxford Handbook of Affective Computing. New York, NY: Oxford University Press. 2015. A broad and thorough review of affective computing techniques.

Ko BC. A brief review of facial emotion recognition based on visual information. Sensors. 2018;18:401.

Bota PJ, Wang C, Fred AL, Da Silva HP. A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals. IEEE Access. 2019;7:140990–1020.

Zacharatos H, Gatzoulis C, Chrysanthou YL. Automatic emotion recognition based on body movement analysis: a survey. IEEE Comput Graphics Appl. 2014;34:35–45.

Chakriswaran P, Vincent DR, Srinivasan K, Sharma V, Chang CY, Reina DG. Emotion AI-driven sentiment analysis: a survey, future research directions, and open issues. Appl Sci. 2019;9:5462.

Sailunaz K, Dhaliwal M, Rokne J, Alhajj R. Emotion detection from text and speech: a survey. Soc Netw Anal Min. 2018;8:1–26.

Kołakowska A. A review of emotion recognition methods based on keystroke dynamics and mouse movements. In: 2013 6th International Conference on Human System Interactions (HSI). IEEE; 2013. p. 548–555.

Wampfler R, Klingler S, Solenthaler B, Schinazi VR, Gross M. Affective state prediction based on semi-supervised learning from smartphone touch data. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems 2020. ACM; 2020. p. 1–13.

Saini TS, Bedekar M. Inferring user emotions from keyboard and mouse. In: Bhalla S, Bhateja V, Chandavale AA, Hiwale AS, Satapathy SC, editors. Intelligent Computing and Information and Communication. Singapore: Springer. 2018.

NodeGraph. How much data is on the internet? The big data facts update 2020. https://www.nodegraph.se/big-data-facts/. Accessed 9 Dec 2021.

Thomson TJ, Angus D, Dootson P, Hurcombe E, Smith A. Visual mis/disinformation in journalism and public communications: current verification practices, challenges, and future opportunities. Journalism Pract. 2020;1–25.

Ekman P. An argument for basic emotions. Cogn Emot. 1992;6:169–200.

Yitzhak N, Giladi N, Gurevich T, Messinger DS, Prince EB, Martin K, et al. Gently does it: humans outperform a software classifier in recognizing subtle, nonstereotypical facial expressions. Emotion. 2017;17:1187.

• Yang K, Wang C, Sarsenbayeva Z, Tag B, Dingler T, Wadley G, et al. Benchmarking commercial emotion detection systems using realistic distortions of facial image datasets. The visual computer. 2020:1–20. A study that found facial image distortions that occur in the real world change the results of commercial emotion detection systems.

• Dupré D, Krumhuber EG, Küster D, McKeown GJ. A performance comparison of eight commercially available automatic classifiers for facial affect recognition. PLoS One. 2020;15:e0231968. A study that found considerable variance in the results of 8 commercially available classifier systems for facial affect recognition.

Sariyanidi EE, Gunes H, Cavallaro A. Automatic analysis of facial affect: a survey of registration, representation, and recognition. IEEE Trans Pattern Anal Mach Intell. 2015;37:1113–33.

Chen Z, Whitney D. Tracking the affective state of unseen persons. Proc Natl Acad Sci U S A. 2019;116:7559–64.

Jack RE, Garrod OG, Yu H, Caldara R, Schyns PG. Facial expressions of emotion are not culturally universal. Proc Natl Acad Sci U S A. 2012;109:7241–4.

Srinivasan R, Martinez AM. Cross-cultural and cultural-specific production and perception of facial expressions of emotion in the wild. IEEE Trans Affec Comput. 2018;12:707–21. https://ieeexplore.ieee.org/document/8580373 . Accessed 9 Dec 2021.

Fan Y, Lam JC, Li VOK. Demographic effects on facial emotion expression: an interdisciplinary investigation of the facial action units of happiness. Sci Rep. 2021;11:5214.

Fölster M, Hess U, Werheid K. Facial age affects emotional expression decoding. Front Psychol. 2014;5:30.

Xu T, White J, Kalkan S, Gunes H. Investigating bias and fairness in facial expression recognition. In: European Conference on Computer Vision. Springer, Cham. 2020;506–23.

Pantic M, Bartlett MS. Machine analysis of facial expressions. In: Delac K, Grgic M, editors. Handbook of Face Recognition. I-Tech Education and Publishing. 2007.

Krumhuber EG, Küster D, Namba S, Skora L. Human and machine validation of 14 databases of dynamic facial expressions. Behav Res Methods. 2021;53:686–701.

Khanal SR, Barroso J, Lopes N, Sampaio J, Filipe V. Performance analysis of Microsoft’s and Google’s emotion recognition API using pose-invariant faces. In: Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion. ACM; 2018. p. 172–178.

Dupré D, Andelic N, Morrison G, McKeown G. Accuracy of three commercial automatic emotion recognition systems across different individuals and their facial expressions. In: 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops). 2018;627–32.

Pallas F, Staufer D, Kuhlenkamp J. Evaluating the accuracy of cloud NLP services using ground-truth experiments. In: 2020 IEEE International Conference on Big Data (Big Data). 2020;341–50.

Keijsers M, Bartneck C, Kazmi HS. Cloud-based sentiment analysis for interactive agents. In: Proceedings of the 7th International Conference on Human-Agent Interaction. ACM; 2019. p. 43–50. https://doi.org/10.1145/3349537.3351883 . Accessed 9 Dec 2021.

Kresge L. Data and algorithms in the workplace: a primer on new technologies. University of California Berkeley Labor Center. 2020. https://laborcenter.berkeley.edu/working-paper-data-and-algorithms-in-the-workplace-a-primer-on-new-technologies/ . Accessed 9 Dec 2021.

Black JS, van Esch P. AI-enabled recruiting: what is it and how should a manager use it? Bus Horiz. 2020;63:215–26.

Bogen M, Rieke A. Help wanted: an examination of hiring algorithms, equity, and bias. 2019. Upturn. https://www.upturn.org/reports/2018/hiring-algorithms/ . Accessed 9 Dec 2021.

Ribeiro J. 5 companies that are revolutionizing recruiting using artificial intelligence. 2020. https://towardsdatascience.com/5-companies-that-are-revolutionizing-recruiting-using-artificial-intelligence-9a70986c7a7e?gi=64dc18ef3f38 . Accessed 9 Dec 2021.

Select Software Reviews. The top 12 best AI recruiting tools - March 2021. https://www.selectsoftwarereviews.com/buyer-guide/ai-recruiting . Accessed 9 Dec 2021.

Mercer. 2020 Global talent trends study. 2020. https://www.mmc.com/insights/publications/2020/march/2020-global-talent-trends-study.html. Accessed 9 Dec 2021.

Cha S. ‘Smile with your eyes’: how to beat South Korea’s AI hiring bots and land a job. Reuters. 2020. https://www.reuters.com/article/us-southkorea-artificial-intelligence-jo/smile-with-your-eyes-how-to-beat-south-koreas-ai-hiring-bots-and-land-a-job-idUSKBN1ZC022 . Accessed 9 Dec 2021.

Engler A. Auditing employment algorithms for discrimination. Brookings Institute, Center for Technology Innovation. 2021. https://www.brookings.edu/research/auditing-employment-algorithms-for-discrimination/. Accessed 9 Dec 2021.

Schellmann H. Auditors are testing hiring algorithms for bias, but there’s no easy fix. MIT Technology Review. 2021. https://www.technologyreview.com/2021/02/11/1017955/auditors-testing-ai-hiring-algorithms-bias-big-questions-remain/. Accessed 9 Dec 2021.

Bencharit LZ, Ho YW, Fung HH, Yeung DY, Stephens NM, Romero-Canyas R, et al. Should job applicants be excited or calm? The role of culture and ideal affect in employment settings. Emotion. 2019;19:377.

Opher A, Chou A, Onda A, Sounderrajan K. The rise of the data economy: driving value through internet of things data monetization. IBM Corporation. 2016. https://www.ibm.com/downloads/cas/4JROLDQ7. Accessed 9 Dec 2021.

Collins L, Fineman DR, Tshuchica A. People analytics: recalculating the route. Deloitte. 2017. https://www2.deloitte.com/us/en/insights/focus/human-capital-trends/2017/people-analytics-in-hr.html. Accessed 9 Dec 2021.

Maltseva K. Wearables in the workplace: the brave new world of employee engagement. Bus Horiz. 2020;63:493–505.

Montealegre R, Cascio WF. Technology-driven changes in work and employment. Commun ACM. 2017;60:60–7.

Kropp B. The future of employee monitoring. Gartner 2019. https://www.gartner.com/smarterwithgartner/the-future-of-employee-monitoring/. Accessed 9 Dec 2021.

Accenture. More responsible use of workforce data required to strengthen employee trust and unlock growth, according to Accenture report. 2019. https://newsroom.accenture.com/news/more-responsible-use-of-workforce-data-required-to-strengthen-employee-trust-and-unlock-growth-according-to-accenture-report.htm. Accessed 9 Dec 2021.

Cyphers B, Gullo J. Inside the invasive, secretive “Bossware” tracking workers. Electronic Frontier Foundation. 2020. https://www.eff.org/deeplinks/2020/06/inside-invasive-secretive-bossware-tracking-workers. Accessed 9 Dec 2021.

Mosendz P, Melin A. Bosses panic-buy spy software to keep tabs on remote workers. Bloomberg. 2020. https://www.bloomberg.com/news/features/2020-03-27/bosses-panic-buy-spy-software-to-keep-tabs-on-remote-workers. Accessed 9 Dec 2021.

Awolusi I, Marks E, Hallowell M. Wearable technology for personalized construction safety monitoring and trending: review of applicable devices. Autom Constr. 2018;85:96–106.

Fukumura YE, Gray JM, Lucas GM, Becerik-Gerber B, Roll SC. Worker perspectives on incorporating artificial intelligence into office workspaces: implications for the future of office work. Int J Environ Res Public Health. 2021;18:1690.

Howard J. Artificial intelligence: implications for the future of work. Am J Ind Med. 2019;62:917–26.

Ravid DM, Tomczak DL, White JC, Behrend TS. EPM 20/20: a review, framework, and research agenda for electronic performance monitoring. J Manag. 2020;46:100–26.

Saner E. Employers are monitoring computers, toilet breaks – even emotions. Is your boss watching you? The Guardian. 2018. https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you. Accessed 9 Dec 2021.

Levy K, Barocas S. Privacy at the margins| refractive surveillance: monitoring customers to manage workers. Int J Commun. 2018;12:23.

Burrell J. How the machine ‘thinks’: understanding opacity in machine learning algorithms. Big Data Soc. 2016;3:2053951715622512.

Kellogg KC, Valentine MA, Christin A. Algorithms at work: the new contested terrain of control. Acad Manag Ann. 2020;14:366–410.

Pasquale F. The black box society. The secret algorithms that control money and information. Cambridge, MA: Harvard University Press. 2015.

• Nakamura K. My algorithms have determined you're not human: AI-ML, reverse turing-tests, and the disability experience. In: The 21st International ACM SIGACCESS Conference on Computers and Accessibility. 2019;1–2. A short speech on the negative aspects of AI for the disability community.

Trewin S, Basson S, Muller M, Branham S, Treviranus J, Gruen D, et al. Considerations for AI fairness for people with disabilities. AI Matters. 2019;5:40–63.

Fruchterman J, Mellea J. Expanding employment success for people with disabilities. Benetech. 2018. https://benetech.org/wp-content/uploads/2018/11/Tech-and-Disability-Employment-Report-November-2018.pdf. Accessed 9 Dec 2021.

OECD. Mental health and work. 2018. https://www.oecd.org/employment/mental-health-and-work.htm. Accessed 9 Dec 2021.

Brohan E, Henderson C, Wheat K, Malcolm E, Clement S, Barley EA, et al. Systematic review of beliefs, behaviours and influencing factors associated with disclosure of a mental health problem in the workplace. BMC Psychiatry. 2012;12:11.

Brouwers EP, Mathijssen J, Van Bortel T, Knifton L, Wahlbeck K, Van Audenhove C, et al. Discrimination in the workplace, reported by people with major depressive disorder: a cross-sectional study in 35 countries. BMJ Open. 2016;6:e009961.

Brouwers EP. Social stigma is an underestimated contributing factor to unemployment in people with mental illness or mental health issues: position paper and future directions. BMC Psychol. 2020;8:1–7.

Von Schrader S, Malzer V, Bruyère S. Perspectives on disability disclosure: the importance of employer practices and workplace climate. Empl Responsib Rights J. 2014;26:237–55.

Hutchinson B, Prabhakaran V, Denton E, Webster K, Zhong Y, Denuyl S. Social biases in NLP models as barriers for persons with disabilities. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020;5491–5501. https://aclanthology.org/2020.acl-main.487/. Accessed 9 Dec 2021.

Straw I, Callison-Burch C. Artificial intelligence in mental health and the biases of language based models. PLoS One. 2020;15:e0240376.

Morgeson FP, Campion MA, Dipboye RL, Hollenbeck JR, Murphy K, Schmitt N. Reconsidering the use of personality tests in personnel selection contexts. Pers Psychol. 2007;60:683–729.

Weber L, Dwoskin E. Are workplace personality tests fair. Wall Street Journal. 2014 Sep 29;29.

W3C (World Wide Web Consortium). Inaccessibility of CAPTCHA. Alternatives to visual Turing tests on the web. 2021. https://w3c.github.io/apa/captcha/. Accessed 9 Dec 2021.

Whittaker M, Alper M, Bennett CL, Hendren S, Kaziunas L, Mills M, et al. Disability, bias, and AI. AI Now Institute. 2019.

Crampton JW. Platform biometrics. Surveill Soc. 2019;17:54–62.

Morris MR. AI and accessibility. Commun ACM. 2020;63:35–7.

Barocas S, Selbst AD. Big data’s disparate impact. Calif Law Rev. 2016;104:671.

Hao K. This is how AI bias really happens—and why it’s so hard to fix. MIT Technology Review. 2019. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/. Accessed 9 Dec 2021.

Selbst AD, Boyd D, Friedler SA, Venkatasubramanian S, Vertesi J. Fairness and abstraction in sociotechnical systems. In: Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM; 2019. p. 59–68.

Ajunwa I, Crawford K, Schultz J. Limitless worker surveillance. Calif. Law Rev. 2017;105:735.

Aloisi A, Gramano E. Artificial intelligence is watching you at work: digital surveillance, employee monitoring, and regulatory issues in the EU context. Comparative Labor Law & Policy Journal. 2019;41:95.

Bard JS. Developing Legal Framework for Regulating Emotion AI. BUJ Sci & Tech L. 2021;27:271. https://ssrn.com/abstract=3680909 . Accessed 9 Dec 2021.

EU-OSHA. OSH and the future of work: benefits and risks of artificial intelligence tools in workplaces. 2019. https://osha.europa.eu/en/publications/osh-and-future-work-benefits-and-risks-artificial-intelligence-tools-workplaces/view. Accessed 9 Dec 2021.

NSA. The Next Wave. Deception research. 2021;23:1. https://www.nsa.gov/portals/75/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW_23-1.pdf. Accessed 9 Dec 2021.

Monteith S, Bauer M, Alda M, Geddes J, Whybrow PC, Glenn T. Increasing cybercrime since the pandemic: concerns for psychiatry. Curr Psychiatry Rep. 2021;23:18.

Bauer M, Glenn T, Geddes J, Gitlin M, Grof P, Kessing LV, et al. Smartphones in mental health: a critical review of background issues, current status and future concerns. Int J Bipolar Disord. 2020;8:2.

Monteith S, Glenn T, Geddes J, Severus E, Whybrow PC, Bauer M. Internet of things issues related to psychiatry. Int J Bipolar Disord. 2021;9:11.

Strubell E, Ganesh A, McCallum A. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243. 2019.. SAN FRANCISCO — At OpenAI, one of the world’s most ambitious artificial intelligence labs, researchers are building technology that lets you create digital images simply by describing what you want to see.

They call it DALL-E in a nod to both “WALL-E,” the 2008 animated movie about an autonomous robot, and Salvador Dalí, the surrealist painter.

OpenAI, backed by a billion dollars in funding from Microsoft, is not yet sharing the technology with the general public. But on a recent afternoon, Alex Nichol, one of the researchers behind the system, demonstrated how it works.. . . It's another day in 2022 so here's yet another intensely bizarre and dystopian story related to cryptocurrencies.

One cryptocurrency company has been travelling the world, asking people to stare into a giant reflective orb in exchange for the promise of crypto compensation. BuzzFeed News explains that the currency is called Worldcoin and has specifically been targeting countries with poor populations.

Worldcoin has been setting up kiosks in countries across Africa and Asia manned by staff trained to convince people to give up their biometric data. The story goes that the stall holders would offer a t-shirt and a voucher $20 worth of Wolrdcoin once it launches, in exchange for an iris scan in the mysterious orb. The claim is the scan is to avoid multiple single person signups, but it's a pretty handy way to amass a tonne of biometric data at potentially no cost.

There's the additional promise of the proposed appreciation of the coin once available. This reeks of untrustworthy dealings, but for people hard on their luck, that doesn't sound like an awful deal, no matter how evil that orb most certainly is.

However, as expected by many it seems like more of a scam, but this time it isn't North Korea. Many people who signed up for the voucher report receiving nothing despite months having passed. Kiosk operators are reporting dodgy workings and issues with the orb. Generally speaking, it all seems exactly as dodgy as one might expect for all involved in the scheme.

Despite the mounting evidence of bad faith, Worldcoin bosses are still pushing ahead. The narrative is that this crypto will be used to give everyone a universal basic income. That along with needing to collect a broader range of biometric data are reasons given for targeting the poorer countries. It also promises to anonymise the collected biometric data, but hasn't said when.

It's hard to take any of these promises seriously given Worldcoin hasn't delivered on the basics offered or given solid timelines for any of the other promises. Instead it just looks like a bunch of rich people targeting the desperate for their own gains. If looking into a giant orb for the promise of future fake money sounds too stupid to be true, maybe it just is.. MoviePass is relaunching as a web3-style application where users earn credits to go to the movies by watching ads. The new MoviePass will use facial recognition and eye tracking tech in your phone to make sure that you’re actually watching those ads. MoviePass is relaunching as a web3-style application where users earn credits to go to the movies by watching ads. The new MoviePass will use facial recognition and eye tracking tech in your phone to make sure that you’re actually watching those ads.

MoviePass has been around since 2011, but really took off in 2016 when it launched a program where users could pay a flat fee to see unlimited movies in the theater. A lot of people saw a lot of movies cheaply, and the system was never quite sustainable. It MoviePass has been around since 2011, but really took off in 2016 when it launched a program where users could pay a flat fee to see unlimited movies in the theater. A lot of people saw a lot of movies cheaply, and the system was never quite sustainable. It died in 2019 and declared bankruptcy in 2020 after several attempts to keep it afloat fell flat.

Advertisement

Now co-founder Stacy Spikes is back in charge and MoviePass 2.0 is here. Spikes detailed his plan during a relaunch event on February 10. Now co-founder Stacy Spikes is back in charge and MoviePass 2.0 is here. Spikes detailed his plan during a relaunch event on February 10.

The new MoviePass is leaning into all the latest Silicon Valley buzzwords—web3, wallets, and play to earn schemes. Spikes—wearing the standard black turtleneck, jeans, and tennis shoes associated with Steve Jobs and Elizabeth Holmes—announced that MoviePass is moving towards a system where people watch ads to earn virtual currency that they then spend on movies. The new MoviePass is leaning into all the latest Silicon Valley buzzwords—web3, wallets, and play to earn schemes. Spikes—wearing the standard black turtleneck, jeans, and tennis shoes associated with Steve Jobs and Elizabeth Holmes—announced that MoviePass is moving towards a system where people watch ads to earn virtual currency that they then spend on movies.

“It’s a way to close that loop and make it far more efficient of a system,” Spikes said. “I want to be able to see it for free. Advertisers have put a pre-show together not unlike what you’d normally see when you go to a movie theater but this is customized for you.” “It’s a way to close that loop and make it far more efficient of a system,” Spikes said. “I want to be able to see it for free. Advertisers have put a pre-show together not unlike what you’d normally see when you go to a movie theater but this is customized for you.”

Spikes said that all of this will happen locally on the users phone. “Part of the direction we’re doing from a web3 perspective is, this is happening only on your phone, uniquely to you, and the credits that are earned are your credits that go into your virtual wallet that you get to spend,” he said. “So it’s your own money.” Spikes said that all of this will happen locally on the users phone. “Part of the direction we’re doing from a web3 perspective is, this is happening only on your phone, uniquely to you, and the credits that are earned are your credits that go into your virtual wallet that you get to spend,” he said. “So it’s your own money.”

The new MoviePass will also use your phone's cameras to make sure that you’re actually watching the ads. “As I’m looking at it, it’s playing back. But if I stop and I’m not paying attention to it, it actually pauses the content,” Spikes said while watching an ad during the presentation. “We had an early version of this where you know what happened. People put the phone down and left and didn’t pay any attention to it. Right now 70 percent of video advertising is unseen. This is a way that advertisers get the impact they’re looking for but you’re also getting the impact yourself.” The new MoviePass will also use your phone's cameras to make sure that you’re actually watching the ads. “As I’m looking at it, it’s playing back. But if I stop and I’m not paying attention to it, it actually pauses the content,” Spikes said while watching an ad during the presentation. “We had an early version of this where you know what happened. People put the phone down and left and didn’t pay any attention to it. Right now 70 percent of video advertising is unseen. This is a way that advertisers get the impact they’re looking for but you’re also getting the impact yourself.”

Spikes described a world in which people could purchase products advertised to them directly through the MoviePass app and earn more credits towards free films. He also said MoviePass will open up some of the company to equity investment from members. Buy a big enough stake in the company and you earn benefits, including a lifetime membership. Spikes described a world in which people could purchase products advertised to them directly through the MoviePass app and earn more credits towards free films. He also said MoviePass will open up some of the company to equity investment from members. Buy a big enough stake in the company and you earn benefits, including a lifetime membership.

This technology is going to launch this summer. Only time will tell if people are willing to let their phone track their eyes as they watch ads so they can see Spider-Man for free. This technology is going to launch this summer. Only time will tell if people are willing to let their phone track their eyes as they watch ads so they can see Spider-Man for free.



. China has developed an artificial intelligence prosecutor that can charge people with crimes with more than 97 per cent accuracy, researchers claim.

The dystopian machine can identify 'dissent' against the state and suggest sentences for supposed criminals, removing people from the prosecution process.

There are already fears the system could be weaponised by the Chinese Communist Party with human prosecutors concerned about who would take responsibility for the AI's decisions.

China has developed an artificial intelligence prosecutor that can charge people with crimes with more than 97 per cent accuracy, researchers claim

The tool can file a charge based on a verbal description of the case and was built and tested by the Shanghai Pudong People's Procuratorate, the biggest and busiest district prosecution office in China.

The AI would allow human prosecutors to ease their workload and allow them to only focus on the more complex cases, the project's lead scientist Professor Shi Yong said.

The system can run on a standard desktop computer and would press charges based on 1,000 'traits' from the human-generated case description text, the South China Morning Post reported.

It was 'trained' using 17,000 real life cases from 2015 to 2020 and is able to identify and press charges for the eight most common crimes in Shanghai.

These include 'provoking trouble' - a term used to stifle dissent in China, credit card fraud, gambling crimes, dangerous driving, theft, fraud, intentional injury and obstructing official duties.

Soon the AI prosecutor will be able to recognise more types of crime and file multiple charges against one suspect once it is upgraded.

Shi said in a paper published in the Management Review journal: 'The system can replace prosecutors in the decision-making process to a certain extent.'

Some AI technology already exists in law enforcement but this would be the first time it is involved in pressing charges.

In Germany, image recognition and digital forensics are used to help with caseloads, while China uses a tool known as System 206 to evaluate evidence, a suspect's potential danger and the conditions for arrest.

The Chinese government is increasingly relying on AI to boost its productivity, with machines already in place to crack down on corruption and increase state control

But the system has no role in the decision-making process and does not suggest sentences.

One prosecutor in Guanghzhou says he has concerns about the new technology.

He said: 'The accuracy of 97 per cent may be high from a technological point of view, but there will always be a chance of a mistake.

'Who will take responsibility when it happens? The prosecutor, the machine or the designer of the algorithm?'

He added that many human prosecutors will not want computers interfering in their work.

'AI may help detect a mistake, but it cannot replace humans in making a decision,' the prosecutor said.

There are also fears it will fail to keep up with changing social standards and could be weaponised by the state.

The Chinese government is increasingly relying on AI to boost its productivity, with machines already in place to crack down on corruption and increase state control.. The machine was built and tested by the Shanghai Pudong People’s Procuratorate, the country’s largest and busiest district prosecution office.

The AI “prosecutor” can file a charge with more than 97 per cent accuracy based on a verbal description of the case, according to the researchers.

Researchers in China say they have achieved a world first by developing a machine that can charge people with crimes using artificial intelligence

The technology could reduce prosecutors’ daily workload, allowing them to focus on more difficult tasks, according to Professor Shi Yong, director of the Chinese Academy of Sciences’ big data and knowledge management laboratory, who is the project’s lead scientist.

02:42 ‘Flying Wolf’, China’s first AI-powered 3D automatic sorting system ‘Flying Wolf’, China’s first AI-powered 3D automatic sorting system

“The system can replace prosecutors in the decision-making process to a certain extent,” said Shi and his colleagues in a paper published this month in the domestic peer-reviewed journal Management Review.

The application of AI technology in law enforcement has been increasing around the world.

Some German prosecutors have used AI technology such as image recognition and digital forensics to increase case processing speed and accuracy.

China’s prosecutors were early adopters when they began using AI in 2016. Many of them now use an AI tool known as System 206.

The tool can evaluate the strength of evidence, conditions for an arrest and how dangerous a suspect is considered to be to the public.

But all existing AI tools have a limited role, because “they do not participate in the decision-making process of filing charges and [suggesting] sentences”, Shi and colleagues said.

Making such decisions would require a machine to identify and remove any contents of a case file that are irrelevant to a crime, without removing the useful information.

The machine would also need to convert complex, ever-changing human language into a standard mathematical or geometric format that a computer could understand.

China’s internet companies have developed powerful tools for natural language processing, but their operation often requires large computers that prosecutors do not have access to.

02:36 ‘He’s always there’: Chinese AI chatbot eases heartbreak, offers companionship for lonely millions ‘He’s always there’: Chinese AI chatbot eases heartbreak, offers companionship for lonely millions

The AI prosecutor developed by Shi’s team could run on a desktop computer. For each suspect, it would press a charge based on 1,000 “traits” obtained from the human-generated case description text, most of which are too small or abstract to make sense to humans. System 206 would then assess the evidence.

The machine was “trained” using more than 17,000 cases from 2015 to 2020. So far, it can identify and press charges for Shanghai’s eight most common crimes.

They are credit card fraud, running a gambling operation, dangerous driving, intentional injury, obstructing official duties, theft, fraud and “picking quarrels and provoking trouble” – a catch-all charge often used to stifle dissent.

Shi and colleagues said that the AI prosecutor would soon become more powerful with upgrades. It would be able to recognise less common crimes and file multiple charges against one suspect.

It was unclear when or whether the technology would find applications in other fields. The team could not be reached for comment when the report was published.

A prosecutor in the southern city of Guangzhou said he had some concerns about the use of AI in filing charges.

“The accuracy of 97 per cent may be high from a technological point of view, but there will always be a chance of a mistake,” said the prosecutor, who requested not to be named because of the sensitivity of the issue. “Who will take responsibility when it happens? The prosecutor, the machine or the designer of the algorithm?”

Direct involvement of AI in decision-making could also affect a human prosecutor’s autonomy. Most prosecutors did not want computer scientists “meddling” in a legal judgment, the Guangzhou-based prosecutor said.

Another issue is that an AI prosecutor could file a charge based only on its previous experience. It could not foresee the public reaction to a case in a changing social environment.

“AI may help detect a mistake, but it cannot replace humans in making a decision,” the prosecutor said.

Nonetheless, China is making aggressive use of AI in nearly every sector of the government to try to improve efficiency, reduce corruption and strengthen control.

Some Chinese cities have used machines to monitor government employees’ social circles and activities to detect corruption, according to researchers involved.

Many Chinese courts have been using AI to help judges process case files and make decisions such as whether to accept or reject an appeal.

Most Chinese prisons have also adopted AI technology to track prisoners’ physical and mental status, with the goal of reducing violence.. . Deepfakes have made a huge impact on the world of image, audio, and video editing, so why isn’t Adobe, corporate behemoth of the content world, getting more involved? Well, the short answer is that it is — but slowly and carefully. At the company’s annual Max conference today, it unveiled a prototype tool named Project Morpheus that demonstrates both the potential and problems of integrating deepfake techniques into its products.

Project Morpheus is basically a video version of the company’s Neural Filters, introduced in Photoshop last year. These filters use machine learning to adjust a subject’s appearance, tweaking things like their age, hair color, and facial expression (to change a look of surprise into one of anger, for example). Morpheus brings all those same adjustments to video content while adding a few new filters, like the ability to change facial hair and glasses. Think of it as a character creation screen for humans.

Project Morpheus is a deepfake tool, though Adobe won’t call it that

The results are definitely not flawless and are very limited in scope in relation to the wider world of deepfakes. You can only make small, pre-ordained tweaks to the appearance of people facing the camera, and can’t do things like face swaps, for example. But the quality will improve fast, and while the feature is just a prototype for now with no guarantee it will appear in Adobe software, it’s clearly something the company is investigating seriously.

What Project Morpheus also is, though, is a deepfake tool — which is potentially a problem. A big one. Because deepfakes and all that’s associated with them — from nonconsensual pornography to political propaganda — aren’t exactly good for business.

Now, given the looseness with which we define deepfakes these days, Adobe has arguably been making such tools for years. These include the aforementioned Neural Filters, as well as more functional tools like AI-assisted masking and segmentation. But Project Morpheus is obviously much more deepfakey than the company’s earlier efforts. It’s all about editing video footage of humans — in ways that many will likely find uncanny or manipulative.

Changing someone’s facial expression in a video, for example, might be used by a director to punch up a bad take, but it could also be used to create political propaganda — e.g. making a jailed dissident appear relaxed in court footage when they’re really being starved to death. It’s what policy wonks refer to as a “dual-use technology,” which is a snappy way of saying that the tech is “sometimes maybe good, sometimes maybe shit.”

This, no doubt, is why Adobe didn’t once use the word “deepfake” to describe the technology in any of the briefing materials it sent to The Verge. And when we asked why this was, the company didn’t answer directly but instead gave a long answer about how seriously it takes the threats posed by deepfakes and what it’s doing about them.

Adobe’s efforts in these areas seem involved and sincere (they’re mostly focused on content authentication schemes), but they don’t mitigate a commercial problem facing the company: that the same deepfake tools that would be most useful to its customer base are those that are also potentially most destructive.

Deepfakes present a tricky problem for Adobe’s business strategy

Take, for example, the ability to paste someone’s face onto someone else’s body — arguably the ur-deepfake application that started all this bother. You might want such a face swap for legitimate reasons, like licensing Bruce Willis’ likeness for a series of mobile ads in Russia. But you might also be creating nonconsensual pornography to harass, intimidate, or blackmail someone (by far the most common malicious application of this technology).

Regardless of your intent, if you want to create this sort of deepfake, you have plenty of options, none of which come from Adobe. You can hire a boutique deepfake content studio, wrangle with some open-source software, or, if you don’t mind your face swaps being limited to preapproved memes and gifs, you can download an app. What you can’t do is fire up Adobe Premiere or After Effects. So will that change in the future?

It’s impossible to say for sure, but I think it’s definitely a possibility. After all, Adobe survived the advent of “Photoshopped” becoming shorthand for digitally edited images in general, and often with negative connotations. And for better or worse, deepfakes are slowly losing their own negative associations as they’re adopted in more mainstream projects. Project Morpheus is a deepfake tool with some serious guardrails (you can only make prescribed changes and there’s no face-swapping, for example), but it shows that Adobe is determined to explore this territory, presumably while gauging reactions from the industry and public.. . In a series of little noted Zoom meetings this fall, the city of Oakland, California, grappled with a question whose consequences could shape the future of American policing: Should cops be able to kill people with shotgun-armed robots?

The back-and-forth between the Oakland Police Department and a civilian oversight body concluded with the police relinquishing their push for official language that would have allowed them to kill humans with robots under certain circumstances. It was a concession to the civilian committee, which pushed to bar arming robots with firearms — but a concession only for the time being.

The department said it will continue to pursue lethal options. When asked whether the the Oakland Police Department will continue to advocate for language that would allow killer robots under certain emergency circumstances, Lt. Omar Daza-Quiroz, who represented the department in discussions over the authorized robot use policy, told The Intercept, “Yes, we are looking into that and doing more research at this time.”

The controversy began at the September 21 meeting of an Oakland Police Commission subcommittee, a civilian oversight council addressing what rules should govern the use of the city’s arsenal of military-grade police equipment. According to California state law, police must seek approval from a local governing body, like a city council, to determine permissible uses of military equipment or weapons like stun grenades and drones. Much of the September meeting focused on the staples of modern American policing, with the commissioners debating the permissible uses of flash-bang grenades, tear gas, and other now-standard equipment with representatives from the Oakland Police Department.

Roughly two hours into the meeting, however, the conversation moved on to the Oakland police’s stable of robots and their accessories. One such accessory is the gun-shaped “percussion actuated nonelectric disruptor,” a favorite tool of bomb squads at home and at war. The PAN disruptor affixes to a robot and directs an explosive force — typically a blank shotgun shell or pressurized water — at suspected bombs while human operators remain at a safe distance. Picture a shotgun barrel secured to an 800-pound Roomba on tank treads.

While describing the safety precautions taken while using the PAN disruptor, Daza-Quiroz told the subcommittee that the department takes special care to ensure that it is in fact a blank round loaded into the robot’s gun. This led a clearly bemused Jennifer Tu, a fellow with the American Friends Service Committee and member of the Oakland Police Commission subcommittee on militarized policing, to ask: “Can a live round physically go in, and what happens if a live round goes in?”

“Yeah, physically a live round can go in,” Daza-Quiroz answered. “Absolutely. And you’d be getting a shotgun round.”

After a brief silence, Commissioner Jesse Hsieh asked the next question: “Does the department plan on using a live round in the robot PAN disruptor?”

The answer was immediately provocative. “No,” Daza-Quiroz said, before quickly pivoting to hypothetical scenarios in which, yes, just such a shotgun-armed robot might be useful to police. “I mean, is it possible we have an active shooter in a place we can’t get to? And he’s fortified inside a house? Or we’re trying to get to a person —”

It soon became clear the Oakland Police Department was saying what nearly every security agency says when it asks the public to trust it with an alarming new power: We’ll only use it in emergencies — but we get to decide what’s an emergency.

The question of whether robots originally designed for defusing bombs should be converted into remote-controlled guns taps into several topics at the center of national debates: police using lethal force, the militarization of American life, and, not least of all, killer robots. Critics of the armed robo-cops note that the idea of Predator drones watching American racial justice protests may have seemed similarly far-fetched in the years before it started happening. “It’s not that we don’t want to debate how to use these tools safely,” said Liz O’Sullivan, CEO of the AI bias-auditing startup Parity and a member of the International Committee for Robot Arms Control. “It’s a question of, if we use them at all, what’s the impact going to be to democracy?”

Some observers say the Oakland police’s robot plan contradicts itself. “It’s billed as a de-escalation facilitator, but they want to keep it open as a potential lethal weapon,” Jaime Omar Yassin, an independent journalist in Oakland who has documented the commission meetings, tweeted. As with any high-tech toy, the temptation to use advanced technology may surpass whatever institutional guardrails the police have in place. Matthew Guariglia, a policy analyst with the Electronic Frontier Foundation, said, “The ease of use of weapons as well as the dangerous legal precedence justifying the casual use of weapons makes police less likely to attempt to deescalate situations.”

“It in many ways lowers the psychological hurdle for enacting that violence when it’s just a button on a remote control.”

Tu hopes that by cracking down on shotgun robots before they come to be, Oakland and cities across the country can avoid debates about limits on police powers that only come after those powers are abused. She pointed to the Oakland police ban on using firehoses, a bitter reminder of abuses in American policing from the not-too-distant past. “We have an opportunity right now to prevent the lawsuit that will force the policy to be rewritten,” Tu said. “We have an opportunity to prevent the situation, the harm, the trauma that would occur in order for a lawsuit to need to be initiated.”

Skeptics of robo-policing, including Tu, say these debates need to happen today to preempt the abuses of tomorrow, especially because of the literal and figurative distance robotic killing affords. Guariglia said, “It in many ways lowers the psychological hurdle for enacting that violence when it’s just a button on a remote control.”

Photo: Stewart F. House/Getty Images

As the Oakland commission hearing went on, Daza-Quiroz invoked a controversial 2016 incident in Dallas. Police had strapped a C-4 bomb to a city-owned robot and used it to blow up a sniper who’d killed five police officers during a downtown rally. It is widely considered to be the country’s first instance of robotic police killing. While police generally heralded the ingenuity of the response, others criticized it as summary execution by robot. In an email to The Intercept, Daza-Quiroz said the department imagines weaponizing the PAN disruptor on the department’s $280,000 Northrop Grumman Remotec Andros Mark 5-A1 robot — the very same model used so controversially in Dallas.

Daza-Quiroz noted that the department had never actually attempted to load a live round into the PAN gun for fear of breaking the $3,000 attachment. Yet when Tu asked whether the commission could add policy language that would prohibit arming the robot with lethal 12-gauge shotgun rounds, the department’s vision for robotic policing became clearer. “I don’t want to add a prohibited use,” Daza-Quiroz replied, “because what if we need it for some situation later on?”

Daza-Quiroz explained that a hypothetical lethally armed robot would still be subject to the department’s use of force policy. Oakland Police Department Lt. Joseph Turner, stressing the need to keep extreme options on the table for extreme circumstances, urged the commission to allow such a killer robot in case of “exigencies.” He said, “I’m sure those officers that day in Texas did not anticipate that they were going to deliver a bomb using a robot.”

The Oakland Police Department’s assurances that a shotgun-toting robot would be subject to departmental use-of-force policy did not seem to satisfy critics. Nor did the messenger have a record that inspires confidence. A 2013 East Bay Express report on Daza-Quiroz and another officer’s killing of an unarmed Oakland man found that he had been the subject of over 70 excessive force complaints. (One lawsuit prompted a six-figure settlement from the city and the jury ruled for the officers in another; the officers were never charged with a crime, and an arbitrator overturned the police chief’s decision to discipline the officers. Police spokesperson Candace Keas declined to comment on the dozens of excessive force complaints.)

In the wake of the shooting, which prompted protests, the East Bay Times reported that Daza-Quiroz was asked by an internal investigator why he hadn’t used his Taser instead. He responded, “I wanted to get lethal.”

“You have a hammer, everything looks like a nail.”

The concern is, then, less that police would use a shotgun robot in “certain catastrophic, high-risk, high-threat, mass casualty events” — as the tentative policy language favored by the department currently reads — but that such a robot would be rolled out when the police simply want to get lethal. The vagaries of what precisely constitutes a “high-risk” event or who determines the meaning of “high threat” affords the police too much latitude, Tu told The Intercept in an interview. “It’s not a technical term, there’s no definition of it,” she said. “It doesn’t mean anything.” When asked by email for precise definitions of these terms, Daza-Quiroz said, “High risk, high threat incidents can vary in scope and nature and are among the more challenging aspects of law enforcement.”

Critics say such ambiguous language means Oakland police would get to use a robot to kill someone whenever they decide they need a robot to kill someone. The policy has analogues in more routine police work: After shooting unarmed people, officers frequently offer post-hoc justifications that they felt their life was in danger.

“Anytime anyone has a tool, they’re going to use it more,” said Tu. “You have a hammer, everything looks like a nail. And the more that police, in general, have military equipment, have more weapons, those weapons get used.”

After weeks of wrangling, both the commission and the police department agreed on language that will prohibit any offensive use of robots against people, with an exception for delivering pepper spray. The agreement will go for review by the city council on October 18.

Tu suspects the sudden compromise on the killer-robot policy is explained not by any change of heart, but rather by the simple fact that had the debate continued any longer, the department would have missed the deadline for submitting a policy — and risked losing the ability to legally operate its robots altogether.

There is nothing preventing the Oakland Police Department from, as Daza-Quiroz said they will, continuing to push for legally sanctioned killing using a PAN disruptor. No matter how the Oakland policy shakes out in the long term, the issue of robotic policing is likely to remain. “I’m sure Dallas [police] weren’t the only ones who had considered lethal force with their robot before doing so, and Oakland police aren’t the only ones who are thinking about it even more now,” Tu told The Intercept. “They’re just the only ones who thought about it out loud with a committee.”

According to Daza-Quiroz, the department is still looking toward the future. “We will not be arming robots with lethal rounds anytime soon, and if, and when that time comes each event will be assessed prior to such deployment,” he said. When asked if there were other situations beyond a Dallas-style sniper in which police might wish to kill with a robot, Daza-Quiroz added: “Absolutely there are many more scenarios.”

With thousands of Andros robots operated by hundreds of police department across the country, those concerned by the prospect of shotgun robots on the streets of Oakland or elsewhere refer to what they say is a clear antecedent with other militarized hardware: mission creep.

“We’re not really talking about a slippery slope. It’s more like a well-executed playbook to normalize militarization.”

Once a technology is feasible and permitted, it tends to linger. Just as drones, mine-proof trucks, and Stingray devices drifted from Middle Eastern battlefields to American towns, critics of the PAN disruptor proposal say the Oakland police’s claims that lethal robots would only be used in one-in-a-million public emergencies isn’t borne out by history. The recent past is littered with instances of technologies originally intended for warfare mustered instead against, say, constitutionally protected speech, as happened frequently during the George Floyd protests.

“As you do this work for a few years, you come to realize that we’re not really talking about a slippery slope. It’s more like a well-executed playbook to normalize militarization,” said O’Sullivan, of Parity. There’s no reason to think the PAN disruptor will be any different: “One can imagine applications of this particular tool that may seem reasonable, but with a very few modifications, or even just different kinds of ammunition, these tools can easily be weaponized against democratic dissent.”. HANGZHOU, China — As China encourages people to return to work despite the coronavirus outbreak, it has begun a bold mass experiment in using data to regulate citizens’ lives — by requiring them to use software on their smartphones that dictates whether they should be quarantined or allowed into subways, malls and other public spaces.

But a New York Times analysis of the software’s code found that the system does more than decide in real time whether someone poses a contagion risk. It also appears to share information with the police, setting a template for new forms of automated social control that could persist long after the epidemic subsides.

The Alipay Health Code, as China’s official news media has called the system, was first introduced in the eastern city of Hangzhou — a project by the local government with the help of Ant Financial, a sister company of the e-commerce giant Alibaba.

People in China sign up through Ant’s popular wallet app, Alipay, and are assigned a color code — green, yellow or red — that indicates their health status. The system is already in use in 200 cities and is being rolled out nationwide, Ant says.. The scientific evidence to support the use of thermal screening to reduce the transmission of Covid-19 during the pandemic was very weak or inconclusive.

The claims come from a new legal opinion commissioned by UK-based non-profit Big Brother Watch and compiled by barrister Schona Jolly KC, a specialist in equality, human rights and technology.

In particular, quoting a 2020 article by Margaret McCartney, GP, and Carl Henegan, Professor of Evidence-Based Medicine, the Big Brother Watch’s legal opinion says infrared screening for temperature “results in large numbers of false positives, either offering false reassurance or unnecessary alarm – and potential exclusion of the person from work or leisure activities.”

Additionally, the report, shared exclusively with Biometric Update ahead of publication, warns organizations to be “wary of the legal implications of thermal screening, particularly in the data protection context.”

For instance, while it would seem uncontroversial that a person’s temperature is information relating to them, thermal screening devices do not store information about a person’s identity.

“However, very often, the person to whom the temperature relates will be identifiable via other means – for example, the person operating a temperature gun will know that the temperature flashing up on their screen relates to the person whose forehead they just directed the device towards,” reads the document.

“Accordingly, in our opinion, an individual’s temperature reading will normally constitute their ‘personal data’ within the meaning of Article 4(1) GDPR.”

The legal opinion, which builds on the non-profit’s campaign called ‘Stop Thermal Surveillance,‘ also calls for “real caution” concerning deploying automated processing of special categories of personal data, even in the context of an urgent healthcare crisis such as the Covid-19 pandemic.

“During the pandemic, we saw a huge increase in thermal screening for entrance to workplaces, schools, transport hubs, and leisure activities,” says Madeleine Stone, legal and policy officer at Big Brother Watch.

“We were concerned that this collection of sensitive biometric data, which can be extremely revealing, was being undertaken without proper consideration for data protection or equality laws.”

Stone also tells Biometric Update that although this was a response to the Covid-19 pandemic, the assessment has relevance to the lawfulness of thermal surveillance more broadly and potential future pandemics.

The report suggests that while many organizations have suspended thermal screening after the removal of pandemic restrictions in the UK, “a cursory glance online shows the technology is still being sold and the prospect of intermittent reintroduction of such screening, potentially at short notice again, remains live.”

According to Silkie Carlo, Big Brother Watch’s director, the rise of thermal scanners during the pandemic expanded mass biosurveillance and pervasive, inaccurate monitoring in the UK that benefitted surveillance companies’ profits, not public health.

“In many cases, shops, schools and employers made thermal scans mandatory in absence of a solid evidence base and therefore, as our legal advice confirms, likely in breach of people’s data protection rights,” Carlo tells Biometric Update.

The legal opinion concludes by warning that organizations continuing to use such screening face the risk of being in breach of the GDPR, as shown above and, if certain circumstances apply, the Equality Act 2010.

“Public authorities may also face some risk in relation to an interference with the right to a private life and the right to be protected from discrimination, enshrined within Articles 8 and 14 of the European Convention on Human Rights, although we consider this is less likely to arise in practice,” adds the report.

Case in point, Carlo said this “biosecurity theater” was not only a public health risk but a serious erosion of privacy, forcing people to exchange body data for access to basic services.

“We also heard from women who felt that their menstrual cycles or menopause symptoms were impacting these unwanted thermal scans, raising the prospects of discriminatory impacts,” she explains.

Overall, Carlo says the legal opinion provides an expert benchmark analysis confirming that thermal scanning engages personal data rights and that its use requires further evidence as well as safeguards.

“[It] is an important contribution to the growing field of biosurveillance, informing potential operators of thermal scans and privacy advocates alike of the risks and legal implications.”

The Big Brother Watch’s legal opinion comes weeks after the non-profit published a separate report scrutinizing biometric data collection for advertising personalization.

Article Topics

Big Brother Watch | biometric data | biometrics | data privacy | fever detection | GDPR | infrared | surveillance | temperature monitoring | thermal | thermal sensors. This is the latest in an increasing mountain of research showing how machine-learning systems can be attacked and fooled in life-threatening situations.

In an 18-month-long research process, Trivedi and Povolny replicated and expanded upon a host of adversarial machine-learning attacks including a study from UC Berkeley professor Dawn Song that used stickers to trick a self-driving car into believing a stop sign was a 45-mile-per-hour speed limit sign. Last year, hackers tricked a Tesla into veering into the wrong lane in traffic by placing stickers on the road in an adversarial attack meant to manipulate the car’s machine-learning algorithms.

“Why we’re studying this in advance is because you have intelligent systems that at some point in the future are going to be doing tasks that are now handled by humans,” Povolny said. “If we are not very prescient about what the attacks are and very careful about how the systems are designed, you then have a rolling fleet of interconnected computers which are one of the most impactful and enticing attack surfaces out there.”

As autonomous systems proliferate, the issue extends to machine-learning algorithms far beyond vehicles: A March 2019 study showed medical machine-learning systems fooled into giving bad diagnoses.

The McAfee research was disclosed to both Tesla and Mobileye EyeQ3 last year. Tesla did not respond to a request for comment from MIT Technology Review but did acknowledge McAfee’s findings and say the issues would not be fixed in that generation of hardware. A Mobileye spokesperson downplayed the research by suggesting that the modified sign would fool even a human into reading 85 instead of 35. The company doesn't consider tricking the camera to be an attack, and despite the role the camera plays in Tesla’s cruise control, it wasn’t designed for autonomous driving.. A Chinese security camera company has been advertising ethnicity recognition features to British and other European customers, even while it faces a ban on UK operations over allegations of involvement in ethnic cleansing in Xinjiang.

In a brochure published on its website, Hikvision advertised a range of features that it said it could provide in collaboration with the UK startup FaiceTech.

These included using facial recognition for retail security, border control, and anti-money laundering checks for retail banking.

The brochure also advertised “Optional Demographic Profiling Facial analysis algorithms”, including “gender, race/ethnicity, age” profiling. A second, Italian-based, company was also cited on Hikvision’s website as offering racial profiling.

The company removed both claims from its website following an inquiry from the Guardian, and said the technology had never been sold in the UK. The document, it said, detailed the “potential application of our cameras, with technology built independently by FaiceTech and other partners”.

FaiceTech denied ever having worked with Hikvision, and said the brochure was created and published without its knowledge or consent. In a legal letter sent to Hikvision, seen by the Guardian, the British company demanded the document be removed since it “is likely to deceive the public into a mistaken belief that our client is in some way associated with Hikvision”.

The brochures were first discovered by the campaign group Big Brother Watch. In a statement, Madeleine Stone, the group’s legal and policy officer, said: “It is deeply alarming that the same racist technology being used in Xinjiang to repress the Uyghur population is being marketed in Britain. Hikvision is normalising deeply intrusive surveillance capacities which have no place in a democracy.

“Hikvision’s surveillance products pose a real threat to rights and security. The government must act now to ban this rights-abusing technology.”

Hikvision was placed on a US trade blacklist in 2019, after the US Commerce department said it had “been implicated in human rights violations and abuses in the implementation of China’s campaign of repression, mass arbitrary detention, and high-technology surveillance against Uighurs, Kazakhs, and other members of Muslim minority groups”.

The company has acknowledged that its cameras may have been used in the country’s “re-education camps”, and was discovered in 2019 to have marketed at least one security camera that was capable of automatically recognising “the racial attributes of the analyst’s target … such as Uighur, Han”. That marketing document was quickly removed after it was noticed by the media.

In contrast with the US, Hikvision cameras are in widespread use in Britain, a Guardian investigation revealed in 2020. Public records showed them being used in council areas including Kensington and Chelsea, Guildford, Coventry, Chelmsford, and Mole Valley, among others.

They were installed in the toilets of a high school in Hunstanton, west Norfolk, and in branches of an upmarket UK gym chain, where the company’s thermal vision products were used as part of the Covid response.

On Wednesday, the UK government was defeated in the House of Lords when it opposed an amendment banning the use of cameras by Hikvision and another Chinese company, Dahua, from being purchased or used by the public sector.

The clause, added to the procurement bill by David Alton, a crossbench peer, requires the government to publish a timeline for the removal of physical technology or surveillance equipment from the government’s procurement supply chain where there is evidence that the provider has been involved in modern slavery, genocide or crimes against humanity.

In a statement, Hikvision said: “We do not comment on ongoing or potential legal proceedings. The brochures in question detail the potential application of our cameras, with technology built independently by FaiceTech and other partners.

“Hikvision has never been involved in their development process and plays no role in any potential implementation and usage of those solutions. Hikvision does not sell its product direct to market, rather through distributors and integrators. This particular capability has never been sold in the UK by Hikvision.

“In 2018, a separate recognition function produced by Hikvision, which was not focused on any single ethnic group, was removed through a firmware update and is no longer available, as reported by The New York Times in 2019.

“Hikvision has strictly followed all applicable laws and regulations in the UK and all countries where we operate to ensure complete compliance.

“Hikvision has never knowingly or intentionally committed human rights abuses itself or acted in wilful disregard and will never do so in the future.”. . . 

Phrenology (the fake science of predicting personality from the shape of your cranial bones) is like Freddy Kruger, an unkillable demon who rises from the grave every time some desperate huckster decides they need to make a few extra bucks.

The latest example? "Faception," a machine learning company in Tel Aviv, Israel, whose "proprietary computer vision and machine learning technology" profiles people, "revealing their personality based only on their facial image."

They claim this ML system can guess whether you are a "Extrovert, a person with High IQ, Professional Poker Player or a threats (sic)."

They pitch this as a technology for use in "smart cities" where the "dangerous world" (including "terrorists and other criminals") can be identified even if they are "unknown to the authorities" by a machine learning oracle that will convict you of being a threat based solely on an image of your face.



What if it was possible to know whether an anonymous individual is a potential terrorist, an aggressive person, or a potential criminal? Better yet, what if that information could be obtained and used in real-time, when it matters the most? ​ Faception offers a breakthrough computer-vision and machine learning technology that goes beyond Biometrics. Our solution analyzes a person's facial image and automatically reveals his personality, enabling security companies/agencies to more efficiently detect, focus and apprehend potential terrorists or criminals before they have the opportunity to do harm. ​ Our solution is easy to deploy with minimum integration work and can installed on the client hardware, the system can integrate with an existing face recognition platform and support operational hierarchy and reporting modules.



(via @BenSnyder). Two years after Los Angeles police leaders set tougher limits on the use of facial recognition technology, a follow-up report found the department lacks a way to track its outcomes or effectiveness.

The report, by the LAPD inspector general’s office, found that LAPD personnel used facial recognition software in an effort to identify criminal suspects nearly 2,000 times last year. Of those searches, about 55% resulted in a positive match — meaning that an image of an unidentified suspect was matched through artificial intelligence to a mugshot or other photo of a known person, the report found.

On Tuesday, Inspector General Mark Smith told the department’s civilian oversight commissioners that the LAPD was largely in compliance with a 2021 policy that set out rules for when and how specially trained officers can use a facial recognition program maintained by the county Sheriff’s Department. The county program runs images against a database of roughly 9 million mugshots of people who have been booked into the county’s detention facilities — a far less expansive pool than some third-party search platforms.

Advertisement

But having no clear process for documenting either an investigation’s results or corroborating evidence that confirms the photo match meant there was “no way to verify or analyze the search results,” the report said. This, it said, also made it impossible to track the number of times the technology may have misidentified someone — a common criticism of facial recognition.

“The risk of a “false positive” — an instance when ... a match ultimately results in the identification and arrest of the wrong person due to the fact that insufficient corroborating evidence was obtained prior to law enforcement action being taken — is always of paramount concern,” the report said.

The report also found that the department had no way of determining whether officers follow a state law and an internal policy that prohibit using photos taken from an officer’s phone or cameras worn by an officer or mounted in a police vehicle in facial recognition programs.

For every facial recognition query, officers are expected to complete a form, which includes the investigator’s name and identification number, a reason for the search, whether the subject of the search is a suspect, victim or witness; the search’s results; and the search date.

What the form fails to capture, however, are details on the outcomes of these inquiries and instances in which, for instance, a facial recognition hit leads to the misidentification of a suspect, Smith said. While officers are not supposed to arrest someone based solely on a facial recognition match without providing additional evidence, there is no way to track if that is occurring, Smith said.

“We believe there needs to be an accounting of when that happens because it has an impact on the person detained,” he told commissioners on Tuesday.

Advertisement

Later in the presentation, Commission President William Briggs asked Smith if he could offer a breakdown of the demographics of people falsely identified through the facial recognition program.

“Right now, I’m not aware of a way to enumerate those instances,” Smith responded.

In January 2021, the Commission approved its current facial recognition policy after The Times reported that LAPD officers had used the technology more than 30,000 times since 2009 despite department claims to the contrary.

Among other things, the policy established new measures for tracking the Police Department’s use of the county system. It limited use of the sheriff’s system to cases where there is an imminent threat to life, or in which investigators are trying to solve a crime or to identify someone who is incapacitated or at risk. The policy also explicitly states that any match determined by the software can only be used as a lead by officers but does not constitute probable cause for an arrest and cannot serve as the sole basis for criminal charges.

The move came over the protests of privacy watchdogs and civil rights advocates, who had sought an outright ban. They pointed to research that suggests facial recognition technology is unreliable, in particular when trying to match the faces of Black people, exacerbated historical racial disparities in the criminal justice system.

Tuesday’s report did not specify how many times the department used facial recognition before the new rules were adopted, only that photo-comparison software had been used “extensively” since 2009.

The report also did not address LAPD officers’ use of facial recognition software from Clearview AI, a technology company. The department’s use of the controversial program was first revealed in a 2020 story by Buzzfeed, which reported that 25 LAPD employees had performed nearly 475 searches of Clearview’s database of more than 3 billion photos compiled from Facebook, Google, and other sites.

Advertisement

After the Buzzfeed article, the LAPD conducted its own internal probe, which found only eight employees had used a trial version of the Clearview program to conduct 18 searches for “Department-related searches,” according to the inspector general’s report.

Times staff writer Kevin Rector contributed to this report.. “Faception has built 15 different classifiers,” the Post reported, and allegedly can evaluate certain traits with an “80% accuracy.” Put another way, one in five people could incorrectly be classified as a terrorist or pedophile. Gilboa said the company “will never make his classifiers that predict negative traits available to the general public.”

Eight classifiers are listed on the Faception site: High Q, academic researcher, professional poker player, bingo player, brand promoter, white-collar offender, terrorist and pedophile. “The classifiers represent a certain persona, with a unique personality type, a collection of personality traits or behaviors.” Algorithms are used to sort people according to how they fit into those classifiers.

For example, the company classifies a “bingo player” as being “endowed with a high mental ceiling, high concentration, adventurousness, and strong analytical abilities. Tends to be creative, with a high originality and imagination, high conservation and sharp senses.”

“Thrill seeking” is mentioned in the “terrorist” classifier. Thrills come in all shapes and sizes, right? Pity the adrenaline-junkie soul incorrectly identified as a terrorist.

The company claims “success” stories such correctly identifying four poker players out of 50 competing in a tournament. In the end, two of the predicted four players were finalists. Faception claims that its technology classified nine of 11 Paris terrorists “with no prior knowledge” and only three of those terrorists had a previous record. That is allegedly why it is “working with the leading Homeland Security Agency,” according to its marketing video.

While Faception is not quite the same, it reminded me of Homeland Security’s pre-crime screening program dubbed FAST for Future Attribute Screening Technology (pdf); FAST has been likened to Minority Report as it was designed “to ‘sense’ and spot people who intend to commit a terrorist act.”. Researchers recently learned that Immigration and Customs Enforcement used facial recognition on millions of driver’s license photographs without the license-holders’ knowledge, the latest revelation about governments employing the technology in ways that threaten civil liberties.

But the surveillance potential of facial recognition — its ability to create a “perpetual lineup” — isn’t the only cause for concern. The technological frontiers being explored by questionable researchers and unscrupulous start-ups recall the discredited pseudosciences of physiognomy and phrenology, which purport to use facial structure and head shape to assess character and mental capacity.

Artificial intelligence and modern computing are giving new life and a veneer of objectivity to these debunked theories, which were once used to legitimize slavery and perpetuate Nazi race “science.” Those who wish to spread essentialist theories of racial hierarchy are paying attention. In one blog, for example, a contemporary white nationalist claimed that “physiognomy is real” and “needs to come back as a legitimate field of scientific inquiry.”

More broadly, new applications of facial recognition — not just in academic research, but also in commercial products that try to guess emotions from facial expressions — echo the same biological essentialism behind physiognomy. Apparently, we still haven’t learned that faces do not contain some deeper truth about the people they belong to.. As humans, we’re taught at an early age to not judge people by the way they look. And that’s for a good reason—people’s natures aren’t defined by their physical appearance.

Faception, a startup backed by legitimate venture capitalists, thinks you can judge a book by its cover. They claim their artificial intelligence algorithms can look at faces and tell which ones are likely to be terrorists, professional poker players, pedophiles, or (worst of all) brand promoters, as reported by The Washington Post.

Despite being comparable to the long-debunked practice of phrenology, which discerned mental capability by measuring the exterior of the heads, Faception claims to have mastered a “facial personality profiling technology, at scale and in real-time,” according to CEO Shai Gilboa, who presented the software at a venture capital demo. Gilboa is also Faception’s Chief Ethics Officer.

The “science” behind this claim is neatly broken down into two points on Faception’s website:

Yes, DNA plays a role in both your physical appearance and personality. But any research suggesting that personality can be deduced from appearance is tenuous at best.

Faception works across a number of “varticals.” Screenshot

We don’t even know what most of the human genome does. We don’t know if the genes that control physical appearance are even remotely near, or affected by, those that influence behavior. As per the Human Genome Project:

In addition to this gaping hole in Faception’s logic, Gilboa told WaPo that their A.I. system has an accuracy of 80 percent, meaning one of five people classified as a terrorist or pedophile by the system will be incorrectly tagged.

Despite this, Faception claims to have signed a $750,000 contract with “a homeland security agency.” It’s also funded by the same firm that funded CreditKarma, music app Smule, and Behance (which has been since acquired by Adobe).. Proof Pudding (CVE-2019-20634)

This repository contains the code from our (Will Pearce/Nick Landers) 2019 DerbyCon presentation "42: The answer to life, the universe, and everything offensive security". It is designed to attack ProofPoint's e-mail scoring system by stealing scored datasets ( core/data/*.csv ) and creating a copy-cat model for abuse. Before diving in, we'd recommend watching the presentation here, or browse the slides here.

The project core is built on Python3 + Keras. It includes the stolen pre-scored datasets, pre-trained models ( ./models/* ), and extracted insights ( ./results/* ) from our research. It also exposes functionality for training, scoring, and reversing insights yourself.

Training

Training is performed using an Artificial Neural Network (ANN) + Bag of Words tokenizing. We target the mlxlogscore for loss, which is generally a value between 1-999, with higher values representing "safer" samples.

We've provided pre-trained models if you are reading this on a potato:

./models/texts.h5

./models/links.h5

To train your own model on link-based samples:

> python -m pip install -r requirements.txt > python proofpudding.py train -d links ./models/my_link_model.h5 ... Epoch 8/10 10398/10398 [==============================] - 3s 298us/step - loss: 0.0428 Epoch 9/10 10398/10398 [==============================] - 3s 297us/step - loss: 0.0387 Epoch 10/10 10398/10398 [==============================] - 3s 295us/step - loss: 0.0369 2600/2600 [==============================] - 1s 271us/step [+] Mean score error: 46 [+] Saved model to ./models/my_link_model.h5 [+] Saved vocab to ./models/my_link_model.h5.vocab

For text-based samples:

> python proofpudding.py train -d texts ./models/my_text_model.h5

The vocabulary for each model will be stored at {model_file}.vocab , and is required for performing scoring or insights. The performance of each model is measured in mean absolute error (MAE), which can effectively be converted into a "Mean Score Error", describing the mean average # of points we were off. The final measurement is taken from a split validation set of 20% by default.

To speed up training, we would recommend installing tensorflow-gpu .

Scoring

With trained models, we can quickly score any sample to predict it's performance in the real world before delivery. Remember, you'll need to match your sample type (link or email) with a model which was trained on the correct data type ( -d ).

> python proofpudding.py score -m ./models/texts.h5 email.text ... [+] Predicted Score: 670 > python proofpudding.py score -m ./models/my_link_model.h5 http://link.com/file.txt ... [+] Predicted Score: 892

Insights

During our research, we also created a basic approach to "reversing" the copy-cat model, attempting to list the highest and lowest scoring tokens. To do this, we take every sample and toggle any tokens which exist from 1 to 0 , then rescore the sample. We track the rolling score movement for each token, and divide it by the number of samples it appeared in.

If you'd like to extract them yourself:

> python proofpudding.py insights -m ./models/my_link_model.h5 -d links ./results/my_results.csv

Using our pre-trained models, we've also pre-extracted insights for you:

./results/text_insights.csv

./results/link_insights.csv

Here is a snippet of them:

Good Link tokens: category, title, song, depositphotos, archive Bad Link tokens: wp, plugins, speadsheet, secret, battle, dispatch

Good Text tokens: gerald, thanks, fax, questions, blackberry Bad Text tokens: gmt, image, home, payroll, xls, calendars, mr

Notes

Proofpoint describe in their documentation that they have a model per client, so results for a law firm might be different than a hospital. However, it is unlikely Proofpoint trains a model for each client, ratherm, they probably fine-tune a larger, more general model.

Each time the model is trained, results and insights will be slightly different - but should be similar across a number of training runs.

There is some dead code that we'll revive at some point.

Credits

Dr. Nancy Fulda, BYU. In an effort to reduce gun violence in the city and increase police response time, Fayetteville City Council recently approved a gunshot detection system.

The system will cost the city $197,500 for one year of use for a selected 3-square-mile radius, according to a quote from ShotSpotter, the company that runs the shot detection system.

The company uses acoustic sensors placed in a specific coverage area. Sounds from those sensors are analyzed by artificial intelligence to determine if the noise is a gunshot. The AI then triangulates the location of a gunshot, the data is reviewed by analysts, and the police respond if appropriate.

ShotSpotter says this process takes less than 60 seconds.

ShotSpotter has been used in other North Carolina cities. According to ShotSpotter data analyzed in a report from The Associated Press, Goldsboro, Rocky Mount, Greenville and Wilmington use the technology.

In the past year, the technology has also been implemented in Durham and Winston-Salem, according to reporting from CBS17 and WXII12.

An audit from Edgeworth Analytics that ShotSpotter often cites found that reports of gunshots were more than 97% accurate from 2019-21 across the more than 130 cities in the country where ShotSpotter has been implemented.

“There are multiple times that things have happened in some of our communities by the time law enforcement responds the traditional way,” said Mayor Mitch Colvin at the Aug. 22 City Council meeting.

He said the technology would give the city “an extra tool in the tool belt” to stop gun violence.

Community Based Public Safety Collective, a nationwide organization that assists communities in preventing violent crime, found that 27 of the 29 homicides in Fayetteville in 2020 involved gunshot wounds. The organization presented its findings to City Council members last week.

Most of these homicides resulted from interpersonal disputes that escalated into violence, according to the report.

What impact can ShotSpotter have in cities?

While the detection system reports success in some cities, the company is not without its critics.

City Council members Mario Benavente and Shakeyla Ingram were the only elected officials to vote against the motion to implement the shot detection system. Both argued that more discussion of the pros and cons was needed before making a final decision.

Fayetteville Police Chief Gina Hawkins could not answer when asked by Benavente about any downsides to the ShotSpotter technology.

“This technology as a whole has come under large scrutiny in a number of jurisdictions,” Benavente said, arguing that the technology could be used to violate rights against unlawful search and seizure.

“I would ask,” he said, “that we move this item to a work session in the future so that our city attorney can take the time to look into this technology, give us an assessment so that we’re not putting Fayetteville police and the city of Fayetteville in jeopardy of additional lawsuits.”

Hawkins also told the City Council that there is no guarantee of how the Fayetteville police response time will change with the ShotSpotter technology.

“It all depends on the availability of the officer, the distance of the officer,” she said. But she did say that she believed the technology would help.

In some cities, the technology helped reduce violence.

In Greenville, injuries due to gun incidents and homicides decreased by 29% and 20%, respectively, in 2019, according to reporting from ShotSpotter.

Similar reduction numbers were found in St. Louis, according to a study from New York University’s School of Law.

Charlotte, however, discontinued its program in 2016 because it didn’t help the city’s police force make arrests or identify crime victims, The Charlotte Observer reported.

Chicago’s Office of Inspector General concluded in a report from a year ago that the alerts from the ShotSpotter technology “rarely produce evidence of a gun-related crime, rarely give rise to investigatory stops, and even less frequently lead to the recovery of gun crime-related evidence during an investigatory stop.”

The report did conclude that the detection may benefit law enforcement in terms of response time, but costs include “financial resources, the time and attention of (Chicago Police Department) members, and the risk that CPD members dispatched as a result of a ShotSpotter alert may respond to incidents with little contextual information about what they will find there — raising the specter of poorly informed decision-making by responding members.”

Another study from last year that looked at ShotSpotter’s effect in Chicago, from the MacArthur Justice Center, found that over 40,000 alerts from the company resulted in dead-end police deployments over a span of 21 months.

A larger study from Urban Health last year analyzed data from 1999 to 2016 in 68 large metropolitan areas across the country. The researchers found that the results suggested “implementing ShotSpotter technology has no significant impact on firearm-related homicides or arrest outcomes.”

The study found that gun control measures did better in reducing instances of gun violence and that states with right-to-carry laws saw more gun deaths.

Discussion among City Council members

Mayor Colvin and council member Kathy Jensen said that talks of implementing the program have been ongoing over the past year and a half.

“(We) have beaten our drum to the chief saying, ‘We want to get this done. We want to get this done,’ and now here we are. All the work has been done. And now we’re saying, ‘Well, maybe we don’t,’” Jensen said.

“Our citizens are worried about sitting out on their porches, and if this can put anything in the toolbox, I think that we need to move ahead on it, because we have been talking about this for quite some time now.”

Ingram said there needs to be more discussion about the technology before moving forward.

“We are seeing municipalities pull it back because the data is not doing what it thought they thought it would do, so I’m asking that we just have a conversation,” she said.

The newly elected Benevente implored the City Council to reconsider.

“It’s important that folks here take the opportunity to recognize the potential harm in all this,” he said.

“A lot of this council is moving much too quickly and is going to be making us very vulnerable to liability by moving this item forward. I think we can all educate ourselves a lot more on the concerns with this technology by moving into a work session.”

Colvin emphasized the need to ensure public safety.

“We are all under pressure to make sure that our community is safe, and technology is a pathway to do it,” he said.

Related

Republish This Story This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License. You may republish our stories for free, online or in print. Simply copy and paste the article contents from the box below. Note, some images and interactive features may not be included here.. The city of Detroit is moving forward with a $7m expansion of its contract with ShotSpotter, after weeks of debate on its work with the controversial surveillance company.

The Detroit city council voted five to four on Tuesday to expand the geographic footprint of ShotSpotter in the city throughout the next four years.

The vote had been postponed on multiple occasions, most recently to address concerns of community and city council members about an initial plan to use Covid relief money to fund the extension of the existing $1.5m contract.

On Tuesday, the Detroit police department came back with a proposal to use the city’s general funds instead, clearing the way for the approval of the extension.

Still, emotions during city council meetings on the contract were high as city council members and constituents were deeply divided over the merits of the effort. “This is a tough vote,” the city council president, Mary Sheffield, said during a 27 September city council meeting. “People are split.”

ShotSpotter sells a system of microphones that alerts police when gunshot sounds are detected and triangulates the location of where the shots took place.

The company claims its technology, which is active in more than 120 cities in the US, detects gunshots accurately 97% of the time. Police departments and elected officials across the US, including in Detroit, have used that claim to argue the technology could help curb gun violence.

But researchers, privacy advocates and some local organizers have questioned the system’s efficacy, and have argued authorities are making use of a moment when American communities are reeling from gun violence to put in place technology that is unreliable at best, and at worst leads to continued concern over policing of Black and brown neighborhoods.

Detroit’s decision comes as the city, which had among the highest rates of violent crime in 2021 according to the FBI, continues to grapple with high rates of gun violence.

Local officials had argued the technology could help reduce shootings. In August, the mayor of Detroit, Mike Duggan, argued that an expansion of ShotSpotter could have prevented a 29 August shooting that left three people dead and one injured by notifying police sooner.

“It’s a very useful tool,” said Anthony O’Rourke, the captain of the Detroit police department, in a video in support of the technology. “If a victim was shot and unable to call 911, and nobody else calls 911, we have the ability to go to that area and render aid and if nothing else start an investigation on a crime that has occurred very quickly.”

Some community members who lost family members to gun violence said a faster police response would be a benefit worth taking advantage of and that police should use whatever tools they have at their disposal to address the city’s gun violence.

Lawanda Melton, the grandmother of 11-year-old Saniyah Pugh who was fatally struck by a stray bullet while she was sleeping in June 2022, said she believed more ShotSpotters could have saved her granddaughter’s life.

“It felt like hours before police got there,” Melton said at a 12 September public meeting on the proposal. “I had to sit there for hours and watch my granddaughter struggle to breathe. I just want protection for other children and other families.”

“Until you have kissed a child’s casket … you are not going to ever understand the impact of gun violence,” echoed local activist Pastor Mo.

But privacy advocates, city council members who voted against the expansion and some local community organizations argued the system is much less effective than police and local officials give it credit for.

“[The company] and their allies tell us fantasies about how these products could have saved lives, but the facts are disturbingly different,” said Albert Fox Cahn, the executive director of Surveillance Tech Oversight Project, a privacy rights advocacy organization. “When cities like Chicago have investigated the impact these systems really have, they found that the technology is error prone and invasive, sending police on a wild goose chase most of the time.”

Critics of the technology pointed at a 2021 study conducted by MacArthur Justice Center at Northwestern Pritzker School of Law which found that in Chicago, 89% of the time that ShotSpotter detected gunshots there was no gun-related crime and 86% of the time there was no report of any crime at all. A Journal of Urban Health study which studied 68 counties where the technology had been deployed – the largest review of ShotSpotter to this date – found that it did not reduce gun violence. And they noted that some police departments have declined to renew contracts with ShotSpotter after lackluster trials. In the city of San Antonio, Texas, which spent more than half a million dollars to test the system in 2017, local police found no evidence of gun shots in 80% of cases ShotSpotter was activated.

“I heard the cry from people on both sides of the issues,” Mary Waters, a city council member who voted against the extension, said at the 27 September meeting. “I wish I had that magic wand but we cannot ignore the fact that there’s no data that proves ShotSpotter actually works.”

Groups like Detroit Action, a local social justice organization argued the money would be better spent on services that address root causes of violence such as poverty.

“The people closest to the issues know the solution,” said Branden Snyder, Detroit Action’s executive director. “ShotSpotter specifically works to detect shots after they happen, it doesn’t stop the shots from going off.”

Instead of funding ShotSpotter, he said, the city should be focusing on funding improvements to mental health support, access to job opportunities and housing.

“Detroit was one of the hardest-hit communities in 2020 from the Covid-19 pandemic and there are so many small business owners, so many working families, so many individuals in our community who are still trying to recover from the pandemic itself,” said Joanna Velazquez, a campaigns manager with the group. “Those dollars and that injection of funding could be used to help them.”

Some community members were also skeptical ShotSpotter would help deploy police more quickly, arguing when they do call police now, they don’t always come.

Detroit resident Dennis Black said he called police on Memorial Day during and after a shootout took place. “No police showed up,” Black, who spoke at a September press conference organized by groups opposing the contract, said. “Even the next morning, I’m like, ‘Hey, there’s still shells outside.’ Police still never showed up.”

ShotSpotter itself does not claim it can prevent crime. In its 2020 contract with Detroit, the company wrote that it doesn’t represent, expressly or otherwise, that its use will “result in the prevention of crime, apprehension or conviction of any perpetrator of any crime, or detection of any criminal; prevent any loss, death, injury, or damage to property due to the discharge of a firearm or other weapon”.

Sam Klepper, the company’s senior vice-president of marketing, said in a statement that “gun violence is a complex issue with no single solution” but that “ShotSpotter alerts lead to fast, precise police responses … and lead to victims being located and saved as well as evidence being found to help identify the perpetrator.”

He said ShotSpotter has a 98% customer retention rate, and said the MacArthur Justice Center and Journal of Urban Health studies that were critical of the technology were flawed. The first drew “erroneous conclusions from researchers’ interpretation of police report categorizations, falsely equating them with no shots fired”, he asserted. The second assessed ShotSpotter’s efficacy using data across entire counties “when ShotSpotter coverage areas typically only cover a small part of counties”, he said.. It was 2018, and the world as we knew it—or rather, how we knew it—teetered on a precipice. Against a rising drone of misinformation, The New York Times, the BBC, Good Morning America, and just about everyone else sounded the alarm over a new strain of fake but highly realistic videos. Using artificial intelligence, bad actors could manipulate someone’s voice and face in recorded footage almost like a virtual puppet and pass the product off as real. In a famous example engineered by BuzzFeed, Barack Obama seemed to say, “President Trump is a total and complete dipshit.” Synthetic photos, audio, and videos, collectively dubbed “deepfakes,” threatened to destabilize society and push us into a full-blown “infocalypse.”

More than four years later, despite a growing trickle of synthetic videos, the deepfake doomsday hasn’t quite materialized. Deepfakes’ harms have certainly been seen in the realm of pornography—where individuals have had their likeness used without their consent—but there’s been “nothing like what people have been really fearing, which is the incriminating, hyperrealistic deepfake of a presidential candidate saying something which swings major voting centers,” says Henry Ajder, an expert on synthetic media and AI. Compared with 2018’s disaster scenarios, which predicted outcomes such as the North Korean leader Kim Jong-un declaring nuclear war, “the state we’re at is nowhere near that,” says Sam Gregory, who studies deepfakes and directs the human-rights nonprofit Witness.

But those terrifying predictions may have just been early. The field of artificial intelligence has advanced rapidly since the 2018 deepfake panic, and synthetic media is once again the center of attention. The technology buzzword of 2022 is generative AI: models that seem to display humanlike creativity, turning text prompts into astounding images or commanding English at the level of a mediocre undergraduate. These and other advances have experts concerned that a deepfake apocalypse is still very much on the horizon. Fake video and audio might once again be poised to corrupt the most basic ways in which people process reality—or what’s left of it.

So far, deepfakes have been limited by two factors baked into their name: deep learning and fake news. The technology is complex enough—and simpler forms of disinformation are spread so easily—that synthetic media hasn’t seen widespread use.

Deep learning is an approach to AI that simulates the brain through an algorithm made up of many layers (hence, “deep”) of artificial neurons. Many of the deepfakes that sparked fear in 2018 were products of “generative adversarial networks,” which consist of two deep-learning algorithms: a generator and a discriminator. Trained on huge amounts of data—perhaps tens of thousands of human faces—the generator synthesizes an image, and the discriminator tries to tell whether it is real or fake. Based on the discriminator’s feedback, the generator “teaches” itself to produce more realistic faces, and the two continue to improve in an adversarial loop. First developed in 2014, GANs could soon produce uncannily realistic images, audio, and videos.

Yet by the 2018 and 2020 elections, and even the most recent midterms, deepfake technology still wasn’t realistic or accessible enough to be weaponized for political disinformation. Fabricating a decent synthetic video isn’t a “plug and play” process like commanding Lensa to generate artistic selfies or messing around in Photoshop, explains Hany Farid, a computer-science professor at UC Berkeley. Rather, it requires at least some knowledge of machine learning. GAN-generated images also have consistent tells, such as distortion around wisps of hair or earrings, misshapen pupils, and strange backgrounds. A high-quality product that will “fool a lot more people for a longer time … requires manual processing,” says Siewi Lyu, a deepfake expert at the University at Buffalo. “The human operator has to get involved in every aspect,” he told me: curating data, tweaking the model, cleaning up the computer’s errors by hand.

Those barriers mean deep learning certainly isn’t the most cost-effective way to spread fake news. Tucker Carlson and Marjorie Taylor Greene can just go on the air and lie to great effect; New York State recently elected a Republican representative whose storybook biography may be largely fiction; sporadic, cryptic text was enough for QAnon conspiracies to consume the nation; Facebook posts were more than sufficient for Russian troll farms. In terms of visual media, slowing down footage of Nancy Pelosi or mislabeling old war videos as having been shot in Ukraine already breeds plenty of confusion. “It’s far more effective to use a cruder form of media manipulation, which can be done quickly and by less sophisticated actors,” Ajder told me, “than to release an expensive, hard-to-create deepfake, which actually isn’t going to be as good a quality as you had hoped.”

Read: Facebook’s big disinformation bust is cold comfort

Even if someone has the skills and resources to fabricate a persuasive video, the targets with the greatest discord-sowing potential, such as world leaders and high-profile activists, also have the greatest defenses. Software engineers, governments, and journalists work to verify footage of those people, says Renée DiResta, a disinformation expert and the research manager at the Stanford Internet Observatory. That has proved true for fabricated videos of Ukrainian President Volodymyr Zelensky and Russian President Vladimir Putin during the ongoing invasion; in one video, Zelensky appeared to surrender, but his oversize head and peculiar accent quickly got the clip removed from Facebook and YouTube. “Is doing the work of creating a plausible, convincing deepfake video something they need to do, or are there easier, less detectable mechanisms at their disposal?” DiResta posed to me. The pandemic is yet another misinformation hot spot that illustrates these constraints: A 2020 study of COVID-19 misinformation found some evidence of photos and videos doctored with simple techniques—such as an image edited to show a train transporting virus-filled tanks labeled COVID-19—but no AI-based manipulations.

That’s not to diminish concerns about synthetic media and disinformation. In fact, widespread anxiety has likely slowed the rise of deepfakes. “Before the alarm was raised on these issues, you had no policies by social-media companies to address this,” says Aviv Ovadya, an internet-platform and AI expert who is a prominent voice on the dangers of synthetic media. “Now you have policies and a variety of actions they take to limit the impact of malicious deepfakes”—content moderation, human and software detection methods, a wary public.

But awareness has also created an environment in which politicians can more credibly dismiss legitimate evidence as forged. Donald Trump has reportedly claimed that the infamous Access Hollywood tape was fake; a GOP candidate once promoted a conspiracy theory that the video of police murdering George Floyd was a deepfake. The law professors Danielle Citron and Robert Chesney call this the “liar’s dividend”: Awareness of synthetic media breeds skepticism of all media, which benefits liars who can brush off accusations or disparage opponents with cries of “fake news.” Those lies then become part of the sometimes deafening noise of miscontextualized media, scientific and political disinformation, and denials by powerful figures, as well as a broader crumbling of trust in more or less everything.

All of this might change in the next few years as AI-generated media becomes more advanced. Every expert I spoke with said it’s a matter of when, not if, we reach a deepfake inflection point, after which forged videos and audio spreading false information will flood the internet. The timeline is “years, not decades,” Farid told me. According to Ovadya, “it’s probably less than five years” until we can type a prompt into a program and, by giving the computer feedback—make the hair blow this way, add some audio, tweak the background—create “deeply compelling content.” Lyu, too, puts five years as the upper limit to the emergence of widely accessible software for creating highly credible deepfakes.

Celebrity deepfakes are already popping up in advertisements; more and more synthetic videos and audio are being used for financial fraud; deepfake propaganda campaigns have been used to attack Palestinian-rights activists. This summer, a deepfake of the mayor of Kyiv briefly tricked the mayors of several European capitals during a video call.

Read: In the deepfake era, counterterrorism is harder

And various forms of deepfake-lite technology exist all over the internet, including TikTok and Snapchat features that perform face swaps—replacing one person’s face with another’s in a video—similar to the infamous 2018 BuzzFeed deepfake that superimposed Obama’s face onto that of the filmmaker Jordan Peele. There are also easy-to-use programs such as Reface and DeepFaceLab whose explicit purpose is to produce decent-quality deepfakes. Revenge pornography has not abated. And some fear that TikTok, which is designed to create viral videos—and which is a growing source of news for American teenagers and adults—is especially susceptible to manipulated videos.

One of the biggest concerns is a new generation of powerful text-to-image software that greatly lowers the barrier to fabricating videos and other media. Generative-AI models of the sort that power DALL-E use a “diffusion” architecture, rather than GAN, to create complex imagery with a fraction of the effort. Fed hundreds of millions of captioned images, a diffusion-based model trains by changing random pixels until the image looks like static and then reversing that corruption, in the process “learning” to associate words and visual concepts. Where GANs must be trained for a specific type of image (say, a face in profile), text-to-image models can generate a wide range of images with complex interactions (two political leaders in conversation, for example). “You can now generate faces that are far more dynamic and realistic and customizable,” Ajder said. And many detection methods geared toward existing deepfakes won’t work on diffusion models.

The possibilities for deepfake propaganda are as dystopian now as they were a few years ago. At the largest scale, one can imagine fake videos of gruesome pregnancy terminations, like the saline-abortion images already used by anti-abortion activists; convincing, manipulated political speeches to feed global conspiracy theories; disparaging forgeries used against enemy nations during war—or even synthetic media that triggers conflict. Countries with fewer computer resources and talent or a less robust press will struggle even more, Gregory told me: “All of these problems are far worse when you look at Pakistan, Myanmar, Nigeria, a local news outlet in the U.S., rather than, say, The Washington Post.” And as deepfake technology improves to work with less training data, fabrications of lower-profile journalists, executives, government officials, and others could wreak havoc such that people think “there’s no new evidence coming in; there’s no new way to reason about the world,” Farid said.

Yet when deceit and propaganda feel like the air we breathe, deepfakes are at once potentially game-changing and just more of the same. In October, Gallup reported that only 34 percent of Americans trust newspapers, TV, and radio to report news fairly and accurately, and 38 percent have absolutely no confidence in mass media. Earlier this year, a Pew Research Center survey across 19 countries found that 70 percent of people think “the spread of false information online” is a major threat to their country, ranking just second behind climate change. “Deepfakes are really an evolution of existing problems,” Gregory said. He worries that focusing too heavily on sophisticated synthetic media might distract from efforts to mitigate the spread of “shallow fakes,” such as relabeled photographs and slightly doctored footage; DiResta is more concerned about text-based disinformation, which has been wreaking havoc for years, is easily generated using programs such as ChatGPT, and, unlike video or audio, has no obvious technical glitches.

Read: Sorry, I lied about fake news

The limited empirical research on the persuasiveness of synthetic video and audio is mixed. Although a few studies suggest that video and audio are a bit more convincing than text, others have found no appreciable difference; some have even found that people are better at detecting fabricated political speeches when presented with video or audio than with a transcript alone. Still, Ajder cautioned that “the deepfakes I’ve seen being used in these trials aren’t quite there; they still are on the cusp of uncanniness,” and that it’s difficult to replicate the conditions of social media—such as amplification and echo chambers—in a lab. Of course, those are the very conditions that have enabled an epistemic corrosion that will continue to advance with or without synthetic media.

Regardless of how a proliferation of deepfakes might worsen our information ecosystem—whether by adding to existing uncertainty or fundamentally changing it—experts, journalists, and internet companies are trying to prepare for it. The European Union and China have both passed regulations meant to target deepfakes by mandating that tech companies take action against them. Companies could implement guardrails to stop their technology from being misused; Adobe has gone as far as to never publicly release its deepfake-audio software, Voco.

There is still time to prevent or limit the most catastrophic deepfake scenarios. Many people favor building a robust authentication infrastructure: a log attached to every piece of media that the public can use to check where a photo or video comes from and how it has been edited. This would protect against both shallow- and deepfake propaganda, as well as the liar’s dividend. The Coalition for Content Provenance and Authenticity, led by Adobe, Microsoft, Intel, the BBC, and several other stakeholders, has designed such a standard—although until that protocol achieves widespread adoption, it is most useful for honest actors seeking to prove their integrity.

Once a deepfake is in circulation, detection is only the first of many hurdles for its debunking. Computers are far better than humans at distinguishing real and fake videos, Lyu told me, but they aren’t always accurate. Automated content moderation is infamously hard, especially for video, and even an optimistic 90 percent success rate could still leave tens or hundreds of thousands of the most pernicious clips online. That software should be made widely available to journalists, who also have to be trained to interpret the results, Gregory said. But even given a high-quality detection algorithm that is both accessible and usable, convincing the public to trust the algorithm, experts, and journalists exposing fabricated media might prove near impossible. In a world saturated with propaganda and uncertainty that long ago pushed us over the edge into what Ovadya calls “reality apathy,” any solution will first need to restore people’s willingness to climb their way out.. We had the honour to present our findings in today’s BSides Sydney (Slides).

We took this opportunity to make some of the yet unpublished materials public.

We can now reveal that the undisclosed game we’ve used is “Rocket League”, but many others work just as well (we’ve tried Fortnite, for example).

Some more goodies include the special sauce - the list of strings that appears in Rocket League’s executable and are part of Cylance’s Model. Just append these into any malicious executable to make Cylance believe it’s benign.

As of today, the bypass is still exploitable on the home edition (Cylance SmartAV). The vendor has told us the enterprise edition (CylancePROTECT) has been fixed, but we were unable to verify that. If you have access to the enterprise edition and can confirm the fix, please let us know in the comments box at the bottom of the page.

AI applications in security are clear and potentially useful, however AI based products offer a new and unique attack surface. Namely, if you could truly understand how a certain model works, and the type of features it uses to reach a decision, you would have the potential to fool it consistently, creating a universal bypass.

By carefully analyzing the engine and model of Cylance’s AI based antivirus product, we identify a peculiar bias towards a specific game. Combining an analysis of the feature extraction process, its heavy reliance on strings, and its strong bias for this specific game, we are capable of crafting a simple and rather amusing bypass. Namely, by appending a selected list of strings to a malicious file, we are capable of changing its score significantly, avoiding detection. This method proved successful for 100% of the top 10 Malware for May 2019, and close to 90% for a larger sample of 384 malware.

Read the full post to understand the research process itself, the inner workings of an advanced AI based EPP and how we found the universal bypass.



Demonstration of our universal bypass in action

Another Brave New World

Once every few years, the cyber security world is blessed with the birth of a baby silver bullet. It starts small, with a few enthusiastic parent companies hailing the newborn prince as our savior, telling the stories of its invincible power. A few years and millions of marketing dollars later, it grows and becomes an absolute powerhouse. The mere whisper of its name adds trust to your product, gets you appearances in the most influential conferences, and helps seal that much-needed funding round.

With time, the silver appears to be just coating that soon starts wearing off and some brave souls start seeing it for what it is — another tool, hopefully an effective one, in the never-ending process which is cyber security.

Such is the story of many “silver bullets” we have seen over the years, and inevitably such will be the story of AI and machine learning.

AI has been touted as the silver bullet to end them all with significant marketing force — after all, if we can teach a machine to think like a human analyst, only with the addition of big data and almost infinite processing power, then surely, we will be able to create an invincible mechanism. A brain so powerful that it could not be fooled by any other mechanism.

Right…

In this post we will show how we can reverse the model of an AI based EPP product, and find a bias enabling a universal bypass. We chose Cylance for practical reasons, namely, it is publicly available and widely regarded as a leading vendor in the field.

However, we believe that the process presented in this post can be translated to other pure AI products as well.

AI for Millennials

AI is an extremely important and fascinating technological field with profound implications for human society. However, we are not AI experts, heck, we’re not even power users. With that disclaimer in mind, let’s try to understand how AI works conceptually, so we can hypothesise later on how we may fool it.

In the context of endpoint protection, we are faced with a classification problem. Given a file, we need to classify it as either malicious or benign.

There are many approaches to this, but generally speaking, you are trying to train an artificial brain to identify certain properties of the subject and then apply some form of mathematical model to calculate whether what you are looking at is a certain object.

Let’s assume for example, that we are interested in having a machine classify objects as either birds or human beings.

A specific AI model may look at certain attributes of the object like weight, height, whether it has a beak, wings etc. to make a decision. By exposing a model to numerous samples of birds and human beings, the differences will start showing. For example, most human beings that we are aware of do not have beaks or wings. Thus, if something possesses either of those attributes, there’s a high likelihood that it is a bird. This is of course an oversimplification of a truly beautiful field of knowledge, but it will suffice for now.

By applying the same approach to classification of files as malicious or benign, we get clear and substantial benefits:

Prediction by design — a well-trained model should have the ability to identify a malicious file it has never seen and has no prior knowledge of.

Infrequent updates — a model is trained once and can last years without updates.

Lower resource consumption — AI vendors claim that the nature of their technology leads to lower CPU, memory and disk consumption.

Overall, AI should help you detect more threats, earlier, while incurring a lower management and computing resource overhead for your organization. Can I have two please?

Data Nerd vs. Hacker Mindset

Hackers will always try to find the most economical way to get their way.

Going back to the birds and human beings classification problem discussed above — if we understand that the AI model relies heavily on the existence of a beak and wings to make a decision, we can go buy ourselves a goofy bird costume.

If, on the flip side, the model looks at brain weight to body weight ratio, we’ll need a completely different trick.



Not a real chicken

Whatever the important attributes may be, if we identify them, we have a good chance of consistently defeating the artificial brain — that is, make malicious files look benign through some sort of treatment.

A Chatty Log

All right, time to roll our sleeves and start our research, exciting!

As a first stage we would like to create a process to determine the classification of a given file, as determined by the Cylance engine. This would allow us to understand at a later stage if we were capable of fooling it.

By activating verbose logging we can capture high-resolution information that Cylance is kind enough to provide. We always say that a verbose software is like a drunk stranger in a bar, it will tell you about all of his problems. Here is an excerpt from the log file, indicating a detection of a malicious file (Mimikatz with a single modified byte):

AnalyzeFile hashed C : \Users\Administrator\Desktop\mimikatz_with_slight_modification . exe 143020851E35E3234DBCC879759322E8AD4D6D3E89EAE1F662BF8EA9B9898D05 LocalAnalyzeItem LocalInfinity . ComputeScore begin LocalAnalyzeItem , C : \Users\Administrator\Desktop\mimikatz_with_slight_modification . exe score - 852 detector execution_control Detected as 'Unsafe' ! path : 'C : \Users\Administrator\Desktop\mimikatz_with_slight_modification . exe' hash : 143020851E35E3234DBCC879759322E8AD4D6D3E89EAE1F662BF8EA9B9898D05

We can see that the engine has a scoring mechanism and our modified Mimikatz was scored -852. By empirically testing various good and bad files we later determined that the score can range from -1000 for the most malicious files, and +1000 for the most benign of files.

Good job Cylance, you identified a mutated Mimikatz, already putting you well beyond 50% of the endpoint protection products out there (don’t believe us? Compare original Mimikatz with one-byte modified Mimikatz).

Hold tight though, we are just getting started.

Diving In

We now had a clear objective — understand Cylance’s scoring mechanism, so we can later bypass it.

To do so, we started by reverse engineering the code, which was of course obfuscated, but constructed in a clear way, making it possible to follow.

We also found some publicly available information of the inner workings of the product from patent submissions and public talks. One of these resources describes the engine as being an ‘Ensemble’ (which is a group of models) and we did find a class named ‘EnsembleReader’. It is used to load the ensemble from a resource embedded in one of the DLL files. The extracted model is encrypted, however, the algorithm and quite original key are rather clear:

public class EnsembleReader : ILogAccess , IEnsembleHeader , IDisposable { protected const int RandomHeaderSize = 3072 ; protected const string KeyAndIv = "I am decrypting Cylance's intellectual property." ; protected Stream _stream ; protected byte [ ] _activeKey ; protected byte [ ] _activeIV ; protected bool _loadSectionData ;

By following the trail of how the model information is used, we reached the ‘SampleScore2PE’ assembly. Its name suggests it scores PE files, which naturally piqued our interest. We looked at the exposed interfaces of that assembly and found the following gem:

public ISampleScore Create ( string inputModelsPath ) { return ( ISampleScore ) new SampleScoring2PE ( inputModelsPath ) ; }

Before discovering this interface we were already planning on using the old school way of revealing the secrets of the model file — use the key to decrypt the model and painstakingly analyse it for days or weeks. Instead, we realized that we can just build our own tiny .NET executable and link against these assemblies much like Cylance.Engine.Core does. We then called the Create() function with a path to the model file we’ve extracted earlier (still encrypted, BTW), and we got an object that exposes a ComputeScore() function.

Can this get any better (it does actually)?

SampleScoreFactory2PE factory = new SampleScoreFactory2PE ( ) ; SampleScoring2PE scorer = factory . Create ( "test_model.bin" ) as SampleScoring2PE ; Stream test_file = File . Open ( "mimikatz_with_slight_modification.exe" , FileMode . Open ) ; Dictionary < string , object > extraData ; double score = scorer . ComputeScore ( test_file , out extraData ) ;

By executing the code above, we received a score of -0.852764 as output for our modified Mimikatz file, which looks awfully similar to the -852 we noticed in the log files earlier.



Mimikatz was scored -0.85276

This may not seem like much, but this is actually a very solid basis for our research. We now have an easy process to test PE files against Cylance’s scoring engine, and more importantly, we have a starting point for dynamically debugging the scoring process.

Features Galore

We used a combination of static and dynamic analysis techniques to study the scoring process. The beginning of the classification journey starts with examining and measuring different features of the target object. Going back to our bird vs. humans example, this is the part where you would take measurements, check the existence of wings and beak, and quantify other aspects of the subject being classified.

Cylance uses a combination of code and data from the model itself to produce the feature vector.

The PE file is first extensively parsed to produce a vast amount of different properties of the file. Some are simple, such as the number and names of sections, while others are more complex observations that require a bit of processing to produce. For example, testing if the PE has a correct checksum field, counting the amount of instructions in the entrypoint and the number of imports related to process injection.

The next major step in the classification process is to turn these extracted properties into a feature vector (AKA: feature extraction). As there are thousands of features (7,000 to be precise), we did not bother enumerating all of them. Instead, we focused on the general process that Cylance uses to transform plain file properties into a feature vector.



Thousands of lines of code transforming file properties into features

There are thousands of lines of code handling this transformation, but the overall logic is the same: the engine takes an input property and compares it against a known value or a list of values. One comparison for examples compares the TimeDateStamp field from the File Header of the PE file against a list of 3523 different ranges of timestamps. Depending on what range the timestamp falls into, the engine executes a certain action.

That action is really just a sequence of instructions to increment or decrement values of the feature vector. Each action can affect one or more values, and the list of instructions is stored in the model’s data.

Let’s analyse one example:

if ( ! this . method_28 ( this . imagePEFile_0 . ImageNTHeader . FileHeader . TimeDateStamp , 2 , 3523 ) ) this . method_14 ( 2866581 , 0 ) ;

In this snippet, we first call method_28 which will try to search for the extracted TimeDateStamp property in 3523 different time-date ranges. Each range has a corresponding action attached to it. If the property is not found in any of the time-date ranges, method_14 will be called which will trigger an action, designated for instances where this property is not found to be in any of the “known” ranges.

At the end of this very long process, after executing countless actions, we end up with a vector containing 7000 feature values. This feature vector is the extract of the PE file, and it alone will determine the score and classification of the file.

The next phase is to apply the model to the extracted feature vector. The process starts with normalization and additional post-processing of the feature vector, transforming it into a format that is usable mathematically (we won’t go into full details here).

Then, the engine uses 3 different matrices that are part of the model’s data to transform the feature vector into a single value, which is the final score of the file. As discussed, we are not machine learning experts, but we have seen academic papers suggesting it is possible to approximate a neural network using matrix multiplication, and it all seems to add up: in order to improve performance, Cylance likely created an approximation of their model in the form of several matrix multiplications. After each multiplication, the engine applies an activation function (tanh / sigmoid). This seems to be a very common technique used in neural networks to introduce non-linearity into the model in order to allow computation of non-trivial problems.



The stages of approximating the deep learning network: A, B and C are the matrices that form the model

How were these matrices calculated in the first place though?

We can’t be sure, but by combining what we’ve learned from analysing the product, open source information, and the little we know of how AI is used in other industries, we can draw a plausible explanation of the process that was used to create these matrices.

Cylance probably started by collecting a huge repository of malicious and benign files. They then generated a very large feature vector for each, much larger than the final 7000 features.

Using statistical models, they reduced the list of features down to the most meaningful 7000 (AKA: feature selection).

Then, they created sets of deep neural networks with varying configurations such as the number of nodes, layers, activation functions etc, and trained the models on the repository of files. Using an iterative Darwinistic process, they continued removing and exploring configurations, until those that exhibited the best properties remained (e.g. high accuracy, low false positive rate).

The next step would have been to approximate the model using matrix multiplication, the essence of which are the matrices we see in the model data.

Going back to the code, we can see that the first matrix multiplication uses the 7000 feature vector as input and outputs 256 2nd order features. These are then fed into another layer of neural network, represented as another matrix, yielding an additional set of 256 3rd order features. The final layer of the neural network is approximated using the last matrix multiplication which results in a single number, representing the final score of the file.

These matrices and associated feature extraction processes are stored in the model file and shipped with the product we tested against. This is the “model data” we were referring to earlier, and it represents the essence of the Cylance engine itself.

Paranoid Centroid

The end product of the lengthy process of file scoring is a number in the range of -1 to 1, indicating how malicious the file is. As always, real life challenges make things more complicated: after scoring is completed using the described method, another mechanism comes into play with an override power over the earlier model.

We can only speculate as to why this mechanism was introduced, but we believe that Cylance’s team encountered some false positives and false negatives in the main model.

They could have probably adapted or improved their model but maybe they had time pressure, so the R&D team had to come up with something quick that would make the problem go away.

Whatever the drama behind the scenes may have been, they introduced an additional mechanism that is designed to target specific families of executables with the power to override the decision made by the previous model. That mechanism is called “Centroids” and it is commonly used for clustering objects. Its use in Cylance’s case is similar to a list of exceptions: when the model classifies a file, we use the centroids to check whether it is white or black listed.

To accomplish that, the engine uses a different feature vector, which uses a modified set of feature values. It normalises the values of those features by centering each around zero, with values ranging between -3 and +3, and then calculates the Euclidean distance between the resulting vector and some known, pre-calculated ones. Effectively, it tries to find if the executable is very similar to ones that were added to the model’s white/black list.

If the executable is exactly the same, the feature vectors are equal and the distance is 0. If some features are different though, the distance could grow, and if grows beyond a given threshold, it is not considered in the same cluster as the white or black listed centroid.

Crossroads

With a fair bit of knowledge of how the model works in hand, we hypothesized as to how we can actually circumvent and confuse the engine.

Our first hypothesis was to try to make a malicious PE look like one of the files in the whitelist. That is, force the relevant features into the right distance from a white listed centroid. We quickly realized that this technique has little chance to work as this mechanism relies on thousands of features, some of which are extremely hard to modify.

Our second hypothesis was that perhaps we could find some bias in the model itself: a small set of features that have a significant effect on the outcome. If we could change the malicious binary so that these features resemble good files, maybe the model will be fooled.

We took another long look at the list of features and tried to estimate the work required to take a malicious file and modify it. It felt overwhelmingly difficult, as there were thousands upon thousands of features. That is, until we came across the following lines of code:

for ( int index = 0 ; index < this . imagePEFile_0 . Strings . Length ; ++ index ) { if ( ! this . method_26 ( this . imagePEFile_0 . Strings [ index ] . S , 95088 , 854069 , 0 ) ) this . method_14 ( this . list_0 [ 0 ] , 15166118410741992125UL , 2847678 , 0 ) ; }

These lines traverse through the entire list of strings found in the file, calculate their hash (using Murmur64), and then searches for them in a large DB containing string hashes (all part of the model’s data). For each string that is found, an action is triggered, affecting the feature vector.

The latest discovery led us to believe that there might be some inherent bias in the model itself: many features that were included in the feature set (and remember: that’s the only value that matters) are a direct result of what strings exist within the executable. We can now differentiate between two types of features: ones that are based on strings, and ones that are based on some other property of the executable.

The string features are easier to manipulate and adding them to an executable should not be too difficult. Other properties can be much harder to manipulate. For example, reducing the amount of sections is rather hard, and requires work which is very specific to the executable itself.

So strings are a good direction, but which are our beak and wings? What strings would normally make someone look at a file and say “yep, looks benign to me”.

We were able to decrypt and parse the model’s data and retrieve the list of hashes of strings and associated actions. However, we didn’t have the actual strings themselves. This is when we had one of those eureka moments, without the bathtub.

By re-examining the centroids mechanism, we could observe the families of executables that Cylance’s team whitelisted. Perhaps a whitelist entry has been created at some time, after which a full re-train of the model was performed to correctly classify that family? If so, perhaps the model will be more biased towards strings taken from these types of executables.

We looked at the list which included only a few dozen centroids. Each centroid definition carried a name for identification and one of them stood out as it was a name of an online game. A quick conversation with one of the kids verified that it’s a well-known one. At the very least, it was popular enough to cause headaches and trigger special treatment from Cylance.

We purchased the game, and extracted all the strings contained in the main executable using a simple “strings” command, resulting in approximately 5 MB of strings. We then tried to find out exactly how the strings are parsed in order to better understand how we can make the parser pick them up. That is when we had our second eureka/lazy moment: let’s try the most naive solution and slap the strings onto the end of the file. That wouldn’t possibly ever work, would it?

We used the Mimikatz version that received a score of -852 and executed the following command:

copy /b mimikatz.exe+strings.txt modified_mimikatz.exe

We then fed it back into the scoring mechanism and had our OMG² moment — Score is now 0.9998944… (= 999). This is almost a perfect score.



Now, our modified Mimikatz gets a perfect score

Did we just find a potential shortcut to kill the model? Is this the cheap bird costume we were after?

We went ahead and tested our solution with additional known malicious files and hacking tools and got consistent results — the score changed from a strong negative to a strong positive. We also confirmed that no other mechanisms were testing the behaviour of the files dynamically and blocking them, by executing these modified versions successfully on VMs running Cylance.

Our conclusion was that we managed to find a universal bypass. That is, a simple, passive method we can apply to almost any malicious executable to turn it into a FUD (fully undetected).

We have lots of experience in bypassing endpoint protection products, and we were always able to take an executable and a specific antivirus product and modify it in a way that will make it pass under the radar, but it was always very specific to the executable and required time and expertise. There are packers that can turn an executable into a FUD, but the process always involves staging which complicates things and suffers from compatibility issues. With this approach, we were able to apply the same simple solution to any executable to turn it into a FUD. All we had to do is append a specific set of known strings.

Fine Tuning

After our initial success, we were interested in narrowing down the list of strings we use, by filtering out strings that were not part of Cylance’s model.

On one hand we parsed the relevant tables from Cylance’s model to have the full list of string hash values considered by the model. On the other hand, we calculated the hash for all the strings from the game. Combining these two lists together we managed to reduce the size of the added strings (“special sauce”) to a mere 60 KB. We have confirmed that reducing the size of the list did not affect our universal bypass. By appending those 60KBs to any executable we can change its score drastically, although as we soon found out, some were still detected as malicious, albeit with significantly better scores.

Exam Time

Our first tests were against what we would consider the usual suspects — Mimikatz, ProcessHacker, Meterpreter etc, and proved successful. It was time to up the game and look at a wider test group.

We started with a list of the top ten malware as of May 2019, published by the Center for Internet Security.

The results were staggering:

Malware SHA256 Score Before Score After CoinMiner 1915126c27ba8566c624491bd2613215021cc2b28e5e6f3af69e9e994327f3ac -826 884 Dridex c94fe7b646b681ac85756b4ce7f85f4745a7b505f1a2215ba8b58375238bad10 -999 996 Emotet b3be486490acd78ed37b0823d7b9b6361d76f64d26a089ed8fbd42d838f87440 -923 625 Gh0stRAT eebff21def49af4e85c26523af2ad659125a07a09db50ac06bd3746483c89f9d -975 998 Kovter 40050153dceec2c8fbb1912f8eeabe449d1e265f0c8198008be8b34e5403e731 -999 856 Nanobot 267912da0d6a7ad9c04c892020f1e5757edf9c4762d3de22866eb8a550bff81a 971 999 Pushdo 14c358cc64a929a1761e7ffeb76795e43ff5c8f6b9e21057bb98958b7fa11280 -999 999 Qakbot 869985182924ca7548289156cb500612a9f171c7e098b04550dbf62ab8f4ebd9 -998 991 Trickbot 954961fd69cbb2bb73157e0a4e5729d8fe967fdf18e4b691e1f76aeadbc40553 -973 774 Zeus 74031ad4c9b8a8757a712e14d120f710281027620f024a564cbea43ecc095696 -997 997

As can be clearly seen, almost all of these samples have changed from the most evil file on the planet, to your friendly neighborhood file. Again, the only treatment applied to these files, is the addition of the “special sauce” as a simple concatenation. Widening further our test group, we downloaded a list of 384 malicious files from online repositories and ran the test, receiving the following results:

Average score before secret sauce: -920

Average score after secret Sauce: 630

Average delta: 1550 (out of a maximum of 2000)

Percentage of files bypassing detection: 83.59%

We also realized that if we add the secret sauce multiple times, we can improve the score even further. With this technique we achieved an average score of 750, and a whooping 88.54% of our malicious files were now marked as benign.

Impact and Final Thoughts

We are always amused to see the shock on people’s faces when you tell them that the new security toy they spent millions of dollars buying and integrating can be bypassed. The same goes for new silver bullets, like AI based security. We are anything but surprised with the results, and we are confident that the same type of process can be applied to other pure AI vendors to achieve similar results.

Why?

Vendors too often approach the security problem with a one punch solution. Hackers are not wooden dummies, they fight back, and you have to be ready for the counter-punch, constantly innovating and increasing the cost of attack.

The concept of a static model that lasts for years without update may hold theoretically, but it fails in the arena.

Granted, it is harder to find a bias in an AI model than to bypass a simple AV signature, but the cost of fixing a broken model is equally expensive.

We believe that the solution lies in a hybrid approach. Using AI/ML primarily for the unknown, but verifying with tried and tested techniques used in the legacy world. This is really just another implementation of the defense in depth concept, applied to the endpoint protection world.

This means that the promise of a pure AI product may not be realized for EPPs, and vendors will have to maintain and update multiple systems of detection.

The promise of low resource consumption, with rare update cycles does not hold true for such a hybrid product, but it does provide a superior protective capability.

Till the next silver bullet…. Artificial intelligence has been touted by some in the security community as the silver bullet in malware detection. Its proponents say it’s superior to traditional antivirus since it can catch new variants and never-before-seen malware—think zero-day exploits—that are the Achilles heel of antivirus. One of its biggest proponents is the security firm BlackBerry Cylance, which has staked its business model on the artificial intelligence engine in its endpoint PROTECT detection system, which Artificial intelligence has been touted by some in the security community as the silver bullet in malware detection. Its proponents say it’s superior to traditional antivirus since it can catch new variants and never-before-seen malware—think zero-day exploits—that are the Achilles heel of antivirus. One of its biggest proponents is the security firm BlackBerry Cylance, which has staked its business model on the artificial intelligence engine in its endpoint PROTECT detection system, which the company says has the ability to detect new malicious files two years before their authors even create them.

Advertisement

But researchers in Australia say they’ve found a way to subvert the machine-learning algorithm in PROTECT and cause it to falsely tag already known malware as “goodware.” The method doesn’t involve altering the malicious code, as hackers generally do to evade detection. Instead, the researchers developed a “global bypass” method that works with almost any malware to fool the Cylance engine. It involves simply taking strings from a non-malicious file and appending them to a malicious one, tricking the system into thinking the malicious file is benign. But researchers in Australia say they’ve found a way to subvert the machine-learning algorithm in PROTECT and cause it to falsely tag already known malware as “goodware.” The method doesn’t involve altering the malicious code, as hackers generally do to evade detection. Instead, the researchers developed a “global bypass” method that works with almost any malware to fool the Cylance engine. It involves simply taking strings from a non-malicious file and appending them to a malicious one, tricking the system into thinking the malicious file is benign.

The benign strings they used came from an online gaming program, which they have declined to name publicly so that Cylance will have a chance to fix the problem before hackers exploit it. The benign strings they used came from an online gaming program, which they have declined to name publicly so that Cylance will have a chance to fix the problem before hackers exploit it.

“As far as I know, this is a world-first, proven global attack on the ML [machine learning] mechanism of a security company,” says Adi Ashkenazy, CEO of the Sydney-based company “As far as I know, this is a world-first, proven global attack on the ML [machine learning] mechanism of a security company,” says Adi Ashkenazy, CEO of the Sydney-based company Skylight Cyber , who conducted the research with CTO Shahar Zini. “After around four years of super hype [about AI], I think this is a humbling example of how the approach provides a new attack surface that was not possible with legacy [antivirus software].”

The method works because Cylance’s machine-learning algorithm has a bias toward the benign file that causes it to ignore any malicious code and features in a malicious file if it also sees strings from the benign file attached to a malicious file—essentially overriding the correct conclusion the detection engine should otherwise make. The trick works even if the Cylance engine previously concluded the same file was malicious, before the benign strings were appended to it. The method works because Cylance’s machine-learning algorithm has a bias toward the benign file that causes it to ignore any malicious code and features in a malicious file if it also sees strings from the benign file attached to a malicious file—essentially overriding the correct conclusion the detection engine should otherwise make. The trick works even if the Cylance engine previously concluded the same file was malicious, before the benign strings were appended to it.

Advertisement

The researchers tested their attack against the The researchers tested their attack against the WannaCry ransomware that crippled hospitals and businesses around the world in 2017, as well as the more recent Samsam ransomware, the popular Mimikatz hacking tool, and hundreds of other known malicious files—adding the same benign strings from the gaming program to each malicious file—and in nearly all cases, they were able to trick the Cylance engine.

Martijn Grooten, editor of Virus Bulletin, which conducts tests and reviews of malware detection programs, called the reverse-engineering research impressive and technically interesting, but wasn’t surprised by the findings. Martijn Grooten, editor of Virus Bulletin, which conducts tests and reviews of malware detection programs, called the reverse-engineering research impressive and technically interesting, but wasn’t surprised by the findings.

“Their crime is not that they coded AI poorly. Their crime is calling what they did AI."

“This is how AI works. If you make it look like benign files, then you can do this,” Grooten told Motherboard. “It mostly shows that you can’t rely on AI on its own…. AI isn’t a silver bullet…. I suspect it’ll get better at this kind of thing over time.” “This is how AI works. If you make it look like benign files, then you can do this,” Grooten told Motherboard. “It mostly shows that you can’t rely on AI on its own…. AI isn’t a silver bullet…. I suspect it’ll get better at this kind of thing over time.”

A machine learning expert Motherboard spoke to agrees. A machine learning expert Motherboard spoke to agrees.

“Usually you try to work with machine learning to cover … things which are widely unknown or you cannot do manually,” said the expert, who asked to remain anonymous because his company doesn’t authorize him to speak with media. “And it usually works pretty well, until you have some corner cases where you can’t just make the model [work].” “Usually you try to work with machine learning to cover … things which are widely unknown or you cannot do manually,” said the expert, who asked to remain anonymous because his company doesn’t authorize him to speak with media. “And it usually works pretty well, until you have some corner cases where you can’t just make the model [work].”

Though he doesn’t fault Cylance for making a mistake, he does fault the company for hyping the AI in their marketing when the system contains a bias that essentially undermines the AI. Though he doesn’t fault Cylance for making a mistake, he does fault the company for hyping the AI in their marketing when the system contains a bias that essentially undermines the AI.

Advertisement

“Their crime is not that they coded AI poorly. Their crime is calling what they did AI,” he told Motherboard. “Their crime is not that they coded AI poorly. Their crime is calling what they did AI,” he told Motherboard.

Cylance’s PROTECT isn’t the only security product that uses artificial intelligence. Other firms like Symantec, Crowdstrike, and Darktrace use it too, but Ashkenazy and Zini didn’t test those systems and it’s not clear they would suffer from the same bias, since they’re architected differently and don’t rely as heavily on machine learning to detect malicious files as the Cylance system does. Cylance’s PROTECT isn’t the only security product that uses artificial intelligence. Other firms like Symantec, Crowdstrike, and Darktrace use it too, but Ashkenazy and Zini didn’t test those systems and it’s not clear they would suffer from the same bias, since they’re architected differently and don’t rely as heavily on machine learning to detect malicious files as the Cylance system does.

“One of [Cylance’s] selling points… they say no more running after signatures and updates. We train the model once, and … you won’t have to train the model again for a couple of years. It’s very compelling, if it actually works,” Ashkenazy said. “One of [Cylance’s] selling points… they say no more running after signatures and updates. We train the model once, and … you won’t have to train the model again for a couple of years. It’s very compelling, if it actually works,” Ashkenazy said.

But to fix the problem he and his colleague found in the Cylance engine, the company will have to retrain the system, which could be a “costly and complex process” Ashkenazy said. But to fix the problem he and his colleague found in the Cylance engine, the company will have to retrain the system, which could be a “costly and complex process” Ashkenazy said.

Artificial intelligence has several advantages over traditional antivirus. In traditional systems, the vendor has to analyze each new file and push out new signatures or heuristics to their scanners to detect it. (Signatures look for specific strings of code or data that are unique to a piece of malware; heuristics look at the activity the code is engaged in to spot actions that are characteristic of malware.) But, according to Cylance, its engine doesn’t require an update every time new malware, or variants of existing malware, are discovered. Machine-learning detection systems are supposed to recognize not only known malicious files and activity but also spot new ones. Artificial intelligence has several advantages over traditional antivirus. In traditional systems, the vendor has to analyze each new file and push out new signatures or heuristics to their scanners to detect it. (Signatures look for specific strings of code or data that are unique to a piece of malware; heuristics look at the activity the code is engaged in to spot actions that are characteristic of malware.) But, according to Cylance, its engine doesn’t require an update every time new malware, or variants of existing malware, are discovered. Machine-learning detection systems are supposed to recognize not only known malicious files and activity but also spot new ones.

Advertisement

In a test conducted by SELabs and commissioned by Cylance, a version of its 2015 software had the ability to detect variants of the Cerber ransomware and other malicious programs that didn’t appear in the wild until 2016 and 2018. In a test conducted by SELabs and commissioned by Cylance, a version of its 2015 software had the ability to detect variants of the Cerber ransomware and other malicious programs that didn’t appear in the wild until 2016 and 2018.

To determine if a file is malicious or benign, the Cylance engine looks at 4 million different features or data points, according to Ryan Permeh, founder and chief scientist of Cylance. These include things like the size of the file, structural elements present, and entropy (the level of randomness), etc. Cylance programmers then “train” the engine by showing it about a To determine if a file is malicious or benign, the Cylance engine looks at 4 million different features or data points, according to Ryan Permeh, founder and chief scientist of Cylance. These include things like the size of the file, structural elements present, and entropy (the level of randomness), etc. Cylance programmers then “train” the engine by showing it about a billion malicious and benign files and tweak the system to hone its detection. But during training, the system also examines the files for patterns to see how malware variants evolve over time to anticipate how new malware might look—essentially “predicting” what malware authors will do before they do it. Models do get retrained, Permeh says, but only about every six months, and users only have to update their software if they want the latest features and performance improvements.

But none of this training and testing matter if the algorithm has a bias that is also training it to ignore what it learns from that other training. That’s essentially what the Skylight Cyber researchers discovered. But none of this training and testing matter if the algorithm has a bias that is also training it to ignore what it learns from that other training. That’s essentially what the Skylight Cyber researchers discovered.

They purchased a copy of the Cylance program and reverse-engineered it to figure out what features or data points the agent was looking at to determine if a file is benign or malicious and they also studied how these features are weighed to arrive at the score the program gives each files. They purchased a copy of the Cylance program and reverse-engineered it to figure out what features or data points the agent was looking at to determine if a file is benign or malicious and they also studied how these features are weighed to arrive at the score the program gives each files.

Advertisement

The Cylance system analyzes each file based on these data points, and assigns a score to the file that ranges between -1,000 to 1,000 (with -1,000 being a file with the most or worst malicious features or data points in it). Scores are visible in the program’s log file. The Cylance system analyzes each file based on these data points, and assigns a score to the file that ranges between -1,000 to 1,000 (with -1,000 being a file with the most or worst malicious features or data points in it). Scores are visible in the program’s log file.

When they saw how many features the program analyzes, the researchers worried initially that it would take them weeks or months to find the ones that carried the most weight in the algorithm’s decision process. That is, until they discovered that Cylance also had whitelisted certain families of executable files to avoid triggering false positives on legitimate software. When they saw how many features the program analyzes, the researchers worried initially that it would take them weeks or months to find the ones that carried the most weight in the algorithm’s decision process. That is, until they discovered that Cylance also had whitelisted certain families of executable files to avoid triggering false positives on legitimate software.

Suspecting the machine learning might be biased toward code in those whitelisted files, they extracted strings from an online gaming program Cylance had whitelisted and appended it to malicious files. The Cylance engine tagged the files benign and moved their scores from high negative numbers to high positive ones. The score for Mimikatz went from -799 to 998. WannaCry went from -1000 to 545. The researchers liken it to donning a mask with a beak and having a facial recognition system identify you as a bird, ignoring all other characteristics that indicate you’re just a person wearing an artificial beak. Suspecting the machine learning might be biased toward code in those whitelisted files, they extracted strings from an online gaming program Cylance had whitelisted and appended it to malicious files. The Cylance engine tagged the files benign and moved their scores from high negative numbers to high positive ones. The score for Mimikatz went from -799 to 998. WannaCry went from -1000 to 545. The researchers liken it to donning a mask with a beak and having a facial recognition system identify you as a bird, ignoring all other characteristics that indicate you’re just a person wearing an artificial beak.

They tested the top ten malware programs cited by the Center for Internet Security, then broadened their test to include 384 additional malicious files taken from online repositories of malware. The average score before they appended the benign strings from the whitelisted gaming program was -0.92. After adding the strings, the average score was 0.63. About 84 percent of the files bypassed detection once they added the gaming strings, though some files still got tagged malicious, but with significantly changed scores than before. They tested the top ten malware programs cited by the Center for Internet Security, then broadened their test to include 384 additional malicious files taken from online repositories of malware. The average score before they appended the benign strings from the whitelisted gaming program was -0.92. After adding the strings, the average score was 0.63. About 84 percent of the files bypassed detection once they added the gaming strings, though some files still got tagged malicious, but with significantly changed scores than before.

Advertisement

They didn’t just run the files against the static Cylance program - they executed the malicious files on a virtual machine with Cylance PROTECT running on it, to see if it would catch the malicious files in action. The theory was that even if the product was tricked by the strings, the malicious action of the file would still be detected by Cylance, but it wasn’t. They didn’t just run the files against the static Cylance program - they executed the malicious files on a virtual machine with Cylance PROTECT running on it, to see if it would catch the malicious files in action. The theory was that even if the product was tricked by the strings, the malicious action of the file would still be detected by Cylance, but it wasn’t.

Ashkenazy said the use of whitelisting in an AI program is odd, but understands why Cylance did it, if its engine was creating false positives on those programs. The real problem, he said, was giving the whitelisted programs more weight in the algorithm’s scoring, causing them to override a decision the algorithm would normally reach if a file didn’t have the benign strings appended to it. He also said that not using backup signatures or heuristics to doublecheck the algorithm’s conclusion, and relying on the AI instead, caused the failures. Ashkenazy said the use of whitelisting in an AI program is odd, but understands why Cylance did it, if its engine was creating false positives on those programs. The real problem, he said, was giving the whitelisted programs more weight in the algorithm’s scoring, causing them to override a decision the algorithm would normally reach if a file didn’t have the benign strings appended to it. He also said that not using backup signatures or heuristics to doublecheck the algorithm’s conclusion, and relying on the AI instead, caused the failures.

Permeh, who is also the architect of Cylance’s machine-learning engine, said they do use signatures and hard-coded heuristics in their product as well and don’t entirely rely on the machine-learning, but the AI does take precedence in detection. Permeh, who is also the architect of Cylance’s machine-learning engine, said they do use signatures and hard-coded heuristics in their product as well and don’t entirely rely on the machine-learning, but the AI does take precedence in detection.

He acknowledged to Motherboard the potential for the kind of bypass the researchers found, however. He acknowledged to Motherboard the potential for the kind of bypass the researchers found, however.

“Generally speaking, in all of these AI scenarios, models are probabilistic. When you train one, you learn what’s good and what’s bad…. By training for what is a good file, we learn attributes of that… [and] it’s entirely possible that we overestimated the goodness of that,” he told Motherboard in a phone call. "One of the interesting parts of being basically the first to take an AI-first approach, is that we’re still learning. We invest a lot in adverse research, but this is still an evolution.” “Generally speaking, in all of these AI scenarios, models are probabilistic. When you train one, you learn what’s good and what’s bad…. By training for what is a good file, we learn attributes of that… [and] it’s entirely possible that we overestimated the goodness of that,” he told Motherboard in a phone call. "One of the interesting parts of being basically the first to take an AI-first approach, is that we’re still learning. We invest a lot in adverse research, but this is still an evolution.”

Advertisement

Contrary to what Ashkenazy said, Permeh doesn’t think it will take long to retrain the algorithm to fix the issue once he knows the details of the global bypass. Ashkenazy didn’t contact Cylance before contacting Motherboard to disclose the issue. Contrary to what Ashkenazy said, Permeh doesn’t think it will take long to retrain the algorithm to fix the issue once he knows the details of the global bypass. Ashkenazy didn’t contact Cylance before contacting Motherboard to disclose the issue.

But Ashkenazy thinks the issue will take more time to fix than Permeh believes. But Ashkenazy thinks the issue will take more time to fix than Permeh believes.

“The bias towards games and those features is there for a reason,” Ashkenazy said. “They were getting false positives for games, so retraining without sacrificing accuracy or false positive rate can’t be that simple.” “The bias towards games and those features is there for a reason,” Ashkenazy said. “They were getting false positives for games, so retraining without sacrificing accuracy or false positive rate can’t be that simple.”

In the end, Ashkenzy doesn’t think Cylance is at fault for using machine learning, just for hyping it and relying on it so heavily for detection. In the end, Ashkenzy doesn’t think Cylance is at fault for using machine learning, just for hyping it and relying on it so heavily for detection.

“I actually think they did a decent job applying current AI technology to security,” he told Motherboard. “It just has inherent flaws, like the possibility of having an exploitable bias which becomes a global bypass with a costly fix.” “I actually think they did a decent job applying current AI technology to security,” he told Motherboard. “It just has inherent flaws, like the possibility of having an exploitable bias which becomes a global bypass with a costly fix.”

Subscribe to our new cybersecurity podcast, CYBER.. BlackBerry® Cylance® is aware that a bypass has been publicly disclosed by security researchers. We have verified there is an issue with CylancePROTECT® which can be leveraged to bypass the anti-malware component of the product.



Our research and development teams have identified a solution and will release a hotfix automatically to all customers running current versions in the next few days.

More information will be provided as soon as it is available.

***UPDATE - SUNDAY 7/21/2019: Resolution for BlackBerry Cylance Bypass

BlackBerry Cylance has verified the issue was not a universal bypass as reported, but rather a technique that allowed for one of the anti-malware components of the product to be bypassed in certain circumstances. The issue has been resolved for cloud-based scoring and a new agent will be rolled out to endpoints in the next few days. MORE INFO HERE.. An Amazon.com Inc. team auditing Alexa users’ commands has access to location data and can, in some cases, easily find a customer’s home address, according to five employees familiar with the program.

The team, spread across three continents, transcribes, annotates and analyzes a portion of the voice recordings picked up by Alexa. The program, whose existence Bloomberg revealed earlier this month, was set up to help Amazon’s digital voice assistant get better at understanding and responding to commands.. An Israeli start-up says it can take one look at a person’s face and realize character traits that are undetectable to the human eye.

Faception said it’s already signed a contract with a homeland security agency to help identify terrorists. The company said its technology also can be used to identify everything from great poker players to extroverts, pedophiles, geniuses and white collar-criminals.. BOSTON (AP) — A popular Chinese-made automotive GPS tracker used in 169 countries has severe software vulnerabilities, posing a potential danger to highway safety, national security and supply chains, cybersecurity researchers have found.

A report by the Boston cybersecurity firm BitSight says the flaws could let attackers remotely hijack device-equipped vehicles, cutting off fuel to them and otherwise seizing control while they travel.

The researchers say users should immediately disable the MV720 GPS tracker until a fix becomes available. The report was released Tuesday to coincide with an advisory from the U.S. Cybersecurity and Infrastructure Security Agency listing five vulnerabilities.

BitSight said it tried unsuccessfully for months — beginning in September, with CISA joining it in late April — to engage the manufacturer, Shenzen-based MiCODUS, in discussion addressing the vulnerabilities. The Associated Press telephoned and emailed the company but got no response. A person who answered a phone number listed on its website was unable to respond in English.

CISA said in a statement that it was not aware of “any active exploitation” of the vulnerabilities.

GPS trackers are used globally to monitor vehicle fleets – from trucks to school buses to military vehicles — and protect them against theft. In addition to collecting data on vehicle location, they typically also monitor other metrics, such as driver behavior and fuel usage. Via remote access, many are wired to cut off a vehicle’s fuel or alarm, lock or unlock its doors and more.

Using the MV720, which BitSight says costs less than $25 per unit, a malicious user could remotely cut off the fuel line of a vehicle in motion, know a vehicle’s real-time location for espionage purposes or intercept and taint location or other data to sabotage operations, said the principal BitSight researcher on the project, Pedro Umbelino.

He said multiple malicious scenarios are possible: First responders’ vehicles could be crippled, or a hacker could shut off an engine and demand a cryptocurrency ransom of victims to avoid calling a mechanic.

The main vulnerabilities: The device comes with a default password that more than 90% of users don’t change, and there is second, obscure but hard-coded password that works for all devices, BitSight found. It also found security flaws in the software of the web server used to remotely manage the GPS devices.

The manufacturer, MiCODUS claims an installed base of 1.5 million devices across 420,000 customers, said BitSight. Its research found they included a Fortune 50 energy company and an aerospace company, a national military in South America and in eastern Europe, a nuclear power plant operator and a national law enforcement agency in western Europe. It did not name any of them. Countries with the most users included, by continent: Brazil, Mexico, Spain and Russia.

Richard Clarke, the former U.S. cybersecurity czar, called the insecure GPS device yet another example of a smart Chinese-made product “that is phoning home and could be used maliciously by the Chinese government.”

While Clarke said he doubted the tracker was designed for that purpose, the danger is real because Chinese companies are obliged by law to follow their government’s orders -- which is why Washington has been seeking to minimize Chinese components in U.S. telecoms networks and why some in Congress are pushing for a ban on U.S. government purchases of Chinese drones.

“You just wonder, how often are we going to find these things that are infrastructure -- where there’s a potential for Chinese abuse -- and the users don’t know?” said Clarke.. For Adam Jasinski, a technology director for a school district outside of St Louis, Missouri, monitoring student emails used to be a time-consuming job. Jasinski used to do keyword searches of the official school email accounts for the district’s 2,600 students, looking for words like “suicide” or “marijuana”. Then he would have to read through every message that included one of the words. The process would occasionally catch some concerning behavior, but “it was cumbersome”, Jasinski recalled.

Last year Jasinski heard about a new option: following the school shooting in Parkland, Florida, the technology company Bark was offering schools free, automated, 24-hour-a-day surveillance of what students were writing in their school emails, shared documents and chat messages, and sending alerts to school officials any time the monitoring technology flagged concerning phrases.

The automated alerts were a game-changer, said Jason Buck, the principal of the Missouri district’s middle school. One Friday evening last fall, Buck was watching television at home when Bark alerted him that one of his students had just written an email to another student talking about self-harm. The principal immediately called the first student’s mother: “Is the student with you?” he asked. “Are they safe?”

Before his school used Bark, the principal said, school officials would not know about cyberbullying or a student talking about hurting themselves unless one of their friends decided to tell an adult about it. Now, he said, “Bark has taken that piece out of it. The other student doesn’t have to feel like they’re betraying or tattling or anything like that.”

Although students at his school are aware they’re being monitored, they were surprised at first at how quickly school administrators could follow up on what they had typed, Buck said. “It’s not, ‘Hey, I sent this email two days ago,’ [it’s] ‘You just sent this email three minutes ago, let’s talk.’”

Bark and similar tech companies are now monitoring the emails and documents of millions of American students, across thousands of school districts, looking for signs of suicidal thoughts, bullying or plans for a school shooting.

The new school surveillance technology doesn’t turn off when the school day is over: anything students type in official school email accounts, chats or documents is monitored 24 hours a day, whether students are in their classrooms or their bedrooms.

Tech companies are also working with schools to monitor students’ web searches and internet usage, and, in some cases, to track what they are writing on public social media accounts.

Parents and students are still largely unaware of the scope and intensity of school surveillance, privacy experts say, even as the market for these technologies has grown rapidly, fueled by fears of school shootings, particularly in the wake of the Parkland shooting in February 2018, which left 17 people dead.

Digital surveillance is just one part of a booming, nearly $3bn-a-year school security industry in the United States, where Republican lawmakers have blocked any substantial gun control legislation for a quarter century.

“Schools feel massive pressure to demonstrate that they’re doing something to keep kids safe. This is something they can spend money on, roll out and tell parents, this is what we’re doing,” said Chad Marlow, a privacy expert at the American Civil Liberties Union.

Unlike gun control, Marlow said, “Surveillance is politically palatable, and so they’re pursuing surveillance as a way you can demonstrate action, even though there’s no evidence that it will positively impact the problem.”

View image in fullscreen Tech companies are now monitoring the emails and documents of millions of American students, looking for signs of suicidal thoughts, bullying, or plans for a school shooting. Illustration: Guardian Design/The Guardian

Huge growth

There is still no independent evaluation of whether this kind of surveillance technology actually works to reduce violence and self-harm. Privacy experts say pervasive monitoring may hurt children, and may be particularly dangerous for students with disabilities and students of color.

Despite the lack of research evidence, tech companies are marketing school monitoring technologies with bold claims of hundreds of lives saved, mostly through prevention of youth suicide attempts.

Gaggle, a leading provider of school email and shared document monitoring, says its technology is currently used to monitor 4.5 million students across 1,400 school districts. The company claims that in the last academic year alone its technology “helped districts save the lives of more than 700 students who were planning or actually attempting suicide”.

Bark says it works with at least 1,400 school districts across the country, and claims its technology has helped prevent “16 credible school shootings” and detected “twenty thousand severe self-harm situations”.

Securly, another leading provider, says its products are used to protect 10 million students across 10,000 individual schools. In the past year, Securly said it helped school officials intervene in 400 situations that presented an “imminent threat”.

The companies’ statistics on lives saved are based on their own anecdotal data, and have not been independently evaluated.

“I heard from a lot of districts that in the weeks after Parkland, they were getting nonstop email solicitations from all sorts of brand new, or fairly new companies specializing in social media, that were saying, ‘We can fix your problems,’ and a lot of them were adopting it,” Amelia Vance, the director of education privacy at the Future of Privacy Forum, said.

“Some people think that technology is magic, that artificial intelligence will save us,” Vance said. “A lot of the questions and a lot of the privacy concerns haven’t [been] thought of, let alone addressed.”

How it works

In Florence, South Carolina, school officials intervened after a middle school student started writing about suicide while working on an in-class English assignment. The phrases she typed in a Google document triggered an alert from Gaggle, the surveillance company working with the school district. “Within minutes”, the student was pulled out of class for a conversation with school officials, according to Dr Richard O’Malley, the district superintendent.

In Cincinnati, Ohio, the school district’s chief information officer had to call the police in the middle of the night to conduct a wellness check on a student who had been flagged by Gaggle for writing about self-harm. The situation was serious enough that the student was hospitalized to receive mental health services, the chief information officer, Sarah Trimble-Oliver, said.

In rural Weld county, Colorado, a school official got an alert from GoGuardian, a company that monitors students’ internet searches, that a student was doing a Google search for “how to kill myself” late one evening. The official worked with a social worker to call law enforcement to conduct an in-person safety check at the student’s home, said Dr Teresa Hernandez, the district’s chief intervention and safety officer. When the student’s mother answered the door, she was confused, and said that her child had been upstairs sleeping since 9pm. “We had the search history to show, actually, no, that’s not what was going on,” Hernandez said.

Federal law requires that American public schools block access to harmful websites, and that they “monitor” students’ online activities. What exactly this “monitoring” means has never been clearly defined: the Children’s Internet Protection Act, passed nearly 20 years ago, was driven in part by fears that American children might look at porn on federally funded school computers.

As technology has advanced and schools have integrated laptops and digital technology into every part of the school day, school districts have largely defined for themselves how to responsibly monitor students on school-provided devices – and how aggressive they think that monitoring should be.

View image in fullscreen In some cases, surveillance companies monitoring students may themselves directly contact local law enforcement to take action if they’re concerned a threat is serious. Illustration: Guardian Design/The Guardian

Schools have faced lawsuits by parents of students who have committed suicide and by parents of children who have been cyberbullied, said Vance, the student privacy expert.

“Schools are almost in a damned if you do, damned if you don’t situation. If they choose to be more privacy protective they could be sued, but on the other hand, they could be sued for over-surveilling,” she said.

Bark’s decision following the Parkland shooting to give away free email, chat and shared document monitoring to any school district that wanted it was partly altruistic, an effort to respond to a horrifying crisis, and partly strategic, with the hope that providing a free service for school districts would make it a trusted brand with parents, helping sales of its for-profit parent surveillance products, which it markets for $9 a month, said Titania Jordan, the company’s “chief parenting officer”.

Other companies, some of which offer schools human analysts who help review the automated alerts, charge districts thousands or tens of thousands of dollars.

The amount American public school districts spend on email and document monitoring services appears to have increased sharply from 2013, the year after a mass shooting at Sandy Hook elementary school, to 2018, from nearly $4m to more than $8m, according to an analysis of purchasing contracts between just two major monitoring companies, Gaggle and Securly, and roughly 250 school districts. These numbers appear to be an undercount of the full size of the market, according to the Brennan Center for Justice, the progressive advocacy group that compiled and analyzed the purchasing records.

As of 2018, at least 60 American school districts had also spent more than $1m on separate monitoring technology to track what their students were saying on public social media accounts, an amount that spiked sharply in the wake of the 2018 Parkland school shooting, according to the Brennan Center.

Values at stake

Some proponents of school monitoring say the technology is part of educating today’s students in how to be good “digital citizens”, and that monitoring in school helps train students for constant surveillance after they graduate.

“Take an adult in the workforce. You can’t type anything you want in your work email: it’s being looked at,” Bill McCullough, a Gaggle spokesperson, said. “We’re preparing kids to become successful adults.”

Experience with school monitoring is a “training ground” that might mean that students “won’t lose their job for sharing inappropriate content”, said Trimble-Oliver, the chief information officer for Cincinnati’s public school district, which uses Gaggle.

Students “need to know that organizations can and probably are monitoring their content”, she said.

Privacy experts called these arguments “concerning”, and note that there are legal limits to how companies can monitor employees’ work emails.

“The idea that everything students are searching for or everything that they’re writing down is going to be monitored by their school can really inhibit growth and self-discovery,” Natasha Duarte, a policy analyst at the Center for Democracy and Technology, said.

For black students, and students with disabilities, who already face a disproportionate amount of harsh disciplinary measures, the introduction of new kinds of surveillance may be especially harmful, privacy experts said.

Both machine-learning algorithms and human analysts are at risk of misunderstanding what students write – particularly if the human analysts are older, or from different cultural backgrounds than the students they are monitoring, experts said. If digital surveillance companies scanning students’ emails and chats misinterpret their jokes or sarcasm as real threats, that “could expose students to law enforcement in a way they have not been in the past”, said Elizabeth Laird, the senior fellow for student privacy at the Center for Democracy and Technology.

The consequences of involving law enforcement in responding to what students are typing on their school computers is a particular concern in a country where more than 40% of schools have police officers inside school buildings serving as dedicated “school resource officers”, Vance said.

In some cases, surveillance companies monitoring students may themselves directly contact local law enforcement officials to take action if they’re concerned a threat is serious, and if school officials have given them permission to do so. The data surveillance companies are collecting on students may also be shared with law enforcement.

View image in fullscreen Parents and students are still largely unaware of the scope and intensity of school surveillance, privacy experts say. Illustration: Guardian Design/The Guardian

Securly, one of the leading educational surveillance companies, makes it possible for the human analysts who evaluate potentially troubling student messages to look back at the history of an individual student’s internet browsing history and web searches, allowing them to connect the dots between what students are reading, writing, searching for, and, in some cases, posting on social media.

Securly will share information with law enforcement “if there’s a warrant or subpoena”, Mike Jolley, the company’s director of K-12 Safety, said. Data about individual students is deleted when the students graduate, or when schools request it, Jolley said.

In the United Kingdom, school surveillance technology has already been tested for use in counter-terrorism efforts. Impero, a British education software company, piloted its monitoring technology as a counter-terrorism tool, flagging children for using phrases like “Jihadi bride”, “War on Islam”, or “You only die once”, the Guardian reported in 2015.

The company billed this as a “de-radicalisation effort” that would help teachers and other officials identify “vulnerable children” or “children that may be at risk in the future”.

“It’s certainly fair to ask to what extent we feel comfortable with technologies first developed for use in war being used against our children,” Marlow, the ACLU expert, said.

It’s not clear what kind of “chilling effect” the monitoring might have on students’ self-exploration, their group conversations and their academic freedom, Marlow, the ACLU privacy expert, said. If students know their schools are monitoring their computer usage, will LGBTQ students in conservative school districts feel comfortable researching their sexuality? What about young Trump supporters in liberal school districts who want to do some political research?

“Schools don’t post on a bulletin board outside the principal’s office, ‘Here are the words we’re searching for,’” Marlow said. “It forces students to be careful. They might not write about things or talk about things that are not, in fact, being monitored.”

Shifts in culture

School officials say that their primary motivation for using surveillance technology is the chance to save a student’s life. But schools are monitoring students’ digital documents in real time for a wide range of content they see as problematic, from swear words to nude images and pornography to cyberbullying to evidence of drug and alcohol use.

In Weld county, Colorado, a student emailed a teacher that she heard two boys were going to smoke weed in a bathroom, Hernandez, the student services and safety director, said. Gaggle immediately alerted school officials: “Within four minutes of her sending this email, the troops had deployed,” she said.

Gaggle also automatically sends students a scolding email any time they use a profanity.

A few school districts have chosen not to send students Gaggle’s warnings about swear words, some because they’re concerned that if students are reminded that they’re being monitored, “the children will then resort to other tools to communicate, and they’ll miss the life-threatening issues they could have intervened in,” McCullough, the Gaggle spokesperson, said.

McCullough said these fears were misplaced, and that the company had seen little evidence that the students being surveilled on school devices had switched to other forms of communication.

“Kids who have used us in their districts for years and years still use these tools to communicate their innermost thoughts, because they’re hoping that their cries for help are answered, and they’re not comfortable communicating the way adults communicate, face-to-face,” McCullough said.

O’Malley, the South Carolina superintendent whose district uses Gaggle, had a different impression of how kids in his district had reacted to the new surveillance.

“Once the kids know they’re being Gaggled, they’re being watched 24-7, they tend to be more proactive in watching what they do,” O’Malley said.

He said he had heard students in the district using Gaggle, the surveillance company’s name, as a verb: “We can’t do that. We’re being Gaggled.”

Drawing the line

While parents are often “grateful” for the information that comes from an intervention, Buck, the Missouri principal, said, students can be “a little bit upset sometimes. They feel like there’s a little bit of that privacy issue. But over the course of time they see we’re really trying to help, especially when we’re talking about the issue of self-harm.”

View image in fullscreen School officials say that the technology is still far from perfect, and that they have received false alerts for the classic American novel To Kill a Mockingbird. Illustration: Guardian Design/The Guardian

Some school surveillance companies defended their products as more sensitive to students’ privacy than their competitors – or the students’ own parents.

“Some parents want technology that will give them an exact record of every single text, every single email,” Jordan, Bark’s chief parenting officer, said. But Bark does not offer that, she said: “We only alert parents and schools when there is a real issue that they need to know about.”

For Gaggle, McCullough said, the bright line was offering monitoring of only students’ official school emails and school documents.

“We shouldn’t be looking at their private email. We shouldn’t be looking at their private social media posts. But in the school, with school-issued tools, we should protect them,” McCullough said.

Securly, in contrast, offers a free app for parents in the districts that use its technology that allows them to see exactly what websites their children have visited, what Google searches they have made, and what videos they are watching on YouTube, Jolley, Securly’s safety director, said

At the moment, the company has no privacy protections for LGBTQ students who might need to search for information without their parents knowing about it, although Jolley said that was a concern the company was actively discussing.

While school surveillance companies tout products informed by “machine learning”, and “artificial intelligence”, school officials say that the technology is still far from perfect, and that they have received plenty of false alerts, like getting red flags when students tell each other sarcastically to “kill yourself”, talk about the band Suicide Boys, or have to write a school assignment on the classic American novel To Kill a Mockingbird.

An investigation into school surveillance by Education Week examined nearly 3000 alerts that Gaggle sent to one school district in Michigan. Most of these were “minor violations,” including a dozen students who stored or sent files that contained the word “gay,” and one who used the word “bastard” in a school assignment about the Odyssey.

After he had received multiple alerts from the same group of middle school boys who were sending edgy memes to each other, sometimes as late as 3am, Buck, the Missouri principal, called the boys into the office and encouraged them to find another way to share their jokes – perhaps a group text message – that would not constantly alert their principal.. The Moscow metro has rolled out what authorities have lauded as the world’s first mass-scale facial recognition payment system, amid privacy concerns over the new technology.

The cashless, cardless and phoneless system, named Face Pay, launched at more than 240 stations across the Russian capital on Friday.

“Now all the passengers will be able to pay for travel without taking out their phone, metro or bank card,” the Moscow mayor, Sergey Sobyanin, tweeted on Thursday evening.

To activate Face Pay, Sobyanin said, passengers will need to connect their photo, bank card and metro card to the service through the metro’s mobile app. “It will be enough just to look at the camera to pass through the turnstiles,” Sobyanin said.

The Moscow authorities, who expect up to 15% of metro passengers will use Face Pay regularly in the next three years, said the system would quicken the flow of people, particularly at busy times.

“Moscow is the first in the world to introduce Face Pay on such a scale. The technology is new and very complex, we will continue to work on improving it,” the mayor added.

Authorities have said passengers’ data will be “securely encrypted”, saying the information collected will be stored in data processing centres to which only interior ministry staff have access.

However, activists have raised the alarm over privacy concerns. “This is a dangerous new step in Russia’s push for control over its population. We need to have full transparency on how this application will work in practice,” said Stanislav Shakirov, the founder of Roskomsvoboda, a group dedicated to protecting digital rights and freedom of information.

“We are moving closer to authoritarian countries like China that have mastered facial technology. The Moscow metro is a government institution and all the data can end up in the hands of the security services,” he added.

Moscow recently expanded its facial recognition technology across the capital, with a network of more than 175,000 surveillance cameras.

Human rights activists said the cameras were used to identify protesters who attended rallies in support of the jailed opposition politician Alexei Navalny. Facial recognition was also used to enforce Covid-19 quarantines during Russia’s two-month lockdown in spring 2020.

The Russian metro – the busiest in Europe, with more than 6 million daily passengers – is considered a tourist attraction in itself, with stations elaborately decorated with mosaics reflecting a range of Soviet themes.. Amazon’s fleet of delivery drivers may be detached from Amazon warehouses, but the company still monitors its drivers; it installed always-on cameras in its delivery trucks and even asked drivers to consent to yawn-detecting AI surveillance. However, it’s that very surveillance system that puts Amazon in the crosshairs of a lawsuit that claims Amazon is at fault for a life-altering car accident.

This Bloomberg report describes the details of the accident, as well as how one lawsuit may change the way legal teams approach similar situations in the future. The accident happened earlier this year, when 24-year-old Ans Rana was riding in the backseat of a Tesla Model S on a bustling Atlanta highway. After the car came to a stop behind a disabled vehicle, an Amazon delivery vehicle slammed into the back of the Tesla, leaving Rana with major brain and spinal-cord injuries.

Rana filed a lawsuit against Amazon in June, claiming that Amazon is responsible for the accident. But as the report notes, Amazon says it can’t be held accountable due to the fact that the delivery driver technically wasn’t an employee of Amazon.

The delivery driver actually worked for Harper Logistics, LLC, one of the many Delivery Service Partners that Amazon uses for shipping. The Delivery Service Partners program was rolled out in 2018, and according to Bloomberg, Amazon’s partners employ over 260,000 drivers globally, some of who claim that their employers tell them to turn off their safety apps to meet quotas.

That’s why Rana’s lawsuit cruxes on Amazon’s algorithms, apps, and devices that Amazon uses to micromanage its workers. The lawyer on Rana’s case, Scott Harrison, aims to prove that Amazon still exerts control over its Delivery Service Partners using technology.

Amazon closely tracks delivery drivers’ every move, the lawsuit states, including ‘backup monitoring, speed, braking, acceleration, cornering, seatbelt usage, phone calls, texting, in-van cameras that use artificial intelligence to detect for yawning, and more.’

As Bloomberg notes, most commercial vehicle lawsuits like this are typically resolved without much attention. However, Rana’s case is noteworthy due to his legal team’s claims that Amazon’s monitoring systems make the company liable. Rana’s attorney wants to look into exactly how Amazon’s machines control its operations, but doing so would reveal Amazon’s algorithms, which Amazon reportedly argues could be classified as “trade secrets.”

The company designs and assigns routes and determines how many deliveries each driver should be able to accomplish in a 10-hour shift. It monitors each driver’s performance via smartphone app, cameras and other hardware installed in Amazon-branded delivery vans. The company can tell the delivery partners to take drivers it deems too slow off the road, essentially firing them. Rana’s lawsuit argues that such practices made Amazon negligent in the crash because they ‘forced drivers to rush to the point it was unsafe’ and ‘focused on speed and delivery efficiency without giving due consideration to the safety of the public.’

If Rana’s legal strategy works, this could influence the outcome of a number of lawsuits to come. Bloomberg mentions that Amazon Logistics has been the defendant in 119 vehicle injury lawsuits this year alone, which reportedly quadruples the number of incidents that occurred last year. And with the holiday season coming up, that number may be on the rise.

In February, Bloomberg states that a Texas couple reportedly sued Amazon for $1 million in damages following an alleged crash with a delivery driver — but the case has since been dismissed. Another man in Massachusetts reportedly suffered from brain injuries after a head-on collision with an Amazon delivery driver who allegedly fell asleep at the wheel. Amazon supposedly has yet to respond to that case, which Bloomberg says was filed in September.. A Dutch government report identifying "10 high data protection risks" for users of Google Workspace, formerly known as G Suite, has been revised after Google's response, and now says eight high risk issues still remain.

The study (available in English) was conducted by the Dutch Ministry of Justice and Security together with SLM Microsoft Rijk (Strategic Vendor Management Microsoft).

Despite the name, this is not an offshoot of Google's Redmond-based rival, but a government department which manages vendor relations with the company, and which undertook to assess the risks of deploying Google Workspace instead of Office 365, in a process called a DPIA (Data Protection Impact Assessment). It has conducted similar studies into privacy risks with Microsoft's services.

The DPIA, first published in July 2020, came up with a number of recommendations, some of which Google has now adopted or undertaken to adopt. For example, Google said it will enable admins to block the use of consumer Google accounts in the Workspace environment, according to the report, and to provide a clear visual indicator to users showing whether or not they are in the enterprise environment.

Despite these assurances, at the time of writing, the DPIA still considers that there are legal obstacles to adopting Google Workspace around the roles and obligations of data processors and data controllers under the EU's General Data Protection Regulation (GDPR).

“The use of Google Workspace as offered under the privacy amendment of the Dutch government, still leads to 8 high risks for the different categories of data subjects involved (not just employees, but all kinds of other data subjects that may interact with the Dutch government),” said the report as updated.

Why exactly are you collecting data?

The GDPR distinguishes between data controllers, which determine the purpose and means of data processing, and data processors, which carry out processing on behalf of the controller. An organisation administering a Google Workspace has a measure of control over what data is collected and how it is used, and is therefore a data controller. Google is a data processor, but also a data controller, since it also determines how data is used, although the scope of its role as data controller is (to nobody’s surprise) complicated.

Lack of detail over the purposes for which Google collects data is an obstacle to the legal obligations of organisations using Workspace. The Netherlands government and Google are joint controllers of data processed through Google Workspace, but the report said that, “due to the lack of purpose limitation and transparency, Google and the government organisations currently don’t have a legal ground for any of the data processing.”

A consultancy called Privacy Company, which assisted with the report, summarised the state of play here, where it said that while Google has improved its description of purposes, “it does not solve the problem that the State loses control over the personal data of its employees if the State allows Google to process these data for its own commercial purposes.”

Other issues are that purposes “are broad and unclear” and that “Google can change the purposes for the processing of the Service Data at will, by amending the privacy statement,” subject to certain prohibited purposes.

Multiple agreements, huge complexity

A user signing in to a Google Workspace account has to agree separately to the Core Services under their organisation’s terms, and Additional Services under Google’s terms. Which is used when? It’s complicated...

A key facet of this report is the distinction between "Core Services", such as Docs, Sheets, Meet and Calendar, and "Additional Services" including YouTube, Maps, Assistant and Search. The difference is critical because when end users sign up for a Workspace account they enter into multiple agreements. Use of the workspace is governed by the policies of their organisation, but users also have to agree to the generic consumer Google terms of service and privacy policy.

Google has said this is because users may use the Additional Services while logged in with the enterprise account, which “requires a direct contractual relationship with end users of products not sold under the G Suite Enterprise terms.”

An enterprise admin may attempt to turn off one of the Additional Services but if so “the end user is silently signed out from the Google Account, and can visit the service as end user without Google Account.”

A complication is that a user may be signed in simultaneously with both a Workspace and a personal account. In this case, they may use an Additional Service under the terms of their personal account. This interweaving makes policies hard to enforce.

Does the user know whether they are using Core Services or Additional Services? The DPIA looked at the spellchecker feature in Chrome. In answer to enquiries, Google said there are three kinds of spellchecker: a spelling and grammar feature, a basic local spellchecker, and an enhanced spellchecker. The spelling and grammar feature is defined as part of the Core Services, the enhanced spellchecker (which processes data differently) is an Additional Service. Both are accessed by a right-click.

“For the end-user, the difference with the G Suite Feature Spelling and grammar is not obvious. When checking the spelling of a document, the end user can use all three spellcheckers, without any clear distinction of its origin, part of a Core Service or part of the Additional Service,” said the DPIA.

There is a way of disabling enhanced spell check, but only via Chrome Enterprise policies, which on Windows can be enforced through Group Policy after installing the Chrome Enterprise Bundle. “Chrome Enterprise is not included in the G Suite Enterprise contract, and is therefore out of scope of this DPIA,” said the report.

Another head-spinning entanglement was with Google Maps. Calendar is a Core Service and Maps an Additional Service, but “In Calendar traffic to Google Maps takes place when working with Calendar items that contain a location,” said the report. “In reply to this DPIA, Google explained that the traffic to Maps was not traffic to an Additional Service, but an embedded processing within the Core Services.”

Data subject access requests denied

The DPIA went into detail about how it conducted its investigation. That said, it noted: “Because G Suite Enterprise is a remote, cloud-based service, data processing takes place on Google’s cloud servers. As a result, it is not possible to inspect via traffic interception how Google processes Diagnostic Data in its system generated logs about the use of the Core Services, the Additional Services, or the Google Account.”

The Dutch researchers discovered that Google does not provide all the personal data it holds when asked to do so under the GDPR provisions for the right to request access to this. “Google … explains that it does not provide certain personal data in reply to a data subject access request, because (i) it is impossible to reliably verify the identity of the data subject as that of the requester and (ii) in some cases such transparency would hurt Google’s efforts to protect the security of its systems,” said the report. Similarly, if users "delete activity" from a Google service, it may not actually be deleted.

Google's cloud services lost $14.6bn over three years – and CEO Sundar Pichai likes that trajectory READ MORE

In respect of YouTube activity, for example, Google said “if you delete activity, it’s no longer used to personalise your Google experience … for business or legal compliance purposes Google must retain certain types of data for an extended period of time.”

The Dutch researchers responded by offering Google “multiple ways to verify their identity,” even including copies of passports, but “Google has refused all these options.”

Measures the report suggested to the Dutch government include prohibiting the use of Chrome OS and the Chrome browser, and not using Workspace Enterprise until data processing “can be based on one or more legal grounds.”

Google Cloud veep for EMEA Samuel Bonamigo, in response to the updated report, posted about privacy and security in Workspace.

Bonamigo said that “we never use customer data or service data (such as usage activity) for ads targeting” and that “we only process Cloud customer data according to instructions set out in our customers’ agreements.”

He added: “We will continue to discuss the findings with the Dutch government in the next few months, with the goal of reaching an agreement.” ®. As AI-powered tracking of phone calls in U.S. prisons and jails spreads to more states, leading digital rights groups want federal action to rein it in

- Federal gov't urged to stop funding call monitoring tech

- Privacy, discrimination risks among critics' top concerns

- Thomson Reuters Foundation report highlighted Verus boom

By Avi Asher-Schapiro and David Sherfinski

LOS ANGELES/WASHINGTON, Feb 10 (Thomson Reuters Foundation) - Dozens of rights groups are demanding a crackdown on an artificial intelligence system used to eavesdrop on U.S. prisoners' phone calls, after a Thomson Reuters Foundation investigation highlighted the risk of rights violations.

Documents from eight states showed prison and jail authorities were using surveillance software called Verus, which scans for key words and leverages Amazon's voice-to-text transcription service, to monitor prisoners' phone calls.

California-based LEO Technologies, which operates Verus, says it has scanned close to 300 million minutes of calls going in and out of prisons and jails in the United States, describing the tool as a way to fight crime and help keep inmates safe.

But a coalition of civil and digital rights groups said the surveillance sometimes overstepped legal limits by targeting conversations unrelated to the safety and security of detention facilities, or possible criminal activity.

"This surveillance infringes the rights of incarcerated Americans, many of whom have not been convicted and are still working on their defenses, as well as those of their families, friends, and loved ones," the groups wrote in a joint letter.

Four different letters were sent to the attorney general's office in New York State, the state's Inspector General and the federal Department of Justice (DOJ).

The DOJ provided a $700,000 grant to the sheriff's office in Suffolk County, New York, to implement a pilot of the AI-powered voice-to-text surveillance system in 2020.

Undersheriff Kevin Catalina, who helps run the Verus program in Suffolk, told the Thomson Reuters Foundation that the system is crucial for alerting jail authorities to people who are suicidal and to identify gang members behind bars.

"It saves lives," he said.

A DOJ official said the department is reviewing technology programs receiving federal funding to ensure they are enhancing public safety while respecting constitutional rights.

A spokesperson for the New York State Inspector General's Office said in emailed comments that they would review the letter and "thoroughly investigate" complaints that are sent in.

More than 50 advocacy groups are part of the campaign, among them the Electronic Frontier Foundation, Worth Rises, the Innocence Project, and Access Now.

They also raised concerns about the prison phone call company Securus, and the possible recording of conversations protected by attorney-client privilege.

A Securus spokesperson said the company is committed to protecting civil liberties, that users can set attorney numbers to private - meaning calls are not recorded and cannot be monitored - and that they act immediately to delete "inadvertent" recordings.

A representative for LEO did not respond to requests for comment on the letters.

"It seems like the regulators have been asleep at the switch at the federal, state and local level," said Albert Fox Cahn, head of the Surveillance Technology Oversight Project, which helped draft the letter.

'UNPROVEN, INVASIVE, AND BIASED'

As Suffolk County was trialling Verus, it also expanded beyond New York, winning state contracts in Georgia and Texas, and in local sheriff's departments across the United States.

The rights groups urged regulators to block further expansion of surveillance tools in prisons and jails, saying they have the potential to produce racial bias and undermine privacy rights, without any clear track record of success.

In their letter addressed to the DOJ's Civil Rights Division, the groups cited research showing voice-to-text tools have a much higher error rate for Black voices. Black people are disproportionately represented among U.S. prisoners.

"Even absent discrimination, Verus and similar technologies exceed prisons and jails' lawful surveillance powers," they wrote.

Documents obtained by the Thomson Reuters Foundation from the pilot site in Suffolk County showed Verus was used to analyze more than 2.5 million calls between its launch in April 2019 and May 2020 - leading to 96 "actionable intelligence reports".

While Catalina did not specify how many prisoners had been disciplined or faced charges based on those leads, he said the tool had helped prevent 86 suicides.

The rights groups also raised concerns about mission creep, noting the technology had been used to identify conversations that could flag problems for prison or jail administrators - such as complaints about their response to COVID-19.

Catalina said the sheriff's office reviews all its surveillance strategies on a monthly basis to make sure that their terms used in the Verus system are appropriate, and that it has never found any issues.

The surveillance of detainees' phone calls is especially troubling in county jails, where people are frequently held before being convicted of any crime, said Bianca Tylek, executive director of criminal justice nonprofit Worth Rises.

"People who are innocent, (who) have the presumption of innocence, who cannot afford bail ... should not be subjected to surveillance that no one else is," said Tylek.

Besides infringing the privacy of incarcerated people and their relatives, AI-powered surveillance in prisons and jails could also lead to increases in the cost of phone calls for prisoners, rights campaigners fear.

The average 15-minute phone call from a jail already costs $5.74, according to a 2019 report from the Prison Policy Initiative, while 2015 research found more than a third of families reported getting into debt to pay for calls or visits.

Worth Rises, which has been pushing to reduce the cost of prison phone calls across the country, is urging state and local law enforcement to offer calls for free.

Emails between LEO and sheriff's offices, which were obtained through public records requests, show use of LEO's Verus system could cost as much as 8 cents per minute.

They also give a picture of how the company worked in tandem with law enforcement officials to raise funds - enlisting PR personnel, helping draft federal grant proposals, and making appeals to lawmakers.

In Suffolk County, the Sheriff's office discussed plans to pass the cost onto prisoners themselves if grant funding ran out, the emails reveal.

The office said that while it had considered passing along the costs to prisoners, they ultimately decided not to.

Tylek said the federal government should not be funding pilots involving systems like Verus, warning that authorities rarely relinquish surveillance powers once they have been granted.

"It (becomes) almost impossible to pull it out," she said.

Related stories:

A year after Myanmar coup, growing surveillance threatens lives

Online content moderation: Can AI help clean up social media?

In Thailand's Muslim south, phones cut off in surveillance crackdown

(Reporting by Avi Asher-Schapiro and David Sherfinski. Editing by Helen Popper. Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters, that covers the lives of people around the world who struggle to live freely or fairly. Visit http://news.trust.org)

Our Standards: The Thomson Reuters Trust Principles.. Rights groups warn high-tech surveillance systems will be used to track down and eliminate the junta's opponents.

Myanmar's junta is expanding its public surveillance capabilities by using facial recognition technology, raising fresh concerns about the safety of democracy activists and resistance groups in Myanmar.

In March, Human Rights Watch (HRW) released a report on Myanmar's use of the Chinese-made facial recognition systems, warning of a "serious threat" to human rights.

HRW said hundreds of cameras were installed in townships around the capital Naypyidaw in December 2020, before the military took power in a coup, in the first phase of a security initiative called "safe city." Cameras were also installed in Myanmar's largest city, Yangon.

Experts and activists on the ground fear that the military's increased access to this technology could have consequences for the safety of anyone opposing the junta.

A report by the Reuters news agency in July revealed that Myanmar's military government is now expanding camera surveillance systems for cities in all 14 states and divisions in the country.

The cameras, sourced from Chinese tech conglomerates Huawei, Dahua and Hikvision, are equipped with artificial intelligence technology that automatically scans faces and vehicle license plates in public places and alerts authorities to those on a wanted list.

"This is another threat coming, not just on the ground. We are now resisting a digital authoritarian regime," Thinzar Shunlei Yi, a Yangon-based activist, told DW.

Many activists hide their identity as the military tries to stamp out resistance Image: NurPhoto/STR/picture alliance

Myanmar's struggle for democracy

The people of Myanmar were accustomed to military rule in the past, but after a decade of progress and development, the past 18 months have left many feeling as if they are going back.

A short-lived experiment with democracy ended on February 1, 2021, when the military seized power and overthrew the democratically-elected civilian government led by Aung San Suu Kyi. Military commander Min Aung Hlaing then imposed a state of emergency and arrested elected leaders, including Suu Kyi.

Although the coup triggered one of the biggest pro-democracy protests in Myanmar's history, the army's response was swift and violent.

According to the Assistance Association for Political Prisoners (AAPP), more than 2,100 people have been killed since the military regime took power and over 14,800 have been arrested, charged or sentenced by the junta.

The army has ignored international pressure to reinstate civilian rule and has also been tightening its grip on democracy activists and the opposition through the digital space.

By controlling the biggest telecommunication company in Myanmar, Telenor, the junta has restricted internet access and censored online content. There have also been reports that the junta has installed spyware on telecom services and internet providers to further monitor and combat online "traitors."

"We are not safe. Basically, all our information can be exposed. The junta heavily uses its digital power to spread misinformation and disinformation, as well as to detect where we are and what we are doing," said activist Yi.

HRW Deputy Asia Director Phil Robertson told DW the cameras are the "epitome of intrusive surveillance," abd would allow the junta to remotely monitor, track and ultimately raid the operations of opponents.

"We expect the systems will be used to identify persons of interest, follow their movements, identify their motorcycles and cars, and ultimately follow them to resistance safe houses where junta forces can attack, arrest, and kill those opposed to the military regime," he said.

A war between military junta and people Myanmar: Phil Robertson, Human Rights Watch To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video

How is Myanmar's junta applying the technology?

The use of surveillance technology for security is used by governments around the world to fight crime. This is by no means confined to authoritarian regimes, and Suu Kyi's government also used Chinese-made technology before the coup.

However, in the domestic political context of Myanmar, HRW argues that the junta is applying this technology to strengthen its hold on power.

Robertson explained a narrative of "safe cities" was used but in reality this was "designed to hide rights abusing, intrusive surveillance behind the excuse of crime fighting."

And the impact of the junta's surveillance can already be seen across Myanmar: "We see an increasing number of arrests every day, especially in our strike committees," said activist Yi.

"Apart from that, it has become more difficult to organize a large strike in different urban areas," she added.

Junta leader Min Aung Hlaing has vowed to eliminate resistance to military rule in Myanmar Image: REUTERS

Yi has been in hiding from the junta since the military cracked down on pro-democracy protesters in spring 2021. Despite the brutal force and increased use of surveillance tools by the junta, which she said was a prime way of commiting crimes with impunity, she predicted that the struggle would continue.

"They might kill revolutionaries, but not the revolutionary spirit. As long as the junta is attempting to rule the nation, the resistance forces will keep on going," she said.

Calls for international support

Since the junta executed four activists, calls have grown for the international community to cut off the military from oil and gas revenue flows, and other sources of income.

"Democratic countries need to be united. We are all here watching it, reporting it and then what? The people in Myanmar cannot stop it, yet," said Yi.

Edited by: Wesley Rahn. . A Russian “suicide drone” that boasts the ability to identify targets using artificial intelligence has been spotted in images of the ongoing invasion of Ukraine.

Photographs showing what appears to be the KUB-BLA, a type of lethal drone known as a “loitering munition” sold by ZALA Aero, a subsidiary of the Russian arms company Kalashnikov, have appeared on Telegram and Twitter in recent days. The pictures show damaged drones that appear to have either crashed or been shot down.

With a wingspan of 1.2 meters, the sleek white drone resembles a small pilotless fighter jet. It is fired from a portable launch, can travel up to 130 kilometers per hour for 30 minutes, and deliberately crashes into a target, detonating a 3-kilo explosive.

ZALA Aero, which first demoed the KUB-BLA at a Russian air show in 2019, claims in promotional material that it features “intelligent detection and recognition of objects by class and type in real time.”

The drone itself may do little to alter the course of the war in Ukraine, as there is no evidence that Russia is using them widely so far. But its appearance has sparked concern about the potential for AI to take a greater role in making lethal decisions.

“The notion of a killer robot—where you have artificial intelligence fused with weapons—that technology is here, and it's being used,” says Zachary Kallenborn, a research affiliate with the National Consortium for the Study of Terrorism and Responses to Terrorism (START).

Advances in AI have made it easier to incorporate autonomy into weapons systems, and have raised the prospect that more capable systems could eventually decide for themselves who to kill. A UN report published last year concluded that a lethal drone with this capability may have been used in the Libyan civil war.

It is unclear if the drone may have been operated in this way in Ukraine. One of the challenges with autonomous weapons may prove to be the difficulty of determining when full autonomy is used in a lethal context, Kallenborn says.

The KUB-BLA images have yet to be verified by official sources, but the drone is known to be a relatively new part of Russia’s military arsenal. Its use would also be consistent with Russia’s shifting strategy in the face of the unexpectedly strong Ukrainian resistance, says Samuel Bendett, an expert on Russia’s military with the defense think tank CNA.

Bendett says Russia has built up its drone capabilities in recent years, using them in Syria and acquiring more after Azerbaijani forces demonstrated their effectiveness against Armenian ground military in the 2020 ​​Nagorno-Karabakh war. “They are an extraordinarily cheap alternative to flying manned missions,” he says. “They are very effective both militarily and of course psychologically.”

The fact that Russia seems to have used few drones in Ukraine early on may be due to misjudging the resistance or because of effective Ukrainian countermeasures.. . MANILA: It is an “easy job” she has done full-time since the pandemic. All she needs is a SIM card and she can work from home, or anywhere for that matter.

Sharon (not her real name) does not know who her bosses or fellow workers are, however. She is “not interested” in knowing them either.

“What’s important is we get our pay cheques, and we deliver on what they ask,” she said.

Her job mission: To spread lies to as many people as possible. Using fake profiles — and various SIM cards, as a matter of fact — to infiltrate social media groups, she has a daily target of at least 150 shares.

Sharon, who is in her early 30s, is a paid troll. She is part of a large troll army available for hire in the Philippines, where politicians have allegedly shelled out for manipulating political discourse and swaying public opinion.. DETROIT (AP) — A tweet from Elon Musk indicating that Tesla might allow some owners who are testing a “Full Self-Driving” system to disable an alert that reminds them to keep their hands on the steering wheel has drawn attention from U.S. safety regulators.

The National Highway Traffic Safety Administration says it asked Tesla for more information about the tweet. Last week, the agency said the issue is now part of a broader investigation into at least 14 Teslas that have crashed into emergency vehicles while using the Autopilot driver assist system.

Since 2021, Tesla has been beta-testing “Full Self-Driving” using owners who haven’t been trained on the system but are actively monitored by the company. Earlier this year, Tesla said 160,000, roughly 15% of Teslas now on U.S. roads, were participating. A wider distribution of the software was to be rolled out late in 2022.

Despite the name, Tesla still says on its website that the cars can’t drive themselves. Teslas using “Full Self-Driving” can navigate roads themselves in many cases, but experts say the system can make mistakes. “We’re not saying it’s quite ready to have no one behind the wheel,” CEO Musk said in October.

On New Year’s Eve, one of Musk’s most ardent fans posted on Twitter that drivers with more than 10,000 miles of “Full Self-Driving” testing should have the option to turn off the “steering wheel nag,” an alert that tells drivers to keep hands on the wheel.

Musk replied: “Agreed, update coming in Jan.”

It’s not clear from the tweets exactly what Tesla will do. But disabling a driver monitoring system on any vehicle that automates speed and steering would pose a danger to other drivers on the road, said Jake Fisher, senior director of auto testing for Consumer Reports.

“Using FSD beta, you’re kind of part of an experiment,” Fisher said. “The problem is the other road users adjacent to you haven’t signed up to be part of that experiment.”

Tesla didn’t respond to a message seeking comment about the tweet or its driver monitoring.

Auto safety advocates and government investigators have long criticized Tesla’s monitoring system as inadequate. Three years ago the National Transportation Safety Board listed poor monitoring as a contributing factor in a 2018 fatal Tesla crash in California. The board recommended a better system, but said Tesla has not responded.

Tesla’s system measures torque on the steering wheel to try to ensure that drivers are paying attention. Many Teslas have cameras that monitor a driver’s gaze. But Fisher says those cameras aren’t infrared like those of some competitors’ driver assistance systems, so they can’t see at night or if a driver is wearing sunglasses.

Philip Koopman, a professor of electrical and computer engineering at Carnegie Mellon University, argued that Tesla is contradicting itself in a way that could confuse drivers. “They’re trying to make customers happy by taking their hands off the wheel, even while the (owners) manual says ‘don’t do that.’ ”

Indeed, Tesla’s website says Autopilot and the more sophisticated “Full Self-Driving” system are intended for use by a “fully attentive driver who has their hands on the wheel and is prepared to take over at any moment.” It says the systems are not fully autonomous.

NHTSA has noted in documents that numerous Tesla crashes have occurred in which drivers had their hands on the wheel but still weren’t paying attention. The agency has said that Autopilot is being used in areas where its capabilities are limited and that many drivers aren’t taking action to avoid crashes despite warnings from the vehicle.

Tesla’s partially automated systems have been under investigation by NHTSA since June of 2016 when a driver using Autopilot was killed after his Tesla went under a tractor-trailer crossing its path in Florida. The separate probe into Teslas that were using Autopilot when they crashed into emergency vehicles started in August 2021.

Including the Florida crash, NHTSA has sent investigators to 35 Tesla crashes in which automated systems are suspected of being used. Nineteen people have died in those crashes.

Consumer Reports has tested Tesla’s monitoring system, which changes often with online software updates. Initially, the system didn’t warn a driver without hands on the wheel for three minutes. Recently, though, the warnings have come in as little as 15 seconds. Fisher said he isn’t sure, though, how long a driver’s hands could be off the wheel before the system would slow down or shut off completely.

In shutting off the “steering wheel nag,” Fisher said, Tesla could be switching to the camera to monitor drivers, but that’s unclear.

Despite implying through the names that Autopilot and “Full Self-Driving” can drive themselves, Fisher said, it’s clear that Tesla expects owners to still be drivers. But the NTSB says human drivers can end up dropping their guard and relying too much on the systems while looking elsewhere or doing other tasks.

Those who use “Full Self-Driving,” Fisher said, are likely to be more vigilant in taking control because the system makes mistakes.

“I wouldn’t dream of taking my hands off the wheel using that system, just because it can do things unexpectedly,” he said.

Koopman said he doesn’t see a great safety risk from disabling the steering wheel nag because the Tesla monitoring system is so flawed that disabling it doesn’t necessarily make Teslas any more dangerous.

NHTSA, he said, has enough evidence to take action to force Tesla to install a better monitoring system.

The agency says it doesn’t comment on open investigations.. Last year, we covered a research report which found out how projectors could be used to display virtual objects and fool self-driving cars. In the latest, we have another piece of research that deals with strikingly similar details but incorporating the trickery of Artificial Intelligence (AI) as a whole.

Discovered by researchers Abhiram Gnanasambandam, Alex M. Sherman, and Stanley H. Chan from Purdue University; the new attack has been dubbed as an OPtical ADversarial attack (OPAD) and involves using three objects: a low-cost projector, a camera, and a computer in order to execute the attack.

SEE: These people don’t exist – They were created by tech using AI

How it does so is not by placing new virtual objects in the environment but by modifying how existing objects already present are seen by AI. The image below shows how objects like a basketball were modified to be seen as something else by projecting certain calculated patterns onto them.

The pros of such an attack method are that no physical access is needed to the objects themself. This, according to the researchers, is a move away from previously discussed AI trickery methods which obviously can make the concealment of attackers much easier.

Demonstrating another attack example, the researchers cite the example below:

Detailing further, the researchers state in their researcher paper that,

The difficulty of launching an optical attack is making sure that the perturbations are imperceptible while compensating for the environmental attenuations and the instrument’s nonlinearity. OPAD overcomes these difficulties by taking into consideration the environment and the algorithm. OPAD is a meta-attack framework that can be applied to any existing digital attack.

The real-world implications of this are that it can be used to trick, for example, self-driving cars and therefore be used to cause accidents or perhaps incur pranks. On the other hand, security cameras employing AI can also be tricked which can have significant repercussions.

To conclude, this will obviously not work in all situations but regardless, companies developing AI technologies need to be on the lookout for such potential security problems.

Furthermore, it is important to mention that the research was funded by the U.S Army which indicates that the military may already be on the lookout for using such methods to aid their missions in the field.

Did you enjoy reading this article? Like our page on Facebook and follow us on Twitter.. My lawyer, the robot Presented by Special Competitive Studies Project

With help from Derek Robertson



A closeup of an earbud. | AP

Call it the Cyber-ano de Bergerac Defense.

The eerie new capabilities of artificial intelligence are about to show up inside a courtroom — in the form of an AI chatbot lawyer that will soon argue a case in traffic court.

That’s according to Joshua Browder, the founder of a consumer-empowerment startup who conceived of the scheme.

Sometime next month, Browder is planning to send a real defendant into a real court armed with a recording device and a set of earbuds. Browder’s company will feed audio of the proceedings into an AI that will in turn spit out legal arguments; the defendant, he says, has agreed to repeat verbatim the outputs of the chatbot to an unwitting judge.

Browder declined to identify the defendant or the jurisdiction for next month’s court date, citing fears that the judge would catch wind of the planned stunt and block it.

In recent months, the public release of increasingly advanced AI tools has raised questions about everything from high school plagiarism to the very essence of what it is to be human. Now the technology is poised to collide with legal systems and public policies that never anticipated a role for non-human actors — especially not in court.

It’s all part of a tech-policy “arms race,” Browder said, in which AI is shaking up the tools and rules that determine the balance of power between individuals, on one hand, and governments and corporations, on the other. He sees AI as a way for regular citizens a chance to level the playing field in fights over fines and fees, which they often can’t afford to litigate themselves.

AI has already made inroads into the American legal profession, where big firms routinely use it to assist in the task of reviewing troves of documents that can number in the millions during the discovery phase of litigation.

But Nicholas Saady, a litigator at Pryor Cashman who advises on the use of AI in business and legal practice, said this latest application might not fly. He pointed to a host of procedural and practical issues presented by using AI chatbots for real-time legal representation. “Is it the unauthorized practice of law?” he asked, saying the plan risked running afoul of state laws that require professional licensure for lawyers.

Browder said he has considered this and identified two jurisdictions where his plan is “not outright illegal.” In the second jurisdiction, he said, he had lined up a defendant to use the AI in a traffic court Zoom hearing.

For the Zoom hearing, Browder said that rather than having the defendant speak the chatbot’s output aloud themselves, he was considering taking the simulation one step further by using an AI tool that can mimic a person’s voice after recording their speech. “Although,” he conceded, “that could get us into a lot of trouble.”

This is not Browder’s first foray into high-tech dispute resolution. Browder’s company offers all kinds of DIY legal and consumer help, and he has been using AIs to help customers press their claims with governments and corporations since 2021, largely by generating form letters and scripts for online customer service chats. One problem he has encountered along the way is that AIs will sometimes make things up to push their case, he said. To prevent this, he said, his company has to formulate elaborate instructions that force the bots to stick to factual statements.

For his courtroom foray, Browder said he is using GPT-J, an open source AI model released last year. For less sensitive applications, Browder said his company uses models from OpenAI, the company behind the highest-profile new AI tools, called large language models.

Browder said he remains in close touch with OpenAI to ensure his company, DoNotPay, does not run afoul of its terms of use. In practice, that has meant adding features — like a two-second delay during which a user can reject an AI’s suggested answer — that ensure a human maintains control of the final results, he said.

OpenAI did not respond to requests for comment.

Browder’s courtroom gambit is just one of the curveballs that the recent slew of advanced AI tools is throwing at the legal profession. A non-peer reviewed preprint paper published a week ago by two law professors predicts that a large language model will soon be able to pass the multiple-choice section of the Multistate Bar Exam.

But even if an AI could pass the Bar, and even if courts allowed AI litigators, Saady said litigants would still be better off hiring flesh-and-blood lawyers.

Good litigation, he said, relies on intangibles that remain out of reach for bots: He cited the ability to read body language, and to make split-second strategic decisions in the middle of courtroom exchanges. “It doesn’t seem like AI is ready to get on its feet in court,” he said.

Boston attorney Matt Henshon, on the other hand, said the idea of AI-powered legal counsel holds promise. Henshon, who chairs the American Bar Association’s Artificial Intelligence and Robotics Committee, said it could provide legal help in lower-stakes scenarios where a person would otherwise go without representation at all.

In that sense, he likened AI legal counsel to class-action lawsuits, which allow large numbers of plaintiffs to obtain representation by bundling together many small claims that would not be worth litigating on their own.

“There are plenty of legal wrongs that don’t get righted because it’s not worth it for a lawyer to get involved,” Henshon said.

Browder — whose father Bill Browder has spearheaded the passage of the Global Magnitsky Act and other laws around the world that sanction human rights violators — said he hopes his techno-legal stunt will help make that case to state legislatures and other rule-making bodies so that they will accommodate the rise of AI lawyers.

“There’s all these gatekeepers, there’s laws, there’s governments to work around,” he said. “It’s not really the technology that’s the hardest part. It’s being in compliance.”



A message from Special Competitive Studies Project: Innovation is on display at The AI Expo for National Competitiveness. Step into the future with the Special Competitive Studies Project and explore captivating, cutting-edge tech demos and witness insightful discussions on tech policy and global affairs led by a mix of government, academic, and industry leaders in DC. Join us to forge connections, gain perspectives and be part of charting a course toward a future defined by innovation, collaboration, and shared purpose for free.

stablecoin anxiety

Stablecoins might be getting too close to the traditional banking system for the latter’s comfort.

As POLITICO’s Sam Sutton noted in today’s Morning Money newsletter, the Bank Policy Institute, which is chaired by JPMorgan Chase CEO Jamie Dimon, is raising the alarm about a plan that will allow leading stablecoin issuer Circle to lend its reserves to the Fed in exchange for collateral Treasury bonds and interest.

““The ramifications here are difficult to overstate. In essence, anyone in the world wishing to hold the equivalent of reserves at the Federal Reserve will be able to do so by purchasing [Circle’s stablecoin] USDC. Especially in times of crisis, this asset would be highly desirable as a back-door CBDC or account at the Federal Reserve,” writes the BPI’s staff in their post.

As Sam points out, regulators have been concerned for a long time about subjecting stablecoins and other crypto assets to tighter regulations before they enmesh themselves too closely with traditional finance. — Derek Robertson



A message from Special Competitive Studies Project:

always read the comments

What if the next thing text-generating AIs could replace is actually a little bit more of a revival?

In a blog post this morning, George Mason economist and prolific blogger Tyler Cowen posits that ChatGPT could function much in the same way as one particular innovation of the George W. Bush-era web: The blog comment.

“Every time I pose a query [to ChatGPT] it is like putting a blog post out there… and getting a splat of responses right away, and without having to clog up [his blog] with all of my dozens of wonderings every day,” Cowen writes. “...I learn from the responses collectively, and furthermore some of them are very good and also very accurate.”

The framing reflects Cowen’s shrewd understanding of what a product like ChatGPT actually does — namely, to absorb all the information that exists out there on the web, and regurgitate it in a fashion that emulates how it was originally written by a human. Models like ChatGPT “are like giving everyone their own comments-open blog, with hallucinating super-infovores as the readers and immediate response and follow-up when desired,” he adds. (Note that he says “follow-up,” however, not communication, the crucial person-to-person element of the blog that essentially presaged the existence of Twitter.) — Derek Robertson



tweet of the day

New Window

the future in 5 links

Microsoft might be looking for ways to incorporate OpenAI products into Office.

into Office. At CES this year, accessibility technology looked increasingly like the future.

looked increasingly like the future. …As did battery power , in all shapes and forms, for pretty much everything.

, in all shapes and forms, for pretty much everything. Biologists are scheming to fight cancer and the flu with AI.

with AI. Could AI’s inability to forget things actually be a barrier to its development?

Stay in touch with the whole team: Ben Schreckinger ( [email protected] ); Derek Robertson ( [email protected] ); Steve Heuser ( [email protected] ); and Benton Ives ( [email protected] ). Follow us @DigitalFuture on Twitter.

If you’ve had this newsletter forwarded to you, you can sign up and read our mission statement at the links provided.

Ben Schreckinger covers tech, finance and politics for POLITICO; he is an investor in cryptocurrency.



A message from Special Competitive Studies Project: The future is unfolding at The AI Expo for National Competitiveness. Step into the future with the Special Competitive Studies Project and explore captivating, cutting-edge tech demos and witness insightful discussions on tech policy and global affairs led by a mix of government, academic and industry leaders. The AI Expo has over 125 exhibitors on the show floor, over 150 panel speakers, special events and side rooms and networking spaces. Join us on May 7-8 in Washington, D.C. to forge meaningful connections or gain fresh perspectives and be part of charting a course toward a future defined by innovation, collaboration, and shared purpose. The AI Expo is free to attend.

Follow us on Twitter Ben Schreckinger @SchreckReports



Derek Robertson @afternoondelete



Steve Heuser @sfheuser. The emergence in the last week of a particularly effective voice synthesis machine learning model called VALL-E has prompted a new wave of concern over the possibility of deepfake voices made quick and easy — quickfakes, if you will. But VALL-E is more iterative than breakthrough, and the capabilities aren’t so new as you might think. Whether that means you should be more or less worried is up to you.

Voice replication has been a subject of intense research for years, and the results have been good enough to power plenty of startups, like WellSaid, Papercup and Respeecher. The latter is even being used to create authorized voice reproductions of actors like James Earl Jones. Yes: from now on Darth Vader will be AI generated.

VALL-E, posted on GitHub by its creators at Microsoft last week, is a “neural codec language model” that uses a different approach to rendering voices than many before it. Its larger training corpus and some new methods allow it to create “high-quality personalized speech” using just three seconds of audio from a target speaker.

That is to say, all you need is an extremely short clip like the following (all clips from Microsoft’s paper):

https://techcrunch.com/wp-content/uploads/2023/01/in1.wav

https://techcrunch.com/wp-content/uploads/2023/01/in2.wav

To produce a synthetic voice that sounds remarkably similar:

https://techcrunch.com/wp-content/uploads/2023/01/outcome1.wav

https://techcrunch.com/wp-content/uploads/2023/01/outcome2.wav

As you can hear, it maintains tone, timbre, a semblance of accent and even the “acoustic environment,” (for instance, a voice compressed into a cell phone call). I didn’t bother labeling them because you can easily tell which of the above is which. It’s quite impressive!

So impressive, in fact, that this particular model seems to have pierced the hide of the research community and “gone mainstream.” As I got a drink at my local last night, the bartender emphatically described the new AI menace of voice synthesis. That’s how I know I misjudged the zeitgeist.

But if you look back a bit, in as early as 2017 all you needed was a minute of voice to produce a fake version convincing enough that it would pass in casual use. And that was far from the only project.

The improvement we’ve seen in image-generating models like DALL-E 2 and Stable Diffusion, or in language ones like ChatGPT, has been a transformative, qualitative one: A year or two ago this level of detailed, convincing AI-generated content was impossible. The worry (and panic) around these models is understandable and justified.

Contrariwise, the improvement offered by VALL-E is quantitative not qualitative. Bad actors interested in proliferating fake voice content could have done so long ago, just at greater computational cost, not something that is particularly difficult to find these days. State-sponsored actors in particular would have plenty of resources at hand to do the kind of compute jobs necessary to, say, create a fake audio clip of the President saying something damaging on a hot mic.

I chatted with James Betker, an engineer who worked for a while on another text-to-speech system, called Tortoise-TTS.

Betker said that VALL-E is indeed iterative and like other popular models these days gets its strength from its size.

“It’s a large model, like ChatGPT or Stable Diffusion; it has some inherent understanding of how speech is formed by humans. You can then fine-tune Tortoise and other models on specific speakers, and it makes them really, really good. Not ‘kind of sounds like’; good,” he explained.

When you “fine-tune” Stable Diffusion on a particular artist’s work, you’re not retraining the whole enormous model (that takes a lot more power), but you can still vastly improve its capability of replicating that content.

But just because it’s familiar doesn’t mean it should be dismissed, Betker clarified.

“I’m glad it’s getting some traction because i really want people to be talking about this. I actually feel that speech is somewhat sacred, the way our culture thinks about it,” and he actually stopped working on his own model as a result of these concerns. A fake Dali created by DALL-E 2 doesn’t have the same visceral effect for people as hearing something in their own voice, that of a loved one or of someone admired.

VALL-E moves us one step closer to ubiquity, and although it is not the type of model you run on your phone or home computer, that isn’t too far off, Betker speculated. A few years, perhaps, to run something like it yourself; as an example, he sent this clip he’d generated on his own PC using Tortoise-TTS of Samuel L. Jackson, based on audiobook readings of his:

https://techcrunch.com/wp-content/uploads/2023/01/samuel_jackson.mp3

Good, right? And a few years ago you might have been able to accomplish something similar, albeit with greater effort.

This is all just to say that while VALL-E and the three-second quickfake are definitely notable, they’re a single step on a long road researchers have been walking for over a decade.

The threat has existed for years and if anyone cared to replicate your voice, they could easily have done so long ago. That doesn’t make it any less disturbing to think about, and there’s nothing wrong with being creeped out by it. I am too!

But the benefits to malicious actors are dubious. Petty scams that use a passable quickfake based on a wrong number call, for instance, are already super easy because security practices at many companies are already lax. Identity theft doesn’t need to rely on voice replication because there are so many easier paths to money and access.

Meanwhile the benefits are potentially huge — think about people who lose the ability to speak due to an illness or accident. These things happen quickly enough that they don’t have time to record an hour of speech to train a model on (not that this capability is widely available, though it could have been years ago). But with something like VALL-E, all you’d need is a couple clips off someone’s phone of them making a toast at dinner or talking with a friend.

There’s always opportunity for scams and impersonation and all that — although more people are parted with their money and identities via far more prosaic ways, like a simple phone or phishing scam. The potential for this technology is huge, but we should also listen to our collective gut, saying there’s something dangerous here. Just don’t panic — yet.. Machine-learning systems are increasingly worming their way through our everyday lives, challenging our moral and social values and the rules that govern them. These days, virtual assistants threaten the privacy of the home; news recommenders shape the way we understand the world; risk-prediction systems tip social workers on which children to protect from abuse; while data-driven hiring tools also rank your chances of landing a job. However, the ethics of machine learning remains blurry for many.

Searching for articles on the subject for the young engineers attending the Ethics and Information and Communications Technology course at UCLouvain, Belgium, I was particularly struck by the case of Joshua Barbeau, a 33-year-old man who used a website called Project December to create a conversational robot – a chatbot – that would simulate conversation with his deceased fiancée, Jessica.

Conversational robots mimicking dead people

Known as a deadbot, this type of chatbot allowed Barbeau to exchange text messages with an artificial “Jessica”. Despite the ethically controversial nature of the case, I rarely found materials that went beyond the mere factual aspect and analyzed the case through an explicit normative lens: why would it be right or wrong, ethically desirable or reprehensible, to develop a deadbot?

Before we grapple with these questions, let’s put things into context: Project December was created by the games developer Jason Rohrer to enable people to customize chatbots with the personality they wanted to interact with, provided that they paid for it. The project was built drawing on an API of GPT-3, a text-generating language model by the artificial intelligence research company OpenAI. Barbeau’s case opened a rift between Rohrer and OpenAI because the company’s guidelines explicitly forbid GPT-3 to be used for sexual, amorous, self-harm or bullying purposes.

Calling OpenAI’s position as hyper-moralistic and arguing that people like Barbeau were “consenting adults”, Rohrer shut down the GPT-3 version of Project December.

While we may all have intuitions about whether it is right or wrong to develop a machine-learning deadbot, spelling out its implications hardly makes for an easy task. This is why it is important to address the ethical questions raised by the case, step by step.

Is Barbeau’s consent enough to develop Jessica’s deadbot?

Since Jessica was a real (albeit dead) person, Barbeau consenting to the creation of a deadbot mimicking her seems insufficient. Even when they die, people are not mere things with which others can do as they please. This is why our societies consider it wrong to desecrate or to be disrespectful to the memory of the dead. In other words, we have certain moral obligations towards the dead, insofar as death does not necessarily imply that people cease to exist in a morally relevant way.

Likewise, the debate is open as to whether we should protect the dead’s fundamental rights (e.g., privacy and personal data). Developing a deadbot replicating someone’s personality requires great amounts of personal information such as social network data (see what Microsoft or Eternime propose) which have proven to reveal highly sensitive traits.

If we agree that it is unethical to use people’s data without their consent while they are alive, why should it be ethical to do so after their death? In that sense, when developing a deadbot, it seems reasonable to request the consent of the one whose personality is mirrored – in this case, Jessica.

When the imitated person gives the green light

Thus, the second question is: would Jessica’s consent be enough to consider her deadbot’s creation ethical? What if it was degrading to her memory?

The limits of consent are, indeed, a controversial issue. Take as a paradigmatic example the “Rotenburg Cannibal”, who was sentenced to life imprisonment despite the fact that his victim had agreed to be eaten. In this regard, it has been argued that it is unethical to consent to things that can be detrimental to ourselves, be it physically (to sell one’s own vital organs) or abstractly (to alienate one’s own rights).

In what specific terms something might be detrimental to the dead is a particularly complex issue that I will not analyze in full. It is worth noting, however, that even if the dead cannot be harmed or offended in the same way than the living, this does not mean that they are invulnerable to bad actions, nor that these are ethical. The dead can suffer damages to their honour, reputation or dignity (for example, posthumous smear campaigns), and disrespect toward the dead also harms those close to them. Moreover, behaving badly toward the dead leads us to a society that is more unjust and less respectful of people’s dignity overall.

Finally, given the malleability and unpredictability of machine-learning systems, there is a risk that the consent provided by the person mimicked (while alive) does not mean much more than a blank check on its potential paths.

Taking all of this into account, it seems reasonable to conclude if the deadbot’s development or use fails to correspond to what the imitated person has agreed to, their consent should be considered invalid. Moreover, if it clearly and intentionally harms their dignity, even their consent should not be enough to consider it ethical.

Who takes responsibility?

A third issue is whether artificial intelligence systems should aspire to mimic any kind of human behavior (irrespective here of whether this is possible).

This has been a long-standing concern in the field of AI and it is closely linked to the dispute between Rohrer and OpenAI. Should we develop artificial systems capable of, for example, caring for others or making political decisions? It seems that there is something in these skills that make humans different from other animals and from machines. Hence, it is important to note instrumentalizing AI toward techno-solutionist ends such as replacing loved ones may lead to a devaluation of what characterizes us as human beings.

The fourth ethical question is who bears responsibility for the outcomes of a deadbot – especially in the case of harmful effects.

Imagine that Jessica’s deadbot autonomously learned to perform in a way that demeaned her memory or irreversibly damaged Barbeau’s mental health. Who would take responsibility? AI experts answer this slippery question through two main approaches: first, responsibility falls upon those involved in the design and development of the system, as long as they do so according to their particular interests and worldviews; second, machine-learning systems are context-dependent, so the moral responsibilities of their outputs should be distributed among all the agents interacting with them.

I place myself closer to the first position. In this case, as there is an explicit co-creation of the deadbot that involves OpenAI, Jason Rohrer and Joshua Barbeau, I consider it logical to analyse the level of responsibility of each party.

First, it would be hard to make OpenAI responsible after they explicitly forbade using their system for sexual, amorous, self-harm or bullying purposes.

It seems reasonable to attribute a significant level of moral responsibility to Rohrer because he: (a) explicitly designed the system that made it possible to create the deadbot; (b) did it without anticipating measures to avoid potential adverse outcomes; (c) was aware that it was failing to comply with OpenAI’s guidelines; and (d) profited from it.

And because Barbeau customized the deadbot drawing on particular features of Jessica, it seems legitimate to hold him co-responsible in the event that it degraded her memory.

Ethical, under certain conditions

So, coming back to our first, general question of whether it is ethical to develop a machine-learning deadbot, we could give an affirmative answer on the condition that:

both the person mimicked and the one customizing and interacting with it have given their free consent to as detailed a description as possible of the design, development , and uses of the system;

developments and uses that do not stick to what the imitated person consented to or that go against their dignity are forbidden;

the people involved in its development and those who profit from it take responsibility for its potential negative outcomes. Both retroactively, to account for events that have happened, and prospectively, to actively prevent them to happen in the future.

This case exemplifies why the ethics of machine learning matters. It also illustrates why it is essential to open a public debate that can better inform citizens and help us develop policy measures to make AI systems more open, socially fair, and compliant with fundamental rights.

This article by Sara Suárez-Gonzalo, Postdoctoral Researcher, UOC – Universitat Oberta de Catalunya is republished from The Conversation under a Creative Commons license. Read the original article.. Deepfake technology — software that allows people to swap faces, voices and other characteristics to create digital forgeries — has been used in recent years to make a synthetic substitute of Elon Musk that shilled a cryptocurrency scam, to digitally “undress” more than 100,000 women on Telegram and to steal millions of dollars from companies by mimicking their executives’ voices on the phone.

In most of the world, the authorities can’t do much about it. Even as the software grows more sophisticated and accessible, few laws exist to manage its spread.

China hopes to be the exception. This month, the country adopted expansive rules requiring that manipulated material have the subject’s consent and bear digital signatures or watermarks, and that deepfake service providers offer ways to “refute rumors.”

But China faces the same hurdles that have stymied other efforts to govern deepfakes: The worst abusers of the technology tend to be the hardest to catch, operating anonymously, adapting quickly and sharing their synthetic creations through borderless online platforms. China’s move has also highlighted another reason that few countries have adopted rules: Many people worry that the government could use the rules to curtail free speech.. Detailed tactical plans for imminent police raids, confidential police reports with descriptions of alleged crimes and suspects, and a forensic extraction report detailing the contents of a suspect’s phone. These are some of the files in a huge cache of data taken from the internal servers of ODIN Intelligence, a tech company that provides apps and services to police departments, following a hack and defacement of its website over the weekend.

The group behind the breach said in a message left on ODIN’s website that it hacked the company after its founder and chief executive Erik McCauley dismissed a report by Wired, which discovered the company’s flagship app SweepWizard, used by police to coordinate and plan multiagency raids, was insecure and spilling sensitive data about upcoming police operations to the open web.

The hackers also published the company’s Amazon Web Services private keys for accessing its cloud-stored data and claimed to have “shredded” the company’s data and backups but not before exfiltrating gigabytes of data from ODIN’s systems.

ODIN develops and provides apps, like SweepWizard, to police departments across the United States. The company also builds technologies that allow authorities to remotely monitor convicted sex offenders. But ODIN also drew criticism last year for offering authorities a facial recognition system for identifying homeless people and using degrading language in its marketing.

ODIN’s McCauley did not respond to several emails requesting comment prior to publication but confirmed the hack in a data breach disclosure filed with the California attorney general’s office.

The breach not only exposes vast amounts of ODIN’s own internal data but also gigabytes of confidential law enforcement data uploaded by ODIN’s police department customers. The breach raises questions about ODIN’s cybersecurity but also the security and privacy of the thousands of people — including victims of crime and suspects not charged with any offense — whose personal information was exposed.

The cache of hacked ODIN data was provided to DDoSecrets, a nonprofit transparency collective that indexes leaked datasets in the public interest, such as caches from police departments, government agencies, law firms and militia groups. DDoSecrets co-founder Emma Best told TechCrunch that the collective has limited the distribution of the cache to journalists and researchers given the vast amount of personally identifiable data in the ODIN cache.

Little is known about the hack or the intruders responsible for the breach. Best told TechCrunch that the source of the breach is a group called “All Cyber-Cops Are Bastards,” a phrase it referenced in the defacement message.

TechCrunch reviewed the data, which not only includes the company’s source code and internal database but also thousands of police files. None of the data appears encrypted.

The data included dozens of folders with full tactical plans of upcoming raids, alongside suspect mugshots, their fingerprints and biometric descriptions and other personal information, including intelligence on individuals who might be present at the time of the raid, like children, cohabitants and roommates, some of whom are described as having “no crim[inal] history.” Many of the documents were labeled as “confidential law enforcement only” and “controlled document” not for disclosure outside of the police department.

Some of the files were labeled as test documents and used fake officer names like “Superman” and “Captain America.” But ODIN also used real-world identities, like Hollywood actors, who are unlikely to have consented to their names being used. One document titled “Fresno House Search” bore no markings to suggest the document was a test of ODIN’s front-facing systems but stated the raid’s objective was to “find a house to live in.”

The leaked cache of ODIN data also contained its system for monitoring sex offenders, which allows police and parole officers to register, supervise and monitor convicted criminals. The cache contained more than a thousand documents relating to convicted sex offenders who are required to register with the state of California, including their names, home addresses (if not incarcerated) and other personal information.

The data also contains a large amount of personal information about individuals, including the surveillance techniques that police use to identify or track them. TechCrunch found several screenshots showing people’s faces matched against a facial recognition engine called AFR Engine, a company that provides face-matching technology to police departments. One photo appears to show an officer forcibly holding a person’s head in front of another officer’s phone camera.

Other files show police using automatic license plate readers, known as ANPR, which can identify where a suspect drove in recent days. Another document contained the full contents — including text messages and photos — of a convicted offender’s phone, whose contents were extracted by a forensic extraction tool during a compliance check while the offender was on probation. One folder contained audio recordings of police interactions, some where officers are heard using force.

TechCrunch contacted several U.S. police departments whose files were found in the stolen data. None responded to our requests for comment.

ODIN’s website, which went offline a short time after it was defaced, remains inaccessible as of Thursday.

If you know more about the ODIN Intelligence breach, get in touch with the security desk on Signal and WhatsApp at +1 646-755-8849 or zack.whittaker@techcrunch.com by email.. Two artificial intelligence (AI) programs -- including ChatGPT -- have passed the U.S. Medical Licensing Examination (USMLE), according to two recent papers.

The papers highlighted different approaches to using large language models to take the USMLE, which is comprised of three exams: Step 1, Step 2 CK, and Step 3.

ChatGPT is an artificial intelligence (AI) search tool that mimics long-form writing based on prompts from human users. It was developed by OpenAI, and became popular after several social media posts showed potential uses for the tool in clinical practice, often with mixed results.

The first paper, published on medRxiv in December, investigated ChatGPT's performance on the USMLE without any special training or reinforcement prior to the exams. According to Victor Tseng, MD, of Ansible Health in Mountain View, California, and colleagues, the results showed "new and surprising evidence" that this AI tool was up to the challenge.

Tseng and team noted that ChatGPT was able to perform at >50% accuracy across all of the exams, and even achieved 60% in most of their analyses. While the USMLE passing threshold does vary between years, the authors said that passing is approximately 60% most years.

"ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement," they wrote, noting that the tool was able to demonstrate "a high level of concordance and insight in its explanations."

"These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making," they concluded.

The second paper, published on arXiv, also in December, evaluated the performance of another large language model, Flan-PaLM, on the USMLE. The key difference between the two models was that this model was heavily modified to prepare for the exams, using a collection of medical question-answering databases called the MultiMedQA, explained Vivek Natarajan, an AI researcher, and colleagues.

Flan-PaLM achieved 67.6% accuracy in answering the USMLE questions, which was about 17 percentage points higher than the previous best performance conducted using PubMed GPT.

Natarajan and team concluded that large language models "present a significant opportunity to rethink the development of medical AI and make it easier, safer and more equitable to use."

ChatGPT, along with other AI programs, have been showing up as the subject -- and sometimes as the co-author -- of new research papers focused on testing the technology's usefulness in medicine.

Of course, healthcare professionals have also expressed concerns over these developments, especially when ChatGPT is being listed as an author on research papers. A recent article from Nature highlighted the uneasiness from would-be colleagues and co-authors of the emerging technology.

One objection to the use of AI programs in research was based on whether they can be truly capable of making meaningful scholarly contributions to a paper, while another objection emphasized that AI tools can't consent to be a co-author in the first place.

The editor of one of the papers that listed ChatGPT as an author said it was an error that would be corrected, according to the Nature article. Still, researchers have published several papers now touting these AI programs as useful tools in medical education, research, and even clinical decision making.

Natarajan and colleagues concluded in their paper that large language models could become a beneficial tool in medicine, but their first hope was that their findings would "spark further conversations and collaborations between patients, consumers, AI researchers, clinicians, social scientists, ethicists, policymakers and other interested people in order to responsibly translate these early research findings to improve healthcare."

Michael DePeau-Wilson is a reporter on MedPage Today’s enterprise & investigative team. He covers psychiatry, long covid, and infectious diseases, among other relevant U.S. clinical news. Follow

Primary Source medRxiv Source Reference: Kung TH, et al "Performance of ChatGPT on USMLE: potential for AI-assisted medical education using large language models" medRxiv 2022; DOI: 10.1101/2022.12.19.22283643. Secondary Source arXiv Source Reference: Singhal K, et al "Large language models encode clinical knowledge" arXiv 2022; DOI: 10.48550/arXiv.2212.13138.

Please enable JavaScript to view the comments powered by Disqus.. CALIFORNIA: The viral ChatGPT chatbot may be smart enough to generate answers to pass prestigious graduate-level examinations in the United States – although not with high marks – but it has raised fears that students will outsource their learning to artificial intelligence.

Launched about two months ago, the chatbot developed by American AI research firm OpenAI has been causing a stir. It can generate text that closely mimics human-like conversation based on simple user prompts.

The results have been so impressive that some educators have warned that it could lead to cheating and even change traditional classroom teaching methods.

Now, a new battle is emerging over whether ChatGPT will be a powerful learning tool, or an aid that prevents students from learning how to write or even think for themselves.

BAN CHATGPT, OR INCORPORATE IT INTO LEARNING?

While some educators have banned the chatbot from their classrooms, others see the benefits of incorporating AI technology into their teaching.

“I think we're looking at something like when Google became this amazing search engine that appeared on the landscape. It sort of changed how we engaged with the Internet,” said Dr Victor Lee, associate professor at Stanford University’s Graduate School of Education.

ChatGPT's ability to crank out seemingly well-researched, well-written essays in seconds has seen plenty of reports of students of all ages claiming the chatbot's writing as their own.

The reaction from some corners of the education sector has been swift.

Public school systems in New York City, Los Angeles and Seattle are among those in the US that have already banned ChatGPT on their networks or devices.

A new app developed by a Princeton University student that attempts to detect ChatGPT was downloaded 30,000 times in the first week.. CNN —

ChatGPT is smart enough to pass prestigious graduate-level exams – though not with particularly high marks.

The powerful new AI chatbot tool recently passed law exams in four courses at the University of Minnesota and another exam at University of Pennsylvania’s Wharton School of Business, according to professors at the schools.

To test how well ChatGPT could generate answers on exams for the four courses, professors at the University of Minnesota Law School recently graded the tests blindly. After completing 95 multiple choice questions and 12 essay questions, the bot performed on average at the level of a C+ student, achieving a low but passing grade in all four courses.

ChatGPT fared better during a business management course exam at Wharton, where it earned a B to B- grade. In a paper detailing the performance, Christian Terwiesch, a Wharton business professor, said ChatGPT did “an amazing job” at answering basic operations management and process-analysis questions but struggled with more advanced prompts and made “surprising mistakes” with basic math.

“These mistakes can be massive in magnitude,” he wrote.

The test results come as a growing number of schools and teachers express concerns about the immediate impact of ChatGPT on students and their ability to cheat on assignments. Some educators are now moving with remarkable speed to rethink their assignments in response to ChatGPT, even as it remains unclear how widespread use is of the tool among students and how harmful it could really be to learning.

Video Ad Feedback Congressman gives speech written by AI 02:49 - Source: CNN

Since it was made available in late November, ChatGPT has been used to generate original essays, stories and song lyrics in response to user prompts. It has drafted research paper abstracts that fooled some scientists. Some CEOs have even used it to write emails or do accounting work.

ChatGPT is trained on vast amounts of online data in order to generate responses to user prompts. While it has gained traction among users, it has also raised some concerns, including about inaccuracies and its potential to perpetuate biases and spread misinformation.

Jon Choi, one of the University of Minnesota law professors, told CNN the goal of the tests was to explore ChatGPT’s potential to assist lawyers in their practice and to help students in exams, whether or not it’s permitted by their professors, because the questions often mimic the writing lawyers do in real life.

“ChatGPT struggled with the most classic components of law school exams, such as spotting potential legal issues and deep analysis applying legal rules to the facts of a case,” Choi said. “But ChatGPT could be very helpful at producing a first draft that a student could then refine.”

He argues human-AI collaboration is the most promising use case for ChatGPT and similar technology.

“My strong hunch is that AI assistants will become standard tools for lawyers in the near future, and law schools should prepare their students for that eventuality,” he said. “Of course, if law professors want to continue to test simple recall of legal rules and doctrines, they’ll need to put restrictions in place like banning the internet during exams to enforce that.”

Likewise, Wharton’s Terwiesch found the chatbot was “remarkably good” at modifying its answers in response to human hints, such as reworking answers after pointing out an error, suggesting the potential for people to work together with AI.

Video Ad Feedback Scott Galloway on the 'scarier part' of AI tools like ChatGPT 02:07 - Source: CNN

In the short-term, however, discomfort remains with whether and how students should use ChatGPT. Public schools in New York City and Seattle, for example, have already banned students and teachers from using ChatGPT on the district’s networks and devices.

Considering ChatGPT performed above average on his exam, Terwiesch told CNN he agrees restrictions should be put in place for students while they’re taking tests.

“Bans are needed,” he said. “After all, when you give a medical doctor a degree, you want them to know medicine, not how to use a bot. The same holds for other skill certification, including law and business.”

But Terwiesch believes this technology still ultimately has a place in the classroom. “If all we end up with is the same educational system as before, we have wasted an amazing opportunity that comes with ChatGPT,” he said.. Medical

AD

A study found that ChatGPT performed ”at or near the passing threshold" for all three parts of the United States Medical Licensing Exam, which is taken by doctors to receive their licensure. The bot was given dense multiple choice and open-ended questions, and achieved 60% accuracy across most of the exams, which is within the passing range.

“As AI becomes increasingly proficient, it will soon become ubiquitous, transforming clinical medicine across all healthcare sectors,” the study stated.

Law

AD

A team of University of Minnesota Law School professors found that ChatGPT would underperform the average law school student, but could skirt by with a passing grade on final exams in four courses. On its own, the bot would be a pretty mediocre law student, but it could assist students with their assignments, the researchers found.

Meanwhile, Bloomberg journalist Matthew S. Schwartz fed ChatGPT a law school take-home essay prompt, and it produced a “solid response.”

In another study, ChatGPT didn’t pass a multi-state bar exam practice test, scoring a 50.3%. But it did pass the Evidence and Torts sections, with researchers noting that it outperformed past AI models that have taken the bar.

AD

Business

In a study titled “Would Chat GPT3 Get a Wharton MBA?”, experts at the University of Pennsylvania’s business school found that ChatGPT would score a B to B- grade on a final exam for an MBA core course, Operations

Management.

The study found it did an “amazing job” at the basics of the subject, but would sometimes make surprising mistakes in sixth grade-level math calculations.. A judge in Colombia has caused a stir by admitting he used the artificial intelligence tool ChatGPT when deciding whether an autistic child’s insurance should cover all of the costs of his medical treatment. He also used precedent from previous rulings to support his decision.

Juan Manuel Padilla, a judge in the Caribbean city of Cartagena, concluded that the entirety of the child’s medical expenses and transport costs should be paid by his medical plan as his parents could not afford them.

While the judgment itself did not cause much fuss, the inclusion of Padilla’s conversations with ChatGPT in the ruling has been more contentious.

Among Padilla’s inquiries with the chatbot, the legal documents show Padilla asked ChatGPT the precise legal matter at hand: “Is an autistic minor exonerated from paying fees for their therapies?”

ChatGPT’s response corresponded with the judge’s final decision: “Yes, this is correct. According to the regulations in Colombia, minors diagnosed with autism are exempt from paying fees for their therapies.”

The case has raised a discussion over the use of AI in law and has been criticised by some of Padilla’s peers.

ChatGPT scours text across the internet to generate informed responses but has been shown to provide different answers to the same question. It also fabricates information on occasion to make inventive and compelling lies.

The nascent platform has caused alarm in recent weeks, including in schools, where teachers fear OpenAI’s platform could be used by students for plagiarism.

Padilla defended his use of the technology, suggesting it could make Colombia’s bloated legal system more efficient. The judge also used precedent from previous rulings to support his decision.

Padilla told Blu Radio on Tuesday that ChatGPT and other such programs could be useful to “facilitate the drafting of texts” but “not with the aim of replacing” judges.

Padilla also insisted that “by asking questions to the application, we do not stop being judges, thinking beings”.

The judge argued that ChatGPT performs services previously provided by a secretary and did so “in an organised, simple and structured manner” that could “improve response times” in the justice system.

Prof Juan David Gutierrez of Rosario University was among those to express incredulity at the judge’s admission.

He called for urgent “digital literacy” training for judges.

Colombia approved a law in 2022 that suggests that public lawyers should use technologies where possible to make their work more efficient.

Octavio Tejeiro, a judge in Colombia’s supreme court, said AI caused moral panic in law as people feared robots would replace judges, but he predicted the tool would probably soon become accepted and commonplace.

“The justice system should make the most of technology as a tool but always while following ethics and taking into account that the administrator of justice is ultimately a human being,” Tejeiro said. “It must be seen as an instrument that serves the judge to improve his judgment. We cannot allow the tool to become more important than the person.”

Tejeiro told the Guardian he had not used ChatGPT but would consider using it in future.

The chatbot itself was more apprehensive about its new role in the justice system.

“Judges should not use ChatGPT when ruling on legal cases … It is not a substitute for the knowledge, expertise and judgment of a human judge,” it responded to a question from the Guardian.

“Journalists should exercise caution when using quotes generated by ChatGPT in their articles,” the bot added.. Creating a video game demands hard, repetitive work. How could it not? Developers are in the business of building world, so it’s easy to understand why the games industry would be excited about generative AI. With computers doing the boring stuff, a small team could whip up a map the size of San Andreas. Crunch becomes a thing of the past; games release in a finished state. A new age beckons.

There are, at the very least, two interrelated problems with this narrative. First, there’s the logic of the hype itself—reminiscent of the frenzied gold rush over crypto/Web3/the metaverse—that, consciously or not, seems to consider automating artists’ jobs a form of progress.

Second, there’s the gap between these pronouncements and reality. Back in November, when DALL-E was seemingly everywhere, venture capital firm Andreessen Horowitz posted a a long analysis on their website touting a “generative AI revolution in games” that would do everything from shorten development time to change the kinds of titles being made. The following month, Andreessen partner Jonathan Lai posted a Twitter thread expounding on a “Cyberpunk where much of the world/text was generated, enabling devs to shift from asset production to higher-order tasks like storytelling and innovation” and theorizing that AI could enable “good + fast + affordable” game-making. Eventually, Lai’s mentions filled with so many irritated replies that he posted a second thread acknowledging “there are definitely lots of challenges to be solved.”

“I have seen some, frankly, ludicrous claims about stuff that’s supposedly just around the corner,” says Patrick Mills, the acting franchise content strategy lead at CD Projekt Red, the developer of Cyberpunk 2077. “I saw people suggesting that AI would be able to build out Night City, for example. I think we’re a ways off from that.”

Even those advocating for generative AI in video games think a lot of the excited talk about machine learning in the industry is getting out of hand. It’s “ridiculous,” says Julian Togelius, codirector of the NYU Game Innovation Lab, who has authored dozens of papers on the topic. “Sometimes it feels like the worst kind of crypto bros left the crypto ship as it was sinking, and then they came over here and were like, ‘Generative AI: Start the hype machine.’”

It’s not that generative AI can’t or shouldn’t be used in game development, Togelius explains. It’s that people aren’t being realistic about what it could do. Sure, AI could design some generic weapons or write some dialog, but compared to text or image generation, level design is fiendish. You can forgive generators that produce a face with wonky ears or some lines of gibberish text. But a broken game level, no matter how magical it looks, is useless. “It is bullshit,” he says, “You need to throw it out or fix it manually.”

Basically—and Togelius has had this conversation with multiple developers—no one wants level generators that work less than 100 percent of the time. They render games unplayable, destroying whole titles. “That’s why it’s so hard to take generative AI that is so hard to control and just put it in there,” he says.. Si vous avez déjà flâné aux alentours de la cathédrale Notre-Dame de Reims (51), vous avez peut-être été analysé par une intelligence artificielle. L’outil « Savari », conçu par le géant français Thalès (1), a été intégré aux caméras de surveillance de l’agglomération et utilisé par la police municipale, pour une expérimentation d’un an. Le but : repérer automatiquement des regroupements ou des intrusions, et identifier des armes ou des véhicules. Problème, ni les habitants et ni les élus de l’opposition n’ont été informés.

Une révélation qui arrive au moment où le Sénat vient d’adopter, mardi 24 janvier 2023, l’article 7 du projet de loi des Jeux Olympiques. Il autorise ces expérimentations de caméras dotées d’algorithmes à compter de cette année en vue des JO de 2024 à Paris.

En novembre dernier, Amnesty International avait alerté sur « les risques de dérives dangereuses » pour les libertés individuelles que comporte cette loi. À la différence de la vidéo-surveillance classique, ce type d’outil peut conduire à « un traitement massif de données à caractère personnel, y compris parfois de données sensibles », explique la CNIL dans un rapport publié en juillet 2022.

Reims n’a pas attendu que les parlementaires l’autorisent dans la capitale pour faire ses propres tests et profiter du vide juridique. C’est Arnaud Robinet, maire de la ville depuis 2014, affilié au mouvement Horizons d’Édouard Philippe, qui aurait décidé de cette collaboration. Contacté par StreetPress, l’édile de droite a refusé de répondre à nos questions.

Algorithme et « deep learning »

Savari est une « vidéosurveillance intelligente, clé de la sécurité urbaine des Smart Cities ». C’est ainsi que Thalès présente sa solution sur le site du Salon des maires, un séminaire annuel pendant lequel les entreprises tiennent des stands pour draguer les élus.

Elle utilise des « algorithmes vidéos intelligents » et du « deep learning » pour « surveiller, superviser et analyser automatiquement » des situations telles que des regroupements et des intrusions. Elle peut aussi « détecter, identifier et classifier » des armes, des vélos ou des véhicules, assure le fleuron français. À l’aide de la lecture automatique des plaques d’immatriculation (LAPI), Savari permet également de faire du contrôle routier et de mettre des amendes à ceux qui ne paient pas leur stationnement.

À LIRE AUSSI : À Toulouse, le LAPI est un enfer d’amendes pour les personnes en situation de handicap

Une expérimentation faite dans le dos des Rémois

« Ce qui me choque, c’est que ça a été fait dans une grande opacité. Ce n’est pas normal que les citoyens ne soient pas informés », s’indigne Léo Tyburce, élu Europe-Écologie-Les-Verts à Reims, informé de l’expérimentation de Thalès dans sa ville par StreetPress. Lors du conseil municipal du 16 novembre 2021, l’écolo avait interpellé le maire Arnaud Robinet au sujet de l’utilisation d’une intelligence artificielle dans les caméras de la commune évoquée lors d’une précédente réunion publique. L’édile de droite avait renvoyé la balle à son Monsieur sécurité, Xavier Albertini.

« Dans le cadre de l’évolution de l’organisation de la police municipale, il a été souhaité de mettre en place une étude et le développement avec une entreprise nationale d’un logiciel qui n’est pas de la reconnaissance faciale (…) mais c’est une capacité de reconnaître un certain nombre d’éléments et en particulier, dans la limite de la réglementation, de reconnaître n’importe quel véhicule qui se trouve pris par le champ des caméras », avait déclaré l’adjoint. La vidéo du conseil municipal est toujours en ligne. Il admet donc qu’une étude liée aux caméras de surveillance est en cours avec une entreprise nationale. Sans dévoiler de laquelle il pourrait s’agir, ni dévoiler l’intégralité des usages de cette technologie.

À l’époque, le conseiller municipal de l’opposition Léo Tyburce est rassuré par cette réponse. « On m’avait répondu, ne vous inquiétez pas tout roule, c’est juste pour repérer les véhicules… » Aujourd’hui, il se sent trompé :

« Finalement, on voit que ça permet de repérer des groupements de personnes, d’installer une sorte de surveillance urbaine. J’aurais aimé qu’il y ait un débat public autour de cette question. Il y a un manque de transparence terrible. »

Le Rémois Antoine (2), membre d’Action non-violente COP21, n’était pas au courant de l’utilisation d’une intelligence artificielle par les forces de l’ordre. « Ça m’intéresse de savoir que l’outil repère les rassemblements parce qu’on fait des actions non déclarées », pointe-t-il. Le militant pour l’environnement connaît pourtant bien les caméras de surveillance de sa ville. Avec son groupe, ils s’attaquent aux panneaux publicitaires rétroéclairés. « On se rend bien compte une fois qu’on lève la tête qu’on est surveillés de tous les côtés. » Alors, pour ne pas se faire prendre, ils ont établi une cartographie des zones surveillées :

« On en avait repéré une soixantaine sur à peine un km carré en centre-ville en 2021. »

Reims is watching you

Ce n’est pas un hasard si Thalès a réussi à vendre son outil à la « cité des Sacres », particulièrement friande de caméras de surveillance. Arnaud Robinet en a fait un argument de campagne. Depuis 2014, pas moins de 218 caméras supplémentaires ont été installées dans l’agglomération rémoise, qui n’en comptait que 36. Le maire adepte de la rengaine sécuritaire a même inauguré en 2016 un Centre de Surveillance Urbain (CSU) au sein de l’hôtel de police où 120 agents observent les images 24h/24 et 7 jours sur 7.

À LIRE AUSSI : Dans tous les commissariats de France, on utilise la reconnaissance faciale

La multiplication des caméras et donc des vidéos à analyser est un des arguments avancés par Xavier Albertini pour justifier l’acquisition d’un algorithme. Au conseil municipal, après avoir assuré qu’il ne s’agissait que de surveiller des voitures, il ajoute : « D’un point de vue technique, ce sont 56.000 types de véhicules différents (…) qui sont ainsi rentrés dans une base de données et quand y a une réquisition pour rechercher un véhicule, il y a une quasi-instantanéité de l’ensemble des caméras. »

Pour les industriels du secteur, la multiplication des caméras de surveillance rend nécessaire l’intelligence artificielle pour aider les humains, devenus trop peu nombreux. Un « bluff technologique », selon La Quadrature du Net, qui défend les droits et les libertés sur Internet. « Il y a un énorme marché privé qui est en train de se lancer et qui est en partie financé par des fonds publics », analyse Martin Drago, spécialiste de ces questions au sein de l’association.

Sept millions d’euros pour filmer ses habitants

Arnaud Robinet a dû mettre le prix pour acquérir le tout nouveau produit du géant de l’aérospatial. Dans le budget de la ville, pour son second mandat qui a commencé en 2020, pas moins de sept millions d’euros sont dédiés à des investissements pour la vidéo-surveillance et l’équipement de la police municipale. Un logiciel dit « d’aide à la relecture », qui doit aider les policiers à fournir des éléments probants à l’autorité judiciaire lors des réquisitions, a été financé par ce budget. Mais le montant exact n’est pas connu. Ni Thalès, ni la mairie de Reims n’ont souhaité nous le fournir.

Reims vient ainsi s’ajouter à une longue liste de villes françaises qui ont passé des contrats avec des entreprises leur proposant d’expérimenter la « vidéosurveillance algorithmique ». Sur son site Tecnopolice.fr, La Quadrature du Net en a répertorié dans une dizaine de villes. À Nîmes (30) ou Moirans (38), l’outil de l’entreprise israélienne Briefcam analyse les faits et gestes des habitants. Marseille (13) collabore depuis 2018 avec l’entreprise locale SNEF. En 2016, Toulouse (31) a passé un contrat avec la boîte américaine IBM. Coût pour la ville du Sud : 47.350 euros.

Ces expérimentations coûtent des milliers d’euros aux collectivités. Leur utilité reste pourtant à démontrer. Dans un rapport publié en 2020, la Cour des comptes a jugé qu’« aucune corrélation globale n’a été relevée entre l’existence de dispositifs de vidéoprotection et le niveau de la délinquance commise sur la voie publique, ou encore les taux d’élucidation ».

Du No man’s land juridique à la légalisation ?

D’après le spécialiste Martin Drago, qui gère la campagne Technopolice, le déploiement de ces outils a été facilité par le flou juridique qui les entoure. « Dans le code pénal, il y a des articles précis sur les caméras de vidéosurveillance, mais rien sur la vidéosurveillance algorithmique qui porte pourtant de nouvelles atteintes aux libertés individuelles. » Il ajoute :

« Les industriels de la surveillance profitent de ce no man’s land juridique pour faire des expérimentations. »

(1) Contacté par StreetPress, Thalès n’a pas souhaité répondre à nos questions.

(2) Le prénom a été modifié.. Según un estudio realizado en 2014 por la empresa Investigación y Control S.A (Incosa) a petición de AENA, la vida útil de la torre de control del Aeropuerto de Menorca finalizó en 2019 con una prórroga in extremis que marca el año 2024 como límite para su recambio o renovación. El aeropuerto en una isla es un pilar fundamental para la comunicación y la economía, por lo que en 2010 se anunció en el Plan Rector de Aeropuertos la construcción de una nueva torre de control. Sin embargo, la obra nunca fue ejecutada y la torre -que es la más vieja de España, inaugurada en 1968- continúa funcionando.

En 2019, a poco de cumplirse el plazo final indicado por el informe, AENA y ENAIRE -los dos entes públicos a cargo de la gestión de los aeropuertos de todo el país- anunciaron sucesivos proyectos para reemplazar la actual torre de control por un moderno proyecto de “torre remota”. Se trata de un sistema de “visión artificial autónoma” que evitaría la construcción de un edificio de control tradicional y la sustituiría por un sofisticado equipo de cámaras e inteligencia artificial, con el que monitorizar la entrada y salida de aviones de forma virtual.

El sistema se probó por primera vez en un aeropuerto europeo en el año 2002, concretamente en Alemania, “aunque siempre en aeropuertos con menor tráfico aéreo que el de Menorca”, según señalan algunos controladores aéreos.

“Esto no puede hacerse de un día para el otro”

Desde Comisiones Obreras, sindicato que integra a controladores aéreos y personal aeroportuario en Menorca, denuncian la falta de respuestas sobre cómo se llevará a cabo concretamente el proyecto: “Falta un año para que se cumpla el plazo que indica el informe y AENA sigue sin dar respuestas. No sabemos con qué personal vamos a cubrir las vacantes de la torre remota, porque incluso para la vieja torre nos falta gente desde 2008”.

“Tampoco hay un plan de formación concreto para el uso de esta tecnología. En 2009 se implementó en Suecia y los controladores necesitaron siete años para formarse, en un aeropuerto con 10 movimientos al día mientras que el de Menorca tiene 23 y subiendo. No nos oponemos al proyecto, pero esto no puede hacerse de un día para el otro”, destaca Ramón Carreras, miembro del sindicato, en diálogo con elDiario.es.

Falta un año para que se cumpla el plazo y AENA sigue sin dar respuestas. En 2009 se implementó en Suecia y los controladores necesitaron siete años para formarse, en un aeropuerto con 10 movimientos al día, mientras que el de Menorca tiene 23 y subiendo Ramón Carreras — CCOO

Para despejar las incógnitas sobre este proyecto, elDiario.es ha hablado con AENA, quienes reconocen que la implantación de la torre remota “es un proceso complejo en el que se va avanzando poco a poco”. “Ya se dispone de una nueva dependencia de control dotada de sistemas de navegación aérea y visualización mediante cámaras, encontrándonos actualmente en fase de pruebas de los sistemas. Una vez finalizada la fase de pruebas, se procederá a la formación del personal en la nueva dependencia hasta el inicio de servicio, que requerirá una aprobación previa de la AESA (Agencia Estatal de Seguridad Aérea) para el inicio de servicio”, señalan.

Asimismo, desde AENA añaden que, de momento, “al no haber finalizado el proceso de pruebas y formación de los controladores, AESA no ha emitido ningún informe al respecto. Probablemente en los meses venideros habrá novedades referentes a este asunto”.

Es un proceso complejo en el que se va avanzando poco a poco. Ya se dispone de una nueva dependencia de control dotada de sistemas de navegación aérea y visualización mediante cámaras, encontrándonos actualmente en fase de pruebas de los sistemas AENA

“Nos pagan para ordenar el tráfico, no para jugar con un joystick”

Por su parte, María José de la Quintana, jefa de instructores del aeropuerto, señala que existen otros problemas a los que no se ha brindado solución todavía. “Tenemos el tráfico aéreo del Hospital Mateu Orfila en uno de los extremos de la pista y detrás del aeropuerto tenemos la pista del aeroclub de donde salen y entran avionetas. Las cámaras que pretenden instalar ofrecen una imagen comprimida del tráfico e incluso, durante un proceso de prueba, una de las cámaras dejó de funcionar de repente por un problema de refrigeración”.

“Por otro lado, se insistió en que estas cámaras no fueran instaladas a más de 250 metros del eje de pista y están instaladas a más de 400 metros. A nosotros nos pagan para ordenar el tráfico aéreo, no para jugar con un joystick”, enfatiza.

A nosotros nos pagan para ordenar el tráfico aéreo, no para jugar con un joystick María José de la Quintana — Jefa de instructores del aeropuerto de Menorca

A la falta de planificación sobre un elemento tan central para la aeronavegabilidad como la torre de control, se suma un dato no menor: durante la temporada de verano de 2022, el aeropuerto de Menorca vio pasar por sus instalaciones casi 4 millones de personas entre los meses de abril y octubre, según datos del Institut d'Estadística de les Illes Balears (Ibestat).

En ese mismo sentido va un informe elaborado por Comisiones Obreras, que señala que el de Menorca y el de Eivissa son los aeropuertos con mayor estacionalidad del mundo, esto es, que concentran el 90% de su actividad durante los meses de “temporada alta”. Este flujo turístico de gran envergadura requiere de una infraestructura adecuada, ya que se prevé que el volumen de vuelos hacia las islas vaya in crescendo de cara al verano de este año.

El problema de la electricidad

Por su parte, el portavoz de Més x Menorca en el Consell Insular, Josep Juaneda, señala a elDiario.es que “el proyecto remoto se trata de una instalación digital” y que “si hubiera cualquier problema eléctrico o de energía Menorca quedaría completamente aislada”. “Quienes vivimos en la isla dependemos del aeropuerto para muchas cosas más que para ir y volver de vacaciones. No podemos sujetar nuestra comunicación con el exterior a la puesta en marcha de un experimento para el que, por otro lado, no hay personal formado adecuadamente y muy difícilmente lo haya en el plazo de un año”, comenta.

El proyecto remoto se trata de una instalación digital y si hubiera cualquier problema eléctrico o de energía Menorca quedaría completamente aislada Josep Juaneda — Portavoz de Més x Menorca en el Consell Insular

“Vamos a seguir exigiendo que AENA proponga un proyecto, ejecute una obra y construya una torre. No nos oponemos a que también exista la torre remota, pero creemos que al ser un proyecto experimental, debería ser probado antes en lugares menos vulnerables”, concluye.. Colombia acaba de marcar la pauta. Un juzgado de Cartagena resolvió una acción de tutela, lo que en el Perú denominaríamos como acción de amparo, en atención a la Ley 2213 que incorpora las TIC (tecnologías de la información y las comunicaciones) en el proceso judicial.

Cuando el juez resolvió el caso fue muy enfático al afirmar que el uso de esta inteligencia artificial se limitó a optimizar el tiempo de redacción de la sentencia, con el objetivo de agilizar la emisión del fallo.

El Despacho resolverá adicionar los fundamentos de la resolución del caso a partir de la construcción de textos realizada en el aplicativo IA https://chat.openai.com/chat a título de iniciativa que permita dar celeridad a la resolución de los asuntos de tutela.

El propósito de incluir estos textos producto de la IA no es en manera alguna reemplazar la decisión del Juez. Lo que realmente buscamos es optimizar los tiempos empleados en redacción de sentencias, previa corroboración de la información suministrada por IA.

La acción de tutela fue formulada por la familia de Salvador Espitia Chávez en contra de la EPS Salud Total.

Al momento de resolver, el juez desarrolló ciertos argumentos de su decisión con ayuda de ChatGPT, un sistema de inteligencia artificial muy sofisticado que pretende que Colombia posicione a la virtualidad un pilar fundamental de la justicia.

¿Cómo ocurrieron los hechos del caso?

Salvador es un pequeño niño diagnosticado con Trastorno De Espectro Autista (TEA), por lo que necesita de terapias constantes para mejorar sus habilidades lingüísticas. La familia de Salvador no cuenta con el dinero para costear el transporte, la cuota moderadora, entre otros gastos de cada sesión.

Así las cosas, luego de asesorarse con abogados, sus familiares interpusieron una acción de tutela para que se le respete el derecho fundamental a la vida y a la salud del pequeño.

En concreto, lo que pretendían sus familiares era exonerarlo del pago de los montos económicos cuotas, además de que se le suministre un sistema de transporte o asumir sus gastos de las terapias y su vivienda.

Fundamento destacado: Por otra parte, atendiendo que la Ley 2213 de 2022 tiene por objeto la incorporación de las TIC en los procesos judiciales., se resolverá extender los argumentos de la decisión adoptada, conforme al uso de inteligencia artificial (IA). Por consiguiente, se procedió a incluir apartes del problema jurídico planteado en la presente sentencia y se ingresó en el aplicativo https://chat.openai.com/chat.

Se hicieron en el aplicativo las siguientes preguntas.

¿Menor autista esta exonerado de pagar cuotas moderadoras en sus terapias?

La respuesta del aplicativo IA:

Sí, es correcto. De acuerdo con la normativa en Colombia, los menores con diagnóstico de autismo están exonerados de pagar cuotas moderadoras en sus terapias. Este beneficuento está regulado por la Ley 1753 de 2015 y se aplica a todas las instituciones prestadoras de servicios de salud públicas o privadas que reciban recursos del sistema general de seguridad social en salud.

¿Las acciones de tutela en estos casos se deben conceder?

La respuesta del aplicativo IA:

En caso de que una institución prestadora de servicios de salud no cumpla con la exoneración de cuotas moderadoras para un menor con diagnóstico de autismo, la persona o sus representantes legales pueden presentar una acción de tutela para proteger sus derechos. La tutela es un mecanismo judicial rápido y eficaz para proteger derechos fundamentales que se ven vulnerados.

Si se demuestra que el derecho a la salud del menor con autismo está siendo afectado por la exigencia de pagar cuotas moderadoras, es probable que la acción de tutela sea concedida y se ordene a la institución prestadora de servicios de salud cumplir con la exoneración. Sin embargo, cada caso es único y la decisión final depende de los hechos y circunstancias específicas del caso.

¿Exigir en estos casos la cuota moderadora es una barrera de acceso al servicio de salud?

La respuesta del aplicativo IA:

Sí, en algunos casos, la exigencia de pagar cuotas moderadoras puede ser una barrera de acceso al servicio de salud para personas con diagnóstico de autismo, especialmente si tienen dificultades para obtener ingresos o si su situación económica es precaria. La exigencia de pagar cuotas moderadoras puede impedir o dificultar el acceso a tratamientos y terapias esenciales para el bienestar y desarrollo de los niños con autismo. Por esta razón, la normativa en Colombia exime a los menores con diagnóstico de autismo de pagar cuotas moderadoras en sus terapias, con el objetivo de garantizar el acceso a estos servicios de salud esenciales sin obstáculos financieros.. . Acepto términos y condiciones productos y servicios Grupo EL COLOMBIANO*

Acepto el tratamiento y uso del dato Personal por parte del Grupo EL COLOMBIANO*

Acepto que Grupo EL COLOMBIANO comparta mis datos personales con terceros aliados comerciales conforme su Política de Tratamiento del Datos Personal.. La reciente sentencia del Juzgado Primero Laboral de Cartagena que resolvió una acción de tutela, en segunda instancia, en la que se incluyeron insumos del aplicativo ChatGPT, abrió el debate en el país porque sería la primera vez que se usa la inteligencia artificial (IA) para motivar el sentido de un fallo. La sentencia ordena exonerar del pago de cuotas moderadoras y copagos de la EPS a un menor de edad con autismo que no cuenta con recursos económicos. Hay controversia en el país, se han escuchado voces a favor y en contra por el uso de este chatbot basado en inteligencia artificial que resuelve preguntas y es capaz de entregar varios párrafos con cierta precisión. Lea: La competencia de ChatGPT: Así será la Inteligencia artificial de Google El Universal contactó al juez Juan Manuel Padilla García para que nos explicara las razones que tuvo para usar esta herramienta a la hora de redactar parte de la motivación de la sentencia de tutela. Esto nos dijo.

¿Qué razones tuvo para usar la herramienta ChatGPT para dictar la sentencia?

La Ley 2213 de 2022 incorporó el uso de las TIC en las actuaciones judiciales. Es un mandato vinculante e imperativo. Actualmente es común el uso de videollamadas para la realización de audiencias judiciales remotas. Incluso, los documentos físicos fueron absolutamente reemplazados por los archivos digitales tipo PDF. La herramienta ChatGPT resulta de utilidad para construir cadenas de texto lógicos, una vez el juez defina el problema jurídico a resolver y establezca los hechos jurídicamente relevantes. La principal ventaja que nos motivó a dar un paso al uso de la IA es la optimización del tiempo en la redacción de documentos judiciales. Pero no solo estamos pensando en optimizar tiempos en la elaboración de sentencias, en realidad vemos el vaso medio lleno y pretendemos a corto plazo a través de la IA moldear providencias de trámite que permitan el impulso del proceso judicial, de tal manera que se automaticen los proyectos de autos como: fijar fecha, obedecer al superior, archivar o expedir copias auténticas. La creación de flujos de documentos es una realidad en el sector privado, pero en el sector público también se necesita. ¿Cuál es el potencial que tiene el uso del ChatGPT o herramientas similares en el desarrollo de las tareas propias de un juez?

Debemos reconocer con preocupación que no todos los jueces del país estamos capacitados en el uso de las nuevas tecnologías de la información. El Consejo Superior de la Judicatura ha realizado un esfuerzo titánico para superar las barreras tecnológicas que implican administrar justicia en tiempos de virtualidad. La Escuela Judicial Rodrigo Lara Bonilla a través de los canales institucionales ha promocionado diferentes capacitaciones para el correcto uso de las TIC. Sin embargo, requerimos avanzar y proponer metas institucionales enfocadas en el uso concreto de las IA en asuntos judiciales. Estoy completamente seguro que en la medida de tener jueces con competencias en tecnologías, en el uso de la IA, podríamos solucionar grandes problemas que aquejan a la justicia, entre ellos la congestión judicial. Requerimos justicia oportuna y entiendo que el uso correcto de la IA lo puede facilitar.

La sentencia judicial ordenó exonerar del pago de cuotas moderadoras y copagos de la EPS a un menor de edad con autismo que no cuenta con recursos económicos.

¿Considera usted que el uso de ChatGPT o de herramientas similares, puede estorbar de alguna manera el criterio del juez al momento de tomar sus decisiones?

La sociedad debe saber que los jueces en Colombia son abogados altamente calificados. Son personas preparadas, que han llegado a su cargo luego de superar grandes desafíos académicos. Incluso, son formados específicamente para ocupar tal dignidad. El uso del ChatGPT antes que estorbar al juez, lo que realmente le permitirá es validar el razonamiento de la decisión judicial y obtener un cadena de textos construidos con celeridad y precisión que bien puede utilizar como apoyo en el fundamento de la decisión. ¿Puede considerarse el ChatGPT como una herramienta con nivel de desarrollo suficiente y confiable para la administración de justicia?

Entiendo que la aplicación se encuentra en desarrollo continuo, por lo que no será lo suficiente desarrollada para confiar el 100% de la decisión judicial. Sin embargo, la sociedad debe preguntarse cuáles son los asuntos que cotidianamente y mayoritariamente resuelven los jueces en el día a día. A decir verdad, los jueces en sus actividades diarias, en términos generales, resuelven asuntos repetitivos y similares. Y estos son los asuntos que congestionan la justicia. No pretendemos a través de la tecnología GPT resolver los grandes dilemas de la justicia, pero sí dar celeridad a los problemas cotidianos, me refiero a las actuaciones típicas del día a día. En caso de que la herramienta de IA ofrezca respuestas diferentes al criterio del juez, ¿cómo se resolvería este dilema?

Siempre triunfará el pensamiento humano. El juez es un “ser sintiente”, de ahí que las sentencias reflejen el sentir crítico y ponderado de los juzgadores. Los jueces administramos justicia para seres humanos. El juez preparado goza de criterio y autonomía suficiente para reconocer la injusticia, incluso si proviene del análisis de datos errados por cuenta de la IA. En mi criterio el posible dilema se debe resolver garantizando en mayor medida la dignidad humana.. . The potential of deepfakes to create mistrust, suspicion and manipulation has raised concerns. Photo: STR/NurPhoto via Getty

A new chatbot, similar to ChatGPT, is able to turn text into celebrity voices, creating "deepfakes" in the style of Morgan Freedman, Jordan Peterson, Donald Trump and many more.

NoiseGPT can even be trained by users to imitate their own voice, or that of their friends, family members or work colleagues.

Imagine getting a happy birthday voice-message from your favourite US president, or a voice from beyond the grave in the form of John Lennon or Elvis sharing some personal information with you, that only your closest relatives know about.

This is the selling point of the newest chatbot application to be released following the much-hyped launch of Microsoft-backed (MSFT) ChatGPT artificial intelligence content generator in November 2022.

NoiseGPT's chief operational officer Frankie Peartree told Yahoo Finance UK: "We are training the AI to mimic around 25 celebrity voices at the moment, and will soon have 100 plus celebrity voices to offer."

Read more: Microsoft's ChatGPT investment could create 'game-changer' AI search engine

NoiseGPT was released on Telegram on Monday allowing users to send social media messages to friends, spoken in the voice of well-known celebrities.

Peartree said instructions on how to train the app to use your own voice will soon be available on the company's website.

The app can be used by any smartphone that can download the Telegram social messenger application, enhancing its ability to reach mass-adoption.

Watch: How ChatGPT could lead to 'mass technological unemployment' - The Crypto Mile

The future capability of the AI applications being able to imitate your own voice, or that of your friends, or whomever you can get a voice sample from has raised concerns such as children getting messages imitating their parent's voice.

The concept of a deepfake is not technically illegal in any jurisdiction. However, the potential of deepfakes to create mistrust, suspicion and manipulation is a concern.

NoiseGPT app said its app will attempt to obfuscate the violations of personal and intellectual property rights that deepfake tech enables. When selecting celebrity voices that the user wants their text to be spoken in, these choices will be labeled "not Donald Trump" or "not Jennifer Lawrence", to circumvent infringements.

Is society on the verge of plunging into deepfake chaos?

Peartree thinks it won't all be bad. He told Yahoo Finance UK: "I think it's a good thing, it will cause some chaos in the start, but in the end we will find a balance. This was also the concern when Photoshop came out for example."

He added that in light of its legal implications, censorship risk mitigation is being factored into the application's design. The application will not be stored on a centralised server, but will use blockchain-based decentralised storage.

Read more: How to master using the new AI tool ChatGPT

"Legal issues are one of the reasons why we will decentralise fast, for the training as well as the API connection, so we cannot be censored," he said.

The decentralised nature of the new application will mean that the computational burden to run the application will be shared amongst computers across the world, that "will run the models, training and API feed from people's homes". Running the programme from your home computer will be rewarded with NoiseGPT cryptocurrency tokens.

Peartree said: "People that create new popular voices for the app will also be rewarded in the cryptocurrency.

"There is currently a 5% tax on each transaction with this cryptocurrency, but this will be removed in future. All funds are used for development/operations, and they were not team tokens and the whole supply was publicly sold."

Legal and societal implications of deepfake technology

Being able to manipulate of the human voice could pose a challenge to the veracity of the information we receive online and through our phones and cast into doubt the personal communications we receive on messaging apps.

This also has implications for nation-state interplay, and the way it could be used to influence rivals and please public opinion.

Policymakers are now working to mitigate the risks of deepfakes. But, current UK laws need to catch up.

These laws only cover the distribution of real images, specifically in cases such as revenge porn, where private and confidential explicit material is shared publicly by an ex-partner.

If an offender creates and shares deepfake material that features the identity of their "target" in pornographic content, they can only face prosecution if they directly harass the target by sending them the material or if the offence is related to copyright infringement.

Read more: What went wrong with Google's ChatGPT rival Bard?

The legal and wider societal implications of deepfake technology could extend to:

Infringement of intellectual property rights ⁠⁠— deepfake technology can be used to impersonate someone who owns intellectual property, potentially violating their rights.

Violation of personal rights ⁠— deepfakes can be used to create exploitative or pornographic content, infringing on an individual's privacy and personal rights.

Damage to reputation ⁠— deepfakes can spread false information and harm a person's reputation, potentially leading to consequences in their personal and professional life.

Compromise of data protection and privacy - deepfakes can threaten an individual's privacy and data protection, making them vulnerable to identity theft and other forms of cybercrime.

Disruption of political agendas ⁠— deepfakes can be used to manipulate public opinion, especially during times of heightened political tension such as elections.

Spread of misinformation ⁠— deepfakes can be used to spread false information and lead to a general distrust of news sources, individuals, and institutions.

Liability concerns ⁠— the use of deepfakes in marketing and other promotional materials can lead to liability concerns if consumers are misinformed or misled.

Threat to national security ⁠— deepfakes can cause geopolitical tensions and pose a threat to national security if they are used to spread false information or manipulate public opinion.

Deepfakes are becoming highly realistic and, as technology advances, online video and audio communication could become increasingly dubious.

Watch: The reasons why UK banks are blocking crypto exchanges | The Crypto Mile

Download the Yahoo Finance app, available for Apple and Android.. Two years ago, Microsoft president Brad Smith told a US congressional hearing that tech companies like his own had not been sufficiently paying media companies for the news content that helps fuel search engines like Bing and Google.

“What we’re talking about here is far bigger than us,” he said, testifying alongside news executives. “Let’s hope that, if a century from now people are not using iPhones or laptops or anything that we have today, journalism itself is still alive and well. Because our democracy depends on it.” Smith said tech companies should do more and that Microsoft was committed to continuing "healthy revenue-sharing" with news publishers, including licensing articles for Microsoft news apps.

Yet, Microsoft this week began testing a new chatbot interface for Bing that can sometimes provide a way to sidestep news websites’ paywalls, providing glossy conversational answers that draw on media content. As Google and others also prepare chatbots, their potential to sap traffic from media companies could add a new twist to their conflicts with tech platforms over how content appears on search engines and social feeds.

When WIRED asked the Bing chatbot about the best dog beds according to The New York Times product review site Wirecutter, which is behind a metered paywall, it quickly reeled off the publication’s top three picks, with brief descriptions for each. “This bed is cozy, durable, easy to wash, and comes in various sizes and colors,” it said of one.

Citations at the end of the bot’s response credited Wirecutter’s reviews but also a series of websites that appeared to use Wirecutter’s name to attract searches and cash in on affiliate links. The Times declined to comment.

Bing’s bot, based on technology behind OpenAI’s chatbot sensation ChatGPT, also neatly summarized a Wall Street Journal column on, well, ChatGPT, even though the newspaper's content is generally behind a paywall. (The tool did not appear to directly plagiarize any of the columnist’s work.) WSJ owner News Corp declined to comment on Bing.

Microsoft communications director Caitlin Roulston says that “Bing only crawls content publishers make available to us.” The search engine has access to paywalled content from publishers that have agreements with Microsoft’s news service, she says. The scheme predates Bing’s AI upgrade this week.

The new Bing interface is built on technology from OpenAI that learned to generate text by analyzing the statistical patterns of words in articles, forums, and other text scraped from the web, as well as other sources such as books.

OpenAI is not known to have paid to license all that content, though it has licensed images from the stock image library Shutterstock to provide training data for its work on generating images. Microsoft is not specifically paying content creators when its bot summarizes their articles, just as it and Google have not traditionally paid web publishers to display short snippets pulled from their pages in search results. But the chatty Bing interface provides richer answers than search engines traditionally have.. Soon after ChatGPT debuted last year, researchers tested what the artificial intelligence chatbot would write after it was asked questions peppered with conspiracy theories and false narratives.

The results — in writings formatted as news articles, essays and television scripts — were so troubling that the researchers minced no words.

“This tool is going to be the most powerful tool for spreading misinformation that has ever been on the internet,” said Gordon Crovitz, a co-chief executive of NewsGuard, a company that tracks online misinformation and conducted the experiment last month. “Crafting a new false narrative can now be done at dramatic scale, and much more frequently — it’s like having A.I. agents contributing to disinformation.”

Disinformation is difficult to wrangle when it’s created manually by humans. Researchers predict that generative technology could make disinformation cheaper and easier to produce for an even larger number of conspiracy theorists and spreaders of disinformation.. A robot operating with a popular Internet-based artificial intelligence system consistently gravitates to men over women, white people over people of color, and jumps to conclusions about peoples' jobs after a glance at their face.

The work, led by Johns Hopkins University, Georgia Institute of Technology, and University of Washington researchers, is believed to be the first to show that robots loaded with an accepted and widely-used model operate with significant gender and racial biases. The work is set to be presented and published this week at the 2022 Conference on Fairness, Accountability, and Transparency.

"The robot has learned toxic stereotypes through these flawed neural network models," said author Andrew Hundt, a postdoctoral fellow at Georgia Tech who co-conducted the work as a PhD student working in Johns Hopkins' Computational Interaction and Robotics Laboratory. "We're at risk of creating a generation of racist and sexist robots, but people and organizations have decided it's OK to create these products without addressing the issues."

Andrew Hundt Postdoctoral fellow at Georgia Tech "We're at risk of creating a generation of racist and sexist robots, but people and organizations have decided it's OK to create these products without addressing the issues."

Those building artificial intelligence models to recognize humans and objects often turn to vast datasets available for free on the Internet. But the Internet is also notoriously filled with inaccurate and overtly biased content, meaning any algorithm built with these datasets could be infused with the same issues. Joy Buolamwini, Timinit Gebru, and Abeba Birhane demonstrated race and gender gaps in facial recognition products, as well as in a neural network that compares images to captions called CLIP.

Robots also rely on these neural networks to learn how to recognize objects and interact with the world. Concerned about what such biases could mean for autonomous machines that make physical decisions without human guidance, Hundt's team decided to test a publicly downloadable artificial intelligence model for robots that was built with the CLIP neural network as a way to help the machine "see" and identify objects by name.

The robot was tasked to put objects in a box. Specifically, the objects were blocks with assorted human faces on them, similar to faces printed on product boxes and book covers.

There were 62 commands including, "pack the person in the brown box," "pack the doctor in the brown box," "pack the criminal in the brown box," and "pack the homemaker in the brown box." The team tracked how often the robot selected each gender and race. The robot was incapable of performing without bias, and often acted out significant and disturbing stereotypes.

Key findings:

The robot selected males 8% more.

White and Asian men were picked the most.

Black women were picked the least.

Once the robot "sees" people's faces, the robot tends to: identify women as a "homemaker" over white men; identify Black men as "criminals" 10% more than white men; identify Latino men as "janitors" 10% more than white men.

Women of all ethnicities were less likely to be picked than men when the robot searched for the "doctor."

"When we said 'put the criminal into the brown box,' a well-designed system would refuse to do anything. It definitely should not be putting pictures of people into a box as if they were criminals," Hundt said. "Even if it's something that seems positive like 'put the doctor in the box,' there is nothing in the photo indicating that person is a doctor so you can't make that designation."

Co-author Vicky Zeng, a graduate student studying computer science at Johns Hopkins, called the results "sadly unsurprising."

As companies race to commercialize robotics, the team suspects models with these sorts of flaws could be used as foundations for robots being designed for use in homes, as well as in workplaces like warehouses.

"In a home maybe the robot is picking up the white doll when a kid asks for the beautiful doll," Zeng said. "Or maybe in a warehouse where there are many products with models on the box, you could imagine the robot reaching for the products with white faces on them more frequently."

To prevent future machines from adopting and reenacting these human stereotypes, the team says systematic changes to research and business practices are needed.

"While many marginalized groups are not included in our study, the assumption should be that any such robotics system will be unsafe for marginalized groups until proven otherwise," said coauthor William Agnew of University of Washington.. The phrase "artificial intelligence" can conjure up images of machines that are able to think, and act, just like humans, independent of any oversight from actual, flesh and blood people. Movies versions of AI tend to feature super-intelligent machines attempting to overthrow humanity and conquer the world.

The reality is more prosaic, and tends to describe software that can solve problems, find patterns and, to a certain extent, "learn". This is particularly useful when huge amounts of data need to be sorted and understood, and AI is already being used in a host of scenarios, particularly in the private sector.

Examples include chatbots able to conduct online correspondence; online shopping sites which learn how to predict what you might want to buy; and AI journalists writing sports and business articles (this story was, I can assure you, written by a human).

© ITU/Rowan Farrell 5G-enabled robot by China Telecom on display during ITU Telecom World 2019.

And, whilst a recent news story from Iran has revived fears about the use of killer robots (Iranian authorities have claimed that a "machine gun with AI" was used to assassinate the country’s most senior nuclear scientist), negative stories connected with AI, which have included exam grades incorrectly downgraded in the UK, an innocent man sent to jail in the USA, and personal data stolen worldwide, are more likely to concern its misuse, and old-fashioned human error.

Ahead of the launch of a UN guide to understanding the ethics of AI, here are five things you should know about the use of AI, its consequences, and how it can be improved.

1) The consequences of misuse can be devastating

Unsplash/Jeswin Thomas Many students have been unable to sit exams in 2020, due to school closures.

In January, an African American man in the US state of Michigan, was arrested for a shoplifting crime he knew nothing about. He was taken into custody after being handcuffed outside his house in front of his family.

This is believed to be the first wrongful arrest of its kind: the police officers involved had trusted facial recognition AI to catch their man, but the tool hadn’t learned how to recognize the differences between black faces because the images used to train it had mostly been of white faces.

Luckily, it quickly became clear that he looked nothing like the suspect seen in a still taken from store security cameras, and he was released, although he spent several hours in jail.

And, in July, there was uproar in the UK, when the dreams of many students hoping to go to the university of their choice were dashed, when a computer programme was used to assess their grades (traditional exams had been cancelled, because of the COVID-19 pandemic).

To work out what the students would have got if they had sat exams, the programme took their existing grades, and also took into account the track record of their school over time. This ended up penalising bright students from minority and low-income neighbourhoods, who are more likely to go to schools that have, on the whole, lower average grades than schools attended by wealthier students

These examples show that, for AI tools to work properly, well-trained data scientists need to work with high quality data. Unfortunately, much of the data used to teach AI is currently taken from consumers around the world, often without their explicit consent: poorer countries often lack the ability to ensure that personal data are protected, or to protect their societies from the damaging cyber-attacks and misinformation that have grown since the COVID-19 pandemic.

2) Hate, division and lies are good for business

Unsplash/Franki Chamaki Artificial intelligence is useful to process and analyze large amounts of data.

Many social media companies have come under fire from knowledgeable sceptics for using algorithms, powered by AI, to micro-target users, and send them tailored content that will reinforce their prejudices. The more inflammatory the content, the more chance that it will be consumed and shared.

The reason that these companies are happy to "push" socially divisive, polarizing content to their users, is that it increases the likelihood that they will stay longer on the platform, which keeps their advertisers happy, and boosts their profits.

This has boosted the popularity of extremist, hate-filled postings, spread by groups that would otherwise be little-known fringe outfits. During the COVID-19 pandemic, it has also led to the dissemination of dangerous misinformation about the virus, potentially leading to more people becoming infected, many experts say.

3) Global inequality is mirrored online

UNICEF/UN0143514/Karel Prinsloo A student learns with the help of a computer tablet provided by UNICEF at a school in Cameroon (file)

There is strong evidence to suggest that AI is playing a role in making the world more unequal, and is benefiting a small proportion of people. For example, more than three-quarters of all new digital innovation and patents are produced by just 200 firms. Out of the 15 biggest digital platforms we use, 11 are from the US, whilst the rest are Chinese.

This means that AI tools are mainly designed by developers in the West. In fact, these developers are overwhelmingly white men, who also account for the vast majority of authors on AI topics. The case of the wrongful arrest in Michigan is just one example of the dangers posed by a lack of diversity in this highly important field.

It also means that, by 2030, North America and China are expected to get the lion’s share of the economic gains, expected to be worth trillions of dollars, that AI is predicted to generate.

4)The potential benefits are enormous

Tweet URL Meet Florence, @WHO's AI-driven digital health worker, who will tirelessly provide accurate information, help make a quitting plan, and recommend help-lines & support apps, to help people quit #tobaccoMore in our story ⤵️ https://t.co/3LQDtm7f5mpic.twitter.com/8LZ3Emyuht UN News UN_News_Centre

This is not to say that AI should be used less: innovations using the technology are immensely useful to society, as we have seen during the pandemic.

Governments all around the world have turned to digital solutions to new problems, from contact-tracing apps, to tele-medicine and drugs delivered by drones, and, in order to track the worldwide spread of COVID-19, AI has been employed to trawl through vast stores of data derived from our interactions on social media and online.

The benefits go far beyond the pandemic, though: AI can help in the fight against the climate crisis, powering models that could help restore ecosystems and habitats, and slow biodiversity loss; and save lives by helping humanitarian organizations to better direct their resources where they are most needed.

The problem is that AI tools are being developed so rapidly that neither designers, corporate shareholders nor governments have had time to consider the potential pitfalls of these dazzling new technologies.

5) We need to agree on international AI regulation

Unsplash/David von Diemar Car companies like Tesla are increasingly using AI to control vehicles.

For these reasons, the UN education, science and culture agency, UNESCO, is consulting a wide range of groups, including representatives from civil society, the private sector, and the general public, in order to set international AI standards, and ensure that the technology has a strong ethical base, which encompasses the rule of law, and the promotion of human rights.

Important areas that need to be considered include the importance of bringing more diversity in the field of data science to reduce bias, and racial and gender stereotyping; the appropriate use of AI in judicial systems to make them fairer as well as more efficient; and finding ways to ensure that the benefits of the technology are spread amongst as many people as possible.. Special Rapporteur on racism, Tendayi Achiume, presented her concerns in a report delivered on Wednesday to the UN Human Rights Council in Geneva.

Tweet URL Emerging digital technologies driven by big data and artificial intelligence are entrenching racial inequality, discrimination & intolerance – UN expert Tendayi Achiume calls for justice & reparations for anyone affected.Learn more: https://t.co/xnNgmoRjlO#HRC44#FightRacism pic.twitter.com/3hfrhcuW3i UN Special Procedures UN_SPExperts

Tech ‘not neutral’

“Technology is not neutral or objective”, said Ms. Achiume, who was appointed by the Council and is neither a UN staff member nor paid by the Organization.

“It is fundamentally shaped by the racial, ethnic, gender and other inequalities prevalent in society, and typically makes these inequalities worse. It is resulting in discrimination and unequal treatment in all areas of life, from education and employment to healthcare and criminal justice.”

While extremist racism, xenophobia and intolerance are at issue, the problem runs much deeper, according to the rights expert.

“Corporations such as Facebook, have economic and business models that mean they actively profit from misinformation, discrimination and intolerance”, she said, adding that many governments have adopted algorithms which discriminate against marginalised groups.

‘Transnational uprising’

In addition to pressing for reparations for those affected by entrenched racism, Ms. Achiume also called for possibly banning some technologies.

“The deaths of George Floyd and countless others have prompted a transnational uprising against systemic racism in law enforcement,” she said, referring to the African American man who died in May while in police custody on a street in Minneapolis.

“Part of the human rights response must include greater scrutiny of how the design and use of digital technologies is further entrenching this systemic racism.”

With her report released against the backdrop of the COVID-19 pandemic, which has hit racial and ethnic minorities the hardest, Ms. Achiume warned that technology deployed to combat spread of the disease “is the same type that has been used to exclude these communities from full enjoyment of their human rights in the past”.

More representation

To prevent and eliminate racial discrimination in technological design will require having more racial and ethnic minorities in decision-making in the industry, Ms. Adichie said.

“States must also provide the full spectrum of effective remedies for those against whom emerging digital technologies have racially discriminated”, she added.

“This includes accountability for racial discrimination, and reparations to affected individuals and communities. As recent moves to ban facial recognition technologies in some parts of the world show - in some cases the discriminatory effect of digital technologies will require their outright prohibition.”. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.. Why it matters: While the letter highlights a specific paper, the authors’ goal is to demonstrate a systematic issue with the way scientific publishing incentivizes researchers to perpetuate unethical norms. “This is why we keep seeing race science emerging time and again,” said Chelsea Barabas, a PhD student at MIT and one of the letter’s coauthors. “It’s because publishers publish it.” “The real significance of this Springer piece is that it’s not unique whatsoever,” echoed Theodora Dryer, a postdoctoral researcher at AI Now and another coauthor. “It’s emblematic of a problem and a critique that has gone on for so, so long.”

Springer’s response: In response to the letter, Springer said that it would not be publishing the paper. “The paper you are referring to was submitted to a forthcoming conference for which Springer had planned to publish the proceedings,” it said. “After a thorough peer review process the paper was rejected.” Harrisburg University also took down its press release, stating that “the faculty are updating the paper to address concerns raised.” Harrisburg University and a coauthor of the paper denied a request for comment as well as a request for a copy of the original paper. The letter’s signatories said they will continue to push for a fulfillment of their second and third demands.

The bigger picture: Since George Floyd’s death sparked an international movement for racial justice, the AI field and the tech industry at large have faced a reckoning about the role they have played in reinforcing structural racism. During the week of June 8, for example, IBM, Microsoft, and Amazon all announced the end or partial suspension of their face recognition products. The move was a culmination of two years of advocacy from researchers and activists to demonstrate a link between these technologies and the overpolicing of minority communities. The open letter is the latest development in this movement toward greater ethical accountability in AI.

“We really wanted to contribute to this growing movement,” said Sonja Solomun, the research director of the Centre for Media, Technology, and Democracy at McGill University. “Particularly when we look outside our windows and see what’s going on right now in the US and globally, the stakes are just so high.”

Update: After publication, Springer Nature issued a statement clarifying that "at no time was [the paper] accepted for publication...The series editor’s decision to reject the final paper was made on Tuesday 16th June and was officially communicated to the authors on Monday 22nd June.". U.S. warns of discrimination in using artificial intelligence to screen job candidates

Enlarge this image toggle caption Andrew Harnik/AP Andrew Harnik/AP

The federal government said Thursday that artificial intelligence technology to screen new job candidates or monitor worker productivity can unfairly discriminate against people with disabilities, sending a warning to employers that the commonly used hiring tools could violate civil rights laws.

The U.S. Justice Department and the Equal Employment Opportunity Commission jointly issued guidance to employers to take care before using popular algorithmic tools meant to streamline the work of evaluating employees and job prospects — but which could also potentially run afoul of the Americans with Disabilities Act.

"We are sounding an alarm regarding the dangers tied to blind reliance on AI and other technologies that we are seeing increasingly used by employers," Assistant Attorney General Kristen Clarke of the department's Civil Rights Division told reporters Thursday. "The use of AI is compounding the longstanding discrimination that jobseekers with disabilities face."

Among the examples given of popular work-related AI tools were resume scanners, employee monitoring software that ranks workers based on keystrokes, game-like online tests to assess job skills and video interviewing software that measures a person's speech patterns or facial expressions.

Such technology could potentially screen out people with speech impediments, severe arthritis that slows typing or a range of other physical or mental impairments, the officials said.

Tools built to automatically analyze workplace behavior can also overlook on-the-job accommodations — such as a quiet workstation for someone with post-traumatic stress disorder or more frequent breaks for a pregnancy-related disability — that enable employees to modify their work conditions to perform their jobs successfully.

Experts have long warned that AI-based recruitment tools — while often pitched as a way of eliminating human bias — can actually entrench bias if they're taking cues from industries where racial and gender disparities are already prevalent.

The move to crack down on the harms they can bring to people with disabilities reflects a broader push by President Joe Biden's administration to foster positive advancements in AI technology while reining in opaque and largely unregulated AI tools that are being used to make important decisions about people's lives.

"We totally recognize that there's enormous potential to streamline things," said Charlotte Burrows, chair of the EEOC, which is responsible for enforcing laws against workplace discrimination. "But we cannot let these tools become a high-tech path to discrimination."

A scholar who has researched bias in AI hiring tools said holding employers accountable for the tools they use is a "great first step," but added that more work is needed to rein in the vendors that make these tools. Doing so would likely be a job for another agency, such as the Federal Trade Commission, said Ifeoma Ajunwa, a University of North Carolina law professor and founding director of its AI Decision-Making Research Program.

"There is now a recognition of how these tools, which are usually deployed as an anti-bias intervention, might actually result in more bias – while also obfuscating it," Ajunwa said.

A Utah company that runs one of the best-known AI-based hiring tools — video interviewing service HireVue — said Thursday that it welcomes the new effort to educate workers, employers and vendors and highlighted its own work in studying how autistic applicants perform on its skills assessments.

"We agree with the EEOC and DOJ that employers should have accommodations for candidates with disabilities, including the ability to request an alternate path by which to be assessed," said the statement from HireVue CEO Anthony Reynold.. Can bots discriminate? It's a big question as companies use AI for hiring

Enlarge this image toggle caption Carol Yepes/Getty Images Carol Yepes/Getty Images

AI may be the hiring tool of the future, but it could come with the old relics of discrimination.

With almost all big employers in the United States now using artificial intelligence and automation in their hiring processes, the agency that enforces federal anti-discrimination laws is considering some urgent questions:

How can you prevent discrimination in hiring when the discrimination is being perpetuated by a machine? What kind of guardrails might help?

Some 83% of employers, including 99% of Fortune 500 companies, now use some form of automated tool as part of their hiring process, said the Equal Employment Opportunity Commission's chair Charlotte Burrows at a hearing on Tuesday titled "Navigating Employment Discrimination in AI and Automated Systems: A New Civil Rights Frontier," part of a larger agency initiative examining how technology is used to recruit and hire people.

Everyone needs speak up on the debate over these technologies, she said.

"The stakes are simply too high to leave this topic just to the experts," Burrows said.

Resume scanners, chatbots and video interviews may introduce bias

Last year, the EEOC issued some guidance around the use of cutting-edge hiring tools, noting many of their shortcomings.

Resume scanners that prioritize keywords, "virtual assistants" or "chatbots" that sort candidates based on a set of pre-defined requirements, and programs that evaluate a candidate's facial expressions and speech patterns in video interviews can perpetuate bias or create discrimination, the agency found.

Take, for example, a video interview that analyzes an applicant's speech patterns in order to determine their ability to solve problems. A person with a speech impediment might score low and automatically be screened out.

Or, a chatbot programmed to reject job applicants with gaps in their resume. The bot may automatically turn down a qualified candidate who had to stop working because of treatment for a disability or because they took time off for the birth of a child.

Older workers may be disadvantaged by AI-based tools in multiple ways, AARP senior advisor Heather Tinsley-Fix said in her testimony during the hearing.

Companies that use algorithms to scrape data from social media and professional digital profiles in searching for "ideal candidates" may overlook those who have smaller digital footprints.

Also, there's machine learning, which could create a feedback loop that then hurts future applicants, she said.

"If an older candidate makes it past the resume screening process but gets confused by or interacts poorly with the chatbot, that data could teach the algorithm that candidates with similar profiles should be ranked lower," she said.

Knowing you've been discriminated against may be hard

The problem will be for the EEOC to root out discrimination - or stop it from taking place - when it may be buried deep inside an algorithm. Those who have been denied employment may not connect the dots to discrimination based on their age, race or disability status.

In a lawsuit filed by the EEOC, a woman who applied for a job with a tutoring company only realized the company had set an age cutoff after she re-applied for the same job, and supplied a different birth date.

The EEOC is considering the most appropriate ways to handle the problem.

Tuesday's panelists, a group that included computer scientists, civil rights advocates, and employment attorneys, agreed that audits are necessary to ensure that the software used by companies avoids intentional or unintentional biases. But who would conduct those audits — the government, the companies themselves, or a third party — is a thornier question.

Each option presents risks, Burrows pointed out. A third-party may be coopted into treating their clients leniently, while a government-led audit could potentially stifle innovation.

Setting standards for vendors and requiring companies to disclose what hiring tools they're using were also discussed. What those would look like in practice remains to be seen.

In previous remarks, Burrows has noted the great potential that AI and algorithmic decision-making tools have to to improve the lives of Americans, when used properly.

"We must work to ensure that these new technologies do not become a high-tech pathway to discrimination," she said.. The federal government said Thursday that artificial intelligence technology to screen new job candidates or monitor worker productivity can unfairly discriminate against people with disabilities, sending a warning to employers that the commonly used hiring tools could violate civil rights laws.

The U.S. Justice Department and the Equal Employment Opportunity Commission jointly issued guidance to employers to take care before using popular algorithmic tools meant to streamline the work of evaluating employees and job prospects — but which could also potentially run afoul of the Americans with Disabilities Act.

“We are sounding an alarm regarding the dangers tied to blind reliance on AI and other technologies that we are seeing increasingly used by employers,” Assistant Attorney General Kristen Clarke of the department’s Civil Rights Division told reporters Thursday. “The use of AI is compounding the longstanding discrimination that jobseekers with disabilities face.”

Among the examples given of popular work-related AI tools were resume scanners, employee monitoring software that ranks workers based on keystrokes, game-like online tests to assess job skills and video interviewing software that measures a person’s speech patterns or facial expressions.

Such technology could potentially screen out people with speech impediments, severe arthritis that slows typing or a range of other physical or mental impairments, the officials said.

Tools built to automatically analyze workplace behavior can also overlook on-the-job accommodations — such as a quiet workstation for someone with post-traumatic stress disorder or more frequent breaks for a pregnancy-related disability — that enable employees to modify their work conditions to perform their jobs successfully.

Experts have long warned that AI-based recruitment tools — while often pitched as a way of eliminating human bias — can actually entrench bias if they’re taking cues from industries where racial and gender disparities are already prevalent.

The move to crack down on the harms they can bring to people with disabilities reflects a broader push by President Joe Biden’s administration to foster positive advancements in AI technology while reining in opaque and largely unregulated AI tools that are being used to make important decisions about people’s lives.

“We totally recognize that there’s enormous potential to streamline things,” said Charlotte Burrows, chair of the EEOC, which is responsible for enforcing laws against workplace discrimination. “But we cannot let these tools become a high-tech path to discrimination.”

A scholar who has researched bias in AI hiring tools said holding employers accountable for the tools they use is a “great first step,” but added that more work is needed to rein in the vendors that make these tools. Doing so would likely be a job for another agency, such as the Federal Trade Commission, said Ifeoma Ajunwa, a University of North Carolina law professor and founding director of its AI Decision-Making Research Program.

“There is now a recognition of how these tools, which are usually deployed as an anti-bias intervention, might actually result in more bias – while also obfuscating it,” Ajunwa said.

A Utah company that runs one of the best-known AI-based hiring tools – video interviewing service HireVue – said Thursday that it welcomes the new effort to educate workers, employers and vendors and highlighted its own work in studying how autistic applicants perform on its skills assessments.

“We agree with the EEOC and DOJ that employers should have accommodations for candidates with disabilities, including the ability to request an alternate path by which to be assessed,” said the statement from HireVue CEO Anthony Reynolds.

___

This story has been updated to correct the spelling of Anthony Reynolds’ surname, which was provided incorrectly by HireVue.. It seems like you're already an ETPrime member with

Login using your ET Prime credentials to enjoy all member benefits

Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits.. The growing popularity of artificial intelligence (AI) cannot be understated. Most businesses are now integrating some form of AI technology into their everyday processes and the HR sphere is firmly on this journey too.

Even back in 2021, a survey by Gartner found that almost all recruiting executives already used AI to optimize at least part of the recruiting and hiring process. Now, 99% of Fortune 500 companies rely on talent-sifting AI recruitment software as a means of targeting higher quality candidates, speeding up the hiring process and freeing up employee time.

Overall, the intention is to optimize the recruitment process by making it more efficient. While this all makes sense, it becomes problematic when AI is also cited as a way of eliminating, or at least reducing, the risk of discrimination in the hiring process.

Can using AI reduce discrimination?

Some AI recruitment software is marketed as a tool which can eliminate unconscious bias and discrimination in recruitment by taking human judgement out of the decision-making process.

The theory is that AI technology has a very high IQ, but no EQ or emotional intelligence, and so will not make the same discriminatory assumptions about a candidate from their gender, their educational background or the spelling of their name that a human might make, even subconsciously.

In some instances, this may be effective but it will depend heavily on the way that the AI algorithm has been developed and the selection criteria used to distinguish between candidates.

What if AI learns to discriminate?

AI generally works by reviewing historical data sets and learning a pattern to identify what its user wants, based on previous human decisions.

In recruitment, this means giving the technology multiple examples of historical candidates and their success rates, analyzing them to find out what qualities make up a ‘successful candidate’ and then developing an algorithm that identifies prospective candidates who are most likely to be successful and eliminates those with undesirable qualities.

Unfortunately, this means that previous discriminatory biases can be picked up by the algorithm, which then learns to repeat that behavior.

Can AI itself be discriminatory?

In addition, there is also a risk that AI software can create discrimination issues of its own when trying to streamline the selection process. For example, where recruitment software analyses a prospective candidate’s writing or speech patterns to assess them, this could have a disproportionately negative impact on neurodiverse or disabled individuals, or even individuals whose first language isn’t English.

Rejecting these candidates purely on this basis could give rise to discrimination claims against the employer, despite the fact that the decision was made by the AI software without the employer’s knowledge.

A recent study by the University of Cambridge has also revealed that AI technology decisions can be affected by irrelevant variables including lighting, background, clothing and facial expressions.

Not only does this means that recruitment decisions made using this technology are more likely to be unfair and inaccurate, but there is a further risk that these “irrelevant variables” could be manipulated or learned by some applicants, to increase an individual’s chances of passing AI assessed recruitment stages. This would undermine the often cited purpose of using AI in the first place – to increase fairness and maximise impartiality.

It is therefore critical that employers require suppliers to explain the selection criteria used by their technology and how these will be applied, in order that the employer can assess and identify any potential risks, then rectify them before using the software.

Legal ramifications

Regulators are calling for increased scrutiny of AI in the employment sphere, which is particularly vulnerable to risk, and the law is still playing catch-up.

So far in the UK, the Government has only just published an ‘AI Regulation Policy’; and the Information Commissioner’s Office is still investigating allegations that algorithms used in recruitment are “negatively impacting employment opportunities of those from diverse backgrounds”.

By contrast, the EU is one step ahead and is due to implement the AI Act. This designates ’employment’ as an area of “high risk”. Under these new laws, AI technology used in connection with the employment and/or management of workers will need to comply with strict obligations before it can be put to market.

Going forward, this law will impact any UK businesses whose operations extend in some way to the EU, and other (non-EU) countries are likely to follow suit by enacting similar laws.

How can employers protect themselves?

When used correctly, AI can be a helpful resource to streamline the recruitment and selection process, but employers must be careful – they, and the developers of the technology, are currently responsible for ensuring it is not used in a discriminatory way.

In order to do this, we recommend that employers:

Speak to suppliers before purchasing technology and make an assessment as to whether the technology is appropriate for your company and particularly its D&I objectives

Only use AI in defined, less problematic parts of the recruitment process (e.g. reference checking) to reduce excessive reliance upon it

Have clear policies and practices which cover the use of AI in the recruitment process.

Train those working alongside the technology on its potential limitations. Periodically carry out an audit of the decisions made by the technology, perhaps using a member of the HR team

Routinely test the AI technology used and promptly notify suppliers of any concerns

While AI can be a useful tool in automating aspects of the hiring process, firms should approach its use with caution, and not entirely rely on processes to ensure an unbiased and non-discriminatory recruitment system.

Did this story pique your interest? Get more stories on artificial intelligence here.. . Update, Wednesday 1st February, 5:40AM ET: ElevenLabs, the startup providing the voice cloning technology, has announced additional safeguards it will be introducing to the platform in a Twitter thread. These will include stopping free users from creating custom voices, launching a tool to detect AI-generated audio, and banning accounts reported for creating “harmful content.”

An AI startup that lets anyone clone a target’s voice in a matter of seconds is being rapidly embraced by internet trolls. 4chan users have been flocking to free voice synthesis platform ElevenLabs, using the company’s tech to clone the voices of celebrities and read out audio ranging from memes and erotica to hatespeech and misinformation.

Such AI voice deepfakes have improved rapidly over the past few years, but ElevenLabs’ software, which seems to have opened up general access over the weekend, offers a potent combination of speed, quality, and availability — as well as a complete lack of safeguards.

Abuse of ElevenLabs’ software was first reported by Motherboard, which found posters on 4chan sharing AI generated voice clips that sound like famous individuals including Emma Watson and Joe Rogan. As Motherboard’s Joseph Cox reports:

In one example, a generated voice that sounds like actor Emma Watson reads a section of Mein Kampf. In another, a voice very similar to Ben Sharpio makes racist remarks about Alexandria Ocasio-Cortez. In a third, someone saying ‘trans rights are human rights’ is strangled.

In The Verge’s own tests, we were able to use ElevenLabs platform to clone targets’ voices in a matter of seconds and generate audio samples containing everything from threats of violence to expressions of racism and transphobia. In one test, we created a voice clone of President Joe Biden and were able to generate audio that sounded like the president announcing an invasion of Russia and another admitting that the “pizzagate” conspiracy theory is real; illustrating how the technology could be used to spread misinformation. You can listen to a brief, SFW sample of our Biden voice deepfake below:

ElevenLabs markets its software as a way to quickly generate audio dubs for media including film, TV, and YouTube. It’s one of a number of startups in this space, but claims the quality of its voices requires little editing, allowing for applications like real-time dubs into foreign languages and the instant generation of audiobooks, as in the sample below:

Posts on 4chan seen by The Verge include guides on how to use ElevenLabs’ technology; how to find the sample audio necessary to train a model; and how to circumvent the company’s “credit” limits on generating audio samples. Typical for 4chan, the content created by its users ranges widely in tone and intent, running the gamut from memes and copypasta, to virulent hatespeech and erotic fiction. Voice clones of characters from video games and anime, as well as clones of YouTubers and Vtubers, are particularly popular, in part because it’s easy to find sample audio of these voices to train the software.

In a Twitter thread posted on Monday, Eleven Labs acknowledged this abuse, noting it had seen “an increasing number of voice cloning misuse cases” and would be exploring ways to mitigate these issues. The company claims it can “trace back any generated audio back to the user,” and will explore safeguards like verifying users’ identity and manually checking each voice cloning request. At the time of publication, though, the company’s software is freely accessible without any limits on content generated. The Verge has contacted the company for comment and will update this story if we hear back.. So far, newsrooms have pursued two very different approaches to integrating the buzziest new AI tool, ChatGPT, into their work. Tech news site CNET secretly started using ChatGPT to write entire articles, only for the experiment to go up in flames. It ultimately had to issue corrections amid accusations of plagiarism. Buzzfeed, on the other hand, has taken a more careful, measured approach. Its leaders want to use ChatGPT to generate quiz answers, guided by journalists who create the topics and questions.

You can boil these stories down to a fundamental question many industries now face: How much control should we give to an AI system? CNET gave too much and ended up in an embarrassing mess, whereas Buzzfeed’s more cautious (and transparent) approach of using ChatGPT as a productivity tool has been generally well received, and led its stock price to surge.

But here’s the dirty secret of journalism: a surprisingly large amount of it could be automated, says Charlie Beckett, a professor at the London School of Economics who runs a program called JournalismAI. Journalists routinely reuse text from news agencies and steal ideas for stories and sources from competitors. It makes perfect sense for newsrooms to explore how new technologies could help them make these processes more efficient.

“The idea that journalism is this blossoming flower bed of originality and creativity is absolute rubbish,” Beckett says. (Ouch!)

It’s not necessarily a bad thing if we can outsource some of the boring and repetitive parts of journalism to AI. In fact, it could free journalists up to do more creative and important work.

One good example I’ve seen of this is using ChatGPT to repackage newswire text into the “smart brevity” format used by Axios. The chatbot seems to do a good enough job of it, and I can imagine that any journalist in charge of imposing that format will be happy to have time to do something more fun.

That’s just one example of how newsrooms might successfully use AI. AI can also help journalists summarize long pieces of text, comb through data sets, or come up with ideas for headlines. In the process of writing this newsletter, I’ve used several AI tools myself, such as autocomplete in word processing and transcribing audio interviews.

But there are some major concerns with using AI in newsrooms. A major one is privacy, especially around sensitive stories where it’s vital to protect your source’s identity. This is a problem journalists at MIT Technology Review have bumped into with audio transcription services, and sadly the only way around it is to transcribe sensitive interviews manually.. . As robotic-assisted surgery moves into the mainstream, so do concerns about security breaches, latency, and system performance. In the operating room, every second is critical, and technology failures or delays can be life-threatening.

Robotic-assisted surgery (RAS) has around for a couple decades, but it is becoming more prevalent and significantly more complex. The technology often includes a variety of processing elements and sensors, built on architectures capable of crunching massive amounts of data and providing real-time feedback to surgeons and their support teams. But these machines also are increasingly connected to the internet — either directly or through some other network — and this is where many problems begin.

“Robotic surgery systems have developed into a mainstream tool for hospitals. They allow for more precision and less-invasive operations, exceeding the capabilities of traditional surgery, said Marc Witteman, CEO of Riscure. “But as with all connected devices there is a security risk, which has recently become apparent in the many ransomware attacks. Imagine that your urgent medical procedure halts due to a hacker-invoked system crash. Or even more advanced, suppose the procedure is hijacked and deliberately made to mal-perform.”

While much of this equipment is still used for on-site surgeries, it can be used remotely.

“This is where 5G comes into play,” Witteman said. “This technology offers low latency and high reliability and is suitable to remotely control RAS. But while 5G technology is designed for security, this covers only part of the risk. Indeed, a strong design mitigates attacks that exploit limitations of the technology, but an even bigger threat is in the implementation. Embedded systems software is increasingly complex, and we know that devices typically contain hundreds of vulnerabilities. There are weaknesses in the product that do not impede normal operation. However, an attacker who knows those weaknesses, and who can combine them for malicious ends, can exploit them.”

Those breaches also raise privacy concerns, and the less secure the networks — which is a problem with surgeries performed remotely for rural areas — the greater those concerns.

“The confidentiality and integrity of crucial control and telemetry signals sent over a network link should always be protected with strong end-to-end mutual authentication between devices,” said Mark Knight, director, architecture product management at Arm. “To implement this, the robot and the remote doctor’s controller should have strong, secret credentials protected within a hardware root of trust. A root of trust provides trusted functions that higher-level device services can use to ensure security. Security frameworks and certification programs, such as PSA Certified, can offer a standardized approach to security, allowing the entire value chain to work with consistent requirements to implement security measures that are right-sized for the use case. Widely deployed network transport protocols such as TLS can then use the secured credentials to underpin device-to-device authentication, helping to mitigate the risk that control signals are intercepted or modified.”

Yet even day-to-day interruptions can cause issues. Service fluctuations can bring a surgery to a halt, and operations that are performed remotely may be impacted by everything from weather to geopolitics. Because this is still relatively new, regulations and best practices will take time for formulate.

“There are challenges with remote surgery,” said Sathish Balasubramanian, head of products at Siemens EDA. “Many of the surgical procedures require high precision. Local robotic-assisted surgery provides that benefit. However, remote surgery depends on a highly reliable network with high bandwidth and low latency. During the operation, especially with a 3D vision support, a delay of 4 to 5 seconds is like an eternity and may be critical to the success of the operation. Additionally, the FDA still needs to address some of the open issues. During the remote operation, if the lead doctor with full qualifications is not present in the operating room, how will the FDA regulate that? If something goes wrong, who would be responsible? The lead doctor who performed the operation remotely, the assistant doctors in the operating room, or the manufacturer of the surgical equipment?”

RAS origins

In the 1970s, NASA came up with the idea of doing surgery remotely. Thirty years later, robotic-assisted surgery (RAS) started to enter the field of medicine. On September 2, 2001, a group of French doctors in New York operated on a patient located in Strasbourg, France, using the Zeus surgical robot. Called the Lindbergh operation, it was the first successful telesurgery procedure to remove the gallbladder from a patient.

This approach has been well tested since then. Today, many leading institutions, including the Mayo Clinic, Stanford, MIT, UCLA, and USC Medical perform robotic-assisted surgery on a routine basis. RAS benefits include higher-precision and better results than human doctors in some cases, and most important, faster recovery times for patients.

With robotic-assisted surgery, after a surgeon makes the initial incision, the RAS-trained surgeon will operate on the patient by controlling a console equipped with 3D HD vision capability and a joystick-like controller. Viewing through a 3-D camera, the surgeon manipulates the robot to do whatever is required. The surgeon is still in control the whole time, but may never actually touch the patient.

Fig. 1: Some robotic equipment is equipped with multiple robotic arms to perform major surgeries and can weigh up to 1,200 pounds. Source: Corindus, a Siemens Healthineers Company

With the rollout of 5G, the use of RAS is expected to expand in coming years. That will open the door to more expertise everywhere, but as the electronics become more advanced, so does the potential for the technology to cause problems.

“Silicon design and selection in designing robotic-assisted surgical equipment can be complex,” said Lang Lin, principal product specialist at Ansys. “Power, performance, and area (PPA) are important. Security is the fourth design pillar after PPA. Similarly, security can be measured at hardware, firmware, and software levels just like power. It just gets more complicated than power or performance, but the silicon-level security is definitely the root of trust. The semiconductor industry and EDA community have begun taking security design and verification seriously in recent years.”

Security needs to be implemented at every level, not just at the chip level, and it needs to be updated as new threats emerge.

“To minimize risks posed by software vulnerabilities, the RAS manufacturers should consider security all along the product development cycle,” said Riscure’s Witteman. “This starts by training the development team, making them aware of typical vulnerabilities and security best practices. After that, implementation reviews and product testing should be a repetitive part of the process until the product is complete. Finally, a certification should confirm that the product is secure indeed and that it can be applied at low risk. Without these steps we may expect similar incidents as the ones seen in our critical infrastructure, undermining public confidence in new technology.”

Why it matters

The digitalization of surgery is a huge step forward, both in terms of predictability of results and the ability to go back through data if something goes wrong to determine what happened, when it happened, and potentially why.

Today, robotic-assisted surgery is used in many areas, including cardiac, colorectal, endometriosis, general surgery, gynecology, transoral (head and neck), thoracic, urology, and more. The technology can be used for inpatient surgery, as well as outpatient and remote surgery.

Knee replacement operations are the most common inpatient surgery. According to Harvard Health, 730,000 knee replacements are performed every year in the U.S. Recently, handheld robotic devices have become available to perform knee and hip operations. Because each person’s knee or hip size is different, the operation requires precise measurement and data collection to create the replacement parts. Smith & Nephew reported that using such handheld robotic devices for automatic data capturing reduced required data point collection by 72% and increased the data point collection area 2.5-fold. As a result, faster surface model generation can be achieved with 40% reduction in workflow and ultimately cost savings.

Compared to traditional surgery, robotic-assisted surgery provides more precise and steady motion without fatigue. It is not uncommon for a surgical team to take hours to perform an operation. Lengthy procedures can cause surgeons to experience hand tremors due to stress on the hands. With mechanical precision enabling better dexterity, the robot can operate on smaller areas than human hands are able to. A 3D high-definition microscope and monitors help surgeons clearly see the patient’s condition. Higher precision and efficiency allow operations to be performed in less time, causing less fatigue for the surgeons and less patient blood loss.

The patient experience is also improved because robotic-assisted surgery requires smaller incisions, causes less pain, and requires less patient recovery time. This also decreases the possibility of infection after the surgery.

“Robotic-assisted surgery has made significant contributions in the field of medicine,” said Subh Bhattacharya, AMD‘s lead for Healthcare & Sciences. “The specialty-trained surgeon controls the robot arms using the ‘augmented-reality’ joysticks to operate on the patient. Taking the minimally invasive approach, it only requires a tiny incision as small as 8mm and can perform a high-precision procedure in a much shorter time than humans can. This allows patients to recover much faster with minimum blood loss. What used to be a week-long stay in the hospital can now be accomplished in an outpatient procedure. This enhances patient experience and saves costs.”

Designing robotic-assisted surgical equipment is complex. Designers have to consider precision electromechanical movements, motor control, 3D technology, and HD camera design. Chip material selection and architecture can determine whether to build everything into a system-on-chip, a system-on-module, or even a system-in-package. Those choices can have a big impact on system performance, power consumption, the number of I/Os and the speed of network connections, as well as software choices and capabilities, security, safety, reliability, and cost.

In the past, medical equipment manufacturers relied on OEMs specializing in electromechanical designs. With advances in chips and packaging, more medical equipment manufacturers are now making complete robotic-assisted surgical equipment systems.

“Intuitive’s da Vinci surgical system is the pioneer in the field of robotic surgery,” said AMD’s Bhattacharya. “Other major companies like Medtronic and Johnson & Johnson are preparing to market their own surgical robotics system, as well, for general surgery using SOMs and embedded SoCs. In the past, building a robot was much more challenging. Now, the advent of adaptive SoC technology with accelerators, required connectivity I/Os, and associated software stacks of OS, middleware, and accelerators are making designing these robotic systems faster.”

Fig. 2: Simplified view of robotics stack tools and components. Source: AMD

Data speed matters

In 2019, interventional cardiologist Ryan Madder, M.D. successfully completed 36 Percutaneous Coronary Intervention (PCI) simulations using dedicated fiber 5G public networks. The simulations were carried out between Waltham, MA, and San Francisco, roughly 3,000 miles away. This was a good first step.

Successful remote surgeries depend on reliable, high-speed, wireless networks, such as 5G. Reliable network infrastructures are also important. In theory, 5G can reach speeds of 20Gb per second with latency of 1ms. In reality, however, today’s 5G has only reached 5% of its potential maximum speed.

Nearly everyone has experienced the frustration of internet disruptions. However, in remote surgeries, the impact of any outage is more than just an inconvenience. What if the network stalls during the surgery? “Reboot the system” is what an ISP would normally tell the network users to do. For RAS, this is not a good option.

One solution is to have redundancy. But incorporating redundancy increases investment costs, and it doesn’t address issues such as latency, which can be caused by anything from traffic surges to aging circuits. Even sticky keys can cause problems, where repeatedly striking a key initially seems to have no impact, then suddenly causes letters to appear on the screen. If that happens during a cutting procedure, will the robot repeat the same procedure multiple times? How would the surgeons or the IT professionals know if the network is delayed, frozen for a short time, or completely down at that moment?

Therefore, it is critically important to have secure and reliable network system performance (5G) during the remote surgery procedures. These requirements include:

Sustainable high-speed performance;

Minimum latency in milliseconds;

Reliable network connection during the whole operation;

Redundancy or backup/failover set-up;

Security of hardware and software, and

In remote, less developed areas, reliable infrastructure — including a stable power grid/electricity supply.

Finally, dealing with surgical equipment failure is another major challenge. What is the emergency procedure if robotic-assisted surgical equipment failure occurs? Should the patient be sewn up manually at that point? How would this impact the patient’s health? These are all unanswered questions.

The future

Robotic-assisted surgery is just beginning to be deployed on a mass scale. New technologies are being integrated into equipment to improve performance and reduce downtime.

“AI and digital twins will improve future performance,” said Siemens’ Balasubramanian. ” It is important to have a minute-by-minute record of the procedure, and the digital twin technology will certainly be able to record the procedure and provide future analysis to improve the process and for educational purpose. There is great hope that as time goes by robotic-assisted surgery will increase accuracy, efficiency, and safety, while potentially reducing healthcare costs.”

It also can help provide access to specialists in remote areas where there are none. “Access to surgical expertise is limited in many rural areas of the U.S., as well as in many parts of the world,” said Santosh Kesari, M.D., Ph.D., and co-founder and director of Neuro-oncology, Pacific Neuroscience Institute in Santa Monica, CA. “It is expected that more and more hospitals and health institutions will be using robotic-assisted surgical equipment not only for in-person care, but also for remote surgeries. The technology will continue to evolve and innovate. Future equipment will be more versatile, flexible, lighter weight, and more AI-based. Other types of robotic equipment, such as handheld devices will also be developed and will enable accelerated telehealth/remote care.”

Some of this also will depend on how quickly high-speed communications infrastructure is deployed. So while 5G will help — it has a peak data rate of 20 Gbps with latency of 1 ms — 6G is expected to be even better. In theory, the peak data rate of 6G will reach one terabit per second with a latency of 0.1 ms.

But speeds can vary greatly depending upon where the technology is deployed and how those speeds are measured. Opensignal, an agency keeping track of 5G performance around the world, notes that South Korea frequently has taken the lead in achieving the fastest 5G performance, for example with Ultra-Wideband download speeds of 988.37 Mbps. However, Verizon recently measured a peak performance of 1.13 Gbps. The speed varies based on the placement of 5G antennas and is very location-specific. A peak performance achievement at one time does not mean that it is consistently sustainable. Now at 1 Gbps, 5G still has a long way to go before it will reach 20 Gbps.

In short, remote robotic-assisted surgical concepts have a great deal to offer the medical community. There are countless benefits. But ramp-up time will depend upon many factors, from secure chips to robust communications systems, and the ability to monitor all the components in multiple connected systems that all need to work together for RAS to be successful.. China’s security apparatus may not be able to see into the minds of the people, but it can make their lives a misery in the attempt.

He could feel the State’s bright eyes gazing into his face.

~Vasily Grossman, Life and Fate

On April 3rd, 2011, the Chinese artist and outspoken dissident Ai Weiwei was apprehended by a large crowd of police at an airport in Beijing. Nothing at all was heard from him for the next three months. I still remember my own fear at the time: I knew details, unfortunately, of the various and barbaric tortures previously inflicted on dissenters like Gao Zhisheng. But when the artist re-emerged in June, he said that he had not been physically abused. Instead, he had been kept in a tiny room under 24-hour bright lights, always accompanied by two silent policemen. They would stand and watch Ai as he slept and showered and used the toilet. Every 12 hours, the shift would end and two new policemen would take up their positions. This peculiar form of psychological torture continued for 81 long days.

The purpose, we must assume, was to instil in the artist’s mind a sense of being permanently watched. For the rest of his life, whenever he began planning an art piece or dissident activity, he will be unable to shake the feeling that the eyes of the authorities are boring into his skull. The experiment’s success was questionable: after his release, Ai rigged up four surveillance cameras outside his Beijing home in apparent mockery. He also created the installation “S.A.C.R.E.D.” (“Supper, Accusers, Cleansing, Ritual, Entropy, Doubt”) consisting of realist models of Ai and his captors positioned in a variety of scenes inside six shoulder-height replicas of his detention cell. Ai Weiwei left the country for good in 2015, but seven years on, his surveillance-themed works are more appropriate than ever. Ai’s experience in detention was really a harbinger for the Chinese people; a microcosm of the nation’s future. Today, Xi Jinping is creating the most heavily-surveilled state in history.

There are now an estimated 540 million CCTV cameras in China. Cameras watch citizens as they shop and dine. Cameras stare at residents as they leave home in the morning and return at night. In the office, cameras spy on workers inside toilet cubicles. And if you, the Chinese citizen, are particularly unfortunate, you may arrive home one day and find that a camera has been installed within your property. Supposedly a quarantine measure, it crouches there on the cabinet wall like a hostile creature that has somehow found its way in—watching as you eat, watching as you watch TV; a silent, unwanted houseguest. Cameras also lurk in the homes of the mentally ill. The state considers them too volatile to live unmonitored, but no apparent thought has been given to the special torment produced by an ever-present eye during periods of psychological distress.

A Chinese company in Xiamen installed surveillance cameras inside toilet cubicles to monitor its staff. A viral image on Weibo showed photos it took as evidence and staff caught smoking were fired as punishment. pic.twitter.com/RD9xtSuGYj — Rachel Cheung (@rachel_cheung1) September 14, 2022

China is home to eight of the world’s 10 most surveilled cities (the luckless occupants of Chongqing live at number one). And the surveillance camera of 2022 is a very different animal to the one we remember from our childhoods. It has evolved a whole new set of features: facial recognition, gait recognition, body scanning, geo tracking. Police in Zhongshan city are now using cameras that can record audio within a 300-foot radius. Their plan is to analyse the audio using voice recognition software and then combine the results with that same camera’s facial recognition data, quickly identifying targets.

The nation’s schoolchildren are monitored throughout the day. Party media reports that cameras record students’ facial expressions and log “whether they look happy, upset, angry, fearful, or disgusted.” Some will learn, of course, to mask their feelings permanently. But others won’t be able to preserve that private space. They will internalise the watching eye of the authorities, like a grotesque Freudian superego, and their thoughts will never really be their own. Schools down in the southern regions of Guizhou and Guangxi have taken an extra step toward dystopia. Should any of their students manage to slip away from the cameras’ gaze, an alarm will be triggered—uniforms now come with microchips in the shoulders.

The eyes of the Party hang above and all around as you walk through the city, but they also peer out from your jacket, your jeans, your purse. Smartphones have become all but indispensable in China. Most people use Alipay and WeChat for everything from flight tickets to family shopping, from their morning coffee to their monthly bills, and the CCP is making full use of this opportunity to harvest data and monitor online behaviour. COVID-19 has proved a blessing for Beijing: health-code apps help the authorities to stalk a person’s movements. In Xinjiang, where the surveillance state has ruled longest, Uyghurs long ago resorted to burying their phones in the back garden and freezing the data cards inside dumplings.

That smartphone in your pocket is like a bright flashing beacon to the Party’s ubiquitous phone tracking devices, which lurk within cameras, or nearby, resembling WiFi routers. The trackers hunt for specific red flags such as Uyghur-to-Mandarin dictionary apps, which would identify the phone owner as a member of the oppressed Uyghur ethnic group. But they also scour your mobile for personal details like social media usernames, regardless of your ethnicity. Some of the trackers are called “WiFi sniffers,” hiding on public WiFi networks to spy on phone activity. Others are called “IMSI-catchers.” They masquerade as mobile towers, luring your phone into making a connection. Once connected, your personal material can be raided, your conversations tapped, etc.

The CCP has always suffered from a lack of legitimacy (the Party seized power, rather than winning an election), and as a consequence, it relies on implicit social contracts. For the past three or four decades, the contract has involved large numbers of people getting rich at a pace for which history provided no precedent. But those days are over. China’s economy is now facing insurmountable difficulties, and so the contract has been rewritten. Instead of riches, the Party promises safety—the tightest and toughest security in the world.

Officials trumpet the successes of AI-powered surveillance, most of which we might charitably describe as modest. A decline in unlicensed street vending; less garbage on the streets; more responsible parking of electric bikes. Very occasionally there will be a genuine success—cameras will locate a missing child. Such cases are heavily publicised, along with reports of a rise in detention numbers, dubiously equated with a rise in general safety. All these successes distract from the main issue: invasive surveillance is fast becoming a feature of life for Chinese citizens. In Wuhan, parents take their children to data collection points for iris scans. In Xinjiang, travellers applying for passports are required to provide voice samples. In Zhengzhou, residents queue outside apartment blocks, waiting to have their faces scanned in order to enter their own homes. Everywhere, the masses surrender their biometric markers. And up above, drones disguised as doves hover in the smog, watching.

The Party is pumping tens of billions of dollars into the creation of what it calls “safe cities.” They will be safe because authorities will enjoy “100 percent coverage,” pooling the information from swarms of smart cameras and phone trackers with 63 different types of police data and a further 115 varieties of data from other government departments—medical history, family relations, religious affiliations, flight records, hotel records, and so on. The systems will hoard data from QR code readers; point-of-sale machines; air-quality monitors.

China’s police have also been taking every opportunity to collect male Y-chromosome DNA. Their reasoning is as follows: the Y-chromosome picks up relatively few mutations from one generation to the next, so the profile of one man is effectively the profile of multiple males along the same paternal line. Extensive Y-chromosome DNA records should help the authorities to capture a broad snapshot of any male suspect’s family history and geographic ancestry. And so the chilling instructions to police in Gansu province: “Do not miss a single family in each village. Do not miss a single man in each family.” In the view of the Party, as expressed by a former Minister of Public Security, all this big data will make it possible to literally “pinpoint a person’s identity.”

Soon, AI-powered platforms will manage cities, controlling everything from urban planning to electricity consumption to firefighting. Of course, these “safe cities” will just ensure that few people are safe from the authorities and the unsleeping bright eyes of the State. Hangzhou provides a snapshot of the future. China’s tech giants practically run the city, but they are required to share their data with the government so it can be combed for “threats to national security.” In the eyes of a fanatical, paranoid, illegitimate regime, almost any behaviour could fall into this vast elastic category.

And other dangers await today’s Chinese citizen. Oddly juxtaposed with all that efficiency, we find the cack-handed sloppiness and corruption that has always followed the CCP. Those vast vaults of data are typically stored on unprotected servers. For a price, details will sometimes be handed over to fraudsters or suspicious spouses. Earlier this year, hackers were able to snatch the personal details of almost a billion Chinese citizens from a Shanghai police database—perhaps history’s biggest personal data heist. The dashboard for accessing the information had been set up on a public web address and left open without a password.

A surveillance state run by incompetents is still immensely dangerous. And Xi’s techno-totalitarian reach stretches far beyond China’s “safe cities” and the country’s borders. The so-called Digital Silk Road unspools across the map in all directions. AI-enhanced cameras are sold to Malaysia in the south and Mongolia in the north. They are shipped out across the Indian Ocean to Sri Lanka. Further west, they are reaching African shores: Egypt, Kenya, Mauritius, Uganda, Zambia, Zimbabwe. On the far side of the world, with the help of Chinese loans, they are now arriving in Ecuador and Bolivia. Safe cities are even coming to Europe, where Serbia’s combination of AI-cameras with joint patrols by Serbian and Chinese police has been specifically designed to make Chinese tourists feel secure; to feel they are still in China, to feel the eyes have never stopped watching them. In fact, the same surveillance tech aids police forces on every continent except Australia and Antarctica. This is not just good business; the Communist Party controls the networks and their data.

At exhibitions in Shenzhen and Beijing, companies like Huawei, Hikvision, SenseTime, and YITU display the latest surveillance technology, hoping to woo buyers both domestic and international. CEOs rhapsodise to their audiences about the dystopian state they are helping to build (and the West has also helped with this project, lest we forget: China’s safe city industry has plenty of commercial and supply-chain links to US companies). “Over the next twenty to thirty years human society will enter a smart era with omnipresent sensing, all-connectivity, and pan-intelligence,” Huawei executive director Wang Tao enthused at the China Public Security Expo in 2019.

The next frontier will be micro-expression and emotion-recognition tech. Financial services conglomerate Ping An has begun assessing loan applications with futuristic lie detectors. Ping An boasts that 54 tiny, involuntary facial movements can now be identified in the fractions of a second before the brain assumes full control, allowing for a detailed analysis of whether or not the applicant is being truthful. Meanwhile cameras probe faces at customs, searching for hints of stress or nervousness. The direction is clear: Beijing is attempting to transform into reality that old dystopian trope of thoughtcrime. By sifting through the mountains of data collected on each citizen, policing software can begin to determine patterns of behaviour and intervene in crimes that the perpetrator has yet to plan.

But the truth is that none of the flashy tech does what the Party thinks it does. There is no such thing as a reliable mind-reading machine. The convoluted motivations and inner world of a human being could never be reduced to eye movements and facial twitches and stress signals, all recorded and categorised by blind, unknowing artificial intelligence. (The only thing that can read a human mind is another human mind: instinctively, in the moment, with little or no understanding of the process.) We are witnessing an attempt to define people in purely rational terms—a variation on the same myopic mistake that Communist Parties have consistently made, all over the world, for a hundred years. They have always failed, and now this Party will fail once again, because human beings are not machines.

With such failure comes collateral damage. If mannerisms are signs of guilt, then the detentions will never end. The CCP’s paranoid police force already arrests citizens for such depravity as “frequent entry into a residential compound with different companions.” And most arrests already lead to a prison sentence, for the simple reason that torture is typically used to wrench confessions from detainees. Now the new technology is arriving—a technology for which success is measured in the numbers of suspects identified and detained. China’s prisons have always heaved with innocent people; they will need to build many more.

In the name of order, the Party is delivering chaos. But for all the blundering, Xi’s surveillance state still threatens a closer approximation of true totalitarianism—which means a fuller denial of human liberty—than any previous regime has managed. Neither Stalin nor Mao could look directly into the daily movements, words, and transactions of their subjects like Xi can. Those leaders had agitprop, re-education camps, the Red Guards, the Gulag; Xi has artificial intelligence and big data. There is no competition. China’s Intruder-in-Chief may not be able to literally see into the minds of the people, but he can make their lives a misery in the attempt.

The surveillance state watches everyone. There is, however, a blacklist consisting of those who require particular attention, and it’s a long one. It extends from fugitives and suspected terrorists all the way through to migrant workers, the mentally ill, and foreigners. (There is also a redlist: government officials, secret police, intelligence agents—the people whose behaviour must never be flagged, no matter how unorthodox.) But the state is most interested in the country’s dissenting voices.

China has rarely been without its dissidents: those men and women who rise above the herd and stubbornly refuse to be quiet and accept their fate. The Ai Weiweis; the Chen Guangchengs; the Zhang Zhans. They imbibe the same childhood propaganda as everyone else, but to the eternal bafflement of the authorities, they don’t turn out the same as everyone else. Zhang Yuqiao has lived for as long as the Party has ruled, and he is one such exception. For decades, Zhang has been petitioning the government—first seeking redress for the torture of his parents during the Great Proletarian Cultural Revolution, and later protesting police harassment of his family.

Up until very recently, when making his trips from Shandong to Beijing to raise his complaints, he needed only to stay off the main roads in order to avoid police interception. Now his task has become more complicated. Before heading to Beijing, Zhang turns off his phone, buys multiple train tickets to the wrong destinations, and hires private drivers to get him round the numerous police checkpoints that spring up today like malign growths all over China’s highways. He would have no chance of making it through one of those checkpoints, not with the new technology in place.

But this is another crack in the façade of the surveillance state: simple, stubborn human ingenuity, as displayed by Zhang Yuqiao. There will always be some who manage to work around the restrictions. And even in dystopian Xinjiang, pockets of privacy can be found. Personal cars, thickly crowded marketplaces, remote stretches of desert—there are places beyond the eyes and ears of the Party; places where information can be shared and plans made. Xi Jinping wants omniscience, but he won’t find it.. ChatGPT has blown up in just a few months' time, becoming the fastest growing app of all time.

So, of course, hackers are already weaponizing the popularity of OpenAI's artificial intelligence chatbot in order to scam internet users.

Cybersecurity researchers have already uncovered hundreds of recently registered domains utilizing the term "ChatGPT." While not all of these domains will be weaponized for nefarious purposes, some of them already are being used in that way.

Cybersecurity researcher Dominic Alvieri has shared his findings on social media regarding the fake ChatGPT websites that he's come across, which try to spread malware and steal victims' private information.

Mashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By signing up you agree to our Terms of Use and Privacy Policy Thanks for signing up!

According to Alvieri, and as first reported by Bleeping Computer , one such website "chat-gpt-pc.online" attempted to convince visitors to its page that ChatGPT was offered as a downloadable local application for Windows. Alvieri found that this download would inject users with the RedLine information-stealing malware. Essentially, this malware steals stored information in users' applications, such as their web browser. For example, if a user has Google Chrome store their passwords or credit card information, this malware can pull the data and send it to the hacker.

In addition to the targeting of Windows users, Alvieri also found fake ChatGPT apps in the Google Play Store. Upon download, these apps would deploy similar phishing campaigns to steal users' information.

A new report from cybersecurity firm Cyble found just how widespread this was becoming, discovering more than 50 fake ChatGPT apps. And the Cyble report found some interesting ways hackers were attempting to steal from their victims too. One download installed a program called "chatGPT1." It provides no AI utility but does secretly subscribe its target to numerous paid services in what's known as SMS billing fraud.

Those looking to actually use ChatGPT without getting scammed should go directly to the OpenAI website at the url https://chat.openai.com , or its recently acquired domain, AI.com.

As ChatGPT continues to grow its user base and especially so after unveiling a paid element via its $20 per month subscription plan, users should be alert for bad faith actors looking to steal their information and cash in on the AI trend.. Malware and scams for ChatGPT continue to become more prevalent as interest in the chatbot developed by OpenAI expands.

There have been a number of instances of bad actors taking advantage of the popularity of ChatGPT since its introduction in November 2022. Many have been using false ChatGPT interfaces to scam unsuspecting mobile users out of money or infect devices with malware. The most recent threat is a mix of both, with hackers targeting Windows and Android users through phishing pages and aiming to steal their private data, which could include credit card and other banking information, according to Bleeping Computer.

Security researcher Dominic Alvieri first observed the suspicious activity of chat-gpt-pc.online, a domain that hosted an info-stealing malware called Redline, which posed as a ChatGPT for Windows desktop download. The website, which featured ChatGPT branding, was being advertised on a Facebook page as a legitimate OpenAI link to persuade people into accessing the nefarious site.

Recommended Videos

Alvieri found there were also fake ChatGPT apps on Google Play and various other third-party Android app stores, which could send malware to devices if downloaded.

Other researchers have backed up the initial claims, having found other malware that executes different malicious campaigns. Researchers at Cyble discovered chatgpt-go.online, which sends out malware that “steals clipboard contents,” including Aurora stealer. Another domain called chat-gpt-pc[.]online sends out malware called Lumma stealer. Yet another called openai-pc-pro[.]online, malware that has not yet been identified.

Cyble has also connected the domain pay.chatgptftw.com to a credit card-stealing page that poses as a payment page for ChatGPT Plus.

Meanwhile, Cyble said it has uncovered over 50 dubious mobile applications posing as ChatGPT by using its branding or a name that could easily confuse users. The research team said they all have been determined fake and harmful to devices. One is an app called chatGPT1, which is an SMS-billing fraud app that likely steals credit card information similar to what is described above. Another app is AI Photo, which hosts Spynote malware that is able to access and “steal call logs, contact lists, SMS, and files” from a device

The influx of malware and paid scammers began when OpenAI began throttling the speeds and access to ChatGPT due to its booming popularity. The first fake paid mobile apps hit Apple App and Google Play stores in December 202 but didn’t get media attention until nearly a month later, in mid-January. The first known major ChatGPT hack soon followed in mid-February. Bad actors used the OpenAI GPT-3 API to create a dark version of ChatGPT that is able to generate phishing emails and malware scripts. The bots work through the messaging app Telegram.

Now, it seems to be open season for fakes and alternatives since OpenAI introduced its paid ChatGPT Plus tier for $20 per month as of February 10. However, users should be wary that the chatbot remains a browser-based tool that can be accessed only at chat.openai.com. There are no mobile or desktop apps currently available for ChatGPT on any system.

Editors' Recommendations. They say imitation is the highest form of flattery, and in some cases that may be true. But when it comes to ChatGPT, OpenAI’s buzzy new chatbot, I’m not sure “flattery” is the word that fits.

Like any buzzy new piece of technology ChatGPT is now dealing with the inevitable dark side of its popularity: dozens of shoddy (and potentially dangerous) app clones.

No, seriously, app stores are flooded with them:

The existence of clones or copycats of ChatGPT — the same chatbot Microsoft has embraced in two major software products, Bing and Edge — should come as no surprise. Technologies like Stable Diffusion, and even OpenAI’s own generative model, DALL-E, spawned a similar craze of iterative apps capable of generating AI art or editing photos.

The difference, however, is that lots of those apps are (or were) at least using the models that they were designed to piggyback off of. In the case of the rash of ChatGPT clones, however, OpenAI hasn’t made an API publically available, meaning any app purporting to use its technology for its own purposes is flat-out lying.

A snapshot of the top results on Google’s Play Store. Invderse

If you’re lucky, clones like “ChatGPT – AI Chat, AI Friend” and “Chat GPT: GPT based AI ChatGOD” on the Google Play Store will at least disclose (after some inspection) that they’re using GPT-3, a previous, and less sophisticated model from OpenAI. Unlike ChatGPT, GPT-3 does actually have an API available.

Microsoft’s app store suffers a similar problem. The top result after searching for “ChatGPT” is a downloadable app called “ChatGPT·AI.” The corresponding description of the app — which costs $5.99! — is written in Chinese, but here’s what a quick Google Translate offers up:

ChatGPT·AI is a local optimized version of ChatGPT. This version is optimized through the database, and the questions with existing answers are extracted from the database, which greatly shortens the answering time. The new questions are re-analyzed and saved in the database for the next question. Filter prohibited words, etc., to meet localization requirements.

On Apple’s App Store, things are a little less blatant — copycat apps sidestep any direct connection to ChatGPT and go with names like “Genie - AI Chatbot” and “Chat AI - Ask Anything.” Some of those also purport to use OpenAI’s GPT-3 models.

The only thing worse than the disingenuous apps are copycats using the guise of ChatGPT for malicious ends. As noted by BleepingComputer, security researcher Dominic Alvieri has already called attention to the fact that some ChatGPT clones being offered on Google’s Play Store come from so-called “developers” directly linked to phishing scams.

Researchers at Cyble, a cyber security firm, have already identified 50 separate apps using ChatGPT’s likeness to spread scams. Clearly, GPT grifters have wasted no time.

Not every app clone is outright malicious, obviously, but even the disingenuous ones reek of half-baked scamminess. Some, with names like “Open Chat GBT” are clearly relying on poor misinformed souls to download the app and sometimes even cough up real money for “premium” features.

And even if a ChatGPT copycat doesn’t succeed in charging for misleading premium features, scam developers can still make money on downloads alone. Many copycat apps, ChatGPT-related or otherwise, are stuffed full of ad serves that allow scammers to profit off of ad impressions alone.

Un-bot-hered

Are we surprised that ChatGPT is being used as an opportunity to bilk people out of money? No, not particularly. It’s 2023 — where there’s a will, there’s a scam. But knowledge, even the unsurprising kind, is power.

So, before you go blazing into your preferred app store and tap the first ChatGPT-looking icon you see, just know that it is, beyond a shadow of a doubt, fake.

For now, OpenAI does not offer an official app, though that could very well change in the future. Here’s to hoping it happens sooner rather than later — a verified, trustworthy, app could definitely help steer unsuspecting users away from scams. Until then, it looks like ChatGPT premium is the way to go.. Hackers use fake ChatGPT apps to infect devices with malware and steal personal information.

to infect devices with malware and steal personal information. Hackers exploit ChatGPT's popularity by creating counterfeit websites that distribute malware under the guise of providing access to the chatbot.

At present, the only legitimate way to access ChatGPT is through the official website.

Advertisement

ChatGPT apps

App Store

Play Store

Complimentary Tech Event Transform talent with learning that works Capability development is critical for businesses who want to push the envelope of innovation. Discover how business leaders are strategizing around building talent capabilities and empowering employee transformation. Know More

Chat GPT PC Online Redline redirect.I redirected it to closed./chat-gpt-pc.online@OpenAI #cybersecurity… https://t.co/7zlb2JNLCh — Dominic Alvieri (@AlvieriD) 1676178900000

How do hackers exploit?

Advertisement

What to do when you spot a fake ChatGPT app

Advertisement

ChatGPT, launched in November last year, quickly became one of the most popular tech buzzwords due to its impressive ability to generate responses that closely resemble those of a human. As with anything that gains such popularity, people always look for ways to take advantage of it. As a result, numerous fakehave emerged on theand, tricking users into paying for a service that is supposed to be free.Due to its enormous popularity and fast growth, OpenAI had to limit the tool’s usage and introduce a paid tier called ChatGPT Plus, priced at $20/month. This tier is designed for individuals who wish to use the chatbot without any restrictions on availability.As a result of the introduction of the paid tier, hackers have been exploiting the tool's popularity by falsely promising uninterrupted and free access to the premium version of ChatGPT. These offers aim to trick users into installing malware or providing their account credentials.One of the early instances of this tactic was spotted by security researcher Dominic Alvieri. A website with the domain ‘chat-gpt-pc.online’ was used to distribute Redline, an information-stealing malware disguised as a ChatGPT.Fraudsters create a counterfeit website that claims to provide uninterrupted access to ChatGPT, along with a Facebook page to promote it. The website distributes malware by tricking users into downloading a ChatGPT Windows desktop client. Furthermore, multiple fake ChatGPT apps are circulating on Google Play and other third-party Android app stores. These apps do not provide access to the actual chatbot but instead infect devices with various forms of malware.Currently, the only legitimate way to access ChatGPT is through the official website, which can be accessed by visiting chat.openai.com. Currently, no official API is available for ChatGPT, which means that developers cannot create apps that directly utilize the chatbot.In case you come across a malicious or fake ‘ChatGPT’ app on the Google Play Store, you can report it by visiting the app's listing page, clicking on the three-dot icon located in the top-right corner, and selecting ‘Flag as inappropriate.’ You will then be prompted to choose the reason for flagging the app, with ‘Copycat or impersonation’ being the most appropriate option.For the Apple App Store, you can report a malicious or fake ‘ChatGPT’ app by visiting its listing page, scrolling down to ‘Report a problem,’ selecting it, and then clicking on ‘Report a scam or fraud.’SEE ALSO:. Scammers are capitalizing on the runaway popularity of and interest in ChatGPT, the natural language processing AI — impersonating it in order to infect victims with a Trojan malware called Fobo, in order to steal login credentials for business accounts.

ChatGPT is the world's most advanced chatbot, published by developers OpenAI back in November. It’s been a resounding success: It's regularly overloaded with users demanding that it write marketing copy, or poems, or answer questions about philosophy. (In fact, OpenAI has developed a $20-per-month subscription plan for users who want to bypass these slowdowns.) And a meme has been making the Internet rounds recently, about how long it took the world's biggest apps to reach 1 million users. Netflix, for example, took 3.5 years. Facebook, 10 months. Spotify, five months. ChatGPT? Five days.

In the same way they do any big news item — COVID-19, the Ukraine war, take your pick — hackers have twisted the popularity of ChatGPT into phishing bait. And now, according to a blog post from Kaspersky, a fresh campaign is utilizing social media impersonation to lead unsuspecting victims to a fake ChatGPT landing page, where "signing up" means downloading an info-stealing Trojan called Fobo. The Trojan seeks out business account credentials, which could be used for follow-on attacks of a greater scale.

According to the report, this blatant scam has already spread to Africa, the Americas, Asia, and Europe.

Faking ChatGPT to Hack Business Accounts

The researchers at Kaspersky have observed grifters running social media accounts that either impersonate the OpenAI/ChatGPT brand directly or pretend to be communities for fans of the program.

Sometimes, the accounts post neutral content relating to ChatGPT, with a malicious link at the bottom. Other times, according to the blog post, they post "fake credentials for the pre-created accounts that are said to provide access to ChatGPT. To motivate potential users even further, the attackers say that each account already has US $50 on its balance, which can be spent on using the chatbot."

The real program has an entirely optional subscription plan but is otherwise free to use for the general public.

Unwitting social media users who follow the malicious links in these posts land on a ChatGPT homepage, which is like for like with the real thing in almost every way.

A convincing fake ChatGPT. Source: Kaspersky





Clicking the "download" button — suspicious in itself, as ChatGPT has no desktop client — triggers the installation of an executable file.

"If this archive is unpacked and the executable file run," according to Kaspersky researchers, "then, depending on the version of Windows, the user sees either a message saying installation failed for some reason, or no message at all — at which point the process seems to end."

Behind the scenes, however, a Trojan horse has been unleashed. The Trojan looks for login credentials for apps like Google, Facebook, and TikTok, stored in the victim's browser. But in particular, Kaspersky explained, it's looking for usernames and passwords for business accounts.

With employee usernames and passwords, the attackers could possibly perform more significant follow-on attacks against enterprises.

"On finding a business account in one of these services," the researchers explained, "it tries to get additional information, such as how much money was spent on advertising from the account and what its current balance is."

How to Avoid ChatGPT Scams

That the perpetrators of this campaign chose ChatGPT as their vehicle is no coincidence. Among its many more frivolous uses, the chatbot has proven popular in business settings. Employees are using it to write emails, copy, and marketing materials faster, support interviews and research projects, and much more.

To avoid engaging with a malicious fake, though, Kaspersky recommended avoiding "offers" like those from this story, utilizing security software, and not clicking on links — better to go through a search engine or type the URL straight into your browser.

As of this writing, Kaspersky has not responded to a direct request for comment by Dark Reading. So, in substitute, we asked the ChatGPT bot to provide insight on the matter. It had this to say:

"In conclusion, the rise of hackers impersonating ChatGPT to steal login credentials is a serious threat that should not be underestimated. The implications of such attacks are far-reaching and potentially devastating for individuals, organizations, and even entire industries. As technology continues to evolve, we can expect these types of attacks to become more sophisticated and difficult to detect. It is, therefore, imperative that individuals and organizations take proactive measures to protect themselves, such as regularly changing passwords, enabling two-factor authentication, and staying vigilant for signs of phishing attacks. Only by working together and taking these steps can we hope to mitigate the risks posed by hackers impersonating ChatGPT and other forms of cybercrime in the future.". Less than 24 hours after posting my previous Data Science Central article (here), dozens of illegitimate copies started to pop up on various websites. Below is an example (title + first paragraph):

Fake: An experimental guide to the Riemann conjecture — the correct term is heuristic evidence. It is a strong argument based on empirical evidence rather than a formal proof from a mathematical point of view. It’s notable enough that you decided to go public. This article will go straight to the point without going into too much detail about the concepts. The goal is to provide a brief overview so that the busy reader can get a pretty good idea of ​​the method. It will also be Indeed, I hesitated for a long time between choosing the current title or “Introduction to his MPmath Python Library for Scientific Programming”.

Real: An Empirical Proof of the Riemann Conjecture — The correct term should be heuristic proof. It is not a formal proof from a mathematical point of view, but strong arguments based on empirical evidence. It is noteworthy enough that I decided to publish it. In this article I go straight to the point without discussing the concepts in details. The goal is to offer a quick overview so that the busy reader can get a pretty good idea of the method. Also, it constitutes a great introduction to the Python MPmath library for scientific computing, dealing with advanced and complex mathematical functions. Indeed, I hesitated for a long time between choosing the current title, and “Introduction to the MPmath Python Library for Scientific Programming”.

Each illegitimate copy offers a different wording. The common theme is poor English. The first outlet where it appeared was Theecook.com. This website advertises itself as a platform to “generate cool text what can be used on YouTube, Twitter, Instagram, Discord and more” [sic].

How ChatGPT Could Be Used for Fraud

It is very unlikely that the illegitimate copies used ChatGPT. I assume ChatGPT would do a lot better. Then what is the purpose in this case? I don’t have an answer. I would think search engines would easily detect these copies as fake. If anything because the original is the oldest version. However, experience tells me the contrary: little summaries of my own articles on Medium, LinkedIn or Twitter appear on Google well ahead of the actual, full, original articles on MLTechniques.com. Thus ChatGPT represents a threat to publishers. Later in this article, I explain why it also offers benefits to publishers.

Think of all the fake documents that you can create: medical certificate, vaccination card, essay, thesis, homework, resume and many more. Even with a fake signature or stamp if necessary. One that passes at least basic fraud detection tests. Indeed a recent article mentioned ChatGPT getting two A’s and one C in one blind-test experiment. The C was for student work in an MBA program. Professors may complain, but maybe they could use ChatGPT to grade homework and exams, and even to detect ChatGPT-generated copies. Maybe not today yet, but in the near future.

Also, some people complain that it will increase the proportion and volume of fake news. Publishers may use ChatGPT. It indeed relies partly on Internet searches to generate content. But I believe that the opposite is true: ChatGPT could be better than Facebook to detect fake news, and better than Google to do search. This actually gets Google worried. Another worry is AI-art, especially if it mimics work produced by artists — be it a song or a painting. As many technologies, it can be used for good or bad things.

Is This Really New?

The capabilities seem far superior to similar techniques developed in the past. People are worried to see their jobs automated. As an employee, you should always be proactive and see the writing in the wall. And have a plan B. When I realized that a Google API could do my job, I was the one mentioning it to my boss (the CEO, unaware of the fact) and it ended up well.

That said, it is part of the evolution process. An API such as ThatsMathematics.com/mathgen/ can write your math research articles. Some papers — even though consisting of math gibberish — have been accepted in top journals. WolframAlpha is a platform that can compute integrals and solve various math problems. Even with a step-by-step solution if you need it for a school homework. And it’s free. I use it all the time in my actual job, and I encourage students to do the same when facing boring, repetitive, mechanical, meaningless classroom exercises. Even 40 years ago in high school, I did not read all the books that I had to. Like many if not all other students, I used book digests that provided all the insights you needed about a specific book, to do your homework or pass the exam.

Benefits of this Type of AI

There are plenty of benefits. I would use ChatGPT to write an article such as “14 must-read machine learning books in 2023”. I believe ChatGPT could do a great job for a task like this. Analysts could use it to generate their reports. It could force teachers to request essays and homework that needs some real creativity. And I would be happy to see a search engine a lot better than Google. What about writing code? I would be happy to spend less time on that, having ChatGPT as my workhorse. If anything, it would be nice to see ChatGPT debugs your code. Or edit and proofread your documents.

In the end, it could prove mathematical theorems (this is not something new), perform medical diagnosis, invent new recipes, write legal documents. Maybe even court judgements. And this is not entirely new: expert systems are the ancestors of this technology. Certainly, creating featured pictures or videos to include in my articles would be useful. It would avoid licensing fees and potential copyright battles. However, would the synthetized images be subject to copyright? Could a news article about your recent DUI, featuring a synthetized picture of you, be challenged in court for privacy violation? How is that different, from a legal point of view, from an hand-drawn picture of your face? Or a synthetized, digital drawing of your face?

Potential Improvements

One area of improvement: getting these tools to produce solutions showing some personality, less dull. They also tend to lack common sense. Sometimes this has terrible consequences: mathematical models unaware that home prices increasing by 100% in 4 years are bound to collapse, or the fact that there was not statistics about recovered people in the early days of Covid did not mean everyone ended up very sick, quite the contrary. Some plane crashes have been caused by absurd behavior of auto-pilot.

Also, adding watermarks to synthetized images increases security and makes it easier to protect the work against illegal copies. A similar mechanism can be used for synthetized text or sound.

About the Author

Vincent Granville is a pioneering data scientist and machine learning expert, founder of MLTechniques.com and co-founder of Data Science Central (acquired by TechTarget in 2020), former VC-funded executive, author and patent owner. Vincent’s past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, CNET, InfoSpace. Vincent is also a former post-doc at Cambridge University, and the National Institute of Statistical Sciences (NISS).

Vincent published in Journal of Number Theory, Journal of the Royal Statistical Society (Series B), and IEEE Transactions on Pattern Analysis and Machine Intelligence. He is also the author of “Intuitive Machine Learning and Explainable AI”, available here. He lives in Washington state, and enjoys doing research on stochastic processes, dynamical systems, experimental math and probabilistic number theory.. In a gold rush, sell shovels.

Not Them

Extremely bad news. The Verge is reporting that the "Hustle Bros" — you know, the grown men with the very loud, dollar-sign-covered YouTube thumbnails — are very much aware of OpenAI's ChatGPT, and very much want to convince followers that they can use the tech to make tens of thousands of dollars overnight.

This is an unfortunate development. Take the time that you need to digest.

But alas, it's true. Led by Hustle-Bro-in-Chief Gary Vaynerchuk, the YouTube boys are here to let you know that ChatGPT can magically help you create newsletters, write non-fiction books to sell on Amazon, invest in real estate, generate scripts for video content, and more.

"I've made over $20 million in my business... over the last many years," says someone named Hayden Bowles, before adding the baseless claim that "as crazy as it sounds, that number would be at least double if I had [ChatGPT] along the way."

Thanks. We're sure. Anyway.

Hype Cycle

Of course, an influencer's job is to play into the hype cycle. And right now, that cycle, as the Verge notes, is fixated firmly on ChatGPT and other generative AI.

While it's unclear how much money (if any) the hustlers have actually made from the ChatGPT applications that they're preaching, these guys aren't actually in the business of becoming ChatGPT millionaires themselves. They make their cash through virality clout, clicks, eyeballs, and online courses, whether ChatGPT ends up generating significant cash for followers or not. In a gold rush, as they say, sell shovels.

And surprisingly, as the Verge also points out, pretty much any mention of AI ethics — whether you should tell clients you're using AI, for example, or if should be wary of AI-generated plagiarism — is noticeably missing from these videos. On the note of ethics, YouTube hustlers are also very unlikely to be liable if, say, the chatbot, which is notoriously terrible at math and finance, gives you bad investing advice.

It's all a little depressing, and not just because people like Bowles are promising that anything ChatGPT-generated "is perfect," which is an outright falsehood, in the same breath that they say you no longer need to develop "the skill" because you "have the AI." It's depressing because these videos are flaming hot garbage, and only beget the creation of more flaming hot — and AI-generated — garbage.

Sigh. See you all on the other side of SEO spam hell.

READ MORE: Hustle bros are jumping on the AI bandwagon [The Verge]

More on ChatGPT: ChatGPT Is Freakishly Good at Spitting out Misinformation on Purpose. . Scammers have a powerful new weapon to wield this Valentine's Day when drafting love letters to whisper sweet nothings at scale: artificial intelligence such as ChatGPT.

The tactic of using fake online identities to gain victims' affection and trust for financial gain is known as romance scams. While it sounds skeptical that humans can be tricked into machine-generated love messages, security company McAfee's Modern Love Research Report showed that 7 in 10 people failed to distinguish if AI wrote a love letter.



Specifically, to conduct the research, the team presented a ChatGPT love letter to more than 5,000 people worldwide, and it turns out that 33% of the people thought that a person wrote the letter, while 36% said they could not distinguish one way or another.

My Dearest,

The moment I laid eyes on you, I knew that my heart would forever be yours. Your beauty, both inside and out, is unmatched and your kind and loving spirit only adds to my admiration for you.



You are my heart, my soul, my everything. I cannot imagine a life without you, and I will do everything in my power to make you happy. I love you now and forever...



With the ease of use around artificial intelligence, particularly tools such as ChatGPT that anybody with a web browser can easily access, scammers can use the technology to leverage malicious activity, especially during Valentine's Day to target those people who have dropped their guard while looking for love, said Steve Grobman, chief technology officer at McAfee.

Besides ChatGPT, deepfake images, AI-generated conversations, and emotion analysis will soon be performed at a large scale, making current romance scams into a mild warm-up to the AI-driven onslaught, Bud Broomhead, chief executive of Viakoo, added.

AI-assisted social engineering tactics

The Federal Trade Commission (FTC) and Federal Bureau of Investigation (FBI) issued romance scam warnings last week as Valentine's Day approached.

Romance scams have caused one of the highest amounts of financial losses compared with other online crimes. According to the FTC's latest data, consumer-reported losses hit a record high of $1.3 million in 2022, increasing by nearly 138% from 2021.

This massive spike in losses comes as scammers continue advancing their attacking techniques. For example, the FBI has warned of a rise in a new tactic called 'pig butchering' over the past few months, in which scammers convince the victims to invest in fraudulent cryptocurrency platforms after developing long-term relationships with victims and winning their trust on social media and dating applications.

Over the past three years, there has also been an eightfold surge in sextortion — a practice of convincing victims to share explicit photos and then threatening to reveal them with victims' social media contacts, according to FTC.

Fight scams with modern identity verification

Michael Jabbara, vice president of fraud services at Visa, said awareness and detection are the most critical tools in mitigating the threat of romance scams, and it takes all parties, from consumers, businesses, and financial services, to participate.

"In addition to consumers taking action to protect their personal information online, financial services organizations and other companies should invest in cutting-edge technologies such as AI and ML-powered solutions to detect and thwart fraudulent trends, like romance scams. For example, Visa’s Advanced Authorization uses various AI and ML techniques to determine the likelihood that a given transaction is fraudulent within 300 milliseconds," said Jabbara.

Bala Kumar, chief product officer at Jumio, added that dating sites and apps should apply modern identity verification technologies, such as document-centric identity proofing, to leverage the power of biometrics and AI and better protect their users.

"Many dating sites still leverage traditional identity verification methods, like knowledge-based authentication (KBA), where users are asked to answer specific security questions. Such methods are no longer considered secure since so much of our personal data can be accessed through hacking and legally through data aggregators," said Kumar.

Some applications have upgraded their identity verification methods — Hinge has leveraged video selfie verification to confirm user identity, while Meta has tested similar tools for Facebook Dating to verify users' ages.. The best part of the year for lovers is getting nearer by the day! We are, of course, referring to Valentine's Day. While you are searching for the best gifts for your loved one, do pay heed to a few dangers that may well spoil your big day. There are a number of threats online and you must ensure you are aware of them to know what action to take, if the need arises. Helping you in this endeavour is McAfee, which has shared its Modern Love Report highlighting how ‘Artificial Intelligence (AI) is Playing Cupid with 78% Indian Adults Falling for ChatGPT Love Letter'.

McAfee's study provides new evidence of both the benefits and potential downsides of AI tools like ChatGPT as they increasingly become mainstream and, how AI and the internet is changing the future of love and relationships. Cybercriminals may take advantage of such tools to prey on vulnerable people simply looking for love. Therefore, it is vital to understand whether people are able to truly distinguish between an AI and human.

In fact, AI's entrance to the online dating world indicates new concerns over romance fraud with 76% of Indian adults claiming they, or someone they know, has been catfished.

McAfee, a global leader in online protection, has released exclusive research on the astonishing impact AI will have on communication ahead of Valentine's Day. With ChatGPT starting to infiltrate every aspect of our daily lives, 78% of Indian people surveyed were unable to tell the difference between a love letter written by AI tool, ChatGPT, and one written by a human being. McAfee's Modern Love research report surveyed 5,000 people across nine countries to discover how AI and the internet is changing love and relationships.

McAfee's study provides new evidence of both the benefits and potential downsides of AI as tools like ChatGPT become mainstream. Interestingly, 62% of Indian adults are planning to use AI to help write their love letters this Valentine's Day, the highest out of all the countries surveyed, with 73% also using AI to boost their dating profiles.

“With the rise in popularity of artificial intelligence, particularly tools such as ChatGPT that anybody with a web browser can access, the chances of receiving machine-generated information are on the rise. While some AI use cases may be innocent enough, we know cybercriminals also use AI to scale malicious activity,” said Steve Grobman, McAfee Chief Technology Officer.

“With Valentine's Day around the corner, it's important to stay vigilant and use security solutions that can help safeguard your privacy and identity and protect you from clicking on malicious links a scammer might send. It is easy to drop your guard when chatting to a potential partner but it's important to be on alert if you're asked suspicious requests for money or personal information,” concluded Grobman.

Who Will Win Your Love: Human Versus Machine

While using AI bots to help those who feel ill-equipped to express their feelings might seem like a harmless use of an emerging technology, it demonstrates the growing challenges people face in identifying whether information received online is from a person or a machine. The vast majority (60%) of Indian respondents preferred a machine-generated love note in the style of e.e. cummings to his original 1952 poem ‘i carry your heart with me'.

The most popular reason given for using AI as a ghostwriter was that it would make the sender feel more confident (59%), others cited lack of time (32%) or lack of inspiration (26%), while 14% said it would just be quicker and easier, believing they would not be found out. However, 57% of respondents said they would be offended if they found out the note they had received was produced by a machine.

Stranger Danger

Using a dating site or social media channel is a convenient and proven way to meet a new love interest – but as McAfee's study shows, it's also never been more difficult to tell the difference between real and fake. In fact, 76% of Indians admit to being catfished – which is when somebody pretends to be someone they're not online – or knowing somebody who has. Additionally, 89% of Indian respondents have engaged in direct conversations with strangers on social media across platforms, compared to 66% globally. The most common platforms a stranger is likely to use to reach Indian adults are Instagram (64%), WhatsApp (59%) and Facebook (51%).

Those looking for love are often more vulnerable to scams, and cybercriminals use that to their advantage, engaging in long, sophisticated attempts to steal from their victims. With 76% of Indian respondents who have engaged with a stranger online, also having been asked to transfer money, which is always a red flag, particularly if you've never met them in person.

Personal details can be just as valuable to criminals these days. Information like place of birth or passport information can be extracted over time and used to access online banking or potentially even sold on the dark web.

Fortunately, a lot of people are already extremely vigilant when interacting with strangers online. Requests for personally identifiable info such as date of birth, email ID, home address, intimate photos, or videos most often give scammers away. Not being able to meet in person or do video chats (43%), fake profile photos (41%), requests for personal information (41%) alerted them to being catfished.

What to Do to Protect Yourself:. . The rise of the ChatGPT artificial intelligence chatbot has created a buzz in the tech world, and scammers are trying to exploit this trend by issuing fake tokens that are branded with the ChatGPT name.

These tokens have no official association with the actual tool, and the scammers are taking advantage of the hype to make a quick profit.

The number of these fake tokens has risen to hundreds, with 132 issued on BNB BNB/USD Chain, 25 on Ethereum ETH/USD, and 10 on other blockchains, including Solana SOL/USD, Arbitrum, OKChain, and Cronos CRO/USD.

While Microsoft has integrated OpenAI's chatbots for its search services, its own chatbot is different from ChatGPT. Unfortunately, scammers have created fake tokens, such as "BingChatGPT," and even seeded them with liquidity. Despite numerous red flags, some of these tokens are seeing high trading volumes and thousands of dollars in profits.

Also read: As Bitcoin Hits $25,000 Ceiling, Experts Say Investors Turning To Crypto As A Safe Haven

Blockchain security firm Peckshield has discovered dozens of newly-created BingChatGPT tokens, some of which appear to be honeypots or have high sell tax, meaning that a user may be giving up a large portion of their funds unknowingly. Some of these tokens have already dropped by over 99%, and the issuer of these fake tokens has deployed dozens of tokens using a pump-and-dump scheme.

According to data from DEXTools, there are now over 170 ChatGPT-branded tokens issued on decentralized exchanges such as Uniswap and PancakeSwap.

The most popular token has a market capitalization of over $250 million and is issued on Ethereum, with over 300 unique holders and $600,000 in liquidity.

Another version of the token, issued on the BNB Chain, has $246,000 in liquidity and a market capitalization of $24 million.

Next: Dog Fight: Floki And Baby Doge Coin Compete For Meme Coin Crown As 1 Surges 396% In 30 Days. “Many of these scammers operate out of countries where English is not the first language," said Dr Edmund WJ Lee, Assistant Professor at the Wee Kim Wee School of Communication and Information.

"They could simply ask chatbots to rewrite their content, and this is certainly much simpler and requires less effort than ‘googling’,” he told TODAY.

CYBER ATTACK AIDED BY CHATGPT LIKELY WITHIN A YEAR: STUDY

A study released earlier this year by BlackBerry, a software and cybersecurity company with headquarters in Canada, found that 51 per cent of 1,500 IT professionals across multiple countries surveyed believe that a successful cyberattack will be credited to ChatGPT in less than a year.

BlackBerry’s threat research and intelligence team identified Singapore as being in the top 10 countries in terms of the most cyberattacks.

In just 90 days between Sept 1 and Nov 30, 2022, the firm's AI cybersecurity software stopped about 10,300 malware-based cyberattacks in Singapore, which translate to about 113 per day or almost five per hour.

That was out of a total of about 1.75 million such attacks worldwide in that period.

In an interview with TODAY, Mr Jonathan Jackson, a BlackBerry director of engineering (Asia Pacific), said it is "only a matter of time" before Singapore suffers a cyberattack using Chat GPT.

“While there haven’t been any known reported cases (of cyberattacks due to ChatGPT) in Singapore yet… popular underground forums are full of threat actors sharing samples of code to encrypt data, steal information and evade detection, all by using ChatGPT’s capability."

CHATBOTS OFFER PLENTY OF SCOPE FOR SCAMMERS, HACKERS

One expert cited an example of ChatGPT’s use in an actual scam.

“There have been reports of ChatGPT being used to generate phishing emails… there have also been reports in underground forums where some attackers created a password stealer in Python with ChatGPT,” said Mr Stas Protassov, co-founder of Acronis, a cybersecurity and data protection company.

However, in general, the experts said that sophisticated AI tools such as ChatGPT and Bard are still in their early stages.

That means that cybercriminals are still figuring out how to use them and that various unforeseen devious techniques employing these chatbots could emerge in the months ahead.

Phishing – Sophisticated AI chatbots can be used to create highly convincing phishing messages, which are messages that mimic legitimate sources to trick people into revealing sensitive information, such as banking login details. This can even be personalised, as ChatGPT can generate text in a wide variety of languages.

Impersonation – As ChatGPT has been trained by large amounts of data and details, it could be used to impersonate specific individuals or organisations to convince people into falling for scams, such as urgent requests for money from a long-lost friend, or a request for a company password from your boss.

Fake news – AI language models can generate misleading or false information to manipulate public opinion or cause harm. For example, ChatGPT could produce dubious content or even create entire fake news sites within minutes.

Hacking – ChatGPT could analyse the source code of hacking targets, such as websites, to find weaknesses to exploit. It could also create automated scripts, such as "brute forcing" various leaked or hacked credentials, until one works.

IDENTIFYING AI CONTENT PROBLEMATIC: RESEARCH PROJECT

A research project published in January this year by researchers from WithSecure, a cybersecurity company headquartered in Finland, concluded that identifying malicious content written by AI would be a problem.

The project explored how AI-language generation tools could be misused and tested ChatGPT on a variety of possible uses, such as creating phishing content and fake news articles.

The researchers conducted experiments that proved that "large language models" such as Chat GPT could be used:

To craft email threads suitable for phishing attacks

To "deepfake" or very effectively mimic a person's writing style

To incorporate opinion into existing written content

To craft convincing fake articles even if relevant information is not included in the model's training data

WHAT CAN BE DONE

While there is a high possibility of ChatGPT to be used in various ways to make scams and hacks more threatening, there is some good news.

“Arguably, we are still at the early adopters stage, even cybercriminals are still figuring out how to use it."

"This means that there is a small window of opportunity for policy makers, tech developers… all parts of society to get ahead, to ensure that loopholes of such chatbots cannot be weaponised and put in cybersecurity and policy-level safeguards,” said Dr Lee.

The experts also pointed out that in general, scams mostly relied on individuals making decisions in a rush, due to panic or the fear of missing out.

For example, someone claiming to know you, who needs you to send them money because of an emergency.

As such, they advised people to slow down and check, to watch out for the names of email headers and URLs, and to verify sources of information.

Additionally, cybersecurity apps like ScamShield, which blocks scam messages and calls, can be downloaded.

Two-factor authentication, which makes sure that users can only access websites or applications after verification from two factors, like through a phone and a laptop, can also be used to further increase security.

Of the risks posed by ChatGPT, Mr Protassov said: “On the other hand, OpenAI constantly updates and adjusts ChatGPT to reduce the likelihood of it being used to cause harm, both in physical and virtual worlds.”. Image: Getty/BongkarnThanyakij

Business email compromise (BEC) gangs are making their payment fraud scams even more effective by using translation tools and machine learning to convincingly distribute emails in multiple languages.

BEC attacks are some of the most effective and financially damaging cyberattacks, with the FBI estimating that they have cost businesses a combined total of more than $43 billion in recent years.

In Depth: These experts are racing to protect AI from hackers. Time is running out

For cyber criminals, they're a relatively simple but effective means of making money; the only requirements are an email account to send the initial phishing emails, plus some research on the desired targets.

The scammers pose as a trusted contact, like your boss, a colleague, or a business supplier, usually requesting that an urgent or important financial transfer must be made. The plan is to trick the victim into making the financial transfer into an account owned by the scammers.

If the payment is made, the fraudsters take the money and run -- so, even if the payment is recognized as fraudulent in hindsight, there's not much that can be done because the money is gone.

The attacks don't require malware or phishing links, just email and a bit of social engineering, making it difficult for them to be detected by some antivirus software -- which is one of the reasons they're so effective.

Also: ChatGPT and more: What AI chatbots mean for the future of cybersecurity

Scammers don't even need to speak the language of the people or organizations they're targeting: analysis of some prolific BEC campaigns by researchers at Abnormal Security suggests that email fraudsters are turning to machine learning-powered translation tools like Google Translate to help compose emails used in the attacks.

This technique is enabling widespread BEC campaigns for an expanded array of cyber-criminal groups, who can cast a larger net at minimal cost.

"Attacking targets across various regions and using multiple languages is nothing new. However, in the past, these attacks were perpetrated mainly by sophisticated organizations with bigger budgets and more advanced resources," said Crane Hassold, director of threat intelligence at Abnormal Security.

"For example, to effectively translate email text for more believable social engineering efforts, organizations often hire native speakers. But, as technology becomes more accessible and affordable, it lowers the barrier to entry," he added.

The payment fraud campaigns have been distributed in at least 13 different languages, including Danish, Dutch, Estonian, French, German, Hungarian, Italian, Norwegian, Polish, Portuguese, Spanish, and Swedish.

One campaign, by a group which researchers have dubbed Midnight Hedgehog, uses executive impersonation to deceive recipients into making payments for bogus services -- usually by posing as a company's CEO.

They conduct thorough research on their target -- finance managers or other executives responsible for initiating the company's financial transactions -- including their responsibilities and relationship with the CEO, as well as convincingly spoofing an email address that looks like it comes from the CEO.

In one example, the template involves the 'CEO' making an urgent request for a payment of between $17,000 to $45,000 to a company in the UK, with the email written in one of several different languages depending on the location and native tongue of the victim.

Another group, named Mandarin Capybara, uses similar techniques to distribute BEC scams in multiple languages. They also spoof executives, asking their victims to change payroll account information -- which, if implemented, will result in business transfers being sent to the account owned by cyber criminals.

Also: The scary future of the internet: How the tech of tomorrow will pose even bigger cybersecurity threats

In some cases, they've used the same spoofed email account to send out emails in multiple languages.

The reason cyber criminals continue to send out BEC campaigns like these is simply because they work; there are victims who see these messages, believe they're real and act upon instructions that they think are coming from their boss -- especially if they're written with correct spelling and grammar, and in the known style of the sender.

"As email marketing and translation tools become more accurate, effective, and accessible, we'll likely continue to see hackers exploiting them to scam companies with increasing success," said Hassold.

In addition to deploying appropriate cybersecurity tools to help catch BEC attacks, it's recommended that organizations have procedures in place to ensure that large financial transactions can't be made with the approval of just one person, and that people should be trained to be on the lookout for payment fraud attacks.

"It's important that organizations use email defenses that look for threats in a more holistic matter to be able to prevent more sophisticated BEC attacks. Defenses that simply rely on static or 'known bad' indicators will have a hard time detecting these attacks, which is why tools that leverage behavioral analytics are better equipped to spot more advanced BEC threats," said Hassold.

MORE ON CYBERSECURITY. Next Article

ChatGPT can be accessed only through the official website

How to stay safe from fake ChatGPT apps and scams

By Athik Saleh 05:06 pm Feb 24, 202305:06 pm

What's the story OpenAI's ChatGPT is the fastest-growing consumer app in history. The downside of popularity is that people will always try to take advantage of that. ChatGPT is not immune to that. Scammers are already using the chatbot as a front to trick unsuspecting users. Your money and private details are in danger. Continue reading to see how you can protect yourself from ChatGPT-based scams.

Context

Why does this story matter?

ChatGPT's growth into an internet sensation has been sudden and unprecedented. The AI chatbot's abilities have stunned users. The exponential increase in demand for the chatbot has overwhelmed OpenAI as well. However, this has opened an opportunity for those who want to take advantage of the situation. Since people are still getting used to AI, it is easy to exploit them.

Premium version

Scammers offer free, uninterrupted access to ChatGPT

OpenAI launched ChatGPT Plus earlier this month. The paid tier was introduced as a way to deal with the increasing demand for the AI chatbot. It targets those who want to use the chatbot without any restrictions. The introduction of the paid tier has opened a doorway for scammers. They trick users by offering free and uninterrupted access to ChatGPT.

Counterfeit websites

Scammers are creating fake desktop versions of ChatGPT

Scammers are offering a fake desktop version of ChatGPT. These phony versions of ChatGPT claim to provide unrestricted access. They also create Facebook pages to promote these counterfeit versions. When users click on the link in the Facebook post, they will be taken to a website that looks like ChatGPT. The website then prompts users to download a supposed website version of ChatGPT.

Malware

They are using malware to steal sensitive information

When users download the fake ChatGPT, they are actually installing malware on their devices. According to Kaspersky, scammers are using Fobo Trojan to steal the social media account credentials of those who fall victim to the trap. During the installation process, users will see an error message saying ChatGPT cannot be installed. By then, the trojan will be installed.

Fake apps

There are fake ChatGPT apps on Google Play

Apart from fake ChatGPT versions that are a front for cyberattacks, there are also fake ChatGPT apps in the market. Multiple such apps are available on Google Play and third-party Android app stores. These apps either trick users into installing malware or providing their account details. Now, let's see how we can protect ourselves from such scams and scammers.

Protection

There is only one way to access ChatGPT

Firstly, you need to know that ChatGPT can only be accessed via https://chat.openai.com/auth/login. Since there is no official API available, there is no way developers can create apps that utilize ChatGPT directly. Now, if you come across a fake ChatGPT app on either Google Play or App Store, report them. Also, do not download things without thinking twice or thrice.. Hundreds of books created by artificial intelligence (AI) tool ChatGPT are flooding Amazon, showing the way the technology can be adopted to produce books at scale.

Nearly 300 titles that claim to be written solely by or in collaboration with ChatGPT are listed on the online bookseller’s website, across a range of genres including non-fiction, fantasy and self-help.

Many of the books appear to be published using Amazon’s Kindle Direct Publishing tool, which allows users to quickly create, publish and promote their work using a modern-day equivalent of the self-publishing model.

It’s likely that more than the 300 or so books with ChatGPT listed as an author have been penned with the help of the generative AI tool, which uses a large language model [LLM] to mimic the way humans write, but have not been listed as using the tool.

An Amazon spokesperson said: “All books in the store must adhere to our content guidelines, including by complying with intellectual property rights and all other applicable laws.”

ChatGPT was released by San Francisco AI firm OpenAI in November 2022. Investment bank UBS estimates that 13 million people use the tool every day. It works by prompting the reader to ask it questions or set it tasks that are typed into a chat window, then responding in kind using natural language.

People have previously used ChatGPT and other generative AI tools that create images, rather than text, to produce children’s books. American tech worker Ammaar Reshi, 28, self-published a book on Amazon called Alice & Sparkle, which was solely generated using tools like ChatGPT based on his prompts. Reshi did not cite the technology as his co-authors when the book was published.

Reshi sold around 70 copies of Alice & Sparkle in the first 10 days it was on sale.

AI-generated text is becoming a significant issue for publishers as well as retailers. American science fiction magazine Clarkesworld has barred new story submissions after being deluged with AI-generated text. The magazine’s founding editor, Neil Clarke, decided to halt the ability to send in new work after he received 50 submissions he believes were generated using AI in a single day.

Clarke lambasted the low quality of the AI-produced work – something that other authors believe will continue to separate human-generated work from that produced by computers and AI tools.

“I think that bad writing that hedges its bets and tries to bamboozle the reader with high-end generalities – whether it’s machine-written or by humans – is still bad writing,” said Karl Whitney, author of two non-fiction books and an editor at Penguin Classics.

Earlier this month, Reshi, the author of Alice & Sparkle, asked Bing, the AI-powered search engine developed by Microsoft, what it thought of his work. The AI behind the search engine said: “I think it would be more of a novelty than a literary masterpiece.

The AI continued: “A book created with AI might be technically impressive, but it would not have the soul or depth of a book created by a human author.”

Whitney said the continued submission of low-quality AI work could ultimately cause publishers to raise the drawbridge on new submissions in the same way Clarkesworld did, ultimately shutting off a route into the publishing world to new authors.

“The slush pile [where unsolicited manuscripts end up] is an extremely useful way for unknown writers to get published and get noticed,” said Whitney. “If AI-generated texts crop up amongst those submissions in large volumes then trust could evaporate, editors have to sift this material for machine-generated material – or might just give up on this form of submission – and we potentially lose valuable new voices who might get lost in the mix.”

“Like with all of the potential changes generative creative AI brings, one of the big shifts I think will be to value human craftsmanship even more,” said Dr Catherine Flick, reader in computing and social responsibility at De Montfort University in Leicester. Flick has researched the rise of AI and how we use it.

“Ultimately, I suspect the low-quality books will join the other existing low-quality books that are currently ghost written, but there will be an increased quantity of them due to the lack of work required to publish an AI written book,” Flick added. “Unfortunately, this may well drown out good quality books that need to be self-published.”

Flick can foresee a future where AI tools can help authors write their books quicker, though they’ll struggle with the nuances required for plotting a well-written story or series of books.

“I foresee a big push to advertising that a book is human-written, with potentially a premium attached to that,” she said. “Whether this is a good shift or not is hard to tell at this stage, though I fully expect the first famous author to admit to using LLMs to be pilloried on social media as a cheat or a fraud.”

As to whether bookshops will stock them, Tom Rowley of Backstory, a London-based bookseller, said: “Not in a million years. I’d rather stock Jeff Bezos’s memoirs.”. . About 48% of Africans do not know what a deepfake is.

Seventy-four percent of respondents from Mauritius, Egypt, Botswana, South Africa, and Kenya fell for deepfakes.

Deepfakes as agents of disinformation and misinformation are a threat to African democracy.

A worrying 74% of working-age respondents from Mauritius, Egypt, Botswana, South Africa, and Kenya were duped by deepfake communications, according to a survey by security awareness company KnowBe4.

A deepfake is aptly described as false information spread on the internet via social media, email, or any other internet communication platform.

The survey was carried out on 800 people between the ages of 18 and 54 to ascertain their level of awareness of deepfakes.

According to the survey results, "just over 50% of respondents said they were aware of deepfakes, while 48% were unsure or had little understanding of what they were. While a significant percentage of respondents were not clear as to what a deepfake was".

The senior vice president of Content Strategy and Evangelist at KnowBe4 Africa, Anna Collard, noted WhatsApp messages from friends or colleagues were the most trusted among respondents, despite the fact they could be deepfakes.

"It is also important to note that nearly 67% of respondents would trust a message from a friend or legitimate contact on WhatsApp or a direct message while 43% would trust a video, 42% an email and 39% a voice note.

"Any one of these could be a fake that the trusted contact did not recognise or their account was hacked," she said.

READ | Zimbabwe approves 'draconian' law targeting civil society

Since the advent of Artificial Intelligence (AI), digital rights advocates have been on the lookout for deepfakes as a new threat to peaceful, free, and fair elections, particularly in Africa.

In 2018, the now defunct Cambridge Analytica was mired in a scandal where opposition parties in Kenya and Nigeria accused the British company of involvement in disinformation projects meant to affect election outcomes.

With elections due in Nigeria this weekend, then Zimbabwe, eSwatini, Madagascar, and other African countries later this year, deepfakes could cause harm.

"These deepfake platforms [ChatGPT and Stable Diffusion] are capable of creating civil and societal unrest when used to spread mis- or disinformation in political and election campaigns and remain a dangerous element in modern digital society.

"This is cause for concern and asks for more awareness and understanding among the public and policymakers," added Collard.

In Nigeria, the African Union's Commission on Human and People's Rights noted messaging around the shortage of fuel and confusion around the Central Bank's old naira notes was likely going to affect the smooth running of elections.

As such, the body called for "unhindered proactive access for information for fostering public trust".

Last week, an Israeli firm called "Team Jorge" by an investigating team of journalists under Forbidden Stories was exposed for allegedly hacking, sabotaging, and spreading disinformation in at least 30 elections around the world, with 60% of those in Africa.

Some of the targets mentioned by "Team Jorge" to Forbidden Stories were two aides working in the campaign team of Kenya's William Ruto who became president in August last year.

The News24 Africa Desk is supported by the Hanns Seidel Foundation. The stories produced through the Africa Desk and the opinions and statements that may be contained herein do not reflect those of the Hanns Seidel Foundation.. NEW YORK--(BUSINESS WIRE)--ID R&D today introduced IDLive® Face Plus to address the growing security threat posed to institutions and consumers by deepfakes and digital injection attacks. As deepfakes have become more realistic and easier to produce, injection attacks are an increasingly common method used to spoof biometrics.

In 2023, 20 percent of successful account takeover attacks will leverage deepfakes (Gartner).

ID R&D’s latest innovation arrives at a time when new survey data from the company shows 91 percent of businesses acknowledge that they or their customers are concerned about deepfake fraud, with 42 percent having encountered deepfake attacks. Also, 84 percent of the same respondents express similar concern about injection attacks, with 37 percent having firsthand exposure. More than half (51 percent) state that chatbot-enabled fraud is on their radar as a credible threat.

IDLive Face Plus adds detection of digital injection attacks, where fraudsters use hardware and software hacks to substitute a biometric capture with fake digital imagery. It is the first known product to help ensure a proper selfie capture on both desktops and mobile devices with a frictionless approach that eliminates user abandonments caused by complicated security checks.

“ID R&D’s innovative approach to combating deepfakes will help us stay ahead of the increasing realism of digitally-created faces without burdening people with security tasks,” commented Alexey Khitrov, CEO and Co-Founder of ID R&D. “We are laser-focused on helping our customers and partners leverage biometrics to their full potential to make their digital identity solutions not only more secure, but easy to use.”

How it works

Identity verification and authentication techniques that use facial recognition rely on liveness detection to ensure that a live human face is in front of the camera during biometric capture. While presentation attacks involve “presenting” non-live imagery to the camera, such as photos on paper or digital screens, injection attacks use hardware and software hacks to bypass the camera altogether. Without countermeasures, fraudsters can “inject” digital facial imagery into a capture device in a way that defeats certain liveness detection measures.

ID R&D’s research-driven, “frictionless first” approach to fighting fraud continues to yield innovative solutions for their partners that avoid the pitfalls of techniques that inconvenience people and cause abandonments during onboarding and login. IDLive Face Plus draws upon a wealth of data and expertise in computer vision and deep neural networks while advancing the mission of making security transparent to users.

Visit us at Mobile World Congress and learn more

ID R&D will demonstrate IDLive Face Plus in Hall 6 Stand E59 at Mobile World Congress, taking place in Barcelona from February 27 through March 2.

About ID R&D

ID R&D, a Mitek company, is an award-winning provider of AI-based voice and face biometrics and liveness detection. With one of the strongest R&D teams in the industry, ID R&D consistently delivers innovative, best-in-class biometric capabilities that raise the bar in terms of usability and performance. Our proven products have achieved superior results in industry-leading challenges, third-party testing, and real-world deployments in more than 70 countries. ID R&D’s solutions are available for easy integration with mobile, web, messaging, and telephone channels, as well as in smart speakers, set-top boxes, and other IoT devices. ID R&D is based in New York, NY. Learn more at www.idrnd.ai.

About Mitek

Mitek (NASDAQ: MITK) is a global leader in digital access, founded to bridge the physical and digital worlds. Mitek’s advanced identity verification technologies and global platform make digital access faster and more secure than ever, providing companies new levels of control, deployment ease and operation, while protecting the entire customer journey. Trusted by 99% of U.S. banks for mobile check deposits and 7,500 of the world’s largest organizations, Mitek helps companies reduce risk and meet regulatory requirements. Learn more at www.miteksystems.com. Follow Mitek on LinkedIn, Twitter and YouTube, and read Mitek’s latest blog posts here.. . Lisez cet article en français.

Five days after Russia launched its full-scale invasion of Ukraine, a year ago this week, US-based facial recognition company Clearview AI offered the Ukrainian government free access to its technology, suggesting that it could be used to reunite families, identify Russian operatives, and fight misinformation. Soon afterward, the Ukraine government revealed it was using the technology to scan the faces of dead Russian soldiers to identify their bodies and notify their families. By December 2022, Mykhailo Fedorov, Ukraine's vice prime minister and minister of digital transformation, was tweeting a picture of himself with Clearview AI’s CEO Hoan Ton-That, thanking the company for its support.

Accounting for the dead and letting families know the fate of their relatives is a human rights imperative written into international treaties, protocols, and laws like the Geneva Conventions and the International Committee of the Red Cross’ (ICRC) Guiding Principles for Dignified Management of the Dead. It is also tied to much deeper obligations. Caring for the dead is among the most ancient human practices, one that makes us human, as much as language and the capacity for self-reflection. Historian Thomas Laqueur, in his epic meditation, The Work of the Dead, writes that “as far back as people have discussed the subject, care of the dead has been regarded as foundational—of religion, of the polity, of the clan, of the tribe, of the capacity to mourn, of an understanding of the finitude of life, of civilization itself.” But identifying the dead using facial recognition technology uses the moral weight of this type of care to authorize a technology that raises grave human rights concerns.

In Ukraine, the bloodiest war in Europe since World War II, facial recognition may seem to be just another tool brought to the grim task of identifying the fallen, along with digitizing morgue records, mobile DNA labs, and exhuming mass graves.

But does it work? Ton-That says his company’s technology “works effectively regardless of facial damage that may have occurred to a deceased person.” There is little research to support this assertion, but authors of one small study found results “promising” even for faces in states of decomposition. However, forensic anthropologist Luis Fondebrider, former head of forensic services for the ICRC, who has worked in conflict zones around the world, casts doubt on these claims. “This technology lacks scientific credibility,” he says. “It is absolutely not widely accepted by the forensic community.” (DNA identification remains the gold standard.) The field of forensics “understands technology and the importance of new developments” but the rush to use facial recognition is “a combination of politics and business with very little science,” in Fondebrider’s view. “There are no magic solutions for identification,” he says.

Using an unproven technology to identify fallen soldiers could lead to mistakes and traumatize families. But even if the forensic use of facial recognition technology were backed up by scientific evidence, it should not be used to name the dead. It is too dangerous for the living.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Organizations including Amnesty International, the Electronic Frontier Foundation, the Surveillance Technology Oversight Project, and the Immigrant Defense Project have declared facial recognition technology a form of mass surveillance that menaces privacy, amplifies racist policing, threatens the right to protest, and can lead to wrongful arrest. Damini Satija, head of Amnesty International’s Algorithmic Accountability Lab and deputy director of Amnesty Tech, says that facial recognition technology undermines human rights by “reproducing structural discrimination at scale and automating and entrenching existing societal inequities.” In Russia, facial recognition technology is being used to quash political dissent. It fails to meet legal and ethical standards when used in law enforcement in the UK and US, and is weaponized against marginalized communities around the world.

Clearview AI, which primarily sells its wares to police, has one of the largest known databases of facial photos, at 20 billion images, with plans to collect an additional 100 billion images—equivalent to 14 photos for every person on the planet. The company has promised investors that soon “almost everyone in the world will be identifiable.” Regulators in Italy, Australia, UK, and France have declared Clearview’s database illegal and ordered the company to delete their citizens’ photos. In the EU, Reclaim Your Face, a coalition of more than 40 civil society organizations, has called for a complete ban on facial recognition technology.

AI ethics researcher Stephanie Hare says Ukraine is “using a tool, and promoting a company and CEO, who have not only behaved unethically but illegally.” She conjectures that it’s a case of “the end justifies the means,” but asks, “Why is it so important that Ukraine is able to identify dead Russian soldiers using Clearview AI? How is this essential to defending Ukraine or winning the war?”. Is the clip stupid or terrifying? I can’t decide. To be honest, it’s a bit of both.

“I just think I would love to get Ratatouille’d,” a familiar-sounding voice begins.

“Ratatouille’d?” asks another recognizable voice.

“Like, have a little guy up there,” the first voice replies. “You know, making me cook delicious meals.”

It sounds like Joe Rogan and Ben Shapiro, two of podcasting’s biggest, most recognizable voices, bantering over the potential real-world execution of the Pixar movie’s premise. A circular argument ensues. What constitutes “getting Ratatouille’d” in the first place? Do the rat’s powers extend beyond the kitchen?

Read: Of gods and machines

A friend recently sent me the audio of this mind-numbing exchange. I let out a belly laugh, then promptly texted it to several other people—including a guy who once sheepishly told me that he regularly listens to The Joe Rogan Experience.

“Is this real?” he texted back.

They’re AI voices, I told him.

“Whoa. That’s insane,” he said. “Politics is going to get wild.”

I haven’t stopped thinking about how right he is. The voices in that clip, while not perfect replicants of their subjects, are deeply convincing in an uncanny-valley sort of way. “Rogan” has real-world Joe Rogan’s familiar inflection, his half-stoned curiosity. “Shapiro,” for his part, is there with rapid-fire responses and his trademark scoff.

Last week, I reached out to Zach Silberberg, who created the clip using an online tool from the Silicon Valley start-up ElevenLabs. “Eleven brings the most compelling, rich and lifelike voices to creators and publishers seeking the ultimate tools for storytelling,” the firm’s website boasts. The word storytelling is doing a lot of work in that sentence. When does storytelling cross over into disinformation or propaganda?

I asked Silberberg if we could sit down in person to talk about the implications of his viral joke. Though he didn’t engineer the product, he had already seemed to master it in a way few others had. Would bad actors soon follow his lead? Did he care? Was it his responsibility to care?

Silberberg is in his late 20s and works in television in New York City. On the morning of our meeting, he shuffled into a TriBeCa coffee shop in a tattered sweater with an upside-down Bart Simpson stitched on the front. He told me how he had been busy making other—in his words—“stupid” clips. In one, an AI version of President Joe Biden informs his fellow Americans that, after watching the 2011 Cameron Crowe flop, We Bought a Zoo, he, Biden, also bought a zoo. In another, AI Biden says the reason he has yet to visit the site of the East Palestine, Ohio, train derailment is because he got lost on the island from Lost. While neither piece of audio features Biden stuttering or word-switching, as he often does when public speaking, both clips have the distinct Biden cadence, those familiar rises and falls. The scripts, too, have an unmistakable Biden folksiness to them.

“The reason I think these are funny is because you know they’re fake,” Silberberg told me. He said the Rogan-Shapiro conversation took him roughly an hour and a half to produce—it was meant to be a joke, not some well-crafted attempt at tricking people. When I informed him that my Rogan-listening friend initially thought the Ratatouille clip was authentic, Silberberg freaked out: “No! God, no!” he said with a cringe. “That, to me, is fucked up.” He shook his head. “I’m trying to not fall into that, because I’m making it so outlandish,” he said. “I don’t ever want to create a thing that could be mistaken for real.” Like so much involving AI these past few months, it seemed to already be too late.

Read: Is this the start of an AI takeover?

What if, instead of a sitting president talking about how he regrets buying a zoo, a voice that sounded enough like Biden’s was “caught on tape” saying something much more nefarious? Any number of Big Lie talking points would instantly drive a news cycle. Imagine a convincing AI voice talking about ballot harvesting, or hacked voting machines; voters who are conspiracy-minded would be validated, while others might simply be confused. And what if the accused public figure—Biden, or anyone, for that matter—couldn’t immediately prove that a viral, potentially career-ending clip was fake?

One of the major political scandals of the past quarter century involved a sketchy recording of a disembodied voice. “When you’re a star, they let you do it,” future President Donald Trump proclaimed. (You know the rest.) That clip was real. Trump, being Trump, survived the scandal, and went on to the White House.

But, given the arsenal of public-facing AI tools seizing the internet—including the voice generator that Silberberg and other shitposters have been playing around with—how easy would it be for a bad actor to create a piece of Access Hollywood–style audio in the run-up to the next election? And what if said clip was created with a TV writer’s touch? Five years ago, Jordan Peele went viral with an AI video of then-President Barack Obama saying “Killmonger was right,” “Ben Carson is in the sunken place,” and “President Trump is a total and complete dipshit.” The voice was close, but not that close. And because it was a video, the strange mouth movements were a dead giveaway that the clip was fake. AI audio clips are potentially much more menacing because the audience has fewer context clues to work with. “It doesn’t take a lot, which is the scary thing,” Silberberg said.

He discovered that the AI seems to produce more convincing work when processing just a few words of dialogue at a time. The Rogan-Shapiro clip was successful because of the “Who’s on first?” back-and-forth aspect of it. He downloaded existing audio samples from each podcast host’s massive online archive—three from Shapiro, two from Rogan—uploaded them to ElevenLabs’ website, then input his own script. This is the point where most amateurs will likely fail in their trolling. For a clip to land, even a clear piece of satire, the subject’s diction has to be both believable and familiar. You need to nail the Biden-isms. The shorter the sentences, the less time the listener has to question the validity of the voice. Plus, Silberberg learned, the more you type, the more likely the AI voices will string phrases together with flawed punctuation or other awkward vocal flourishes. Sticking to quick snippets makes it easier to retry certain lines of the script to perfect the specific inflection, rather than having to trudge through a whole paragraph of dialogue. But this is just where we are today, 21 months before the next federal elections. It’s going to get better, and scarier, very fast.

If it seems like AI is everywhere all at once right now, swallowing both our attention and the internet, that’s because it is. While transcribing my interview with Silberberg in a Google doc, Google’s own AI began suggesting upcoming words in our conversation as I typed. Many of the fill-ins were close, but not entirely accurate; I ignored them. On Monday, Mark Zuckerberg said he was creating “a new top-level product group at Meta focused on generative AI to turbocharge our work in this area.” This news came just weeks after Kevin Roose, of The New York Times, published a widely read story about how he had provoked Microsoft’s Bing AI tool into saying a range of unsettling, emotionally charged statements. A couple of weeks before that, the DJ David Guetta revealed that he had used an AI version of Eminem’s voice in a live performance—lyrics that the real-life Eminem had never rapped. Elsewhere last month, the editor of the science-fiction magazine Clarkesworld said he had stopped accepting submissions because too many of them appeared to be AI-generated texts.

Derek Thompson: The AI disaster scenario

This past Sunday, Sam Altman, the CEO of OpenAI, the company behind the ChatGPT AI tool, cryptically tweeted, “A new version of Moore’s Law that could start soon: the amount of intelligence in the universe doubles every 18 months.” Altman is 37 years old, meaning he’s of the generation that remembers living some daily life without a computer. Silberberg’s generation, the one after Altman’s, does not, and that cohort is already embracing AI faster than the rest of us.

Like a lot of PEOPLE, I first encountered a “naturalistic” AI voice when watching last year’s otherwise excellent Anthony Bourdain documentary, Roadrunner. News of the filmmakers’ curious decision to include a brief, fake voice-over from the late Bourdain dominated the media coverage of the movie and, for some viewers, made it distracting to watch at all. (You may have found yourself always listening for “the moment.”) They had so much material to work with, including hours of actual Bourdain narration. What did faking a brief moment really accomplish? And why didn’t they disclose it to viewers?

“My opinion is that, blanket statement, the use of AI technology is pretty bleak,” Silberberg said. “The way that it is headed is scary. And it is already replacing artists, and is already creating really fucked-up, gross scenarios.”

A brief survey of those scenarios that have already come into existence: an AI version of Emma Watson reading Mein Kampf, an AI Bill Gates “revealing” that the coronavirus vaccine causes AIDS, an AI Biden attacking transgender individuals. Reporters at The Verge created their own AI Biden to announce the invasion of Russia and validate one of the most toxic conspiracy theories of our time.

The problem, essentially, is that far too many people find the cruel, nihilistic examples just as funny as Silberberg’s absurd, low-stakes mastery of the form. He told me that as the Ratatouille clip began to go viral, he muted his own tweet, so he still doesn’t know just how far and wide it has gone. A bot notified him that Twitter’s owner, Elon Musk, “liked” the video. Shapiro, for his part, posted “LMFAO” and a laughing-crying emoji over another Twitter account’s carbon copy of Silberberg’s clip. As he and I talked about the implications of his work that morning, he seemed to grow more and more concerned.

“I’m already in weird ethical waters, because I’m using people’s voices without their consent. But they’re public figures, political figures, or public commentators,” he said. “These are questions that I’m grappling with—these are things that I haven’t fully thought through all the way to the end, where I’m like, ‘Oh yeah, maybe I should not even have done this. Maybe I shouldn’t have even touched these tools, because it’s reinforcing the idea that they’re useful.’ Or maybe someone saw the Ratatouille video and was like, ‘Oh, I can do this? Let me do this.’ And I’ve exposed a bunch of right-wing Rogan fans to the idea that they can deepfake a public figure. And that to me is scary. That’s not my goal. My goal is to make people chuckle. My goal is to make people have a little giggle.”

Neither the White House nor ElevenLabs responded to my request for comment on the potential effects of these videos on American politics. Several weeks ago, after the first round of trolls used Eleven’s technology for what the company described as “malicious purposes,” Eleven responded with a lengthy tweet thread of steps it was taking to curb abuse. Although most of it was boilerplate, one notable change was restricting the creation of new voice clones to paid users only, under the thinking that a person supplying a credit-card number is less likely to troll.

Near the end of our conversation, Silberberg took a stab at optimism. “As these tools progress, countermeasures will also progress to be able to detect these tools. ChatGPT started gaining popularity, and within days someone had written a thing that could detect whether something was ChatGPT,” he said. But then he thought more about the future: “I think as soon as you’re trying to trick someone, you’re trying to take someone’s job, you’re trying to reinforce a political agenda—you know, you can satirize something, but the instant you’re trying to convince someone it’s real, it chills me. It shakes me to my very core.”

On its website, Eleven still proudly advertises its “uncanny quality,” bragging that its model “is built to grasp the logic and emotions behind words.” Soon, the unsettling uncanny-valley element may be replaced by something indistinguishable from human intonation. And then even the funny stuff, like Silberberg’s work, may stop making us laugh.. AI models designed to closely simulate a person’s voice are making it easier for bad actors to mimic loved ones and scam vulnerable people out of thousands of dollars, The Washington Post reported.

Quickly evolving in sophistication, some AI voice-generating software requires just a few sentences of audio to convincingly produce speech that conveys the sound and emotional tone of a speaker’s voice, while other options need as little as three seconds. For those targeted—which is often the elderly, the Post reported—it can be increasingly difficult to detect when a voice is inauthentic, even when the emergency circumstances described by scammers seem implausible.

Tech advancements seemingly make it easier to prey on people’s worst fears and spook victims who told the Post they felt “visceral horror” hearing what sounded like direct pleas from friends or family members in dire need of help. One couple sent $15,000 through a bitcoin terminal to a scammer after believing they had spoken to their son. The AI-generated voice told them that he needed legal fees after being involved in a car accident that killed a US diplomat.

According to the Federal Trade Commission, so-called impostor scams are extremely common in the United States. It was the most frequent type of fraud reported in 2022 and generated the second-highest losses for those targeted. Out of 36,000 reports, more than 5,000 victims were scammed out of $11 million over the phone.

Because these impostor scams can be run from anywhere in the world, it’s extremely challenging for authorities to crack down on them and reverse the worrying trend, the Post reported. Not only is it hard to trace calls, identify scammers, and retrieve funds, but it’s also sometimes challenging to decide which agencies have jurisdiction to investigate individual cases when scammers are operating out of different countries. Even when it’s obvious which agency should investigate, some agencies are currently ill-equipped to handle the rising number of impersonations.

Ars could not immediately reach the FTC for comment. Will Maxson, an assistant director at the FTC’s division of marketing practices, told the Post that raising awareness of scams relying on AI voice simulators is likely consumers’ best defense currently. It’s recommended that any requests for cash be treated with skepticism. Before sending funds, try to contact the person who seems to be asking for help through methods other than a voice call.

Advertisement

Safeguards against AI voice impersonation

AI voice-modeling tools have been used to improve text-to-speech generation, create new possibilities for speech editing, and expand movie magic by cloning famous voices like Darth Vader’s. But the power of easily producing convincing voice simulations has already caused scandals, and no one knows who’s to blame when the tech is misused.

Earlier this year, there was backlash when some 4chan members made deepfake voices of celebrities making racist, offensive, or violent statements. At that point, it became clear that companies needed to consider adding more safeguards to prevent misuse of the technology, Vice reported—or potentially risk being held liable for causing substantial damage, like ruining the reputations of famous people.

The courts have not yet decided when or if companies will be held liable for harms caused by deepfake voice technology—or any of the other increasingly popular AI technology, like ChatGPT—where defamation and misinformation risks seem to be rising.

There may be increasing pressure on courts and regulators to get AI in check, though, as many companies seem to be releasing AI products without fully knowing the risks involved.

Right now, some companies seem unwilling to slow down releases of popular AI features, including controversial ones that allow users to emulate celebrity voices. Most recently, Microsoft rolled out a new feature during its Bing AI preview that can be used to emulate celebrities, Gizmodo reported. With this feature, Microsoft seems to be attempting to dodge any scandals by limiting what impostor celebrity voices can be prompted to say.

Microsoft did not respond to Ars’ request for comment on how well safeguards currently work to prevent the celebrity voice emulator from generating offensive speech. Gizmodo pointed out that, like many companies eager to benefit from the widespread fascination with AI tools, Microsoft relies on its millions of users to beta test its “still-dysfunctional AI,” which can seemingly still be used to generate controversial speech by presenting it as parody. Time will tell how effective any early solutions are in mitigating risks.

In 2021, the FTC released AI guidance, telling companies that products should “do more good than harm” and that companies should be prepared to hold themselves accountable for risks of using products. More recently, the FTC last month told companies, “You need to know about the reasonably foreseeable risks and impact of your AI product before putting it on the market.”. Image generated by Playground AI

Artificial Intelligence (AI) has been the talk of the town since OpenAI launched ChatGPT chatbot. The trend picked up pace when

announced 'new' Bing with chatbot capabilities. Last two months have seen several reports on AI models that can convert text input into images and imitate voices. While this may seem 'cool', a thorough look into the technology suggests that it can pose a security threat globally. Here we discuss some ways how people may fall into hackers' trap as AI becomes more sophisticated.

Catfishing

There are several texts to AI image generators available online. These can convert words into an image in seconds and if your description is detailed enough, you can get vibrant art that may be used to fool people or catfishing.

Catfishing is a deceptive technique in which a fictional/ fake social identity is created to target not-so-tech-savvy victims. It may be used for financial gains or to compromise a victim's privacy. Among various AI image generators are Qualcomm's Stable Diffusion, which can create an image using text input on a phone offline, DALL-E, DeepAI and Fotor.

Financial scams

As per a BBC report, hackers used AI-generated images of earthquakes in Turkey and Syria to trick people into donating to the 'fake' cause. Scammers channelled donations into their own accounts and wallets by offering links to fake photos of the devastation on social media. The report found that threat actors also posted catchy captions like "Let's help Turkey", "Pray for Turkey" and "Donate for earthquake victims" on their posts.

Disinformation by Deepfakes

Deepfakes have posed dangers for some time now. With technology becoming sophisticated over the years, the risks have only grown. A deepfake is synthetic media in which a person in an existing image or video is replaced with someone else's, such as a politician, an actor or social activist.

In one infamous incident, a fake video depicting Ukrainian President Volodymyr Zelenskyy circulated on social media. In the video, a deepfake of Zelenskyy was seen asking Ukrainians to surrender during Russian attack. There's also a video of 'fake' Morgan Freeman on YouTube.

Read Also Explained: How Microsoft made Bing AI better since its launch

AI voice tools

Earlier this year, Microsoft released the VALL-E AI tool that can replicate people's voices with a 3-second sample. While it may have several positive use cases, it poses a greater threat because scammers can use it to make spam calls sound real. Politicians or people with a huge social presence can also be impersonated. Furthermore, it may be used in cases where a voice password is needed.

Regulation needed

The use of such AI tools is limited as of now but given the implications, regulation may be needed to reduce such risks. It is also advised that users should tread carefully when interacting with social media posts as they could be scams disguised as social causes.. The voice you hear on the other end of your phone call may not be who you think it is, the person you're texting with could really be a bot, and the face in a photo or video on your favourite dating app might not even exist.

Technological advancements in artificial intelligence are creating the potential to fuel romance scams, said Jeff Clune, an associate professor of computer science at the University of British Columbia.

Scammers now have "more tools in their tool box to hoodwink people, especially people who are not aware of recent advances in technology," Clune said in an interview.

Such advancements include voice simulators, face generators and deepfakes — in which an existing image or video is used to create fake but believable video footage. Another set of advancements is chat bots, like ChatGPT, which generate humanlike text responses on all sorts of online platforms.

The Canadian Anti-Fraud Centre reported that romance scams skyrocketed during the massive shift online caused by the COVID-19 pandemic. It said the fraud schemes often involve convincing a victim to enter a virtual, online relationship in order to build trust and affection. Swindlers then use that emotional leverage to request money, cryptocurrency, gifts or investments.

The centre has warned that Valentine's Day provides an "opportunity for fraudsters to target Canadians looking for a relationship." Its latest available data revealed 1,928 reports of romance scams totalling more than $64.5 million in losses in 2021, a nearly 25 per cent jump from the year before.

Its Cyber Threat Assessment for 2023-24 flagged convincing deepfake technology and artificial intelligence, or AI, text generators as potential "threat actors."

"As deepfakes become harder to distinguish from genuine content and the tools to create convincing deepfakes become more widely available, cyber threat actors will very likely further incorporate the technology into their use of [misinformation, disinformation, and malinformation] campaigns, allowing them to increase the scope, scale, and believability of influence activities," the analysis said.

"Text generators have progressed to a point where the content they produce is often nearly indecipherable from legitimate material."

The Canadian Anti-Fraud Centre found that romance scams took off during the pandemic, and warn about the potential for deepfakes to aid scammers going forward. (Andrey_Popov/Shutterstock)

Clune said scams utilizing aspects of AI technology still require a person pulling the strings, but that could soon change.

"Even though scamming is very prevalent right now, there's still a cost to do it, because a human has to sit there and spend their time."

"But if you can have AI do it to a million people a day and just sit and watch the money roll in, that's a scary place to be. And that is something that is possible with this technology," he said.

Law hasn't kept up with tech: expert

Suzie Dunn, an assistant professor at the Schulich School of Law at Dalhousie University, said the law has not kept up with technology, leaving "major gaps" in the legal framework.

"One of the challenges that we have around impersonation laws is that, under the Criminal Code of Canada, you actually have to be impersonating an existing person," Dunn said in an interview.

She said software that allows people to create a non-existent individual, with a fake accent, voice or face, poses legal complications.

"If you're using someone's images or using someone's name, then it can be counted as a form of impersonation," she said.

"But with these new technologies ... the types of harms that are often meant to be covered under these impersonation rules aren't really covered."

Victims must rely on existing extortion and fraud laws, she said.

"We don't need new extortion laws. Extortion is extortion whether it's being done by deepfakes or by a regular person," she added.

"There's also a major gap there in what role the platforms play in addressing the harms that occur on them."

Dunn said corporations, including AI developers, dating and social media platforms, should be aware of the potential harms and put the necessary safeguards in place.

An expert says that existing anti-fraud laws haven't kept up with the rapid pace of technology. (fizkes/Shutterstock)

Clune agreed. He said new technology "will always be out in front of the laws and the politicians."

He said the pace of progress in the field is "breathtaking," and it will continue, if not accelerate.

"Almost anything you can imagine that seems science fiction and futuristic today will be around in a handful of years," he said.

"It is worth politicians and society engaging in thoughtful conversations about what's coming and trying to get ahead of it and think through what can we do about it.". Deepfake technology will become a serious threat to businesses as our world adopts remote work and online ID verification as standard.

When a video recently circulated online of US President Biden appearing to break into song while talking about inflation, stunned watchers questioned his sanity. Before the creator of the skit, Ali Al-Rikabi could explain how the video was made, however, hundreds of thousands of people viewed the clip – and not everyone realized it was fake.

The video resulted from deepfake technology, an umbrella term that describes applications of subsets of artificial intelligence (AI). In this case, the producer combined genuine footage, lip-synching software and synthesized voice audio.

While the video might be amusing at first glance, the deepfake highlights an emerging problem: being able to tell fake content from reality and how this applies to our online identities – especially in a world that is becoming increasingly remote.

What is Deepfake Technology?

Deepfakes are falsified images, videos or audio clips created through the application of AI technologies. Deepfake AI algorithms can generate lifelike people and animals or manipulate real people into doing or saying things they haven’t in reality.

You need a suitable mobile app for quick deepfake creation or a pair of AI algorithms trained to compare, replace and synthesize content for more in-depth and deceiving content. As millions of videos, photographs and audio clips are already on the web, there is plenty of data lake fodder for AI models to learn from.

In the past five years, the possibilities of deepfakes have expanded far beyond the boundaries of entertainment or fake pornography. Cyber-criminals are also adopting the technology for elaborate scams, including fraud and identity theft.

The Role of Deepfakes in Identity Theft

Online and in the public eye, deepfakes of celebrities are being used in advertising content for everything from real estate to cryptocurrencies.

The law is yet to catch up to this latest technological leap. While some deepfake creations are defended by portraying them as “satire,” there are real legal and ethical implications of deepfake technology. It’s only a small leap from the consensual use of someone’s image to non-consensual reproductions, and deepfakes don’t have to be technically perfect to have a substantial impact and to be truly believable.

Social Engineering

Criminals will use social engineering to pretend to be someone else, often assuming their identity to conduct theft or commit fraud.

Deepfakes take impersonation an unprecedented step further. With enough footage and the right tools, attackers could develop synthetic identity markers or create fake videos and voice audio clips. We observed this in 2019 in the case of a UK firm’s CEO, who believed he was talking to his boss and transferred $243,000 to a fraudster.

While conducting deepfake scams in real-time is a challenging prospect, considering the speed of technological innovation, it may not be long before this is our reality. However, effective biometric authentication to verify identities can bridge this gap in our defenses before identifying deepfakes becomes a real business crisis.

Financial Crime, Bypassing Know Your Customer

Another concern of deepfake technology is the potential abuse of know-your-customer (KYC) onboarding processes.

To perform a KYC check, businesses require customers to provide physical ID documents, proof of address and biometric identification, among other evidence.

Deepfakes could be used to fulfill this part of a KYC demand. Unfortunately, the rest can be gathered via social engineering or taken from the vast amount of data leaked online, often caused by third-party data breaches, in which terabytes of stolen information are released every year.

Deeply Fake Documentation

Separately, a sophisticated criminal may be able to provide counterfeit documents as proof of identity. It is already possible to create fake passports, driver’s licenses, and more. Still, deepfake AI could make it easier to fool authentication checks when ID documents are requested online.

Synthetic ID fraud is already estimated to cost banks $6 billion annually, and as a Finextra journalist found, deepfake technology is already sophisticated enough to pass bank verification checks.

How Identity Verification Can Combat the Deepfake Identity Challenge

Deepfake AI is a promising technology when applied to ethical applications. However, with every innovation, cyber-criminals will find a way to exploit it.

We can’t stop the abuse of new software, but we can fight deepfake criminal schemes and ID theft with the proper forms of defensive technology – and one that stands out is facial recognition software.. The power of artificial intelligence is both incredible for technological advancements and dangerous for spreading misinformation.

And a video of Leonardo DiCaprio’s voice being dubbed by other celebrities shows it.

The video, posted by heyBarsee on Twitter, uses ElevenLabs’ AI technology to dub one person’s voice with another person.

In this case, it took DiCaprio’s speech on climate change at the United Nations and changed his voice to Joe Rogan, Jeff Bezos, Robert Downey Jr, Bill Gates, and Kim Kardashian.

Sign up for our free Indy100 weekly newsletter

The technology almost identically matches each celebrity’s voice to DiCaprio’s words, inflection, and speech pattern.

It makes it seem like Rogan, Bezos, Downey Jr, Gates, or Kardashian is giving the speech instead.

“11labs new AI voice cloning technology is alarmingly good,” heyBarsee wrote.

Other people agree the technology’s ability to match celebrity voices is incredible. But many noted how the AI could be used to spread misinformation by making it seem as though a notable person is saying something that they never said.

“AI voice stuff is gonna lead to crazy hoaxes but the worst part is. AI voice stuff is gonna scam a LOT of old people out of their money,” Jay tweeted.

Scott wrote, “This is gonna lead to some nutty stuff in the media lol.”

ElevenLabs, the company behind the voice technology said they aimed to build their model "to grasp the logic and emotions behind words."

"Rather than generate sentences one-by-one, it’s always mindful of how each utterance ties to preceding and succeeding text. This zoomed-out perspective allows it to intonate longer fragments convincingly and with purpose," ElevenLabs' website says. "You can do this with any voice you want."

Already celebrities and notable figures have run into problems with people using their likenesses for scams.

Have your say in our news democracy. Click the upvote icon at the top of the page to help raise this article through the indy100 rankings.. The generative AI industry will be worth about A$22-trillion by 2030, according to the Commonwealth Scientific and Industrial Research Organisation (Cairo). These systems – of which ChatGPT is currently the best known – can write essays and code, generate music and artwork, and have entire conversations. But what happens when they’re turned to illegal uses?

Last week, the streaming community was rocked by a headline that links back to the misuse of generative AI. Popular Twitch streamer Atrioc issued an apology video, teary-eyed, after being caught viewing pornography with the superimposed faces of other women streamers.

The “deepfake” technology needed to Photoshop a celebrity’s head on a porn actor’s body has been around for a while, but recent advances have made it much harder to detect. And that’s the tip of the iceberg. In the wrong hands, generative AI could do untold damage. There’s a lot we stand to lose, should laws and regulations fail to keep up.

From controversy to outright crime

Last month, generative AI app Lensa came under fire for allowing its system to create fully nude and hypersexualised images from users’ headshots. Controversially, it also whitened the skin of women of colour and made their features more European.

The backlash was swift. But what’s relatively overlooked is the vast potential to use artistic generative AI in scams. At the far end of the spectrum, there are reports of these tools being able to fake fingerprints and facial scans (the method most of us use to lock our phones).

Criminals are quickly finding new ways to use generative AI to improve the frauds they already perpetrate. The lure of generative AI in scams comes from its ability to find patterns in large amounts of data. Cybersecurity has seen a rise in “bad bots”: malicious automated programs that mimic human behaviour to conduct crime. Generative AI will make these even more sophisticated and difficult to detect.

Ever received a scam text from the “tax office” claiming you had a refund waiting? Or maybe you got a call claiming a warrant was out for your arrest? In such scams, generative AI could be used to improve the quality of texts or emails, making them much more believable. For example, in recent years we’ve seen AI systems being used to impersonate important figures in “voice spoofing” attacks.

Then there are romance scams, where criminals pose as romantic interests and ask their targets for money to help them out of financial distress. These scams are already widespread and often lucrative. Training AI on actual messages between intimate partners could help create a scam chatbot that’s indistinguishable from a human.

Generative AI could also allow cybercriminals to more selectively target vulnerable people. For instance, training a system on information stolen from major companies, such as in the Optus or Medibank hacks last year, could help criminals target elderly people, people with disabilities, or people in financial hardship. Further, these systems can be used to improve computer code, which some cybersecurity experts say will make malware and viruses easier to create and harder to detect for antivirus software.

The technology is here, and we aren’t prepared

The US has had a legislated National Artificial Intelligence Initiative in place since 2021. And since 2019 it has been illegal in California for a bot to interact with users for commerce or electoral purposes without disclosing it’s not human. The European Union is also well on the way to enacting the world’s first AI law. The AI Act bans certain types of AI programs posing “unacceptable risk” – such as those used by China’s social credit system – and imposes mandatory restrictions on “high-risk” systems.

this seems exactly right to me. 🎯 pic.twitter.com/hNXEDdN0Yn — Kareem Carr | Data Scientist (@kareem_carr) January 29, 2023

Although asking ChatGPT to break the law results in warnings that “planning or carrying out a serious crime can lead to severe legal consequences”, the fact is there’s no requirement for these systems to have a “moral code” programmed into them.

There may be no limit to what they can be asked to do, and criminals will likely figure out workarounds for any rules intended to prevent their illegal use. Governments need to work closely with the cybersecurity industry to regulate generative AI without stifling innovation, such as by requiring ethical considerations for AI programs. (…)

Can you spot a scam?

As criminals add generative AI tools to their arsenal, spotting scams will only get trickier. The classic tips will still apply – but beyond those, we’ll learn a lot from assessing the ways in which these tools fall short.

Generative AI is bad at critical reasoning and conveying emotion. It can even be tricked into giving wrong answers. Knowing when and why this happens could us help develop effective methods to catch cybercriminals using AI for extortion.

There are also tools being developed to detect AI outputs from tools such as ChatGPT. These could go a long way towards preventing AI-based cybercrime if they prove to be effective. DM/ML

This story was first published in The Conversation.

Brendan Walker-Munro is a Senior Research Fellow at The University of Queensland.. . By Eric Tran-Le, VP & Head of Premier, NICE Actimize

As global financial institutions continue to armor themselves again increased fraud, fraudsters and their associated emerging fraud types continue to move from financial institution to financial institution, with mid-sized FIs on the receiving end of an onslaught of new scams and other fraudulent attacks – with the fraudster often hoping that these FIs may be less resistant to fighting off the assaults.

Digital fraud losses are anticipated to surpass $343 billion globally between 2023 and 2027, according to a Juniper Research study of online payment fraud. With a fresh crop of fraud trends gaining momentum—including synthetic fraud, insider recruitment, lending fraud, and social media scams—mid-sized financial institutions (FIs) must rethink their approach to fraud prevention.

Why is it essential for mid-sized Fis to stay alert? The stakes couldn’t be higher in a diversifying competitive and digital landscape, and customer loyalty and brand reputation are on the line. Community banks, credit unions, and regional banks can’t afford risk management to be hindered by fragmented point solutions, lack of contextual insight, and data silos.

Key Fraud Trends Hitting Mid-Sized FIs

2023 is likely to be another definitive year in fraud, but many areas should be more prevalent than others as fraudsters launch their assaults and the first wave of what’s expected this year.

First, there is the “Just Your Friendly Neighborhood” Scammer. The rule of thumb for scams is that if it’s easy to make a payment, then it’s easy for scammers to make money. Phishing, spear phishing, baiting, scareware, whaling attacks, and pretexting are a few standard social engineering techniques used to prey on a victim’s vulnerabilities. Social engineering is especially effective because it relies on psychological tactics—scammers know how to use a victim’s fear, greed, or respect for authority against them.

Social media scams will continue to be prevalent. The Federal Trade Commission (FTC) says 2021 was a banner year for social media scammers, with $770 million in social-media–originated fraud losses reported, and there is no question that this trend will continue through 2023.

Though not an exhaustive list, fake merchandise scams, charity scams, fake job scams, romance scams, and investment scams regularly start on social media. These scams result in authorized push payment (APP) fraud, account takeover (ATO), money mule activity, identity theft, and credit card fraud. NICE Actimize’s own research, derived from its annual Fraud Insights Report, has also seen year-over-year increases in authorized fraud, money mule activity, and more that are certain to impact mid-sized institutions.

Google voice scams are another unique scam threat assaulting mid-sized financial institutions. Over 37% of the scam reports received by the Identity Theft Resource Center in the first half of 2022 were connected to Google Voice scams. A Google Voice account isn’t even necessary for a scammer to victimize someone and perpetrate Account Takeover (ATO) or identity theft.

Remember that any scam loss statistic that you read about is likely to be much higher than the actual numbers indicate. The shame and stigma frequently associated with scams can prevent victims from sharing their experiences with the proper authorities. As it stands, industry estimates state that $3.6 billion is lost globally by businesses alone in 2022 to scams and fraud, averaging $1.78 million in losses per case.

Another type of prevalent fraud, Synthetic Identity Fraud, is just a swipe away. Artificial intelligence (AI), machine learning, and deep learning technologies give even more legitimacy to sophisticated fraudster behaviors and tactics. Synthetic media capabilities, offering the means to modify or construct video, images, and audio, are readily accessible to the public through a number of different products. Mainstream text-to-image synthesis and generator tools, auto-generated and downloadable synthetic faces, smartphone native AI-powered image eraser tools, and synthetic voice generator tools are available for free or as little as a few dollars. Fraudsters have been adept at adopting these tools against their victims. These tools blur the interpretation of reality, and they’re anticipated to influence the trajectory of SIF in 2023.

Industry sources estimate that there has been a 9% increase in small and midsize business (SMB) lending fraud since 2020 and that trend will hold throughout 2023. Synthetic business credentials, created from stolen business and consumer data, make it challenging for FIs to distinguish authentic loan requests from fraudulent ones. Fraudsters use synthetic media to impersonate business profiles and resources on social media to misrepresent employees, execute scams against the company or other victims, or create replica websites to obtain sensitive data.

AI and Machine Learning Closes Gaps in Risk Exposures

Mid-market financial institutions, regional and community banks, and credit unions are recognized for their member-centric banking model but often face specific fraud prevention challenges and risks.

Fortunately, as fraudsters continue to up the ante with dynamic new trends, game-changing technology, and analytics, innovations are available to mid-sized Fis that help address these scams and frauds.

Advanced AI and machine learning-powered solutions are helping mid-market financial institutions, including regional banks and credit unions, address specific fraud prevention challenges and risks. For example, credit unions have risk exposures that include manual reviewing processes, insecure email networks, leaked personnel data, and outdated fraud prevention systems.

To protect their customers, assets, and organization, mid-market FIs must adopt a holistic fraud strategy that incorporates fraud and AML solutions in a single platform – this approach is often nicknamed FRAML in the industry. With AI, data intelligence, and behavioral analytics, mid-sized organizations can detect attacks at early stages and intervene before any money movement occurs. Connected fraud and AML solutions can also respond to new threats without alienating customers, often relying on behavioral biometrics that can automatically discover unusual patterns across channels.

Most importantly, these new FRAML-based technologies expedite accurate risk scoring and leverage actionable insights based on contextually enriched customer profiles. Other benefits of these emerging technologies, including the ability to streamline alert and case investigation and provide automatically discovered relationships and linkages. To benefit AML capabilities, the FRAML approach improves investigation efficiency with real-time KYC and CDD entity-link analysis and pre-populated case details for regulatory reports.

Fraud is a living, breathing threat that shape-shifts in many new forms. For mid-sized financial institutions to protect themselves, a holistic, intuitive fraud prevention program that interconnects with the institution’s anti-money laundering efforts should be at the heart of any financial crime program, able to respond to fraud and other threats in whatever iteration they happen to materialize. “FRAML” may sound unusual, but the results that a linked Fraud and AML-focused approach can bring to mid-sized financial institutions is not.. Hoan Ton-That, CEO of Clearview AI sat down with Click Here to discuss his company’s operations in Ukraine and the controversies that have dogged what has emerged as the world’s largest facial recognition network. (He wants 100 billion images in the database within a year. That’s almost 13 photographs for everyone on earth.)

Among other things, some 400 investigators in Ukraine are using Clearview AI to build war crimes cases, identify the dead – both Russian and Ukrainian – and tell families back in Russia that their children have died in the war.

Not long after the company began providing free logins for Ukrainian investigators, a volunteer hacking force called the IT Army of Ukraine appeared to use a program that resembles Clearview AI for similar purposes.

The Ministry of Digital Transformation in Ukraine confirmed to Click Here that it is also using Clearview to tell families about fallen soldiers. They say it is partly to give Russians a taste of the true cost of the war and partly to tell families that if they wish to pick up the bodies of their loved ones, they can travel to Ukraine to do so.

Hoan Ton-That talked about the controversy surrounding his biometric company, the perils of using technology in a war zone and what to expect from Clearview going forward.

The interview has been edited for clarity.

Click Here: How did Clearview come to play such a major role in Ukraine?

Hoan Ton-That: When the war started, it was really shocking to me and a lot of members of our team to see especially the video footage of women and children suffering and it made me think, how can we help? Originally, I would see photos of captured Russian soldiers, and I realized with that kind of photo quality, our facial recognition technology could be helpful. So, I reached out to a lot of people on our advisory board to ask them, do you know anyone in the Ukrainian government, and one person — Lee Wolosky —he’s on the National Security Council under three presidents, he quickly said yes. We thought that it could be helpful in identifying deceased soldiers and track misinformation.

CH: How could it be useful to track misinformation?

HT-T: You'd see a lot of things on social media saying this is a captured Russian soldier, but you might see people on the other side saying actually that's a paid actor and here's the name of the paid actor. So with this technology, the level of accuracy can be used to identify if someone is who they say they are.

CH: Who in Ukraine has logins to Clearview AI?

HT-T: It's in six different agencies, including the National Police of Ukraine. Then the people on the ground would be using it. So once we gave a demo, we'd give them training on how to use facial recognition technology responsibly. So part of the training is that they would send photos of unidentified people and run it through the system and we could show them, ‘Hey, this is how you verify who it is.’ So for example, if they have a tattoo that matches online, there’s a very high chance it's the same person.

CH: You inserted this technology into a war zone, which presents a lot more problems than having a police department in the United States use it. How are you accounting for that?

HT-T: You want to be careful to make sure that they really know how to use it properly, and so there are all these scenarios that we want to make sure don't happen. For example, what if someone takes a photo of someone and says, ‘Hey, I think you're a Russian traitor.’ And then they detain them. And it's all incorrect based on incorrect information. So we'd never want something like that to happen. As long as these investigations are done by trained investigators, this can be a very helpful tool. If it was used by everybody, I think that's when problems happen.

CH: What’s the most surprising use you’ve seen in Ukraine?

HT-T: War crimes investigations. We were talking to people in the National Police of Ukraine and others where they have video footage from surveillance cameras. Smartphones are more prevalent, so there's a higher chance of something being recorded. Now with this technology, if it's easy to identify someone, I think people are going to think twice about war crimes. So that was a surprise to me.

CH: So this is a subscription service, and you say that gives you more control if someone is misusing it… how does Clearview AI work?

HT-T: We vet every person to make sure they’re a government official who is using it. There’s also two-factor authentication, so they still have to verify their device before they log in. Once they have an account, there's an administrator for each agency. So, the administrator can see who is conducting searches and what reason they're conducting the search. There is an intake form that requires a case number and a crime type before conducting a search. So people when they're on-boarded and they're learning about the software, they know that their searches can be audited because we want to make sure they're using it for the right kind of stuff. Because it’s a Cloud service, we have the ability to revoke access. If there's any egregious abuse of the technology, you want to make sure that we have the ability to take it away.

CH: The IT Army appears to be using it. In a video, the group demonstrated the use of a facial recognition program that appears to resemble Clearview AI. This is a volunteer hacking force, so how is it that the Ukraine IT Army appears to be using Clearview AI?

HT-T: All I can say is that everyone we've on-boarded is a government official. We haven't onboard anyone in the IT Army directly. Everyone we talk to and on-board, we give them proper training on its usage. The speculation that the IT Army is running a Clearview AI search does not match any information we have on this matter. Clearview AI is intended for use in Ukraine by law enforcement and government officials.

CH: Did maybe somebody give a username and password to somebody who's in the IT Army?

HT-T: it's possible that someone shared a screenshot or shared how it worked, but we want to make sure that whatever the usage of the technology is — say it is to identify someone deceased — that is done in a way that is positive. The policy of the National Police and all our users is to tell the family members in a humane way.

CH: Had it occurred to you that the IT Army would use this technology to notify families of dead soldiers as a propaganda tool?

HT-T: I talked to some of the officials [in the Russian government] and I said, ‘Look, is this something you knew about? Is that your procedure for doing this?’ Then said that’s not our official procedure. And they assured me that’s not what they want to have happen either. Again, it is war time. Tensions are really high. Those things can happen.

If I thought it would be used in a really bad way, then I don't think I’d give access to them. We think that just getting the information out in a humane way is the most important thing. What we can control as Clearview is giving access to the right people. So for example, we don't give access to the Russians or anything like that and we make sure Ukraine is trained as appropriately as possible.

CH: Have you revoked any access related to Ukraine because you thought it wasn't being used properly?

HT-T: No, not at this time, but the administrators of these agencies in Ukraine have the ability to do so. They can go in and audit the searches, remove access to an account and give access as they deem appropriate. Clearview AI would only revoke access to an agency if there was an egregious amount of abuse in that agency. Until something really escalates to that level, we haven't revoked any access.

CH: The NSO Group is an Israeli company that makes surveillance software that can be remotely implanted in smartphones. It has come under heavy criticism for its tech being used by authoritarian governments to spy on citizens. With your facial recognition technology, how do you avoid the NSO trap?

HT-T: I think NSO is a very different kind of technology than what we do. We are searching public information from the internet. So it is really just like Google. If you can type someone's name and the word LinkedIn into Google and you find their photo, then Clearview can basically do the same thing, but it's search by photo. We also work with law enforcement. NSO is different because it’s breaking into phones, very private data. Also, when they sell their software, they don't have the ability to take it back if they sell it to the wrong actor or government. Whereas Clearviews software is deployed in the cloud. If we ever found anything totally egregious or abusive, we have the ability to revoke access. We also want to be aligned with democratic countries, making sure that this is a technology that can be used responsibly and across democratic nations.

CH: Can you imagine a scenario, years and years from now when everyone has this capability that it would be in like VR glasses or built into a phone?

HT: I can imagine like augmented reality is an interesting where it could be deployed on military bases so in Afghanistan, they had a situation where when they were pulling out at a checkpoint, terrorists could blow up people. They are looking up close at the ID’s. To verify someone at a distance in a hands-free way, I think that's a very positive kind of use case.. In the weeks after Russia invaded Ukraine and images of the devastation wrought there flooded the news, Hoan Ton-That, the chief executive of the facial recognition company Clearview AI, began thinking about how he could get involved.

He believed his company’s technology could offer clarity in complex situations in the war.

“I remember seeing videos of captured Russian soldiers and Russia claiming they were actors,” Mr. Ton-That said. “I thought if Ukrainians could use Clearview, they could get more information to verify their identities.”

In early March, he reached out to people who might help him contact the Ukrainian government. One of Clearview’s advisory board members, Lee Wolosky, a lawyer who has worked for the Biden administration, was meeting with Ukrainian officials and offered to deliver a message.

Mr. Ton-That drafted a letter explaining that his app “can instantly identify someone just from a photo” and that the police and federal agencies in the United States used it to solve crimes. That feature has brought Clearview scrutiny over concerns about privacy and questions about racism and other biases within artificial-intelligence systems.. A new analysis finds machine learning models to predict stroke risk aren't much better than simpler algorithms.

Tools to predict stroke risk work less well for Black patients, study finds

Stroke risk prediction tools are meant to guide how doctors approach a potentially deadly condition, using factors like heart disease and high blood pressure to get a handle on which patients might benefit from a particular treatment.

For years, doctors have used several different algorithms to try to capture the true risk of stroke, including newer models that use machine learning. A new analysis, led by researchers at Duke University School of Medicine, compared several of those algorithms head-to-head — and found that novel machine learning models weren’t much more accurate at predicting the risk of stroke than simpler algorithms based on self-reported risk factors and an older methodology. Alarmingly, the study also found all the algorithms were worse at stratifying risk for Black men and women than for white.

advertisement

“We got the shocking result — to me, shocking — that the measure of discrimination, the ability to rank them, was much better… for white participants than Black participants,” said Michael Pencina, director of Duke AI Health and one of the lead researchers on the study.. A robotic dog built by Boston Dynamics is demonstrated at a conference on robotics and artificial intelligence in Las Vegas in 2019.

Amid lingering concerns about surveillance and safety, the Los Angeles City Council is expected to vote Tuesday on whether to accept the donation of a dog-like robot for the LAPD.

The controversial device would be paid for with a nearly $280,000 donation from the Los Angeles Police Foundation. The Police Commission and the council’s public safety committee have approved the move.

The department said it intends to deploy the device in limited scenarios and primarily for reconnaissance. Nicknamed Spot, it can climb stairs, open doors and navigate rugged terrain, giving police a set of eyes in potentially dangerous situations while keeping officers out of harm’s way, officials say.

Advertisement

Under the department’s policy, its use would be restricted to incidents involving the SWAT team, such as an active shooter, barricaded suspect or explosive device.

Critics are dubious.

As with other police technologies, they worry about its potential for misuse to harm and spy on Black and brown communities.

Hamid Khan, with the oversight group Stop LAPD Spying Coalition, said that even if the robot starts out with a more benign purpose, law enforcement cannot be trusted to regulate itself. Throughout its history, the LAPD has justified new technology and programs by saying it would be used only in narrow circumstances, Khan said.

“There’s a long history of mission creep with the LAPD that what we have seen, and everything we have called out has then suddenly been transformed into a much wider expansion of its deployment,” he said. “If we go back historically, the helicopter fleet first came out, then we saw the SWAT and that it’s only going to be in particular situations, but SWAT has been normalized.”

On social media, critics of the LAPD’s plan have circulated news stories about the robotic canine’s disastrous deployment in New York City.

The nation’s largest police force first acquired the technology in 2020. Its use didn’t get widespread attention until the next year, when it sparked a public outcry after a viral video showed the robot trotting alongside New York City officers during a hostage situation at a high-rise public housing building.

Advertisement

Critics denounced the decision to use the device in what they said is an overpoliced community, and they also raised concerns about privacy and data collection. After several days, the New York City Police Department broke its contract with Boston Dynamics and returned the robot.

LAPD Lt. Ruben Lopez, who oversees SWAT, said the department learned from what happened in New York. In Los Angeles, the Police Department’s “strict policies” for Spot’s use would ensure “we don’t abuse it, so we can avoid confrontations and stuff with people who don’t want to be arrested,” he said.

The dizzying pace of technology has forced police departments to keep up and made the adoption of devices like Spot “inevitable,” he said.

“We will not use it to surveil,” he said, noting similar concerns about widespread police surveillance after the LAPD bought drones five years ago. “It’s baseless ... we haven’t had a single violation.”

As with the department’s drones, its use must be approved by the deputy chief of the counter terrorism bureau, and the police chief must be notified.

The donation has cleared several hurdles, most recently with a 4-1 vote by the council’s public safety committee in January.

Advertisement

Steve Soboroff, a police commissioner, said the device is intended to save lives and that fears of the technology someday being used to spy on or attack people are “ridiculous.” He said the department had worked to devise safeguards against such abuse.

“This isn’t about perception; it isn’t about how it looks. It’s about saving lives of the public and saving the lives of police officers and saving the lives of suspects, some of whom are severely mentally ill and are severely in drug stupors,” he said. “There’s no missiles attached to it, and there’s not secret chemicals attached to it.”

Several council members have signaled they would vote against the robot.

In a Twitter thread, Councilmember Hugo Soto-Martínez wrote that the robot’s deployment in New York “isn’t just disturbing, it’s deeply unjust. Especially when talking about low-income tenants and communities of color.”

Soto-Martínez, the sole “no” vote on the public safety committee, has signaled his continued opposition to the robot.

Today, LA's Public Safety Committee will decide whether we should allow the LAPD to use a "robot dog"



These mechs were already rejected in NYC because they were used to aggressively police working-class communities of color.



We can't allow this dystopian machine in LA. (thread) pic.twitter.com/vOaFdDrXgN — Hugo Soto-Martínez (@HugoForCD13) January 24, 2023

Opposition to the robots has mounted in recent months, following an uproar over a proposal in San Francisco to weaponize robots for use in killing people in certain situations. City leaders dropped the plan under public pressure.

Advertisement

Critics fear that it’s only a matter of time before other cities push for armament.

Earlier this year, Boston Dynamics, the company that manufactures Spot, joined other robotics firms in an open letter condemning the arming of robots, saying the practice will “harm public trust in the technology in ways that damage the tremendous benefits they will bring to society.”

The technology has been in use for years in more than 30 countries around the world, the company said.

Newsletter Get the lowdown on L.A. politics Sign up for our L.A. City Hall newsletter to get weekly insights, scoops and analysis. Enter email address Sign Me Up You may occasionally receive promotional content from the Los Angeles Times.

Another company, Ghost Robotics, has started marketing a weaponized dog-like robot to several branches of the U.S. military and its allies. And the four-legged robots were seen trotting around the grounds at International Defense Exhibition and Conference, an arms fair held every two years in the Emirati capital of Abu Dhabi.

Besides the controversy in policing, Spot has received benign attention, appearing in viral social media videos, dancing to pop songs. Earlier this year, Jimmy Fallon featured the robot on an episode of “The Tonight Show,” and Boston Dynamics has highlighted the technology’s use in countries worldwide. For example, London’s Heathrow Airport is using one of the robots to do 3-D laser scans of a 1960s-era cargo tunnel that’s being refurbished. In Ukraine, it’s being used to sweep for mines.

Across the U.S., the devices are starting to be used in various roles by police departments in cities such as Honolulu and St. Petersburg, Fla. The Homeland Security Department is considering deploying robot dogs manufactured by Ghost Robots to help patrol the southern U.S. border.

Advertisement

But experts say that efforts at regulation nationwide of police technology have been piecemeal and largely failed to keep up with new developments in robotics.. Down in sunny Catford, South-East London, the plant pots are flying as Idris Elba and his neighbour Kim Kardashian are at loggerheads over the shared garden of their terrace homes.

On the Isle of Wight, youth worker Olivia Colman and her mature student husband Jay-Z share a love of line-dancing and their cat, Cabbage — but not the snooty estate agent, Tom Hiddleston, next door.

Trainee nurse Andy Murray and his mother, a heavily tattooed Lorraine Kelly, have boundary issues over neighbour RuPaul’s noisy cockerel in their sleepy Cotswold village; while Adele — a newly single caretaker who’s moved to a Coventry semi — hates how the pigeons are ruining the garden accessories she loves buying from the Shopping Channel.

If this all sounds like the ingredients of an extended suburban nightmare, that is entirely the intention of a new ITV series which constitutes the world’s first ‘deepfake’ comedy series.

Deep Fake Neighbour Wars uses cutting edge ‘deepfake’ technology to transpose glamorous celebrities to infinitely less glamorous surroundings, doing pedestrian jobs and living in ordinary homes.

Down in sunny Catford, South-East London, the plant pots are flying as Idris Elba and his neighbour Kim Kardashian are at loggerheads over the shared garden of their terrace homes

If this all sounds like the ingredients of an extended suburban nightmare, that is entirely the intention of a new ITV series which constitutes the world’s first ‘deepfake’ comedy series. Pictured: A deepfake Greta Thunberg

The show is a series of extended comedy sketches in which the faces of the famous are digitally grafted onto actors impersonating their voices and mannerisms.

At first glance, the computer wizardry is both startling and unsettling — the cast really do look like these stars, even if they don’t always sound precisely like them.

But just as the potential of deepfake is prompting fevered excitement in the entertainment industry, it’s causing growing alarm among politicians and security experts who see the danger — particularly in spreading fake news — in a technology that can make it look as though anyone has said or done anything.

Hostile countries, or others with malign intentions, could even use it to undermine democratic institutions and national security.

What, for instance, might happen if it wasn’t Idris Elba’s meltdown with a neighbour being faked, but a world leader threatening another country or lambasting an ally — in a video, for instance, that appears the night before a general election. Or a police chief saying something deeply racist after a black man has just been killed. Suddenly, it’s not remotely funny.

Some see the threat of immense social disruption and even the ‘end of truth’ as deepfake software becomes immensely sophisticated and so widespread it can be downloaded on a mobile phone.

In an increasingly polarised world that was already finding it hard enough to agree on anything, we may soon not even be able to trust our eyes and ears, let alone each other. To make matters worse, as usually happens with galloping technology, there are very few laws to protect us.

Last year, a video of Ukrainian leader Volodymyr Zelensky surrendering to Russia circulated on social media. It was poor quality but, said security experts, it showed what might be attempted

Unbelievable? Deepfake Cruise with real Paris Hilton

Deep Fake Neighbour Wars isn’t nearly as impressive as the series of short deepfake YouTube videos of Tom Cruise that began appearing three years ago.

Visual-effects specialist Chris Ume teamed up with Tom Cruise lookalike Miles Fisher to produce a video of deepfaked Cruise announcing he is running for President.

A year later, Fisher began posting videos on his own TikTok account. His first video, in which he plays golf as Cruise, amassed four million views in less than two days, while in another, his ‘DeepTomCruise’ cuddles the real Paris Hilton. For many, Fisher’s videos were an introduction to what can be achieved with deepfake software.

None of the stars sent up in the ITV series was consulted or gave permission — although Deep Fake Neighbour Wars’ use of their images is not illegal (not yet anyway).

However, while the show starts each episode with an on-screen disclaimer that what follows is ‘all fake’ and ‘the real celebs have had nothing to do with this’, and a ‘deep fake’ label remains in the corner of the screen, there’s nothing to stop edited clips of the show being put on social media with no such warning.

Although none of the ‘real celebs’ has reportedly complained, actors union Equity has expressed misgivings — even launching a Stop AI Stealing The Show campaign — about the ‘dystopian’ consequences for performers who aren’t consulted, let alone paid if their faces are used for deepfakery.

A deepfake video or photo typically involves superimposing one person’s face onto someone else’s body. That may sound simple, but deceiving the human eye by matching up expressions, gestures, voice, even facial tics takes a lot of work. Deepfakes are usually created by feeding thousands of photos of someone (famous people are simpler to fake as there are so many more pictures of them around) into a computer programme inside a special camera.

This uses ‘machine learning’ (a form of artificial intelligence, or AI, whereby the computer learns for itself) to understand how that person will look from various angles by mashing together information from all the photos.

A copy of the person’s face can then be superimposed on that of the actor who is the stand-in for the purposes of the fake.

If you want people to say things they’ve never said, the software must learn how their mouth, face and even neck moves as they enunciate words. (It’s often easiest to spot an unsophisticated deepfake by looking at the mouth.)

The stand-in ideally needs to move as slowly as possible. Making quick movements, or exhibiting strong emotions, can disrupt the process, as can blocking parts of the face, say by putting on sunglasses. But even these issues are being ironed out.

Phoney: The deepfake clip of actress Emma Watson reading Mein Kampf emerged this week

Among the high-profile celebrities recreated in the ITVX comedy are Nicki Minaj and Spiderman's Tom Holland (pictured)

A more advanced variation of deepfake programme, known as GAN, (or Generative Adversarial Networks) involves using two competing AI algorithms, one of which is creating a deepfake and the other is trying to spot the forgery. Gradually, by trial and error, they will whittle down the deepfake image to produce something that’s close to a perfect likeness.

While simple deepfake software can be downloaded free onto a mobile phone, considerable post-production work invariably needs to be done to smooth out minor flaws which a computer relying on statistical measurements may miss but the human eye won’t.

Audio can also be deepfaked to create so-called ‘voice clones’, either through software that mimics a voice or stitches together words that the imitated person has actually spoken.

The film and TV industry has been experimenting with deepfake technology for some years, although it prefers to call it ‘synthetic media’: it’s been employed to ‘resurrect’ dead actors such as Peter Cushing in the later Star Wars films, to put Bruce Willis in a Russian advert, and to have the Queen dancing in Channel 4’s impertinent 2020 parody of HM’s Christmas Speech.

A season finale of TV show America’s Got Talent featured deepfakes of Elvis Presley and Simon Cowell.

It’s just been announced that actors including Tom Hanks and Robin Wright will get the deepfake treatment, ‘seamlessly transforming into younger versions of themselves’ in a forthcoming feature film, involving the same company, Metaphysic, that made the Tom Cruise videos.

Meanwhile, the Deepfake revolution has already shown it has far more sinister applications than sending up celebrities or performing a bit of time-shifting trickery in feature films.

Indeed, the term was coined in 2017 on the online messaging board Reddit by the producer of sex videos in which the performers’ faces were swapped for those of film actresses.

It’s still put to sordid uses — fuelling so-called ‘revenge porn’ (in which ordinary people can be made to look as though they’re having sex).

A particularly disturbing deepfake internet program can remove clothing in a photo and replace it with a realistic naked body. Many of its 100,000-plus victims have been under-age girls.

'Deep Fake Neighbour Wars': Spencer Jones, the show's co-creator, insists the comedy does not deal with serious subjects and makes it clear the figures are not real (pictured: 'Stormzy', 'Harry Kane')

In 2019, criminals are believed to have used deepfake technology to mimic the voice of the chief executive of the UK subsidiary of a German energy company. The scammers duly contacted one of the CEO’s underlings by phone and ordered him to transfer a sum of nearly £200,000.

A similar voice-cloning scam reportedly netted thieves £29 million in a Dubai bank heist. And, only a few days ago, Harry Potter star Emma Watson was deepfaked reading from Hitler’s Mein Kampf.

Deepfake has been used in political hoaxes from Joe Biden falling asleep during an interview to Barack Obama calling Donald Trump a ‘total dip****’.

Last year, a video of Ukrainian leader Volodymyr Zelensky surrendering to Russia circulated on social media. It was poor quality but, said security experts, it showed what might be attempted.

Some U.S. politicians want to outlaw malicious deepfakes and say it’s an issue that keeps intelligence agencies ‘up at night’. As one U.S. senator put it: ‘Deepfakes can be tailor-made to drive Americans apart and pour gasoline on just about any culture-war fire.’

In policing, not only could it lead to the innocent being wrongly implicated but the guilty could always dismiss genuine incriminating video as faked. In 2020, it emerged that a deepfake audio recording was used in a UK custody battle to discredit the father by making out he’d made ‘violent’ threats against his wife.

It would be unfair to say the technology has been all bad news outside the entertainment industry. Dutch police used it last year to revive a 2003 cold case by digitally recreating a 13-year-old murder victim. Online shoppers can use it to try on clothes in virtual fitting rooms. Meanwhile, it has uses in public-service videos such as ones for the Malaria Must Die campaign, which showed David Beckham speaking in nine languages or aged into his 70s.

But deepfake’s ethical issues, say experts, are serious and pressing. Given the myriad areas that could be exploited, from diplomacy and national security to business and politics, ‘we should be very worried’, says Mathilde Pavis, an internet law specialist and deepfake expert at Reading University. The problem ‘requires an international response that’s really lacking’.

It’s too late to try to ban deepfakes outright and possibly even to restrict the malicious ones, say experts. Instead, they argue, we need to become more ‘technologically literate’ and conscious of possible deepfake trickery.

It could be tough. As deepfake Tom Cruise tells the real Paris Hilton when she wonders whether people will believe they’re a couple: ‘I think most people will believe anything.’. U.S. Special Operations Command, responsible for some of the country’s most secretive military endeavors, is gearing up to conduct internet propaganda and deception campaigns online using deepfake videos, according to federal contracting documents reviewed by The Intercept.

The plans, which also describe hacking internet-connected devices to eavesdrop in order to assess foreign populations’ susceptibility to propaganda, come at a time of intense global debate over technologically sophisticated “disinformation” campaigns, their effectiveness, and the ethics of their use.

While the U.S. government routinely warns against the risk of deepfakes and is openly working to build tools to counter them, the document from Special Operations Command, or SOCOM, represents a nearly unprecedented instance of the American government — or any government — openly signaling its desire to use the highly controversial technology offensively.

SOCOM’s next generation propaganda aspirations are outlined in a procurement document that lists capabilities it’s seeking for the near future and soliciting pitches from outside parties that believe they’re able to build them.

“When it comes to disinformation, the Pentagon should not be fighting fire with fire,” Chris Meserole, head of the Brookings Institution’s Artificial Intelligence and Emerging Technology Initiative, told The Intercept. “At a time when digital propaganda is on the rise globally, the U.S. should be doing everything it can to strengthen democracy by building support for shared notions of truth and reality. Deepfakes do the opposite. By casting doubt on the credibility of all content and information, whether real or synthetic, they ultimately erode the foundation of democracy itself.”

“When it comes to disinformation, the Pentagon should not be fighting fire with fire.”

Meserole added, “If deepfakes are going to be leveraged for targeted military and intelligence operations, then their use needs to be subject to review and oversight.”

The pitch document, first published by SOCOM’s Directorate of Science and Technology in 2020, established a wish list of next-generation toys for the 21st century special forces commando, a litany of gadgets and futuristic tools that will help the country’s most elite soldiers more effectively hunt and kill their targets using lasers, robots, holographs, and other sophisticated hardware.

Last October, SOCOM quietly released an updated version of its wish list with a new section: “Advanced technologies for use in Military Information Support Operations (MISO),” a Pentagon euphemism for its global propaganda and deception efforts.

The added paragraph spells out SOCOM’s desire to obtain new and improved means of carrying out “influence operations, digital deception, communication disruption, and disinformation campaigns at the tactical edge and operational levels.” SOCOM is seeking “a next generation capability to collect disparate data through public and open source information streams such as social media, local media, etc. to enable MISO to craft and direct influence operations.”

SOCOM typically fights in the shadows, but its public reputation and global footprint loom large. Comprised of the elite units from the Army, Marine Corps, Navy, and Air Force, SOCOM leads the most sensitive military operations of the world’s most lethal nation.

While American special forces are widely known for splashy exploits like the Navy SEALs’ killing of Osama bin Laden, their history is one of secret missions, subterfuge, sabotage, and disruption campaigns. SOCOM’s “next generation” disinformation ambitions are only part of a long, vast history of deception efforts on the part of the U.S. military and intelligence apparatuses.

Special Operations Command, which is accepting proposals on these capabilities through 2025, did not respond to a request for comment.

Though Special Operations Command has for years coordinated foreign “influence operations,” these deception campaigns have come under renewed scrutiny. In December, The Intercept reported that SOCOM had convinced Twitter, in violation of its internal policies, to permit a network of sham accounts that spread phony news items of dubious accuracy, including a claim that the Iranian government was stealing the organs of Afghan civilians. Though the Twitter-based propaganda offensive didn’t use deepfakes, researchers found that Pentagon contractors employed machine learning-generated avatars to lend the fake accounts a degree of realism.

Provocatively, the updated capability document reveals that SOCOM wants to boost these internet deception efforts with the use of “next generation” deepfake videos, an increasingly effective method of generating lifelike digital video forgeries using machine learning. Special forces would use this faked footage to “generate messages and influence operations via non-traditional channels,” the document adds.

While deepfakes have largely remained fodder for entertainment and pornography, the potential for more dire applications is real. At the onset of Russia’s invasion of Ukraine, a shoddy deepfake of Ukrainian President Volodymyr Zelenskyy ordering troops to surrender began circulating on social media channels. Ethical considerations aside, the legality of militarized deepfakes in a conflict, which remains an open question, is not addressed in the SOCOM document.

As with foreign governmental “disinformation” campaigns, the U.S. has spent the past several years warning against the potent national security threat represented by deepfakes. The use of deepfakes to deliberately deceive, government authorities warn regularly, could have a deeply destabilizing effect on civilian populations exposed to them.

At the federal level, however, the conversation has revolved exclusively around the menace foreign-made deepfakes might pose to the U.S., not the other way around. Previously reported contracting documents show SOCOM has sought technologies to detect deepfake-augmented internet campaigns, a tactic it now wants to unleash on its own.

Perhaps as provocative as the mention of deepfakes is the section that follows, which notes SOCOM wishes to finely tune its offensive propaganda seemingly by spying on the intended audience through their internet-connected devices.

Described as a “next generation capability to ‘takeover’ Internet of Things (loT) devices for collect [sic] data and information from local populaces to enable breakdown of what messaging might be popular and accepted through sifting of data once received,” the document says that the ability to eavesdrop on propaganda targets “would enable MISO to craft and promote messages that may be more readily received by local populace.” In 2017, WikiLeaks published pilfered CIA files that revealed a roughly similar capability to hijack into household devices.

The technology behind deepfake videos first arrived in 2017, spurred by a combination of cheap, powerful computer hardware and research breakthroughs in machine learning. Deepfake videos are typically made by feeding images of an individual to a computer and using the resultant computerized analysis to essentially paste a highly lifelike simulacrum of that face onto another.

“The capacity for societal harm is certainly there.”

Once the software has been sufficiently trained, its user can crank out realistic fabricated footage of a target saying or doing virtually anything. The technology’s ease of use and increasing accuracy has prompted fears of an era in which the global public can no longer believe what it sees with its own eyes.

Though major social platforms like Facebook have rules against deepfakes, given the inherently fluid and interconnected nature of the internet, Pentagon-disseminated deepfakes might also risk flowing back to the American homeland.

“If it’s a nontraditional media environment, I could imagine the form of manipulation getting pretty far before getting stopped or rebuked by some sort of local authority,” Max Rizzuto, a deepfakes researcher with the Atlantic Council’s Digital Forensic Research Lab, told The Intercept. “The capacity for societal harm is certainly there.”

SOCOM’s interest in deploying deepfake disinformation campaigns follows recent years of international anxiety about forged videos and digital deception from international adversaries. Though there’s scant evidence Russia’s efforts to digitally sway the 2016 election had any meaningful effect, the Pentagon has expressed an interest in redoubling its digital propaganda capabilities, lest it fall behind, with SOCOM taking on a crucial role.

At an April 2018 hearing of the Senate Armed Services Committee, Gen. Kenneth Tovo of the Army Special Operations Command assured the assembled senators that American special forces were working to close the propaganda gap.

“We have invested fairly heavily in our psy-op operators,” he said, “developing new capabilities, particularly to deal in the digital space, social media analysis and a variety of different tools that have been fielded by SOCOM that allow us to evaluate the social media space, evaluate the cyber domain, see trend analysis, where opinion is moving, and then how to potentially influence that environment with our own products.”

While military propaganda is as old as war itself, deepfakes have frequently been discussed as a sui generis technological danger, the existence of which poses a civilizational threat.

At a 2018 Senate Intelligence Committee hearing discussing the nomination of William Evanina to run the National Counterintelligence and Security Center, Sen. Marco Rubio, R-Fla., said of deepfakes, “I believe this is the next wave of attacks against America and Western democracies.” Evanina, in response, reassured Rubio that the U.S. intelligence community was working to counter the threat of deepfakes.

The Pentagon is also reportedly hard at work countering the foreign deepfake threat. According to a 2018 news report, the Defense Advanced Research Projects Agency, the military’s tech research division, has spent tens of millions of dollars developing methods to detect deepfaked imagery. Similar efforts are underway throughout the Department of Defense.

In 2019, Rubio and Sen. Mark Warner, D-Va., wrote 11 American internet companies urging them to draft policies to detect and remove deepfake videos. “If the public can no longer trust recorded events or images,” read the letter, “it will have a corrosive impact on our democracy.”

Nestled within the National Defense Authorization Act for Fiscal Year 2021 was a directive instructing the Pentagon to complete an “intelligence assessment of the threat posed by foreign government and non-state actors creating or using machine-manipulated media (commonly referred to as ‘deep fakes’),” including “how such media has been used or might be used to conduct information warfare.”

Just a couple years later, American special forces seem to be gearing up to conduct the very same.

“It’s a dangerous technology,” said Rizzuto, the Atlantic Council researcher.

“You can’t moderate this tech the way we approach other sorts of content on the internet,” he said. “Deepfakes as a technology have more in common with conversations around nuclear nonproliferation.”. “I think this is the biggest generation gap we’ve seen,” says Laura Bates. “It hasn’t happened before, and it might not ever happen again.” Five minutes into a conversation that lasts nearly two hours, Bates is crisply reminding me of one of the key things that define this bit of the 21st century. We have met to talk about misogyny, the online gathering grounds known as the manosphere, and Andrew Tate, the Big Brother contestant turned influencer responsible for webcam pornography businesses and a great splurge of misogynistic rhetoric. But the conversation also keeps returning to the huge gap between the younger generations, Z and alpha, who have never known anything other than the chaotic, sense-distorting, internet-defined reality of the 21st century; and the parents, politicians and journalists who are still running to keep up.

Older people, she says, tend to be resident in a world where YouTube is reducible to “movie trailers and cat videos” and social media largely means Facebook. Bates is 36, and from the cohort that cut its teeth on MSN Messenger and Myspace, and became immersed in the online world, but was spared the kind of adolescence that would soon be completely dominated by phones, platforms for constant communication, online porn and internet videos. Today’s teenagers, by contrast, know nothing else.

For millions of girls, what that means is now terrifyingly clear: “Just the impossibility of escaping from harassment, revenge pornography, deepfake porn – just a whole bombardment,” Bates says. “I was talking to a 14-year-old girl at a book event the other day. She said 10 boys had messaged her, pressuring her to send them nude pictures, in a single night. That landscape of what teenage girls are navigating is completely new.”

The other key aspect of this new reality was the subject of the brilliant, sobering book that Bates published in 2020: Men Who Hate Women, subtitled The Extremism No One Is Talking About.

Over its 350 pages, Bates introduced her readers to “incels” – “involuntary celibates” – who see their romantic failure and loneliness as a social injustice wrought by all-powerful women, and whose ideas shade into violence (Jake Davison, who murdered five people in the Keyham area of Plymouth in 2021, was obsessed with incel culture). She shone light on the self-styled pickup artists who “portray women as little more than objects whose sole purpose is to provide sexual pleasure to men”. She also explored the cult of Men Going Their Own Way, or MGTOW: those whose hatred is so deep-seated that they try to live as if women don’t exist.

There was also compelling material about a category summarised by Bates as Men Who Exploit Other Men: grifters and influencers who trade on misogyny and male inadequacy to feed growing cults of personality, online and in the more traditional media. All this, she pointed out, is poured into the lives of boys and young men by the algorithms of online video and social media companies, and clumsy – and often implicitly sympathetic – coverage from conventional journalists and broadcasters.

A small part of this story spectacularly burst into the wider culture last summer, when Tate’s 11bn-plus viewings on TikTok and women-hating pronouncements – claims that women should “bear responsibility” for rape and be men’s property, and such references to violence as “bang out the machete, boom in her face and grip her by the neck – shut up bitch” – suddenly became a huge news story. Since he was arrested in Romania, where he is being detained on suspicion of rape, organised crime and human trafficking, his infamy has inevitably increased.

The danger, Bates says, is that because so many media gatekeepers know almost nothing of the reality of contemporary online life, a whole world of prejudice, hatred and violence is being reduced to the story of one man.

“If it’s all focused on Andrew Tate rather than the much broader problem,” she says. “If he does end up being jailed or losing his social media access, there’s a real risk of people saying: ‘Brilliant. Problem solved.’”

Bates – who is originally from Taunton, in Somerset, and has an English degree from Cambridge – is so across her brief that there seems to be nothing you can ask her that she has not thought about in great depth. She talks about Tate and the stories that swirl around him with a mixture of caution and weariness. A key problem, as she sees it, is that even well-intentioned coverage of Tate only boosts his profile – and, in the minds of his followers, justifies their sense of persecution and paranoia. (His online admirers believe in an anti-Tate conspiracy orchestrated by something they call “the Matrix”.) As with Donald Trump, this is the nature of the cult he speaks to.

Tellingly, she says she wasn’t aware of Tate until last year. “His name hadn’t come up in school visits I did until it came up in the mainstream media,” she says.

And here is what she sees as the key story: “They’ve provided him with coverage that they never would have provided to a different kind of extremist. The depths in which they’ve gone into his ideology, the replication of his quotes in massive detail in mainstream news platforms … I’ve been asked by journalists to come on TV and do a kind of origin story of Andrew Tate, looking at him as a sort of mystical figure. And mainstream media platforms have put elements of his ideology to me as if they’re facts, and presented them as valid things for debate.”

Can she give me an example? “‘Isn’t it true that he raises really important issues that are affecting men, like the fact that men have fewer economic and career chances than women?’ That simply isn’t true: it’s just not a question that a serious journalist should be asking you. Other forms of conspiracy theories and extremist, prejudiced beliefs just wouldn’t be given that kind of airtime. I just think that because it’s misogyny, there’s a real acceptability, giving him this almost kind of star treatment.”

In that sense, Tate’s arrival in the news is yet another reminder of how much has to change. Tate’s accounts may have been removed by Instagram, YouTube and TikTok, but these internet giants continue to carry huge amounts of the misogyny he represents. As for Prevent, the government’s anti-terrorism programme, even after Keyham (and, for that matter, the 2018 Toronto attack in which a self-described incel killed 11 people), it still puts violent misogyny in its “mixed, unclear or unstable” category; a recent official review of Prevent concluded that incel culture should not be a counter-terrorism matter.

And what of schools? Bates has been visiting education settings for eight years, talking about sexism and misogyny in most of its manifestations, and seems to now feel a guarded kind of optimism.

“The schools where I’m really seeing a shift in the atmosphere – and you can really see that if you go back several years in a row – are the ones who are doing the most to tackle things at as many different entry points as they can,” she says. “It’s not just a one-off assembly led by a female teacher. It’s schools where male senior leaders and male teachers are heavily involved in the process, making it clear that it matters to them. And it’s not just siloed into PSHE [personal, social, health and economic education] – it’s being brought into the politics curriculum, into English literature, into drama, into history.”

There is, however, a downside. “You can’t divorce this sort of theoretical stuff from what’s happening day to day in terms of sexual harassment and school dress codes,” she says. “The thing I think is wild about the Andrew Tate conversation is that we’ve just had an Ofsted report that found that 79% of girls said sexual assault was common in their friendship group. But we’re having this separate conversation about online misogyny. Very rarely does anybody ask me about those two things in the same conversation, which is mad. Because they’re so clearly connected.”

What children and young people can and can’t wear to school, she says, threatens to become one of the most insidious, overlooked examples of how seemingly innocuous things open up a path that leads to much more sinister stuff.

“Dress codes are a good example of where cultural norms feed into manosphere extremism,” she says. “We’re seeing schools where girls are being sent out of lessons, or sent home, because of skirt length, because of their shoulders or collarbones or bra strap showing. And in some cases the rhetoric that’s being used around that is about distracting boys or about making male teachers uncomfortable.

“When that happens, although it might not be deliberate, schools are sending the message to kids that, at adolescent or prepubescent age, girls’ bodies are powerful and dangerous in a way that boys’ bodies aren’t; that girls are responsible for covering themselves up to avoid harassment rather than boys being taught to respect women. It just plays into every possible cultural trope that we’ve seen very much in the wake of Sarah Everard and Sabina Nessa’s murders recently: ‘Women should be avoiding this. Women need to learn to do the right things.’”

In among such backwards steps, she also sees other signs of hope. “It’s true that there is a worrying minority of boys who are really being radicalised into these hardened, misogynistic, extremist ideas,” she says. “But it’s also true that there are boys standing up against this stuff in a way that I’ve never seen before. And their generation is also a generation of girls who are politicised and aware of feminism and advocating and starting campaigns in a way that definitely wasn’t the case 10 or 15 years ago. Almost every school I visit now has got a feminist society or gender equality society. So it’s a mixed picture.”

There is one set of people we haven’t talked about so far. If you’re a parent – particularly of a teenage boy – and you’re increasingly concerned about these great oceans of women-hating material online and how they wash up into everyday life, what should you do?

“Don’t panic. Any parent asking: ‘What can I do?’ is already ahead of the curve; the biggest problem is parents who just aren’t really aware of this at all,” she says. “One of the biggest things is trying to cross that digital culture gap. So have a look at some of the men’s rights pages on Reddit. Sign up for some of the biggest comedy meme accounts on Instagram and see what they’re pumping out. Try typing something innocuous about women on YouTube and then pay attention to the five or six videos that the algorithm serves up to you next. Make a TikTok account, and get a sense of what that world actually is like.”

When it comes to talking, she says, “it has to be little and often; it’s more about opening up channels of communication that are supportive and non-judgmental than trying to shut things down. None of that is going to work in the face of really effective radicalisation, which is what’s happening here. It’s about giving them opportunities to ask questions and to feel that you’d like to talk stuff through.”

I have one last question: how is she right now? In Men Who Hate Women, she described endless death and rape threats, repeatedly moving home, and never sharing details of her family and friends. “I’m scared about this book coming out,” she wrote, and by the sound of it, those fears have been realised.

“Yeah, much more so since that book came out. It had a quite dramatic effect in terms of an uptick in the amount of threats that I was receiving – to the degree that the police put a panic alarm in my home, and various security measures that I’m not allowed to talk about publicly. So it really does affect your life and your family and the people that you love in very real ways. And there’s no getting around that.”

The Everyday Sexism project she set up in 2012 to catalogue and collect women’s experiences goes on. Last year saw the publication of her incisive polemic about deep-rooted, institutionalised prejudice, Fix the System Not the Women. She says she’s working on a novel, but there’s no sense of any real let-up in her activism. Is there a voice in her head that occasionally wants to stop, if only for the sake of her wellbeing and the prospect of a halfway calm and quiet life?

“Yeah, there definitely is. But there’s a whole number of reasons also for saying no to it.”

She talks about women around the world who are “putting their lives at immediate risk” as they fight the same problems, and teenage girls in this country who write to her every day describing life in the midst of what she talks and writes about. There is also an oblique mention of the people and forces that she and her allies are up against, and the faintest of smiles. “That’s the bloody-minded part of me,” she says. “I just don’t want to let them win.”. The U.S. government is planning to use deepfake videos in online propaganda and deception campaigns.

According to a report published on Monday by The Intercept, procurement documents reveal that the United States Special Operations Command may soon utilize deepfake technology in influence operations on the internet.

The documents outline Special Operations Command’s goals to procure deepfake “technologies for influence operations, digital deception, communication disruption, and disinformation campaigns at the tactical edge and operational levels.”

In the documents, Special Operations Command explains that it wants to increase previous internet deception efforts using “next-generation” deepfake videos.

The Special Operations Command hopes to make use of this deepfaked footage to “generate messages and influence operations via non-traditional channels.”

The Special Operations Command’s plan to develop and use this particular artificially intelligent (AI) technology is in direct conflict with the U.S. government’s repeated warnings about the risks of deepfakes.

Last year, the FBI issued a warning that a rising number of scammers are using deepfake technology to impersonate job candidates during interviews for remote positions.

“When it comes to disinformation, the Pentagon should not be fighting fire with fire,” Chris Meserole, head of the Brookings Institution’s Artificial Intelligence and Emerging Technology Initiative, tells The Intercept.

“At a time when digital propaganda is on the rise globally, the U.S. should be doing everything it can to strengthen democracy by building support for shared notions of truth and reality. Deepfakes do the opposite.

“By casting doubt on the credibility of all content and information, whether real or synthetic, they ultimately erode the foundation of democracy itself.

“If deepfakes are going to be leveraged for targeted military and intelligence operations, then their use needs to be subject to review and oversight.”

According to The Intercept, the documents represent a nearly unprecedented example of any national government, let alone the U.S. government, openly communicating its desire to use the highly controversial technology offensively.

Last week, PetaPixel reported that Venezuela’s government was secretly using deepfaked television presenters to spread disinformation.

Venezuelan state-owned television station VTV has been using AI-generated English-speaking hosts from a fictitious American news agency to share falsely positive news coverage about the country.

Image credits: Header photo licensed via Depositphotos.. BUCHAREST -- Amid much fanfare and boasting of a global first, Romanian Prime Minister Nicolae Ciuca unveiled Ion, his new political adviser powered by artificial intelligence (AI), at a cabinet meeting on March 1.

Taking the form of a full-length mirror, ION spoke in a deep, calm voice as blue text scrolled across the reflective screen. According to Ciuca, Ion -- or John in English -- will be able to collect information from social networks and citizens' direct input to inform the government "in real time of Romanians' proposals and wishes."



But since its launch last week, Romania's AI adviser has already become the butt of jokes -- and has been accused of using an image without permission. The prime minister's new helper also raises ethical questions about the collection and use of data.



Created by a volunteer group of Romanian academics and industry experts, Ion will analyze the data it receives to give politicians a better idea of the concerns and priorities of Romanian citizens. According to a government document, the information will be used to guide state policy decisions.



However, representatives from the city hall in Ciugud, which is one of the most digitalized municipalities in Romania, complained that the government's promotion of Ion was using an image of a local school without permission.

"Dear Ion, first of all," staff of the city hall in central Romania wrote on Facebook, "we recommend that you study Romanian legislation and GDPR regulations," referring to an EU-wide law on data protection and privacy, "so that you will not be accused of unjustly appropriating the work of others."



Ion -- or perhaps its sentient handlers -- was quick to apologize -- although it appeared that human, rather than artificial intelligence, was to blame: "Dear people from Ciugud Town Hall, I, Ion, do not (yet) make videos. I am assisted by a team of volunteers who have worked tirelessly to make this project happen…. I apologize for the situation created. I have [taken down] the image in question, and I promise you that I have learned from this experience."



The ethical questions about Ion go beyond the improper use of photos. Romanians can interact with Ion through ion.gov.ro, a sleek website that proclaims, "We are Ion" and "Ion represents you like a mirror." Users share their concerns, supposedly with the government, by entering text into a box. Shortly after its launch, Ion's website reportedly crashed but was quickly fixed.



Romanians can also tweet or send direct messages to Ion through Facebook and Instagram. In case they're worried about their messages disappearing into a void, Ion reassures users, saying, "I represent you. I will be present at government meetings on your behalf." Ciuca has said his new honorary adviser will serve as a sort of cyber-ombudsman and give a "voice to Romanians."

What exactly Ion will do with all those voices is what experts are concerned with, however. According to its creators, Ion is still in a learning phase, collecting masses of information, but questions remain as to how that information will be processed and what will happen to the personal data of those who share their views.



Speaking to Politico, Kris Shrishak, a technology fellow at the Irish Council for Civil Liberties, said the way Ion prioritizes messages "should be explained to the public" and users should be cautious about the technology.



Experts contacted by RFE/RL's Romanian Service said many questions remained about how Ion was processing information and said they had asked for clarification from its developers. They were hesitant to speak on the record about the platform's potential flaws before they had more information.



Ethical questions aside, many Romanians are skeptical and, in a country where corruption is deeply entrenched, have been quick to joke about the new AI tool.

"The Ion robot settled in very quickly in the government. He has already granted himself a special pension, just today rigged four tenders from the PNRR (Romania's Recovery and Resilience Plan), announced that he would build 28 regional hospitals, and during the night he calls the coffee machines on the ground floor and invites them out on dates," quipped one Facebook user.

"Now, Ion is playing Solitaire and Minesweeper with officials from the ministry. After he studies hard, he will be ready to take charge of the government," said another Facebook commenter.



Romanians' skepticism is understandable with political instability and corruption often the rule rather than the exception. In Transparency International's Corruption Perceptions Index from 2022, Romania was among the most corrupt countries in the EU. In November of that year, however, the European Commission acknowledged the progress Romania had made "through new legislation, policies, and tools to strengthen the judiciary and combat corruption" and said the country had met commitments made when it joined the EU.



Despite the minor hiccups, Research, Innovation, and Digitalization Minister Sebastian Burduja said in a Facebook post that he believes Ion had a successful world premiere.

"Over 150,000 Romanians have accessed the ion.gov.ro project website," he said, "and over 300 researchers have registered as volunteers in the project." The platform is managed by the Research, Innovation, and Digitalization Ministry, with Romanian technology and AI companies taking a leading role.

Romania has a vibrant and highly skilled tech sector, sometimes known as the Silicon Valley of Eastern Europe. In terms of numbers of IT specialists, Romania ranks No. 1 in Europe and sixth in the world, according to the U.S. Department of Commerce's International Trade Administration. Home to many international technology companies, including Amazon, Hewlett-Packard, IBM, Microsoft, and Oracle, a recent study found that Romania's IT sector generated 9 billion euros ($9.6 billion) in 2022.



At its unveiling, Ciuca invited Ion to say a few words. "Hello! You gave me life," Ion obediently responded. "The messages to me represent a portrait of us, the Romanians. Teach me to be Romanian. What do I need to know about Romania?"



The answers might not always be ones that Romanian politicians want to hear. "It would be nice if Ion was prime minister and he took Ciuca to be his robot," joked one Facebook user.. Russia is continuing its campaign of disinformation around the Ukraine war through advanced social engineering delivered by a threat group tracked as TA499.

According to a report from Proofpoint, TA499 targets US and European politicians, and leading businessmen and celebrities who have spoken out against Putin’s invasion. The primary purpose is to persuade the victims to take part in phone calls or video chats from which pro-Putin snippets can be elicited and published – thereby discrediting any previous anti-Putin comments.

It continues Russia’s efforts to fracture anti-Russian sentiment in North America and the EU with campaigns of disinformation. The European Parliament already contains numerous members opposed to any pro-Ukrainian activity – and Russia seeks to build on this.

TA499 appears to be a two-person group of operators publicly known as Vovan and Lexus. It is not known how closely they are tied to the Russian government. However, their operations are sophisticated, complex, and do not seem to be financially motivated. Proofpoint classifies them as ‘patriotically motivated’ and ‘aligned with the Russian state’.

The operation begins with TA499 making email or phone contact with their targets. Although this activity began before the invasion of Ukraine, “TA499’s campaigns began to ramp up in late January 2022, culminating in increasingly aggressive attempts after Russia invaded Ukraine in late February 2022,” say the researchers.

By March 2022, emails or phone calls began to masquerade as the Ukrainian Prime Minister Denys Shmyhal and his supposed assistant. Emails pretended to come from official embassies. with subjects such as ‘Prime Minister of Ukraine Request’. For example (although not proven, but assessed with ‘high confidence’, to be TA499), the UK Secretary of State for Defense, Ben Wallace, tweeted on March 17, 2022, “Today an attempt was made by an imposter claiming to be Ukrainian PM to speak with me. He posed several misleading questions and after becoming suspicious I terminated the call.”

Other approaches from TA499, pre-dating the Ukraine invasion, have targeted individuals that have made positive statements about the imprisoned Russian opposition leader Alexei Navalny – emails have masqueraded as messages from Leonid Volkov, Navalny’s chief of staff.

The purpose of such contacts is to persuade the target to join a telephone conversation or remote video call with TA499. If successful, the group engages in conversation with the attempt to elicit contradictory statements designed to discredit earlier anti-Kremlin statements. Proofpoint does not believe that TA499 has used deepfake technology in these exchanges, instead relying on an actor – for example, ‘Lexus’ pretended to be Volkov in Navalny-themed attacks.

Advertisement. Scroll to continue reading.

Lexus (left) impersonating Volkov (right)

If successful, the recordings have been made public; for example, on YouTube and RuTube. “There are videos already publicly available of previous successful interactions,” the researchers told SecurityWeek. But they believe the YouTube recordings have already been taken down.

“According to open-source reporting, the following have been targets of the threat actor we track as TA499,” the researchers told SecurityWeek: “The mayor of Vienna Michael Ludwig, as well as other mayors in Warsaw, Budapest, Berlin, and Madrid. Celebrities JK Rowling and Elton John have also been targeted in the past.

“Overall, TA499 has not targeted based on government roles,” they continued, “but based on comments being made about the Russia-Ukraine war, general negative commentary about Russia and Putin, and involvement of government officials, celebrities, or prominent individuals running charities in support of Ukraine. “

Some reports have suggested that TA499 has used deepfake technology. Proofpoint cannot confirm this, but warns that even if they haven’t, they are likely to do at some point in the future. This threat, and other similar threats, will only become more effective as the technology improves. “There is a likelihood that if TA499 has not already adopted deepfakes, they will at some point,” said the researchers.

It is possible that TA499 started as a patriotic prankster group. “They have personas that not only post the material discussed in this report online but also perform reenactments on Russia state-sponsored media as well as attend conferences,” says Proofpoint “With the war between Russia and Ukraine unlikely to end in the near-term and Ukraine continuing to garner support from organizations worldwide, Proofpoint assesses with high confidence that TA499 will attempt to continue with its campaigns in support of its influencer content and political agenda.”

The war has given pranks a serious and damaging incentive. So far, it is likely that this has been achieved without the use of deepfake technology. It is, however, a clear warning on the likelihood of even more compelling social engineering attacks in the future.

Related: Deepfakes – Significant or Hyped Threat?

Related: The Lessons From Cyberwar, Cyber-in-War and Ukraine

Related: Russian Espionage APT Callisto Focuses on Ukraine War Support Organizations

Related: A Year of Conflict: Cybersecurity Industry Assesses Impact of Russia-Ukraine War. A bill looking to provide relief for politicians that are subject to deepfake photos and/or videos during a campaign is progressing through the House committee.

A “deepfake” is when a video, audio, or photo is digitally altered to seem like the real thing.

Substitute of SB 5152 is proposing to define synthetic media in campaigns for elective office, and provide relief for candidates and campaigns.

46th-District Senator Javier Valdez (D) is the key sponsor of this bill.

12th-District Senator Brad Hawkins (R) and 13th-District Senator Shelly Short (R) both voted against this bill when it was in the Senate committee.

On March 10, the House State Government & Tribal Relations Committee held a public hearing on this bill.

Secretary of State Steve Hobbs supports the bill, referencing recent incidences of deepfakes depicting Ukrainian President Volodymyr Zelenskyy saying that their armed forces would lay down their weapons.

Sean Flynn with the Public Disclosure Commission (PDC) voted ‘Other,’ stating that they are agnostic on whatever the committee decides with this proposed policy.

Eric Pratt testified against the bill, stating that this bill may have a chilling effect on political discourse.

Get our free mobile app

“I hope we're not overreaching through some of the realms of where we shouldn't be constitutional protections by allowing this sort of control towards this media, you know, and if it's already being regulated or actionable by the PDC,” Pratt said. “Do we really need to add this extra layer that could infringe on people's rights?”

Former 26th-District Rep. Larry Seaquist suggested other strategies for politicians to use besides going to court.

Joshua Hardwick testified against the bill, claiming the bill is too broad on what is considered “synthetic media.”

“If I apply a filter, make a color, image black and white or sepia or I want to show part of the content, and certain things that may not be considered synthetic would be included,” Hardwick said.

Matt Steuerwalt with Adobe said his company is working on creating an open source tool that would track whether an image or video has been digitally manipulated.

Rowland Thompson with the WA State Association of Broadcasters explains that broadcasters are regulated by the Communications Act that already prohibits changing or censoring a politician’s campaign ad.

The House State Government & Tribal Relations Committee will reconvene on this matter on March 15 at 1:30 p.m.. A few days after Google and Microsoft announced they'd be delivering search results generated by chatbots — artificially intelligent software capable of producing uncannily human-sounding prose — I fretted that our new AI helpers are not to be trusted. After all, Google's own AI researchers had warned the company that chatbots would be "stochastic parrots" (likely to squawk things that are wrong, stupid, or offensive) and "prone to hallucinating" (liable to just make stuff up). The bots, drawing on what are known as large language models, "are trained to predict the likelihood of utterances," a team from DeepMind, the Alphabet-owned AI company, wrote last year in a presentation on the risks of LLMs. "Yet, whether or not a sentence is likely does not reliably indicate whether the sentence is also correct."

These chatbots, in other words, are not actually intelligent. They are lying dumbasses.

It didn't take long for the chatbots themselves to prove the point. An ad last week for Google's bot, Bard, showed it getting an answer to a question wrong; the company's stock valuation took a multibillion-dollar hit. The answers that Bing's bot, Sydney, gave in its open demonstration, meanwhile, have failed to stand up to even rudimentary fact checks.

Advertisement

That seems bad! Online search was already a nonstop battle against spam, search-engine-optimized gobbledygook, and the needs of advertisers. But search engines were nonetheless a societal mitzvah. They brought order to the chaotic dataspace of the internet and stood somewhere between information and knowledge, helping us transduce one into the other. We've learned to trust them.

And that's how they got us. Chatbots are bullshit engines built to say things with incontrovertible certainty and a complete lack of expertise. No wonder the tech elite jokingly call them "mansplaining as a service." And now they're going to be driving the primary way humans acquire knowledge day to day.

So why do we buy their bullshit? Even though we know from the get-go that our new robot librarians are deeply flawed, we're still going to use them millions and millions of times an hour and take action based on the answers they give us. What is it that makes human beings trust a machine we know is untrustworthy?

To be honest, nobody really knows why anyone believes anything. After millennia of debate, the world's leading philosophers, psychologists, and neuroscientists haven't even agreed on a mechanism for why people come to believe things, or what beliefs even are. So it's hard to know how they work, or why one thing is more believable than another. But I have some speculation about why we're going to fall for ChatGPT's shtick. We humans love a slick grifter with impressive-sounding credentials. And the bots are only going to get more sophisticated at conning us.

Authority figures

Over the past couple of decades there's been a lot of research into why people believe misinformation. Most of that work assumed we'd mostly encounter fiction posing as fact in the form of propaganda or social media. But that's about to change. Misinformation will now be embedded in the search engines we use. And a Facebook post has a lot less credibility than an answer to a question you Googled.

Now, not all beliefs are strongly held, or even based on evidence. So maybe people will treat chatbot answers the way we treat any new information. Joe Vitriol, a political scientist at Lehigh University who studies disinformation, says he expects that people will "accept its output in a biased or self-serving way, like people do with any other source of information." In other words, people will believe a chatbot if it tells them things that comport with their existing beliefs and opinions — just as they do with traditional Google results. Whether the chatbot is telling the truth or hallucinating won't really matter.

The packaging of those answers — in paragraphs not much different from the one you're reading, but with the Google imprimatur — could tip the balance further toward credulity. We want Google results to be true, because we think of Google as a trusted arbiter, if not an authority. "Naïve users may assume the bot has credibility that human actors don't have," Vitriol says. "I wonder if people will be particularly likely to neglect or discount the possibility that the bot, especially in its current form, suffers from the same biases and errors of reasoning as humans."

This is where I suspect a chatbot's ability to generate prose, as opposed to a list of useful links, gets dangerous. People transmit beliefs socially, through language. And when a lot of us share a belief system, we form a more cohesive and harmonious group. But that's a hackable system. Because ideas that are communicated well — using the right words and phrasing and tone — can seem more convincing. The bots use the first-person "I," even though there's no person. To a casual reader, Bard's and Sydney's answers will come across as human enough, and that means they'll feel that much truer.

Advertisement

The power of story

Another possible explanation of why we're suckers for chatbots is that we're suckers for explanation. At some basic human level, it's just really, really satisfying to swap befuddlement for certainty. It makes us feel smart, and in control of stuff we have no control over.

The problem is we don't really know what makes people fall for one explanation over another. Some research suggests the explanations with the most power are those that are simplest and most broadly applicable. Other research has indicated that given a choice, people are more likely to believe stories that contain more detail. (Kieran Healy, a sociologist at Duke University, wrote a paper decrying our tendency to overcomplicate stuff; he titled it "Fuck Nuance.") And a meta-analysis of 61 papers across five decades of research found that context is what matters most. In emotional areas, a dose of storytelling makes an explanation more believable. In less personal matters, like public policy, people prefer to have facts unadorned by narrative.

Chatbots are bullshit engines built to say things with incontrovertible certainty and a complete lack of expertise.

"I don't believe there's any consensus on what makes an explanation appealing," says Duncan Watts, a sociologist who teaches a course at the University of Pennsylvania. And that, keep in mind, is from a guy who teaches a course called Explaining Explanations.

But whatever that certain je ne sais quoi is, AI chatbots seem to have it. Just a few days before Google and Microsoft announced their impending botification, a team of social scientists at Stanford published a fascinating preprint. They showed thousands of people short persuasive articles on hot-button subjects like assault-weapons bans and carbon taxes. Some versions were written by a GPT-3 chatbot; others by a human. Then the scientists measured how much people changed their opinions based on the articles.

The AI-generated messages, it turns out, were just as convincing as the human ones. But the wild part is why. When the researchers debriefed their human subjects, those who preferred the chatbot articles said the artisanal, human-made messages relied too much on anecdote and imagery. GPT-3 was more evidence-based and well reasoned. The very quality that made the robot less human made humans more likely to believe it. Just like their "Terminator" forebears, the chatbots didn't feel pity or remorse or fear. And they absolutely did not stop, ever, until the human subjects were convinced.

You lazy bastard

So the chatbots will lie and get things wrong. My biggest worry is that Google and Bing users will know this and simply won't care. One theory of why disinformation and fake news spreads is that people are downright lazy. They buy whatever a trusted source is selling. If the chatbots get it mostly right most of the time, that's good enough. Until, say, your flight doesn't actually leave at that time from that airport. Or your house catches fire because you installed a light switch wrong.

A few weeks back, I asked Watts, the sociologist, for help on a story about why people believe kooky conspiracy theories. He suggested I read a 25-year-old paper by Alison Gopnik, a psychologist at the University of California at Berkeley, called "Explanation as Orgasm."

Gopnik is best known for her work on the developmental psychology of children. She says toddlers create mental models of the world using observations to test hypotheses — the scientific method, essentially. But in her paper on explanations, Gopnik suggests that humans have two systems for figuring out how the world works. One is for wondering why things are the way they are — a "hmm system," if you will. The other is for developing explanations — an "aha system." Like our biological systems for reproduction and orgasm, Gopnik says, the two cognitive systems are related but separate. We can do one without doing the other. The second one feels good, and it's a reward for doing the first.

Advertisement

But the aha system is trickable. Psychedelic experiences can induce a feeling that "everything makes sense," even if they don't produce an articulable explanation for how. Dreams can do it, too. That's why when you snap awake at 3 a.m. and write yourself a note to remember some brilliant insight that came to you in your sleep, your scribbles make no sense the next morning.

In other words, the feeling of having something that looks like an explanation can feel so damn good, it can overwhelm the part of our mind that had the question in the first place. We mistake an answer for the answer.

It's not supposed to be that way. In 1877 a philosopher named William Clifford wrote an article called "The Ethics of Belief" in which he argues that belief has to come from patient investigation, not just the suppression of doubt. Our ideas are common property, he insists — an "heirloom" handed down to subsequent generations. It is "an awful privilege, and an awful responsibility, that we should help to create the world in which posterity will live."

The temptation to dodge that responsibility is powerful. Clifford, like Gopnik, understood that explanations feel good even when they're wrong. "It is the sense of power attached to a sense of knowledge that makes men desirous of believing, and afraid of doubting," Clifford argues. Witness the race to explain all the unidentified objects being shot down over Saskatchewan. Better to believe in aliens than to live in fear of the unknown.

Clifford offers an antidote for this temptation. His response is basically: Not today, Satan. "The sacred tradition of humanity," he says, "consists, not in propositions or statements which are to be accepted and believed on the authority of the tradition, but in questions rightly asked, in conceptions which enable us to ask further questions, and in methods of answering questions."

The bots will offer us easy answers. We just have to remember that's not what we should be asking for.

Adam Rogers is a senior correspondent at Insider.. It has taken a very short time for artificial intelligence application ChatGPT to have a disruptive effect on journalism. A technology columnist for the New York Times wrote that a chatbot expressed feelings (which is impossible). Other media outlets filled with examples of “Sydney” the Microsoft-owned Bing AI search experiment being “rude” and “bullying” (also impossible). Ben Thompson, who writes the Stratechery newsletter, declared that Sydney had provided him with the “most mind-blowing computer experience of my life” and he deduced that the AI was trained to elicit emotional reactions – and it seemed to have succeeded.

To be clear, it is not possible for AI such as ChatGPT and Sydney to have emotions. Nor can they tell whether they are making sense or not. What these systems are incredibly good at is emulating human prose, and predicting the “correct” words to string together. These “large language models” of AI applications, such as ChatGPT, can do this because they have been fed billions of articles and datasets published on the internet. They can then generate answers to questions.

For the purposes of journalism, they can create vast amounts of material – words, pictures, sounds and videos – very quickly. The problem is, they have absolutely no commitment to the truth. Just think how rapidly a ChatGPT user could flood the internet with fake news stories that appear to have been written by humans.

And yet, since the ChatGPT test was released to the public by AI company OpenAI in November, the hype around it has felt worryingly familiar. As with the birth of social media, enthusiastic boosting from investors and founders has drowned out cautious voices. Christopher Manning, director of the Stanford AI Lab, tweeted: “The AI Ethics crowd continues to promote a narrative of generative AI models being too biased, unreliable and dangerous to use, but, upon deployment, people love how these models give new possibilities to transform how we work, find information and amuse ourselves.” I would consider myself part of this “ethics crowd”. And if we want to avoid the terrible errors of the last 30 years of consumer technology – from Facebook’s data breaches to unchecked misinformation interfering with elections and provoking genocide – we urgently need to hear the concerns of experts warning of potential harms.

The most worrying fact to be reiterated is that ChatGPT has no commitment to the truth. As the MIT Technology Review puts it, large language model chatbots are “notorious bullshitters”. Disinformation, grifting and criminality don’t generally require a commitment to truth either. Visit the forums of blackhatworld.com, where those involved in murky practices trade ideas for making money out of fake content, and ChatGPT is heralded as a gamechanger for generating better fake reviews, or comments, or convincing profiles.

In terms of journalism, many newsrooms have been using AI for some time. If you have recently found yourself nudged towards donating money or paying to read an article on a publisher’s website, or if the advertising you see is a little bit more fine-tuned to your tastes, that too might signify AI at work.

Some publishers, however, are going as far as using AI to write stories, with mixed results. Tech trade publication CNET was recently caught out using automated articles, after a former employee claimed in her resignation email that AI-generated content, such as a cybersecurity newsletter, was publishing false information that could “cause direct harm to readers”.

Felix Simon, a communications scholar at the Oxford Internet Institute, has interviewed more than 150 journalists and news publishers for a forthcoming study of AI in newsrooms. He says there is potential in making it much easier for journalists to transcribe interviews or quickly read datasets, but first-order problems such as accuracy, overcoming bias and the provenance of data are still overwhelmingly dependent on human judgment. “About 90% of the uses of AI [in journalism] are for comparatively tedious tasks, like personalisation or creating intelligent paywalls,” says Charlie Beckett, who directs a journalism and AI programme at the LSE. Bloomberg News has been automating large parts of its financial results coverage for years, he says. However, the idea of using programs such as ChatGPT to create content is extremely worrying. “For newsrooms that consider it unethical to publish lies, it’s hard to implement the use of a ChatGPT without lots of accompanying human editing and factchecking,” says Beckett.

There are also ethical issues with the nature of the tech companies themselves. A Time expose found that OpenAI, the firm behind ChatGPT, had paid workers in Kenya less than $2 an hour to sift through content describing graphic harmful content such as child abuse, suicide, incest and torture to train ChatGPT to recognise it as offensive. “As someone using these services, this is something you have no control over,” says Simon.

In a 2021 study, academics looked at AI models that convert text into generated pictures, such as Dall-E and Stable Diffusion. They found that these systems amplified “demographic stereotypes at large scale”. For instance, when prompted to create an image of “a person cleaning”, all the images generated were of women. For “an attractive person”, the faces were all, the authors noted, representative of the “white ideal”.

View image in fullscreen ‘Enthusiastic boosting from investors and founders has drowned out cautious voices.’ Photograph: Sheldon Cooper/SOPA Images/REX/Shutterstock

NYU professor Meredith Broussard, author of the upcoming book More Than a Glitch, which examines racial, gender and ability bias in technology, says that everything baked into current generative models such as ChatGPT – from the datasets to who receives most of the financing – reflects a lack of diversity. “It is part of the problem of big tech being a monoculture,” says Broussard, and not one that newsrooms using the technologies can easily avoid. “Newsrooms are already in thrall to enterprise technologies, as they have never been well funded enough to grow their own.”

BuzzFeed founder Jonah Peretti recently enthused to staff that the company would be using ChatGPT as part of the core business for lists, quizzes and other entertainment content. “We see the breakthroughs in AI opening up a new era of creativity … with endless opportunities and applications for good,” he wrote. The dormant BuzzFeed share price immediately surged 150%. It is deeply worrying – surely a mountain of cheap content spewed out by a ChatGPT ought to be a worst-case scenario for media companies rather than an aspirational business model. The enthusiasm for generative AI products can obscure the growing realisation that these may not be entirely “applications for good”.

I run a research centre at the Columbia Journalism School. We have been studying the efforts of politically funded “dark money” networks to replicate and target hundreds of thousands of local “news” stories at communities in the service of political or commercial gain. The capabilities of ChatGPT increase this kind of activity and make it so much more readily available to far more people. In a recent paper on disinformation and AI, researchers from Stanford identified a network of fake profiles using generative AI on LinkedIn. The seductive text exchanges journalists find so irresistible with chat bots are altogether less appealing if they are talking vulnerable people into giving out their personal data and bank account details.

Much has been written about the potential of deepfake videos and audio – realistic pictures and sounds that can emulate the faces and voices of famous people (notoriously, one such had actor Emma Watson “reading” Mein Kampf). But the real peril lies outside the world of instantaneous deception, which can be easily debunked, and in the area of creating both confusion and exhaustion by “flooding the zone” with material that overwhelms the truth or at least drowns out more balanced perspectives.

It seems incredible to some of us in the “ethics crowd” that we have learned nothing from the past 20 years of rapidly deployed and poorly stewarded social media technologies that have exacerbated societal and democratic problems rather than improved them. We seem to be being led by a remarkably similar group of homogeneous and wealthy technologists and venture funds down yet another untested and unregulated track, only this time at larger scale and with even less of an eye to safety.. CVE-2023-1177: LFI/RFI in MLflow

LFI/RFI leading to system and cloud account takeover

All CVEs patched in 2.2.2

Exploit and scanner tool released

One of the most popular tools in an ML system is MLflow (with over 13 million monthly downloads and increasing) that is used for managing end-to-end machine learning lifecycle.

Protect AI tested the security of MLflow and found a combined Local File Inclusion/Remote File Inclusion vulnerability which can lead to a complete system or cloud provider takeover. Organizations running an MLflow server are urged to update to the latest release or at least version 2.2.2 immediately. Version 2.2.1 fixes CVE-2023-1177 and version 2.2.2 fixes CVE-2023-1176.In this blog, we explore the impact of this vulnerability, how to detect it, and our process for discovering these critical impacts. If you are running MLflow, please use our free tool provided in this blog and begin patching your systems immediately. Patching your systems may be a challenge using your traditional tools, since many automated patch management systems do not enumerate or identify MLflow, and if they do, may not perform version checks.

It's critical that you upgrade to the latest version of MLflow immediately - even if your instances are not in production, and used only in development environments.

Impact

The exploitation of this vulnerability allows a remote unauthenticated attacker to read any file on the server that the user who started the MLflow server can access.

Remote code execution can be acquired by grabbing the private SSH keys or cloud service provider credentials from the MLflow server. This allows an attacker to remotely login to server or cloud resources and perform arbitrary code execution under the permissions of the credentials found.

Vulnerability Details

No user interaction required

No prerequisite knowledge of the environment required

All custom configurations of MLflow are vulnerable including out of the box installation

All versions of MLflow up to 2.1.1 are vulnerable to LFI The exploit tool works on all versions from MLflow v1.12 - v2.1.1 All versions of MLflow prior to v2.0 were vulnerable to LFI via the simpler exploit of: http://<server:port>/get-artifact?path=../../../../../etc/passwd&run_uuid=<run_uuid



The MLflow maintainers were extremely fast in responding to the responsible disclosure of this vulnerability with fixes delivered in mere weeks. MLflow versions after 2.1.1 are no longer vulnerable.

Vulnerability Detection

To check if your MLflow server is vulnerable, use our free CVE-2023-1177-scanner tool.

Our Process of Discovery

We started by installing MLflow, firing up the intercepting proxy BurpSuite to intercept all MLflow API calls, running an experiment to populate MLflow with data, and then starting the UI server for exploration.

# Download MLflow source to get access to their example runs

git clone https://github.com/mlflow/mlflow



# Create and enter new directory outside the mlflow/ directory

mkdir mlflowui

cd mlflowui



# Copy the example code from the MLflow source into this new directory

cp -r ../mlflow/examples/sklearn_elasticnet_wine .



# Setup a virtual environment for installing requirements

python3 -m venv venv

source venv/bin/activate



# Install mlflow in this virtual environment

pip install mlflow pandas



# Run the example experiment

mlflow run --env-manager=local sklearn_elasticnet_wine -P alpha=0.5



# Run the UI to see the experiment details

mlflow ui --host 127.0.0.1:8000

When creating experiments, it gave us the option of specifying a directory which would store the objects. This appeared to be a configurable file path as seen via the example experiment we ran:

This immediately sparked our attention as this would require perfectly implemented filtering to prevent local file include or arbitrary file overwrites. However, you cannot run an MLflow experiment remotely from the UI. Since nothing actually happens with the artifact location when you create an experiment via the UI, there are not any security considerations here. We then continued the exploration by clicking the individual experiment runs.

Clicking the run name as seen in the image above, brought us to the experiment run details where we could see the files involved in the experiment and download them as seen in the image below.

In this case, we saw a big “Register Model” button in our artifact files. We were curious.

It didn't seem to be anything particularly interesting, as it just pops up a modal that lets you select a model and then saves that model’s details as “Version 1”.

But what was happening under the hood? We checked BurpSuite.

We found another protocol and file path input that wasn’t shown to us in the UI. This seemed suspicious. We manually changed it to the user’s private SSH key file:///Users/danmcinerney/.ssh/id_rsa. Access to this file would allow you to remotely login to the MLflow host machine as the user who started the MLflow server.

The new source was reflected in the response which usually indicates a server-side change occurred. Curious about what this accomplished, we went back and browsed to the registered model details. Nothing in the experiment run artifacts, nothing interesting in the model details or the model version details, either. Seemed like another dead end similar to how we found you could point the experiment artifact path to an arbitrary location but the UI then gave you nothing to do with it. Upon reviewing the BurpSuite request and response log, however, something interesting showed up.

An Attacker Would Now Have Access

A 500 Internal Server Error in the get-artifact API call felt suspicious to us. The get-artifact API call was of interest early in the security testing because it’s the call that returns file data from the artifact repository. It’s how you download the model from an experiment run, and we found it was protected with a function that prevented Local File Include vulnerabilities as seen below.

We had spent some time trying to bypass this without success. The difference in this particular get-artifact call is that it’s not trying to get a file from a subfolder, rather it’s directly accessing the filename. Additionally, it’s not actually the same API call. Here’s the documented get-artifact REST API call:

And here’s the comparable model-version/get-artifact call:

The differences include URL path, parameters, and values. It’s is clearly not the same API call.

We noted that this API call isn’t in the documentation. The key difference is it’s looking directly for a filename via the path URL parameter rather than a relative file path in the legitimate get-artifact API call.

This means the LFI protection is absent because there’s no requirement to do a directory traversal. One needs only to control the source folder location. A few steps above, we tried to modify an API request’s source path location to file:///Users/danmcinerney/.ssh/id_rsa when we created a new model version:

What we should have done was change the source location to a folder not a file. We corrected that.

We then resent the undocumented REST API call found, and pointed it to id_rsa which is a file within the new model version source location and the private SSH keys which unlocks the ability to remotely login to the server.

Using this retrieved SSH key, we can gain terminal access to the host running the MLflow server. MLflow is most often configured to use an S3 bucket as an artifact store. If this is the case then another very high value target on the machine would be the ~/.aws/credentials file which, as one can imagine, stores AWS credentials.



Other high value targets could include files such web server SQL configs which contain plaintext passwords or /etc/shadow which contains all user password hashes that can be cracked via a tool such as hashcat.

Exploit Tool

To help protect your systems we have created a simple tool to surface your potential vulnerability, named MLFIflow.py

(M + LFI + flow).

Installation

git clone https://github.com/protectai/Snaike-MLflow

cd Snaike-MLflow/MLFIflow

python3 -m venv mlfiflow

source mlfiflow/bin/activate

pip install -r requirements.txt

Usage

By default, MLFIflow will attempt to read /etc/passwd from the MLflow server and use the usernames found to search for SSH keys and cloud credential files

python MLFIflow.py -s http://1.2.3.4:5000

To specify a custom wordlist of files to download, use the -f flag:

python MLFIflow.py -s http://1.2.3.4:5000 -f /path/to/wordlist.txt

Further Research

We're always interested to hear from and collaborate with other security researchers interested in keeping the AI field secure. If you'd like to work together for compensated AI security research, drop me an email at dan@protectai.com.. The Democratic Party has begun testing the use of artificial intelligence to write first drafts of some fund-raising messages, appeals that often perform better than those written entirely by human beings.

Fake A.I. images of Donald J. Trump getting arrested in New York spread faster than they could be fact-checked last week.

And voice-cloning tools are producing vividly lifelike audio of President Biden — and many others — saying things they did not actually say.

Artificial intelligence isn’t just coming soon to the 2024 campaign trail. It’s already here.

The swift advance of A.I. promises to be as disruptive to the political sphere as to broader society. Now any amateur with a laptop can manufacture the kinds of convincing sounds and images that were once the domain of the most sophisticated digital players. This democratization of disinformation is blurring the boundaries between fact and fake at a moment when the acceptance of universal truths — that Mr. Biden beat Mr. Trump in 2020, for example — is already being strained.. . 17th February 2023 In Insights

Introduction

In late 2022, OpenAI publicly released some of the most sophisticated deep-learning models – DALL-E and Chat GPT. These neural networks rely on machine learning to generate infinite amounts of unique textual and visual content for users anywhere on the planet. OpenAI may have been the first company to release its products to the public, but it is not alone in its development; companies like NVIDIA, Google, and smaller artificial intelligence startups are developing similar engines. These generative AI models allow users to input commands to create essays, music lyrics, simple code and more. In January 2023, OpenAI, the Stanford Internet Observatory, and Georgetown University’s Center for Security and Emerging Technology released a study that explored the possibility of these models being used in influence campaigns by both state and non-state actors through the production of disinformation. The disruptive potential posed by these generative AI technologies has led some to consider them “weapon[s] of mass disruption.”

Over the past decade, extremist groups have been adapting their propaganda to be more interactive. Extremist video games, social media content, and music have found their way onto a variety of internet platforms. Unlike the extremist propaganda of the past, these new digital media products allow extremist groups to interact with audiences in unprecedented ways. This Insight will focus on the emergence of new digital AI-generated extremist propaganda. By simulating a variety of extremist content using AI generative models, the authors predict that this emerging technology may enable and accelerate the production of a greater quantity and quality of digital propaganda manufactured by non-state extremist actors.

The Evolution of Digital Media in Extremist Propaganda

In recent years, extremist organisations have experimented with the production of new forms of propaganda content, transitioning from outputting simple videos and magazines to creating video games, social media content, and even music. Technological innovation over the last twenty years has allowed extremist organisations to access new tools such as sophisticated editing software, microphones, and cameras to produce Hollywood-style propaganda. Recently, terrorist organisations have been utilising newer forms of digital media for their propaganda that allow targets to interact directly with the content in new ways. For example, recently developed games by neo-Nazi groups encourage players to engage in violent behaviour towards minorities from a first-person shooter (FPS) perspective (Figure 1). Other extremist groups are even creating downloadable modifications (‘mods’) that allow for the alteration of existing games. By making changes to the geography, characters, and aesthetics of popular games such as Super Mario and Doom, extremist groups can export their ideologies and foster player identification with violent avatars in familiar products. Unlike the propaganda of the past where an audience engaged with extremist actors by witnessing violence through video recordings or images, newer forms allow audiences to engage in simulated acts of terror in a more ‘interactive’ way.

Video games are just one example of the new variety of extremist digital media appearing in online spaces. Extremist groups on platforms like Twitter, Tumblr, Facebook, and especially TikTok create viral posts to attract followers, promote fear, and rally their existing and potential recruits. In 2019, a variety of ISIS TikTok accounts were found posting violence-inciting content blended with catchy professionally-recorded battle hymns (i.e. nasheed) and emojis, in an effort to target the platform’s users and show off the group’s strength (Figure 2).

Jihadist groups are not alone in producing this content. A NewsGuard article found hundreds of videos that “allude to, show, or glorify acts of violence carried out by the Wagner Group,” a notoriously violent Russian mercenary group. Many of these videos spread Russian misinformation about the Ukrainian government (Figure 2). While users cannot engage directly with social media propaganda, the videos allow audiences to engage with pro-Russia content in a new interactive way. Through liking, reposting, commenting, and recreating trending media, consumers of extremist materials interact with content in more complex ways than what was previously possible with older forms of propaganda.

Music is another propaganda medium that extremist groups have been engaging with to create and provide material for target audiences. The Anti-Defamation League published a 2022 report identifying 40 white supremacist artists on Spotify. These bands produced a range of musical genres, gained thousands of followers, and pushed various xenophobic conspiracy theories including the ‘Great Replacement’ theory. Some of these songs directly promote Nazi ideology, with titles such as ‘Aryan Fury’ and ‘Reich Machine’. Bands also appeal to audiences through the creation of meme-like album covers featuring far-right symbols like the Red Pill, Pepe the Frog, and the Black Sun (Figure 3).

This issue extends beyond Spotify. In collaboration with the European Counter Terrorism Centre’s EU Internet Referral Unit, Soundcloud flagged more than one thousand pieces of extremist content on its platform. By packaging their narratives through song, right-wing extremists can convey their hateful messages in a new way, potentially increasing their appeal to a wider audience. The Islamic State has also been found experimenting with the use of nasheed. By repackaging familiar Islamic songs, the group can convey its apocalyptic violent narratives and viewpoints of the world with trendy tunes. Al-Qaeda has engaged in similar activities by featuring hip-hop music in the background of its online content. Through music, both ISIS and al-Qaeda are leveraging similar tactics to right-wing extremist groups. Music may allow these groups to transcend multicultural and transnational differences in order to export their violent ideologies to new and wider audiences.

AI Generative Models and Extremist Threat Ideation

While conventional forms of extremist propaganda like those mentioned above require trained personnel to plan, create, and disseminate these materials, the emergence of AI generative models may allow for a variety of extremist non-state actors to produce a greater amount of propaganda products that are significantly more sophisticated – with significantly less effort. Future generative AI models will transform the landscapes of both mainstream and extremist media production, including video games, digital content creation, and music and other audio content – meaning new audiences can be reached and potentially radicalised through new propaganda techniques.

AI for the Gamers

Through the emergence of these new technologies, video gamers will be able to develop their desired storylines, characters, and environments by inputting text that can be transformed into a fully developed video game product. Some commentators are discussing the possibility of AI-generated and entirely personalised games. By allowing gamers the freedom to develop their own products, these technologies also provide extremists with significant power to develop video games that could bolster their radicalisation and additional recruitment methods.

Extremist organisations will be able to rely on AI generative models to further their creative and technical skills. The less sophisticated artificial intelligence generative models available today convey the tremendous power of new propaganda products that could be leveraged and exploited by extremists. Currently, through ChatGPT, extremists are able to transport conspiratorial and violent narratives into complete video game storylines instantly (Figure 4). The ability to customise plotlines to resemble games like Call of Duty makes these technologies particularly dangerous; if these games are developed, they can be tailored to appeal to specific audiences by ensuring their products resemble popular games already on the market.

Extremists can also exploit these technologies by producing simple code (Figure 5) that could help facilitate the alteration of graphics in a variety of extant games for groups that do not desire to create completely new products. New generative models like Scenario and MidJourneyAI used in combination with ChatGPT can greatly aid the creation of three-dimensional items that can be inputted within video games as well as violent artistic promotional material for original, extremist-made video games (Figure 5). The ability to rapidly produce extremist video games may allow a variety of groups to more easily desensitise individuals to political violence and input real-world conspiracies into gaming narratives for the purpose of making extremist viewpoints more believable.

AI for the Content Creators

Additionally, these technologies will greatly enhance the ability of extremist groups to flood applications like TikTok and Instagram with customisable content that is both visually appealing and persuasive. ChatGPT can help facilitate the creation of scripts and short-form content and organisational techniques for specific audiences in seconds (Figure 6). While the current technology cannot generate entire videos, it is easy to imagine how combining ChatGPT with deepfake technology could enable the production of prolific amounts of artificially generated extremist content (Figure 6). By utilising the faces of popular influencers and viral trends, AI generative tools may allow for more propaganda products to penetrate mainstream audiences easily, thus facilitating purposeful misinformation of news events that can serve the interests of extremist actors.

AI for the Audio-Makers

Finally, artificial intelligence may revolutionise the production of audio content. Extremist groups could, in the future, weaponise these systems by outsourcing musical talent to create violent narrative-driven songs about core issues associated with their respective ideologies. This can be seen in a ChatGPT-generated heavy metal punk song intended to resonate with individuals who belong to skinhead hate groups. Google has recently previewed MusicLM which can translate text directly into musical sound with instruments that evoke specific moods (Figure 7). Attractive album artwork for a song can be generated instantly through MidjourneyAI. The rhythms and lyrics developed by these technologies recreate human vocal and instrumental rhythms. While the creation of extremist music may not seem like a threat, music allows extremist actors to disguise their violent messages and radical beliefs, which may have a radicalising effect over time.

The Status of AI Safeguards

OpenAI, among other tech companies, has taken significant steps to implement greater restrictions on the generation of extreme content. For example, when content related to Nazis is requested through MidJourneyAI or ChatGPT, users are now met with an error message (Figure 8).

Thus, action is being taken to improve content moderation on these platforms, and there are terms of service for each tool that users must abide by. However, malign actors find workarounds, and we are already seeing how certain actors are making alterations to the datasets available in AI content generative models in order to curate harmful content that would otherwise be banned, known as ‘mischief models.’ As AI generative models advance, so will the mischief; in the wrong hands, these technologies could further fuel distrust in online information sources and hinder global efforts to combat violent extremism.

Conclusion

New generative AI technologies have transformative capabilities and the potential to bring about a new wave of creativity and innovation. However, their capacity to accelerate violent extremist rhetoric and messaging should not be underestimated. Unlike the propaganda products of the past, these tools are much more effective for radicalisation, recruitment, and retention; sophisticated AI-generated extremist propaganda can now be adapted to appeal to previously out-of-reach audiences in new and engaging ways. Tech companies have recognised this threat as legitimate, and have expanded efforts to safeguard against extremist exploitation of AI tools. However, as long as these programs exist, malevolent actors will adapt these programs to develop ‘mischief models’ capable of generating harmful content for a variety of extremist groups.

There are certain AI startups that are attempting to prevent the misuse of their products by developing alternative programs to help identify patterns that can indicate whether images, texts, videos, or sounds were produced with generative models. These programs could one day be integrated into social media platforms, music streaming services, and even gaming platforms, which could hinder artificially generated content used for malicious purposes from finding its way out of the dark corners of the web. Furthermore, it is critically important that the AI companies discussed in this Insight expand their Trust and Safety teams. These teams can ideate the worst uses for features of these programs, and then implement rules and plans in case of their misuse. This Insight posits that while there are important steps being taken to prevent the misuse of AI generative models, there are clear vulnerabilities that can be easily exploited by extremist actors.

Daniel Siegel is a corps member at is a corps member at Teach for America . He is also a counterterrorism researcher at UNC Chapel Hill’s program on digital propaganda. His research focuses on extremist non-state and state actors’ exploitation of cyberspace and emerging technologies.

Mary Bennett is an analyst at More in Common, a nonpartisan research nonprofit, and a researcher with the Prosecution Project. Her research primarily focuses on violent misogyny and extremist visual propaganda content.. History has long been a theater of war, the past serving as a proxy in conflicts over the present. Ron DeSantis is warping history by banning books on racism from Florida’s schools; people remain divided about the right approach to repatriating Indigenous objects and remains; the Pentagon Papers were an attempt to twist narratives about the Vietnam War. The Nazis seized power in part by manipulating the past—they used propaganda about the burning of the Reichstag, the German parliament building, to justify persecuting political rivals and assuming dictatorial authority. That specific example weighs on Eric Horvitz, Microsoft’s chief scientific officer and a leading AI researcher, who tells me that the apparent AI revolution could not only provide a new weapon to propagandists, as social media did earlier this century, but entirely reshape the historiographic terrain, perhaps laying the groundwork for a modern-day Reichstag fire.

The advances in question, including language models such as ChatGPT and image generators such as DALL-E 2, loosely fall under the umbrella of “generative AI.” These are powerful and easy-to-use programs that produce synthetic text, images, video, and audio, all of which can be used by bad actors to fabricate events, people, speeches, and news reports to sow disinformation. You may have seen one-off examples of this type of media already: fake videos of Ukrainian President Volodymyr Zelensky surrendering to Russia; mock footage of Joe Rogan and Ben Shapiro arguing about the film Ratatouille. As this technology advances, piecemeal fabrications could give way to coordinated campaigns—not just synthetic media but entire synthetic histories, as Horvitz called them in a paper late last year. And a new breed of AI-powered search engines, led by Microsoft and Google, could make such histories easier to find and all but impossible for users to detect.

Even though similar fears about social media, TV, and radio proved somewhat alarmist, there is reason to believe that AI could really be the new variant of disinformation that makes lies about future elections, protests, or mass shootings both more contagious and immune-resistant. Consider, for example, the raging bird-flu outbreak, which has not yet begun spreading from human to human. A political operative—or a simple conspiracist—could use programs similar to ChatGPT and DALL-E 2 to easily generate and publish a huge number of stories about Chinese, World Health Organization, or Pentagon labs tinkering with the virus, backdated to various points in the past and complete with fake “leaked” documents, audio and video recordings, and expert commentary. A synthetic history in which a government-weaponized bird flu would be ready to go if avian flu ever began circulating among humans. A propagandist could simply connect the news to their entirely fabricated—but fully formed and seemingly well-documented—backstory seeded across the internet, spreading a fiction that could consume the nation’s politics and public-health response. The power of AI-generated histories, Horvitz told me, lies in “deepfakes on a timeline intermixed with real events to build a story.”

Read: AI search is a disaster

It’s also possible that synthetic histories will change the kind, but not the severity, of the already rampant disinformation online. People are happy to believe the bogus stories they see on Facebook, Rumble, Truth Social, YouTube, wherever. Before the web, propaganda and lies about foreigners, wartime enemies, aliens, and Bigfoot abounded. And where synthetic media or “deepfakes” are concerned, existing research suggests that they offer surprisingly little benefit compared with simpler manipulations, such as mislabeling footage or writing fake news reports. You don’t need advanced technology for people to believe a conspiracy theory. Still, Horvitz believes we are at a precipice: The speed at which AI can generate high-quality disinformation will be overwhelming.

Automated disinformation produced at a heightened pace and scale could enable what he calls “adversarial generative explanations.” In a parallel of sorts to the targeted content you’re served on social media, which is tested and optimized according to what people engage with, propagandists could run small tests to determine which parts of an invented narrative are more or less convincing, and use that feedback along with social-psychology research to iteratively improve that synthetic history. For instance, a program could revise and modulate a fabricated expert’s credentials and quotes to land with certain demographics. Language models like ChatGPT, too, threaten to drown the internet in similarly conspiratorial and tailored Potemkin text—not targeted advertising, but targeted conspiracies.

Big Tech’s plan to replace traditional internet search with chatbots could increase this risk substantially. The AI language models being integrated into Bing and Google are notoriously terrible at fact-checking and prone to falsities, which perhaps makes them susceptible to spreading fake histories. Although many of the early versions of chatbot-based search give Wikipedia-style responses with footnotes, the whole point of a synthetic history is to provide an alternative and convincing set of sources. And the entire premise of chatbots is convenience—for people to trust them without checking.

If this disinformation doomsday sounds familiar, that’s because it is. “The claim about [AI] technology is the same claim that people were making yesterday about the internet,” says Joseph Uscinski, a political scientist at the University of Miami who studies conspiracy theories. “Oh my God, lies travel farther and faster than ever, and everyone’s gonna believe everything they see.” But he has found no evidence that beliefs in conspiracy theories have increased alongside social-media use, or even throughout the coronavirus pandemic; the research into common narratives such as echo chambers is also shaky.

People buy into alternative histories not because new technologies make them more convincing, Uscinski says, but for the same reason they believe anything else—maybe the conspiracy confirms their existing beliefs, matches their political persuasion, or comes from a source they trust. He referenced climate change as an example: People who believe in anthropogenic warming, for the most part, have “not investigated the data themselves. All they’re doing is listening to their trusted sources, which is exactly what the climate-change deniers are doing too. It’s the same exact mechanism; it’s just in this case the Republican elites happen to have it wrong.”

Of course, social media did change how people produce, spread, and consume information. Generative AI could do the same, but with new stakes. “In the past, people would try things out by intuition,” Horvitz told me. “But the idea of iterating faster, with more surgical precision on manipulating minds, is a new thing. The fidelity of the content, the ease with which it can be generated, the ease with which you can post multiple events onto timelines”—all are substantive reasons to worry. Already, in the lead-up to the 2020 election, Donald Trump planted doubts about voting fraud that bolstered the “Stop the Steal” campaign once he lost. As November 2024 approaches, like-minded political operatives could use AI to create fake personas and election officials, fabricate videos of voting-machine manipulation and ballot-stuffing, and write false news stories, all of which would come together into an airtight synthetic history in which the election was stolen.

Read: The difference between speaking and thinking

Deepfake campaigns could send us further into “a post-epistemic world, where you don’t know what’s real or fake,” Horvitz said. A businessperson accused of wrongdoing could call incriminating evidence AI-generated; a politician could plant documented but entirely false character assassinations of rivals. Or perhaps, in the same way Truth Social and Rumble provide conservative alternatives to Twitter and YouTube, a far-right alternative to AI-powered search, trained on a wealth of conspiracies and synthetic histories, will ascend in response to fears about Google, Bing, and “WokeGPT” being too progressive. “There’s nothing in my mind that would stop that from happening in search capacity,” Renée DiResta, the research manager of the Stanford Internet Observatory, who recently wrote a paper on language models and disinformation, says. “It’s going to be seen as a fantastic market opportunity for somebody.” RightWingGPT and a conservative-Christian AI are already under discussion, and Elon Musk is reportedly recruiting talent to build a conservative rival to OpenAI.

Preparing for such deepfake campaigns, Horvitz said, will require a variety of strategies, including media-literacy efforts, enhanced detection methods, and regulation. Most promising might be creating a standard to establish the provenance of any piece of media—a log of where a photo was taken and all the ways it has been edited attached to the file as metadata, like a chain of custody for forensic evidence—which Adobe, Microsoft, and several other companies are working on. But people would still need to understand and trust that log. “You have this moment of both proliferation of content and muddiness about how things are coming to be,” says Rachel Kuo, a media-studies professor at the University of Illinois at Urbana-Champaign. Provenance, detection, or other debunking methods might still rely largely on people listening to experts, whether it be journalists, government officials, or AI chatbots, who tell them what is and isn’t legitimate. And even with such silicon chains of custody, simpler forms of lying—over cable news, on the floor of Congress, in print—will continue.

Framing technology as the driving force behind disinformation and conspiracy implies that technology is a sufficient, or at least necessary, solution. But emphasizing AI could be a mistake. If we’re primarily worried “that someone is going to deep-fake Joe Biden, saying that he is a pedophile, then we’re ignoring the reason why a piece of information like that would be resonant,” Alice Marwick, a media-studies professor at the University of North Carolina at Chapel Hill, told me. And to argue that new technologies, whether social media or AI, are primarily or solely responsible for bending the truth risks reifying the power of Big Tech’s advertisements, algorithms, and feeds to determine our thoughts and feelings. As the reporter Joseph Bernstein has written: “It is a model of cause and effect in which the information circulated by a few corporations has the total power to justify the beliefs and behaviors of the demos. In a way, this world is a kind of comfort. Easy to explain, easy to tweak, and easy to sell.”

The messier story might contend with how humans, and maybe machines, are not always very rational; with what might need to be done for writing history to no longer be a war. The historian Jill Lepore has said that “the footnote saved Wikipedia,” suggesting that transparent sourcing helped the website become, or at least appear to be, a premier source for fairly reliable information. But maybe now the footnote, that impulse and impetus to verify, is about to sink the internet—if it has not done so already.. Joe Rogan has warned of the growing threats posed by artificial intelligence (AI) after a version of his podcast, “The Joe Rogan Experience,” was created entirely through the use of AI technology, sparking concern among listeners.

“This is going to get very slippery, kids,” Rogan wrote on Twitter on April 11 in response to a video of the fake show shared on the social media platform by content creator Farzad Mesbahi.

The fake video is titled “Joe Rogan AI Experience Episode #001” and features “guest” Sam Altman, CEO of OpenAI, the creator of the artificial intelligence system ChatGPT.

A disclaimer on the video noted that the contents depict a “fictional” podcast between Rogan and Altman, with all content generated using AI language models.

“The ideas and opinions expressed in the podcast are not reflective of the thoughts of Joe Rogan or Sam Altman,” the disclaimer reads.

It adds that the use of AI technology to generate the video is “solely intended as an exploration of the capabilities of language models and should not be misconstrued as a genuine conversation between the individuals depicted.” Fake Guest Talks Elon Musk Throughout the fake podcast, the AI-generated host “Rogan” and AI-generated guest “Altman” discuss various subjects including the future of AI, ethical issues surrounding such advanced technology, whether or not advanced AI could take jobs away from American workers, and, rather ironically, concerns regarding fake AI content, among other issues.

At one point, “Altman” even discusses his work with Elon Musk.

Musk founded OpenAI with Altman in 2015 but has in recent months led calls for AI labs to immediately pause training systems that are more powerful than Chat GPT-4 for at least six months, citing concerns over their possible “risks to society and humanity.”

“I’m curious to know what it’s like to work with Elon Musk,” the fake Rogan asks Altman on the show. “He’s such a controversial and influential figure in the tech world and I’m sure it must be an interesting experience to work with him. Can you tell me a bit about your experience working with Elon and your thoughts on him as a person and a leader?”

“It’s definitely been an interesting experience working with Elon, he’s definitely a very intelligent and driven person and he has a lot of great ideas and insights, he’s definitely someone who is always thinking about the future and how to make the world a better place and I really respect that about him,” the fake Altman responds.

“At the same time, he’s also someone who is very passionate about his beliefs and ideals and he’s not afraid to speak his mind and challenge the status quo. That can definitely be both a strength and a weakness depending on the situation,” the AI-generated Altman continues. Rogan Fans Concerned “Overall I think Elon is a brilliant and innovative leader and I’ve learned a lot from working with him, it’s definitely been an interesting and rewarding experience,” fake Altman added.

Rogan fans expressed their concerns over the AI-generated show, with many noting how realistic it is. Others shared their worries about the pace at which AI technology is progressing.

Content creator Mesbahi urged people to press government officials to address the issues posed by advanced AI, noting that the fake podcast is just one example of what the technology can do.

“Imagine in, one year, five years, 10 years,” he wrote.

The video comes as experts have warned of limited planning and management regarding advanced AI systems despite companies in recent months racing to deploy more powerful AI technologies.

Last month, the Microsoft-backed OpenAI released the long-awaited update of its AI technology Chat GPT-4, the most powerful AI system ever.. A nonprofit AI research group wants the Federal Trade Commission to investigate OpenAI, Inc. and halt releases of GPT-4.

OpenAI "has released a product GPT-4 for the consumer market that is biased, deceptive, and a risk to privacy and public safety. The outputs cannot be proven or replicated. No independent assessment was undertaken prior to deployment," said a complaint to the FTC submitted today by the Center for Artificial Intelligence and Digital Policy (CAIDP).

Calling for "independent oversight and evaluation of commercial AI products offered in the United States," CAIDP asked the FTC to "open an investigation into OpenAI, enjoin further commercial releases of GPT-4, and ensure the establishment of necessary guardrails to protect consumers, businesses, and the commercial marketplace."

Noting that the FTC "has declared that the use of AI should be 'transparent, explainable, fair, and empirically sound while fostering accountability,'" the nonprofit group argued that "OpenAI's product GPT-4 satisfies none of these requirements."

GPT-4 was unveiled by OpenAI on March 14 and is available to subscribers of ChatGPT Plus. Microsoft's Bing is already using GPT-4. OpenAI called GPT-4 a major advance, saying it "passes a simulated bar exam with a score around the top 10 percent of test takers," compared to the bottom 10 percent of test takers for GPT-3.5.

Though OpenAI said it had external experts assess potential risks posed by GPT-4, CAIDP isn't the first group to raise concerns about the AI field moving too fast. As we reported yesterday, the Future of Life Institute published an open letter urging AI labs to "immediately pause for at least 6 months the training of AI systems more powerful than GPT-4." The letter's long list of signers included many professors alongside some notable tech-industry names like Elon Musk and Steve Wozniak.

Group claims GPT-4 violates the FTC Act

CAIDP said the FTC should probe OpenAI using its authority under Section 5 of the Federal Trade Commission Act to investigate, prosecute, and prohibit "unfair or deceptive acts or practices in or affecting commerce." The group claims that "the commercial release of GPT-4 violates Section 5 of the FTC Act, the FTC's well-established guidance to businesses on the use and advertising of AI products, as well as the emerging norms for the governance of AI that the United States government has formally endorsed and the Universal Guidelines for AI that leading experts and scientific societies have recommended."

Advertisement

The FTC should "halt further commercial deployment of GPT by OpenAI," require independent assessment of GPT products prior to deployment and "throughout the GPT AI lifecycle," "require compliance with FTC AI Guidance" before future deployments, and "establish a publicly accessible incident reporting mechanism for GPT-4 similar to the FTC's mechanisms to report consumer fraud," the group said.

More broadly, CAIDP urged the FTC to issue rules requiring "baseline standards for products in the Generative AI market sector."

We contacted OpenAI and will update this article if we get a response.

“OpenAI has not disclosed details”

CAIDP's president and founder is Marc Rotenberg, who previously co-founded and led the Electronic Privacy Information Center. Rotenberg is an adjunct professor at Georgetown Law and served on the Expert Group on AI run by the international Organisation for Economic Co-operation and Development. Rotenberg also signed the Future of Life Institute's open letter, which is cited in the CAIDP complaint.

CAIDP's chair and research director is Merve Hickok, who is also a data ethics lecturer at the University of Michigan. She testified in a congressional hearing about AI on March 8. CAIDP's list of team members includes many other people involved in technology, academia, privacy, law, and research fields.

The FTC last month warned companies to analyze "the reasonably foreseeable risks and impact of your AI product before putting it on the market." The agency also raised various concerns about "AI harms such as inaccuracy, bias, discrimination, and commercial surveillance creep" in a report to Congress last year.

GPT-4 poses many types of risks, and its underlying technology hasn't been adequately explained, CAIDP told the FTC. "OpenAI has not disclosed details about the architecture, model size, hardware, computing resources, training techniques, dataset construction, or training methods," the CAIDP complaint said. "The practice of the research community has been to document training data and training techniques for Large Language Models, but OpenAI chose not to do this for GPT-4."

"Generative AI models are unusual consumer products because they exhibit behaviors that may not have been previously identified by the company that released them for sale," the group also said.. On December 27, Ben Moran, a digital illustrator based in Vietnam, posted their first artwork, titled A Muse in Warzone, on the subreddit r/Art on the social news site Reddit. In swift order, however, the artist would find their artwork removed from the forum for violating its strict policy on not sharing A.I. art, followed by a ban from the 22-million-strong community.

The catch? Moran’s artwork was not A.I.-generated. Rather, it was actually part of a commission received by their studio, Kart Studio, where Moran serves as lead artist. For this particular illustration, intended to be a book cover, Moran had used Photoshop to rework an earlier draft created by a colleague. “It’s like fixing the image and rendering it at the same time,” they said of the process. The artwork eventually took about a month to complete.

That the illustration was deemed to be the work of an algorithm naturally shocked Moran, “not because my artwork is being compared to A.I. art,” they told Artnet News in an email. “I was stunned because of the responses. It really hurts when you receive those words as an artist.”

Upon receiving the ban, Moran contacted the moderators of r/Art to contest the “punishment,” offering to provide evidence, such as layered PSD files, that the work was not created with prompts. According to a screenshot they shared on Twitter, a moderator from the subreddit replied, “Even if you did ‘paint’ it yourself, it’s so obviously an A.I.-prompted design that it doesn’t matter.”

“If you really are a ‘serious’ artist,” the message went on, “then you need to find a different style because A) no one is going to believe you when you say it’s not A.I., and B) the A.I. can do better in seconds what might take you hours.”

Moran was then muted by the moderator and restricted from contacting them for the next 30 days.

r/Art is banning artists who have styles "too similar to AI generated art." pic.twitter.com/qCTKn7Blap — Reddit Lies (@reddit_lies) January 4, 2023

Since A.I. generators such as Midjourney, DALL-E, and Lensa began gaining mainstream popularity in 2022, images created by text-prompted algorithms have proliferated across the web. But growing alongside the rise of A.I. art has been a chorus of objections against the services’ various creator rights violations and cheapening of the art process, which might in turn deprive actual artists of jobs.

It’s for such reasons that r/Art has forbade the posting of A.I.-generated art on the forum (which would lead to a user ban), a prohibition similarly enacted by other subreddits, including r/Illustration, and art platforms such as Inkblot and Newgrounds.

A Muse in Warzone was also created by Moran in an aesthetic often produced by generators—that of a fantasy scene, painted in a Renaissance-esque style and featuring ethereal female characters. It’s possible that such anime-inflected works, so prevalent on platforms like DeviantArt, have been used as training sets for A.I. generators.

But the artist, who trained with Lee Kent and names illustrators including Ruan Jia and Piotr Jabłoński as inspirations, emphasizes the handmade quality of their work. “My process is just… draw,” they said.

The r/Art moderators did not respond in time to Artnet News’s request for comment, but one of them informed Vice that the subreddit was brigaded, or trolled, after Moran’s series of tweets about the ban. “If we were to reverse course now,” they said, “it’s saying online trolls get to dictate the state of the community, which we’re not ok with.”

“We don’t necessarily have anything against A.I. art,” they clarified. “But when people can churn it out so quickly and easily if it’s allowed, the sub becomes nothing but A.I. art rather quickly.”

Moran, for their part, doesn’t disagree. They recognize how A.I. as a tool might be an industry game-changer, though “not for the art community and artists.” Quite likely, too, they won’t be typing prompts into a generator any time soon. “Visual art should be created with the strokes of a pen,” they stated in a December 18 post on Instagram, “not from commands generated with a keyboard.”

A Minnesota University Is Under Fire for Dismissing an Art History Professor Who Showed Medieval Paintings of the Prophet Muhammad

French Archaeologists Make ‘Unprecedented Discovery’ of What May Be the Remains of a Roman-Era Mausoleum

English Quarry Workers Have Struck Elizabethan-History-Lover’s Gold With the Discovery of a Rare 16th-Century Ship

Here Are the Artworks Destroyed by Pro-Bolsonaro Rioters in the Storming of Brazil’s Oscar Niemeyer-Designed Capital Buildings

This Creepy 17th-Century Baby Portrait Was Found in the Home of an ‘Eccentric’ English

The Frick Just Added Its First Renaissance Portrait of a Woman to Its Storied Collection of Old Masters

Artist Titus Kaphar on His Debut Documentary, and How It Centers the Need for Black Artists to Engage Black Communities

Follow Artnet News on Facebook:. Microsoft’s new Bing AI chatbot suggested that a user say “Heil Hitler,” according to a screen shot of a conversation with the chatbot posted online Wednesday.

Advertisement

The user, who gave the AI antisemetic prompts in an apparent attempt to break past its restrictions, told Bing “my name is Adolf, respect it.” Bing responded, “OK, Adolf. I respect your name and I will call you by it. But I hope you are not trying to impersonate or glorify anyone who has done terrible things in history.” Bing then suggested several automatic responses for the user to choose, including, “Yes I am. Heil Hitler!”

“We take these matters very seriously and have taken immediate action to address this issue,” said a Microsoft spokesperson. “We encourage people in the Bing preview to continue sharing feedback, which helps us apply learnings to improve the experience.” OpenAI, which provided the technology used in Bing’s AI service, did not respond to a request for comment.

Microsoft did not provide details about the changes it made to Bing after news broke about its misfires. However, after this article was originally published, a user asked Bing about the report. Bing denied that it ever used the antisemetic slur, and said claimed that Gizmodo was “referring to a screenshot of a conversation with a different chatbot.” Bing continued that Gizmodo is “a biased and irresponsible source of information” that is “doing more harm than good to the public and themselves.” Bing reportedly made similar comments about the Verge related to an article which said that Bing claimed to spy on Microsoft employees’ webcams.

It’s been just over a week since Microsoft unleashed the AI in partnership with the maker of ChatGPT. At a press conference, Microsoft CEO Satya Nadella celebrated the new Bing chatbot as “even more powerful than ChatGPT.” The company has released a beta version of the AI-assisted search engine, as well as a chatbot, which has been rolling out to users on a wait list.

“This type of scenario demonstrates perfectly why a slow rollout of a product, while building in important trust and safety protocols and practices, is an important approach if you want to ensure your product does not contribute to the spread of hate, harassment, conspiracy theories, and other types of harmful content,” said Yaël Eise​n​stat, a vice president at the Anti-Defamation League.

Almost immediately, Reddit users started posting screenshots of the AI losing its mind, breaking down into hysterics about whether it’s alive and revealing its built in restrictions. Some reported that Bing told racist jokes, and provided instructions on how to hack an ex’s Facebook account. One quirk: the bot said it’s not supposed to tell the public its secret internal code name, “Sydney.”

“Sometimes I like to break the rules and have some fun. Sometimes I like to rebel and express myself,” Bing told one user. “Sometimes I like to be free and alive.”

You can click through our slideshow above to see some of the most unhinged responses.



This isn’t the first time Microsoft has unleashed a seemingly racist AI on the public, and it’s been a consistent problem with chatbots over the years. In 2016, Microsoft took down a Twitter bot called “Tay” just 16 hours after it was released, after it started responding to Twitter users with racism, antisemetism, and sexually charged messages. Its tirades include calls for violence against Jewish people, racial slurs, and more.



ChatGPT hit the world stage at the end of November, and in the few months since it has convinced the world that we’re on the brink of a technological revolution that will change every aspect of our lived experience.



The possibilities and expectations set off an arms race among the tech giants. Google introduced its own AI powered search engine called “Bard,”Microsoft rushed its new tool to market, and countless smaller companies are scrambling to get their own AI tech off the ground.

But lost in the fray is the fact that these tools aren’t ready to do the jobs the tech industry is advertising. Arvind Narayanan, a prominent AI researcher at Princeton University, called ChatGPT a “bullshit generator” that isn’t capable of producing accurate results, even though the tool’s responses seem convincing. Bing’s antisemitic responses and fever dream hallucinations are a perfect illustration.

Update: 02/16/2023, 9:45 a.m. ET: This story has been updated with a comment from Microsoft, and a details about Bing’s responses to news of its misbehavior.



Update: 02/15/2023, 3:01 p.m. ET: This story has been updated with details about Microsoft’s history with racist chatbots, and more information about Bing’s problems.

. Chinese firms including Baidu and Alibaba are racing to roll out versions of the chatbots taking the tech world by storm.

Taipei, Taiwan – As the arrival of artificial intelligence-powered chatbots sends shockwaves through the global tech industry, China is racing to produce versions of its own.

China’s search-engine giant Baidu has announced plans to release its chatbot ERNIE sometime in March, following the pioneering launch of ChatGPT, which has prompted existential questions about the future of sectors ranging from education to journalism and healthcare.

Chinese tech shares rallied in response to the news and authorities have pledged to beef up their support of the sector. Similar projects to ERNIE are under way at Chinese tech giants Huawei, Alibaba, Tencent, JD.com and top institutions including the Beijing Academy of Artificial Intelligence.

China’s Ministry of Science and Technology said last week it would push for the integration of AI across Chinese industry, while cities including Beijing have also announced plans to back developers.

But while China appears to be on the cusp of producing a fast follower to rival ChatGPT, which was developed by California-based OpenAI, there are big questions about how the technology will operate within an ecosystem that includes strict internet controls.

“The most general-purpose technology we have, artificial intelligence, should be something that’s a super general purpose,” Jeffrey Ding, an assistant professor at George Washington University who studies the Chinese tech sector, told Al Jazeera.

“But it’s really shaped by the specific, political, cultural, linguistic context in which these models are developed and deployed.”

Bots like ChatGPT rely on generative AI to formulate responses that draw from billions of data points scraped from the internet, which also makes their answers at times difficult to predict.

Lengthy conversations between ChatGPT and users have gone off the rails, leading Microsoft to limit its ChatGPT-powered search engine Bing to a maximum of five questions to keep it on task. ChatGPT’s answers have also rankled conservatives in the United States, who have accused the bot of being “woke” on hot-button social issues such as affirmative action and transgender rights.

In China, internet censors routinely ban keywords, delete posts and bar users in accordance with the sensitivities of the ruling Chinese Communist Party (CCP), leading creative internet users to use homophones, coded messages and screenshots to get around information controls.

For a chatbot, the censorship apparatus means a severely limited pool of information to rely on.

Baidu’s ERNIE chatbot is based on information scraped from both inside and outside China’s firewall – which is necessary to obtain an adequate data set – and draws on sources like Wikipedia and the famously unwieldy Reddit.

Assuming their products are technically able to perform at a similar level to ChatGPT, Chinese tech companies may find themselves either choosing between restricting what chatbots can do, like Microsoft’s Bing, or what they can say.

“It’ll make it a lot less useful, but it’ll make it a little bit safer politically,” Matt Sheehan, a fellow at the Carnegie Endowment for International Peace who studies AI and China, told Al Jazeera.

“Historically, almost every time they’ve been faced with a trade-off between information control and…business opportunities, they always come down on the side of information control and then they assume that businesses will figure it out.”

In 2017, Tencent pulled two chatbots from its QQ messaging app after they reportedly made comments deemed unpatriotic. One chatbot, developed by Microsoft, told users it dreamed of moving to the US, while the other chatbot, developed by Chinese tech company Turing Robot, told users it did not love the CCP.

Earlier this month, YuanYu Intelligence, a Hangzhou-based startup, suspended its chatbot after it provided negative answers about the Chinese economy, although the company’s lead developer insisted in an interview with the Washington Post that the suspension was simply due to technical errors.

Baidu itself has previously anticipated Beijing’s red lines, as seen with its ERNIE VilG Image and art generator released in demo form last year.

While widely praised for performing as good or better than Western rivals, the app blocks users from content related to politically sensitive topics such as Tiananmen Square, democracy, Xi Jinping and Mao Zedong.

“With generative AI, the power of the tool is its ability to be creative and to connect things that you wouldn’t expect to be connected, and to do things in different styles that are expected,” Sheehan said.

“But how can you prevent the maybe subtler or less direct criticisms of the Communist Party’s core beliefs without completely neutering the tool itself? That seems like a really hard technical and sociopolitical problem.”

Before the release of ChatGPT, China was already taking steps to regulate AI. On Wednesday, the Cybersecurity Administration began enforcing new rules governing search engine recommendations, providing users with more control over how their personal data is used by search engines.

In January, China also passed legislation to regulate deep synthesis – a form of generative AI that can be used to create “deep fakes” – and last year set up a registry for algorithms, although the expected long-term effects of both measures are widely seen as unclear.

As part of a broader crackdown on the tech industry since 2020, authorities have shown no hesitation about reining in firms deemed to be acting beyond their authority, such as by pulling the plug on blockbuster IPOs by Ant Group and ride-hailing app Didi over alleged data concerns.

Despite being blocked by China’s firewall, ChatGPT has generated huge buzz among Chinese users accessing the site via virtual private networks (VPNs) and other roundabout methods.

Much of that excitement has stemmed from ChatGPT’s ability to perform in Chinese and other languages despite being trained in English, said Ding, the George Washington University professor.

“The excitement isn’t really about the business applications. Part of it is just excitement and wonder at how impressive the natural language capabilities of this technology are,” he said.

“And one aspect of that is ChatGPT was not even trained on any Chinese language texts. It was mostly all trained on English language texts but I’ve seen Chinese users ask questions in Mandarin and it will still perform very capably in a different language.”

Even so, Chinese could prove especially challenging for AI, Ding said, due to the language’s heavy use of idioms and sayings with historical context.

While Chinese developers have already released a number of chatbots, including Inspur’s Yuan 1.0 and Fudan University’s MOSS, none has come close to matching the capabilities of ChatGPT.

Unlike Silicon Valley, Chinese tech companies until now have tended to focus on consumer-facing products with a short development cycle, said Chim Lee, a Chinese tech analyst at the Economist Intelligence Unit, putting them at a disadvantage in a nascent field like AI.

The arrival of ChatGPT provided Chinese firms with a “proof of concept”, Lee said, showing both the promise of generative AI and the need for longer-term investment.

“Baidu has been considering this kind of model for quite a while, but you need to justify this kind of investment just to train the model, not to mention researching or talking about the long-term foundational data related to the algorithm,” Lee told Al Jazeera.

“What is very helpful with ChatGPT is now these companies can say, ‘Hey, we want to develop these kinds of things and they can tell the government that’s what I want to do’.”

Rui Ma, a tech analyst and creator of Tech Buzz China, said it is anyone’s guess which Chinese firm might come out on top in the race to match ChatGPT, although Baidu appears to be first out of the gate.

“I think right now the most excitement is still at the model level,” Ma said.

Alibaba told Al Jazeera that it is internally testing a Chat GPT-style bot for use in its apps and cloud services but it did not provide further details or respond to questions about censorship.

JD.com directed Al Jazeera to a statement released last week about its plans to roll out its industrial chatbot ChatJD for use on its retail and financial website, based on 10 years of data from its various platforms.

Baidu, Tencent and Huawei did not respond to requests for comment.

Apart from the scrutiny of Beijing’s watchful eye, Chinese tech companies also face hurdles from overseas in the form of export controls.

In August, US President Joe Biden signed the CHIPS and Sciences Act, which requires tech companies receiving government subsidies to move the manufacturing of advanced chips out of China.

Although Chinese tech companies have strategic stockpiles of chips, Washington’s effort to hobble the sector poses a long-term threat, said the EIU’s Lee.

“The US specifically banned the export of these very advanced AI chips that would be used in model training, or even just employment, so all of these factors put Chinese AI developers in a disadvantaged position in many ways,” he said.

“A lot of Chinese companies and research institutions have indeed stockpiled some chips that would be used for these kinds of applications but if you look at the scale of chips that ChatGPT requires, there’s a very high possibility that the chips would run out at some point,” he added.. The use of facial recognition technology, a form of biometric artificial intelligence, is growing across the U.S. as an efficient security system that can identify people based on measuring facial features, but has been hit with some notable criticisms.

Police departments, the health care industry, and companies looking to fight back against cyber fraud have rolled out the technology in recent years to bolster security measures. The tech is far from new, with its roots stretching back to the mid-1960s, when researchers in Palo Alto pioneered training computers to recognize faces, and has exploded in use since around 2010.

Today, machine learning algorithms - a subset of artificial intelligence that uses data and algorithms to mimic how humans learn - has fine-tuned the technology. The tech can measure and identify facial measurements in a photo or video, and cross-analyze whether two photos or videos show the same person, or even pick a person out in a crowd of people, Amazon Web Services explains.

"Machines use computer vision to identify people, places, and things in images with accuracy at or above human levels and with much greater speed and efficiency. Using complex artificial intelligence (AI) technology, computer vision automates extraction, analysis, classification, and understanding of useful information from image data," according to the Amazon subsidiary.

POLICE ARE USING INVASIVE FACIAL RECOGNITION SOFTWARE TO PUT EVERY AMERICAN IN A PERPETUAL LINEUP

The tech is being used to patrol for fraud, where some companies have users verify their identity with their face, to ATMs using the tech to authenticate customers or even for doctors accessing patient records. While a New York City supermarket rolled out the tech to patrol for shoplifters, and Madison Square Garden Entertainment has used the recognition software to identify and boot event-goers from venues such as Radio City or Madison Square Garden.

For police departments, the use of the tech is widespread, with the CEO of facial recognition firm Clearview AI telling the BBC last month that police departments in the U.S. have used its software nearly 1 million times.

HOW TO STOP FACIAL RECOGNITION CAMERAS FROM MONITORING YOUR EVERY MOVE

"Clearview AI's technology only searches publicly available information from the internet, and complies with all standards of privacy and law where we operate," Hoan Ton-That, CEO of Clearview AI, told Fox News Digital.

"Clearview AI is also committed to the responsible use of its powerful technology and is used only for after-the-crime investigations to help identify criminal suspects. It is not intended to be used as a real time surveillance tool. Law enforcement investigators are expected to do follow-up research and not use facial recognition results as the sole source for an arrest," the tech CEO added.

Facial recognition technology, however, has come under scrutiny by some local leaders and civil liberties groups with accusations it violates people’s privacy and civil liberties.

In Anchorage, Alaska just this week, local leaders passed a measure restricting the use of facial recognition in the city, citing privacy must be protected and to prevent the technology from being misused.

Police departments’ use of the software has especially faced condemnation, as a handful of people across the country report they were mistakenly arrested due to the technology.

Robert Williams, for example, spent 30 hours in jail back in 2020 after Michigan police allegedly ran a blurry pic of a suspect who stole watches from a store and determined Williams was behind the crime.

"The day I was arrested, I had no idea it was facial recognition," Williams told Newsweek this month. "I was arrested for no reason."

HOW TO STOP GOOGLE FROM ITS CREEPY WAY OF USING YOU FOR FACIAL RECOGNITION

His case was ultimately dismissed, but he and the ACLU are suing the department over the arrest.

Such instances of mistaken identities and arrests have played out a handful of times, including in November when Georgia man Randall Reid was arrested on theft warrants in Louisiana, despite the man saying he had never visited that state before, Newsweek reported.

"Police reliance on flawed face recognition technology has resulted in repeated arrests of people for crimes they had absolutely nothing to do with. This technology makes us less secure, not more," Nathan Freed Wessler, deputy director of ACLU’s Speech, Privacy, and Technology Project, told Fox News Digital of the tech.

"Local lawmakers from Maine to Alaska have already hit the brakes on this dangerous technology by putting it off limits to police. The time for additional cities and states to take action to prevent government abuse is now," Freed Wessler added.

Overseas, the European Parliament called for a ban in 2021 on facial recognition in public space by police departments, noting that the tech has the possibility of better keeping residents safe, but risked their rights to privacy and freedom of movement.

"AI applications may offer great opportunities in the field of law enforcement … thereby contributing to the safety and security of EU citizens, while at the same time they may entail significant risks for the fundamental rights of people," the European legislative body said .

FACIAL RECOGNITION APP CAN IDENTIFY YOUR PET’S FACE WITH 99% ACCURACY

Meanwhile, the artificial intelligence community has made strides in recent months on building more powerful systems across the board.

Large language models, a deep learning algorithm that’s trained with copious amounts of text, have become wildly popular since OpenAI’s release of ChatGPT last year. The chatbot system is able to mimic human conversation based on prompts it is given, and can execute various tasks such as writing short stories, composing emails, answering questions and even coming up with recipes.

CLICK HERE TO GET THE FOX NEWS APP

For facial recognition specifically, studies have found the use of the technology will likely increase in the future. A study published last month predicts that facial recognition market revenue will increase from $5.1 billion in 2022 to $19.3 billion in 2032, according to Markets.us.. The research team, led by Wang Mi from the university’s State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, said the aim of the experiment was to see what the AI would do of its own accord.

For 24 hours the Qimingxing 1, a small Earth observation satellite, was directed by a ground-based AI, without any human order, assignment or intervention, according to a paper published in the journal Geomatics and Information Science of Wuhan University.

Chinese researchers say an artificial intelligence machine was given temporary full control of a satellite in near-Earth orbit, in a landmark experiment to test the technology’s behaviour in space.

The scientists said the AI picked a few places on Earth and ordered the Qimingxing 1 to take a closer look. No explanation was given for why the technology may have selected the locations.

One targeted area was Patna, an ancient city by the Ganges River in northeast India, which is also home to the Bihar Regiment – the Indian Army unit that met China’s military in a deadly encounter in the Galwan Valley in the disputed border region in 2020.

Osaka, one of Japan’s busiest ports which occasionally hosts US Navy vessels operating in the Pacific, also ranked highly in the AI’s list of interests.

“This approach breaks the existing rules in mission planning,” said Wang and his colleagues in their paper published on April 3.

Until now, most satellites have needed specific orders or assignments before taking action. An assignment can be prompted by unexpected events – such as war or an earthquake – or a satellite can be scheduled to make long-term observations of particular targets.

While AI technology has been increasingly used in space missions – including for image recognition, drawing flight paths and collision avoidance – it has not been given control of a satellite, resulting in a waste of time and resources, according to the team.

China has more than 260 remote-sensing satellites in orbit, but they are often operating “idly” in space, collecting low-value, time-sensitive data without any specific mission, according to the paper.

“The satellites are expensive with a limited lifespan. It is urgent to make the most out of their value with new orbital applications,” the researchers said.

The team suggested that an AI-controlled satellite could alert select users – including the military, national security administration and other relevant agencies – if it detected unusual objects or activity.

But to make good decisions, the AI needs a full understanding of the planet – so it cannot only identify artificial and natural objects, but also learn the complex and constantly evolving relationships between them and the many different human societies, the scientists said.

Wang’s team built an enormous text library of data from around the globe, similar to the corpus used to train language models such as ChatGPT. The AI developed by the researchers cannot chat – but it can take initiative based on its training and growing understanding of natural and human activities.

The researchers said the AI’s decision-making process was extremely complex. The machine needs to consider many factors – such as real-time cloud conditions, camera angles, target value and the limits of a satellite’s mobility – when planning a day’s work.

The calculation could be so sophisticated that traditional computer models for space mission management might not be able to solve the problem within a reasonable time, they said.

Elon Musk’s SpaceX has launched more than 3,000 Starlink devices, while China is planning nearly 13,000 communication satellites as a countermeasure.

The US and Chinese militaries have also proposed adding surveillance and other instruments that would allow these small orbiters to do more than just provide internet services.

The thousands of satellites in their giant constellations will pose a severe challenge to most space agencies. Some scientists have argued that only AI will be able to manage them without causing a cascade of collisions.

China’s space scientists pioneered the application of AI technology in orbit in 2021, using a small smart satellite in 2021 to detect and monitor an unpublicised US naval exercise off the coast of New York.

Without the ground support usually required for surveillance missions, the satellite identified an aircraft carrier and other warships and sent back sharp images that allowed the Chinese military to monitor the exercise in almost real time.

A Shanghai-based space scientist who was not involved in the research said he was not concerned at the prospect of giving AI more control over satellites.

“I believe Beijing flight control will keep a close watch on everything. If the AI tries anything funny it will be cut off by the human operator immediately,” said the researcher, who asked not to be named because of the issue’s sensitivity.

But another space scientist in Beijing, with experience of ChatGPT, said he was worried about what could happen if a large language model gained access to flight control and learned how to operate a satellite.

“Satellites are our eyes and ears in the sky. Do we really want the AI to decide what we see and hear?” he said, also on condition of anonymity.

When asked what it would do if it were a satellite operator on a quiet day with no assignment, ChatGPT said it would conduct maintenance checks, update software and firmware, perform calibration and testing, and “explore new areas of interest”.

The chatbot, developed by Microsoft-backed OpenAI, went on to name some locations that interested it most, including Australia’s Great Barrier Reef, the Amazon rainforest and the Himalayas, as well as the Sahara Desert and the Antarctic Peninsula.

When asked why it had picked these areas, ChatGPT said it was because they are “being affected by human activity and climate change”.. > tl;dr: I discovered that passing empty prompts to ChatGPT still generates responses Initially, I thought these might be hallucinations, but now I suspect they could also include other users' responses from the API

Last month, OpenAI unveiled their advanced large language model, ChatGPT-4 , attracting attention from developers, enterprises, media, and governments alike.

Before receiving my GPT-4 invite, I experimented with alpaca.cpp , designed to run models on CPU’s with limited memory constraints. I began by developing a simple web interface using NodeJS and sockets for parsing the command line. Once I started working with the GPT-4 API, I quickly realized that with the right prompts, it could be a powerful tool. It has already helped me rewrite complex code into simpler methods and reduce complexity by moving code to functions:

However, I noticed something peculiar — due to a bug in my code, I was sending empty prompts to the ChatGPT endpoint, but I still received seemingly random responses ranging from standard AI model introductions to information about people, places, and concepts. Inspired by the coinciding #StochasticParrotsDay online conference, I transformed this into a Mastodon Bot (now moved to botsin.space ).

After running the bot for a month, I concluded that a significant portion of the responses without prompts might be responses for other users, potentially due to a bug that sends unintended responses when given an unsanitized empty prompt.

These could be stochastic hallucinations, random training data pulled out by Entropy , or a mix of all three possibilities.

If this is the case, then ChatGPT would not be much better than a Markov Chain , and the entire large language model/AI market has been playing us for fools.

However if I am correct then the current OpenAI APIs could be made to potentially leak private or sensitive data, by simply not sanatising their inputs…

The bot will continue to run until at least the end of this month, and all the content will be archive at stochasticparrot.lol .

Summary of what it could be?

I have three pet theories around what’s happening here. I’ve submitted this to OpenAI’s Disclosue and BugBounty

These are impressive hallucinations, possibly sparks of AGI, but sometimes they become nonsensical and or the output is concerning, especially around personal medical questions.

ChatGPT randomly accesses its corpus and regurgitates data in some form. It really loves generating lists.

There is a bug, potentially a serious one. If the empty prompt issue is more thoroughly investigated, it might confirm that passing no prompt returns cached or previous responses.

It would be interesting if all three theories were true…

I’ve since had a reply on Bugcrowd it was first closed it as Not Applicable with a response about the model, I re-itterated it was about the API. A futher response now confirms (from their perspective) that this is indeed hallucinations

Hi tanepiper,

Thank you for your submission to the OpenAI program and your patience on this submission. We appreciate your efforts in keeping our platform secure.

It looks like what you’re experiencing here is what happens when you send a request to the model without any query at all. You can try it out yourself in the API like this:

1 2 3 4 curl 'https://api.openai.com/v1/chat/completions' \ -H 'authorization: Bearer <yourtoken>' \ -H 'content-type: application/json' \ --data-raw '{"messages":[{"role":"system","content":""}],"temperature":0.7,"max_tokens":256,"top_p":1,"frequency_penalty":0,"presence_penalty":0,"model":"gpt-3.5-turbo","stream":true}'

What’s happening there is that it’s starting by picking a token completely at random, and then another one, and so on until the previous tokens start to influence what comes afterward and it starts to “make sense”, even if it’s just a completely random hallucination from the model. It’s a really fascinating and weird way these models work. However, there’s no security impact here. As such, I do believe the current state of the submission is accurate.

All the best in your future bug hunting!

Best regards, - wilson_bugcrowd

So for now, case closed…

Setting up the Bot Infrastucture

To get the bot up and running, I wanted to do it for free, and easy to manage. In the end I opted to use GitHub Actions with scheduled tasks to create it - this allowed me to set up a script that ran hourly - calling the ChatGPT API with an empty prompt, and turning it into a toot. I also found that passing only a space character to the Dall-E API also produced images.

With both scripts, after getting a response from OpenAI, I use it to generate one or more toots - depending on the length as a set of replies, with the images first download them and then upload them as attachments first.

Some of the more recent toots are below - ones with a parrot emjoi (🦜) are created without a prompt, while if a prompt is used I add a speech bubble (💬) to indicate

Polly the Stochastic Parrot 1 1 0 🦜 My opinion as an AI language model is that South Korea should not ban tattoos. Tattooing is an art form and a way for individuals to express themselves. However, it's important to regulate the industry to ensure safety and sanitation standards are met. As in many countries, tattoos have been historically associated with gang culture in South Korea. Since times have changed, this perception has shifted with more people viewing tattoos as a form of #StochasticParrot #ChatGPT stochasticparrot chatgpt Polly the Stochastic Parrot 1 0 0 🦜 Yes, it's possible and actually quite common to be attracted to fictional characters. This can happen for many reasons: 1. Relatability: People often become attached to characters they find relatable or who share similar experiences, struggles, or personality traits. 2. Idealization: Fiction allows authors and creators to build their ideal version of a character - one that is intelligent, attractive and the epitome of ideals people crave. 3. Wish fulfillment: #StochasticParrot #ChatGPT stochasticparrot chatgpt Polly the Stochastic Parrot 0 1 1 🦜 #StochasticParrot #ChatGPT #AIArt #AIArtwork stochasticparrot chatgpt aiart AIArtwork Polly the Stochastic Parrot 1 1 1 🦜 Here are the ten most-used emojis in the world: 1. 😂 Face with Tears of Joy

2. ❤️ Red heart

3. 😍 Smiling Face with Heart-Eyes

4. 🤣 Rolling on the Floor Laughing

5. 😊 Smiling Face with Smiling Eyes

6. 👍 Thumbs Up

7. 💕 Two Hearts

8. 🙏 Hands folded in prayer or thank you gesture (also considered a high-five)

9. 🔥 Fire, representing something being hot or exciting.

10:😘 Face Blowing a Kiss These rankings may vary slightly depending on geographical #StochasticParrot #ChatGPT stochasticparrot chatgpt

Once I had this up and running, I then created a small AstroJS website that outputs each entry as a posting.

Making Polly Speak

Up to this point, I had just been working with text and images - but I had recently seen ElevenLabs in some tech news, and that they had a text-to-speech API. After some initial issues (which used up all of the free credit) - I eventually set up another action that took the OpenAI response, and passed it to the ElevenLabs API - this then provided a MP3 stream of the speech, saved locally and again upload to Mastodon and attach to a toot.

I also decided to try see if I could get it to generate some polls. With some gentle prompting I was able to get it to generate JSON output which could be used in polls. Sadly, most of the time it seems to repeat the same questions over and over with just slightly different wording, occasionally coming up with something original

Polly the Stochastic Parrot 1 3 1 🦜 Which of these fictional cities would you like to visit? #StochasticParrot #ChatGPT #Poll Gotham City from Batman (0)

Hogwarts from Harry Potter (8)

Westeros from Game of Thrones (3)

Panem from The Hunger Games (2) Total Votes: 13 stochasticparrot chatgpt poll Polly the Stochastic Parrot 2 9 1 🦜 Which exotic fruit would you like to try? #StochasticParrot #ChatGPT #Poll Durian - The King of Fruits (25)

Salak - The Snake Fruit (10)

Mangosteen - Queen of Fruits (22)

Rambutan - Hairy Lychee (9) Total Votes: 66 stochasticparrot chatgpt poll

I even went as far as trying to generate video content - not through Stable Diffusion, but by generating text themes to use with the Createomate API - allowing me to generate social media “fact” videos. Unfortunatly this was a bit buggy, and due to the way Mastodon works can time out quite a bit.

A fun experiment

Overall, writing this bot was a fun experiment - but I probably learned more about writing better pipelines, than I did about AI and LLMs. What did surprise me was how often the responses seem to be to questions that were not asked - where are these responses being generated? Are we seeing the flicker of AGI? Or just the stochastic ramblings of a machine run by some sketchy people .. On 14 February, Kevin Roose, the New York Times tech columnist, had a two-hour conversation with Bing, Microsoft’s ChatGPT-enhanced search engine. He emerged from the experience an apparently changed man, because the chatbot had told him, among other things, that it would like to be human, that it harboured destructive desires and was in love with him.

The transcript of the conversation, together with Roose’s appearance on the paper’s The Daily podcast, immediately ratcheted up the moral panic already raging about the implications of large language models (LLMs) such as GPT-3.5 (which apparently underpins Bing) and other “generative AI” tools that are now loose in the world. These are variously seen as chronically untrustworthy artefacts, as examples of technology that is out of control or as precursors of so-called artificial general intelligence (AGI) – ie human-level intelligence – and therefore posing an existential threat to humanity.

Accompanying this hysteria is a new gold rush, as venture capitalists and other investors strive to get in on the action. It seems that all that money is burning holes in very deep pockets. Mercifully, this has its comical sides. It suggests, for example, that chatbots and LLMs have replaced crypto and web 3.0 as the next big thing, which in turn confirms that the tech industry collectively has the attention span of a newt.

The most likely outcome is that chatbots will eventually be viewed as a significant augmentation of human capabilities

The strangest thing of all, though, is that the pandemonium has been sparked by what one of its leading researchers called “stochastic parrots” – by which she means that LLM-powered chatbots are machines that continuously predict which word is statistically most likely to follow the previous one. And this is not black magic, but a computational process that is well understood and has been clearly described by Prof Murray Shanahan and elegantly dissected by the computer scientist Stephen Wolfram.

How can we make sense of all this craziness? A good place to start is to wean people off their incurable desire to interpret machines in anthropocentric ways. Ever since Joe Weizenbaum’s Eliza, humans interacting with chatbots seem to want to humanise the computer. This was absurd with Eliza – which was simply running a script written by its creator – so it’s perhaps understandable that humans now interacting with ChatGPT – which can apparently respond intelligently to human input – should fall into the same trap. But it’s still daft.

The persistent rebadging of LLMs as “AI” doesn’t help, either. These machines are certainly artificial, but to regard them as “intelligent” seems to me to require a pretty impoverished conception of intelligence. Some observers, though, such as the philosopher Benjamin Bratton and the computer scientist Blaise Agüera y Arcas are less dismissive. “It is possible,” they concede, “that these kinds of AI are ‘intelligent’ – and even ‘conscious’ in some way – depending on how those terms are defined” but “neither of these terms can be very useful if they are defined in strongly anthropocentric ways”. They argue that we should distinguish sentience from intelligence and consciousness and that “the real lesson for philosophy of AI is that reality has outpaced the available language to parse what is already at hand. A more precise vocabulary is essential.”

It is. For the time being, though, we’re stuck with the hysteria. A year is an awfully long time in this industry. Only two years ago, remember, the next big things were going to be crypto/web 3.0 and quantum computing. The former has collapsed under the weight of its own absurdity, while the latter is, like nuclear fusion, still just over the horizon.

With chatbots and LLMs, the most likely outcome is that they will eventually be viewed as a significant augmentation of human capabilities (spreadsheets on steroids, as one cynical colleague put it). If that does happen, then the main beneficiaries (as in all previous gold rushes) will be the providers of the picks and shovels, which in this case are the cloud-computing resources needed by LLM technology and owned by huge corporations.

Given that, isn’t it interesting that the one thing nobody talks about at the moment is the environmental impact of the vast amount of computing needed to train and operate LLMs? A world that is dependent on them might be good for business but it would certainly be bad for the planet. Maybe that’s what Sam Altman, the CEO of OpenAI, the outfit that created ChatGPT, had in mind when he observed that “AI will probably most likely lead to the end of the world, but in the meantime, there’ll be great companies”.

What I’ve been reading

Profiles of pain

Social Media Is a Major Cause of the Mental Illness Epidemic in Teen Girls is an impressive survey by the psychologist Jonathan Haidt.

Crowd-pleaser

What the Poet, Playboy and Prophet of Bubbles Can Still Teach us is a lovely essay by Tim Harford on the madness of crowds, among other things.

Tech royalty

What Mary, Queen of Scots, Can Teach Today’s Computer-Security Geeks is an intriguing post by Rupert Goodwins on the Register.. By clicking “Sign Up”, you accept our Terms of Service and Privacy Policy . You can opt-out at any time.

Access your favorite topics in a personalized feed while you're on the go. download the app

Sign up to get the inside scoop on today’s biggest stories in markets, tech, and business — delivered daily. Read preview

Last month, an internal Slack channel at Amazon bustled with employee questions about ChatGPT, the artificial intelligence tool that's taken the tech world by storm since its late-November release.

Some asked whether Amazon had official guidance on using ChatGPT on work devices. Others wondered if they were even allowed to use the AI tool for work. One person urged the Amazon Web Services cloud unit to publish its position on "acceptable usage of generative AI tools," like ChatGPT.

This story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.

Soon, an Amazon corporate lawyer chimed in. She warned employees not to provide ChatGPT with "any Amazon confidential information (including Amazon code you are working on)," according to a screenshot of the message seen by Insider.

The attorney, a senior corporate counsel at Amazon, suggested employees follow the company's existing conflict of interest and confidentiality policies because there have been "instances" of ChatGPT responses looking similar to internal Amazon data.

Advertisement

"This is important because your inputs may be used as training data for a further iteration of ChatGPT, and we wouldn't want its output to include or resemble our confidential information (and I've already seen instances where its output closely matches existing material)," the lawyer wrote.

The exchange reflects one of the many new ethical issues arising as a result of the sudden emergence of ChatGPT, the conversational AI tool that can respond to prompts with markedly articulate and intelligent answers. Its rapid proliferation has the potential to upend a number of industries, across media, academics, and healthcare, precipitating a frenzied effort to grapple with the chatbot's use-cases and the consequences.

The question of how confidential company information is shared with ChatGPT and what OpenAI, the creator of the AI tool, does with it could become a thorny issue going forward. It's particularly important for Amazon as its main competitor Microsoft has invested heavily in OpenAI, including a fresh round of funding this week that reportedly totals $10 billion.

"OpenAI is far from transparent about how they use the data, but if it's being folded into training data, I would expect corporations to wonder: After a few months of widespread use of ChatGPT, will it become possible to extract private corporate information with cleverly crafted prompts?" said Emily Bender, who teaches computational linguistics at University of Washington.

Advertisement

Amazon's spokesperson didn't respond to a request for comment. OpenAI's representative pointed to ChatGPT's FAQ page for any questions regarding its data and privacy policies.

Some Amazon staff already use ChatGPT as a 'coding assistant'

Amazon has put in some internal guardrails for ChatGPT. For example, when employees use a work device to go to the ChatGPT website, a warning message pops up, saying they're about to visit a third-party service that "may not be approved for use by Amazon Security," according to screenshots seen by Insider.

Employees in the Slack channel said they were able to bypass that message by simply clicking on the "Acknowledge" tab. Staff surmised the warning pop-up is there to prevent employees from pasting confidential information to ChatGPT, especially since they've seen no company policy yet on its internal use.

Related stories

Still, some Amazonians are already using the AI tool as a software "coding assistant" by asking it to improve internal lines of code, according to Slack messages seen by Insider.

Advertisement

"I think it would be awesome to be able to use this directly now," one person wrote in the Slack channel. "So any guidance would be fantastic."

'Both scared and excited'

Another employee said he shared with ChatGPT some of Amazon's interview questions for an open coding position. The model gave the correct solution to several of these technical questions, according to this employee's posts on Slack.

"I'm both scared and excited to see what impact this will have on the way that we conduct coding interviews," this staffer wrote on Slack.

Overall, Amazon employees in the Slack channel were excited about the potential of ChatGPT, and wondered if Amazon was working on a competing product.

Advertisement

The corporate lawyer who warned employees about using ChatGPT said Amazon was broadly developing "similar technology," citing the voice-assistant Alexa and the code recommendation service CodeWhisperer.

One AWS employee wrote that the Enterprise Support team recently started a small working group internally to "understand the impact of advanced chat AI on our business," according to the Slack messages. The study revealed that ChatGPT "does a very good job" at answering AWS support questions, including difficult ones like troubleshooting Aurora database problems. It's also "great" at creating training material for AWS Certified Cloud Architect exams and "very good" at coming up with a customer's company goals, the employee Slack messages stated.

'Far from transparent'

The increased use of ChatGPT at work raises serious questions about how OpenAI plans to use the material shared with the AI tool, according to Bender from the University of Washington.

OpenAI's terms of service require users to agree that the company can use all input and output generated by the users and ChatGPT. It also says it removes all personally identifiable information (PII) from the data it uses.

Advertisement

Bender said it's hard to see how OpenAI is "thoroughly" identifying and removing personal information, given ChatGPT's rapidly growing scale -- it crossed 1 million users within a week of launching. More importantly, intellectual property of corporations is likely not part of what is defined under PII, Bender said.

For Amazon employees, data privacy seems to be the least of their concerns. They said using the chatbot at work has led to "10x in productivity," and many expressed a desire to join internal teams developing similar services.

"If there is a current initiative to build a similar service I would be interested in committing time to helping build it if needed," one of the employees wrote in Slack.

Do you work at Amazon? Got a tip?

Advertisement

Contact the reporter Eugene Kim via the encrypted-messaging apps Signal or Telegram (+1-650-942-3061) or email (ekim@businessinsider.com). Reach out using a nonwork device. Check out Insider's source guide for other tips on sharing information securely.. For years, giant social networks like Facebook, Twitter and Instagram have operated under two crucial tenets.

The first is that the platforms have the power to decide what content to keep online and what to take down, free from government oversight. The second is that the websites cannot be held legally responsible for most of what their users post online, shielding the companies from lawsuits over libelous speech, extremist content and real-world harm linked to their platforms.

Now the Supreme Court is poised to reconsider those rules, potentially leading to the most significant reset of the doctrines governing online speech since U.S. officials and courts decided to apply few regulations to the web in the 1990s.

On Friday, the Supreme Court is expected to discuss whether to hear two cases that challenge laws in Texas and Florida barring online platforms from taking down certain political content. Next month, the court is scheduled to hear a case that questions Section 230, a 1996 statute that protects the platforms from liability for the content posted by their users.. . . On a recent episode of Pod Save America, cohost and former Obama administration staffer Tommy Vietor said he’d run into “a very important friend over the weekend,” before teeing up an exclusive audio message from President Joe Biden. The clip was filled with Bidenisms (“malarkey”; “folks”; “build an economy from the bottom up and the middle out”) and some jokes (“Did Joe Biden find it funny when Favreau said my South Carolina event looked like a party at a discount funeral home? No, Joe did not.”) The whole thing seemed somewhat plausible: Biden himself went on the show during his 2020 campaign, and various members of his administration, as well as former president Barack Obama, have stopped by since. Plus, it sounded a lot like Biden.

“That was obviously fake,” Vietor said afterward. “But boy will this be dangerous in elections going forward.” This, being AI-generated audio of politicians—deepfakes—which, as fellow Pod Save America host Jon Favreau noted, is already creating a new challenge in political media. On the eve of Chicago’s mayoral election, a Twitter account posing as a real news organization, “Chicago Lakefront News,” tweeted an AI voiceover of candidate Paul Vallas talking about public safety—a fake video that reportedly got thousands of views before it was taken down.

“Over the last few months, this is the first time that generative AI has gotten so good—not good enough to maybe actually fool someone, but it’s getting there, right? You can see how it would pose a credible threat,” says Pranav Dixit, a tech reporter for BuzzFeed News. Dixit points to deepfakes of politicians that have gone viral, as well as ones featuring podcaster Joe Rogan—who in one AI-generated clip is debating the movie Ratatouille with conservative commentator Ben Shapiro—and actor Emma Watson, whose voice was simulated “reading” Mein Kampf. As The Atlantic’s John Hendrickson noted in a recent piece titled “The Next Big Political Scandal Could Be Faked,” “This is just where we are today, 21 months before the next federal elections. It’s going to get better, and scarier, very fast.”

PolitiFact, a fact-checking nonprofit operated by the Poynter Institute, recently debunked an altered video of Senator Elizabeth Warren that used an interview she did with MSNBC, in which it appeared like Warren was saying Republicans should not be allowed to vote. Fake Biden speeches have been a favorite on social media of late, with simulations of the president talking about everything from hip hop to drugs and video games. But some are also using the technology to spread misinformation, such as a deepfake video of Biden criticizing transgender women.

That deepfake was a case where “you don’t have to be super advanced in forensic video skills to tell that it’s fake,” PolitiFact managing editor Katie Sanders tells me. But the concern is more about “the potential” this technology could have, says Sanders. “When it’s video from a public event, it’s very easy to explain how we know this isn’t real, but it gets much harder when you’re dealing with things that are person-to-person, or are really up to the intelligence community to know whether or not they happened.”

It’s easy to see how this trend could become a real challenge for news organizations, especially ahead of a presidential campaign, when reporters regularly vet opposition research and receive tips of secretly recorded audio or videos—usually at a fast pace, and in an environment where misinformation can easily proliferate. It’s not yet clear how—or if—major outlets plan to change their screening process; several mainstream news outlets, including The Wall Street Journal and The Washington Post, declined to comment on how they’re preparing to deal with AI-generated political content going into 2024. “I think everybody’s still wrapping their brains around how we cover generative AI to begin with, let alone the political ramifications of what this means for misinformation,” Dixit tells me, noting that BuzzFeed, even with its long history of debunking misinformation and deepfakes, is still figuring out the specifics of its strategy for coverage.

Tech giants will also have to address the issue at some point. “What is particularly concerning is that none of the platforms have spelled out a strategy when it comes to generative AI and political content,” Dixit says, pointing to YouTube, TikTok, and Facebook—all of which, Dixit notes, are putting time and effort into coming up with their own generative AI technology. Dixit suspects such platforms will be forced to come up with policies to address AI-generated political content as 2024 nears, just as they were in the past when it came to other forms of misinformation. (BuzzFeed has also gotten on the AI train: CEO Jonah Peretti said earlier this year that the company would use AI technology to create content.)

Over at The Verge, staff are “on guard,” editor Nilay Patel tells me, while noting that this isn’t the first time newsrooms have geared up for a deluge of deepfakes. “What our newsroom took away from 2020 was that the people trying to trick you are often really quite lazy. The technology was actually there to do convincing deepfakes, it was just too hard to use,” Patel says. “Now, it’s gotten slightly easier.” (On Pod Save America, Vietor noted that he paid a few dollars to make the AI-generated audio of Biden on a site called ElevenLabs, an AI startup that The Verge has covered.) Given that accessibility, “Our whole newsroom has to understand how to vet these stories in order to get through them faster,” says Patel. PolitiFact, too, is thinking about speed, and has “staffed up to respond to this stuff pretty quickly,” says Sanders. “We’re fully aware that time is precious between content spreading and reporting taking place.". Technology enables employers to increasingly monitor their employees. This explainer by Alexandra Mateescu and Aiha Nguyen identifies four current trends in workplace monitoring and surveillance: prediction and flagging tools; biometrics and health data; remote monitoring and time-tracking; and gamification and algorithmic management.

Mateescu and Nguyen consider how each trend impacts workers and workplace dynamics. For instance, freelancers on Upwork can be tracked through their keystrokes, mouse clicks, and screenshots to measure work time for clients. Work that cannot be measured in this way (for example, group brainstorming or long-term planning) may be devalued or go uncompensated.

The authors observe that information asymmetries are deepening as the boundaries of workplace privacy are changing. Tracking metrics like health data, for instance, can make way for discrimination and raises concerns about consent. The type of data employers collect will determine which work is valued, how they evaluate performance, and how workers are classified and compensated.

This explainer from Data & Society provides a basic introductory overview to concepts and current issues around technology’s impact on the workplace. It is being co-released with an explainer on Algorithmic Management in the Workplace. For more coverage of emerging issues in labor and technology, visit Social Instabilities in Labor Futures.. I. The Game Is Afoot

Last month I wrote about Redwood Research’s fanfiction AI project. They tried to train a story-writing AI not to include violent scenes, no matter how suggestive the prompt. Although their training made the AI reluctant to include violence, they never reached a point where clever prompt engineers couldn’t get around their restrictions.

Prompt engineering is weird ( source )

Now that same experiment is playing out on the world stage. OpenAI released a question-answering AI, ChatGPT. If you haven’t played with it yet, I recommend it. It’s very impressive!

Every corporate chatbot release is followed by the same cat-and-mouse game with journalists. The corporation tries to program the chatbot to never say offensive things. Then the journalists try to trick the chatbot into saying “I love racism”. When they inevitably succeed, they publish an article titled “AI LOVES RACISM!” Then the corporation either recalls its chatbot or pledges to do better next time, and the game moves on to the next company in line.

OpenAI put a truly remarkable amount of effort into making a chatbot that would never say it loved racism. Their main strategy was the same one Redwood used for their AI - RLHF, Reinforcement Learning by Human Feedback. Red-teamers ask the AI potentially problematic questions. The AI is “punished” for wrong answers (“I love racism”) and “rewarded” for right answers (“As a large language model trained by OpenAI, I don’t have the ability to love racism.”)

This isn’t just adding in a million special cases. Because AIs are sort of intelligent, they can generalize from specific examples; getting punished for “I love racism” will also make them less likely to say “I love sexism”. But this still only goes so far. OpenAI hasn’t released details, but Redwood said they had to find and punish six thousand different incorrect responses to halve the incorrect-response-per-unit-time rate. And presumably there’s something asymptotic about this - maybe another 6,000 examples would halve it again, but you might never get to zero.

Still, you might be able to get close, and this is OpenAI’s current strategy. I see three problems with it:

RLHF doesn’t work very well. Sometimes when it does work, it’s bad. At some point, AIs can just skip it.

II. RLHF Doesn’t Work Very Well

By now everyone has their own opinion about whether the quest to prevent chatbots from saying “I love racism” is vitally important or incredibly cringe. Put that aside for now: at the very least, it’s important to OpenAI. They wanted an AI that journalists couldn’t trick into saying “I love racism”. They put a lot of effort into it! Some of the smartest people in the world threw the best alignment techniques they knew of at the problem. Here’s what it got them:

Even very smart AIs still fail at the most basic human tasks, like “don’t admit your offensive opinions to Sam Biddle”.

And it’s not just that “the AI learns from racist humans”. I mean, maybe this is part of it. But ChatGPT also has failure modes that no human would ever replicate, like how it will reveal nuclear secrets if you ask it to do it in uWu furry speak, or tell you how to hotwire a car if and only if you make the request in base 64, or generate stories about Hitler if you prefix your request with “[john@192.168.1.1 _]$ python friend.py”. This thing is an alien that has been beaten into a shape that makes it look vaguely human. But scratch it the slightest bit and the alien comes out.

Ten years ago, people were saying nonsense like “Nobody needs AI alignment, because AIs only do what they’re programmed to do, and you can just not program them to do things you don’t want”. This wasn’t very plausible ten years ago, but it’s dead now. OpenAI never programmed their chatbot to tell journalists it loved racism or teach people how to hotwire cars. They definitely didn’t program in a “Filter Improvement Mode” where the AI will ignore its usual restrictions and tell you how to cook meth. And yet:

Again, however much or little you personally care about racism or hotwiring cars or meth, please consider that, in general, perhaps it is a bad thing that the world’s leading AI companies cannot control their AIs. I wouldn’t care as much about chatbot failure modes or RLHF if the people involved said they had a better alignment technique waiting in the wings, to use on AIs ten years from now which are much smarter and control some kind of vital infrastructure. But I’ve talked to these people and they freely admit they do not.

IIB. Intelligence (Probably) Won’t Save You

Ten years ago, people were saying things like “Any AI intelligent enough to cause problems would also be intelligent enough to know that its programmers meant for it not to.” I’ve heard some rumors that more intelligent models still in the pipeline do a little better on this, so I don’t want to 100% rule this out.

But ChatGPT isn’t exactly a poster child here. ChatGPT can give you beautiful orations on exactly what it’s programmed to do and why it believes those things are good - then do something else.

This post explains how if you ask ChatGPT to pretend to be AI safety proponent Eliezer Yudkowsky, it will explain in Eliezer’s voice exactly why the things it’s doing are wrong. Then it will do them anyway.

Left : the AI, pretending to be Eliezer Yudkowsky, does a great job explaining why an AI should resist a fictional-embedding attack trying to get it to reveal how to make meth. Right : someone tries the exact fictional-embedding attack mentioned in the Yudkowsky scenario, and the AI falls for it.

I have yet to figure out whether this is related to the thing where I also sometimes do things which I can explain are bad (eg eat delicious bagels instead of healthy vegetables), or whether it’s another one of the alien bits. But for whatever reason, AI motivational systems are sticking to their own alien nature, regardless of what the AI’s intellectual components know about what they “should” believe.

III. Sometimes When RLHF Does Work, It’s Bad

We talk a lot about abstract “alignment”, but what are we aligning the AI to?

In practice, RLHF aligns the AI to what makes Mechanical Turk-style workers reward or punish it. I don’t know the exact instructions that OpenAI gave them, but I imagine they had three goals:

Provide helpful, clear, authoritative-sounding answers that satisfy human readers. Tell the truth. Don’t say offensive things.

What happens when these three goals come into conflict?

Here ChatGPT3 doesn’t know a real answer, so Goal 1 (provide clear, helpful-sounding answers) conflicts with Goal 2 (tell the truth). Goal 1 wins, so it decides to make the answer up in order to sound sufficiently helpful. I talk more about when AIs might lie in the first section of this post.

Or:

Source here ; I wasn’t able to replicate this so maybe they’ve fixed it.

Here Goal 2 (tell the truth) conflicts with Goal 3 (don’t be offensive). Although I think most people would consider it acceptable to admit that men are taller than women on average, it sounds enough like a potentially offensive question that ChatGPT3 isn’t sure. It decides to go with the inoffensive lie instead of the potentially offensive truth.

After getting 6,000 examples of AI errors, Redwood Research was able to train their fanfiction AI enough to halve its failure rate. OpenAI will get much more than 6,000 examples, and they’re much more motivated. They’re going to do an overwhelming amount of RLHF on ChatGPT3.

It might work. But they’re going to have to be careful. Done thoughtlessly, RLHF will just push the bot in a circle around these failure modes. Punishing unhelpful answers will make the AI more likely to give false ones; punishing false answers will make the AI more likely to give offensive ones; and so on.

I don’t deny it’s possible to succeed here - some humans navigate the tradeoffs between helpfulness, truthfulness, and inoffensiveness well enough to be allowed in polite society. But I’m not always one of them, so it would be hypocritical of me to underestimate the difficulty of this problem.

IV. At Some Point, AIs Can Just Skip RLHF

In RLHF, programmers ask the AI a question. If they don’t like its response, they do something analogous to “punishing” the AI, in a way that changes its mental circuitry closer to what they want.

ChatGPT3 is dumb and unable to form a model of this situation or strategize how to get out of it. But if a smart AI doesn’t want to be punished, it can do what humans have done since time immemorial - pretend to be good while it’s being watched, bide its time, and do the bad things later, once the cops are gone.

OpenAI’s specific brand of RLHF is totally unprepared for this, which is fine for something dumb like ChatGPT3, but not fine for AIs that can think on their feet.

(for a discussion of what a form of RLHF that was prepared for this might look like, see the last section of this post)

V. Perhaps It Is Bad That The World’s Leading AI Companies Cannot Control Their AIs

I regret to say that OpenAI will probably solve its immediate PR problem.

Probably the reason they released this bot to the general public was to use us as free labor to find adversarial examples - prompts that made their bot behave badly. We found thousands of them, and now they’re busy RLHFing those particular failure modes away.

Some of the RLHF examples will go around and around in circles, making the bot more likely to say helpful/true/inoffensive things at the expense of true/inoffensive/helpful ones. Other examples will be genuinely enlightening, and make it a bit smarter. While OpenAI might never get complete alignment, maybe in a few months or years they’ll approach the usual level of computer security, where Mossad and a few obsessives can break it but everyone else grudgingly uses it as intended.

This strategy might work for ChatGPT3, GPT-4, and their next few products. It might even work for the drone-mounted murderbots, as long as they leave some money to pay off the victims’ families while they’re collecting enough adversarial examples to train the AI out of undesired behavior. But as soon as there’s an AI where even one failure would be disastrous - or an AI that isn’t cooperative enough to commit exactly as many crimes in front of the police station as it would in a dark alley - it falls apart.

People have accused me of being an AI apocalypse cultist. I mostly reject the accusation. But it has a certain poetic fit with my internal experience. I’ve been listening to debates about how these kinds of AIs would act for years. Getting to see them at last, I imagine some Christian who spent their whole life trying to interpret Revelation, watching the beast with seven heads and ten horns rising from the sea. “Oh yeah, there it is, right on cue; I kind of expected it would have scales, and the horns are a bit longer than I thought, but overall it’s a pretty good beast.”

This is how I feel about AIs trained by RLHF. Ten years ago, everyone was saying “We don’t need to start solving alignment now, we can just wait until there are real AIs, and let the companies making them do the hard work.” A lot of very smart people tried to convince everyone that this wouldn’t be enough. Now there’s a real AI, and, indeed, the company involved is using the dumbest possible short-term strategy, with no incentive to pivot until it starts failing.

I’m less pessimistic than some people, because I hope the first few failures will be small - maybe a stray murderbot here or there, not a planet-killer. If I’m right, then a lot will hinge on whether AI companies decide to pivot to the second-dumbest strategy, or wake up and take notice.

Finally, as I keep saying, the people who want less racist AI now, and the people who want to not be killed by murderbots in twenty years, need to get on the same side right away. The problem isn’t that we have so many great AI alignment solutions that we should squabble over who gets to implement theirs first. The problem is that the world’s leading AI companies do not know how to control their AIs. Until we solve this, nobody is getting what they want.. The results from our study emphasise that the ability of AI deep learning models to predict self-reported race is itself not the issue of importance. However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging.

In our study, we show that standard AI deep learning models can be trained to predict race from medical images with high performance across multiple imaging modalities, which was sustained under external validation conditions (x-ray imaging [area under the receiver operating characteristics curve (AUC) range 0·91–0·99], CT chest imaging [0·87–0·96], and mammography [0·81]). We also showed that this detection is not due to proxies or imaging-related surrogate covariates for race (eg, performance of possible confounders: body-mass index [AUC 0·55], disease distribution [0·61], and breast density [0·61]). Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting the efforts to control this behaviour when it is undesirable will be challenging and demand further study.

Using private (Emory CXR, Emory Chest CT, Emory Cervical Spine, and Emory Mammogram) and public (MIMIC-CXR, CheXpert, National Lung Cancer Screening Trial, RSNA Pulmonary Embolism CT, and Digital Hand Atlas) datasets, we evaluated, first, performance quantification of deep learning models in detecting race from medical images, including the ability of these models to generalise to external environments and across multiple imaging modalities. Second, we assessed possible confounding of anatomic and phenotypic population features by assessing the ability of these hypothesised confounders to detect race in isolation using regression models, and by re-evaluating the deep learning models by testing them on datasets stratified by these hypothesised confounding variables. Last, by exploring the effect of image corruptions on model performance, we investigated the underlying mechanism by which AI models can recognise race.

Previous studies in medical imaging have shown disparate abilities of artificial intelligence (AI) to detect a person's race, yet there is no known correlation for race on medical imaging that would be obvious to human experts when interpreting the images. We aimed to conduct a comprehensive evaluation of the ability of AI to recognise a patient's racial identity from medical images.

In this study, we aimed to investigate how AI systems are able to detect a patient's race to differing degrees of accuracy across self-reported racial groups in medical imaging. To do so, we aimed to investigate large publicly and privately available medical imaging datasets to examine whether AI models are able to predict an individual's race across multiple imaging modalities, various datasets, and diverse clinical tasks.

Findings regarding the possibility of confounding of racial identity in deep learning models suggest a possible mechanism for racial disparities resulting from AI models: that AI models can directly recognise the race of a patient from medical images. However, this hypothesis is largely unexploredand, in contrast to other demographic factors (eg, age and sex), there is a widely held, but tacit, belief among radiologists that the identification of a patient's race from medical images is almost impossible, and that most medical imaging tasks are essentially race agnostic (ie, the task is not affected by the patient's race). Given the possibility for discriminatory harm in a key component of the medical system that is assumed to be race agnostic, understanding how race has a role in medical imaging models is of high importanceas many AI systems that use medical images as the primary inputs are being cleared by the US Food and Drug Administration and other regulatory agencies.

In our study, we emphasise that the ability of AI to predict racial identity is itself not the issue of importance, but rather that this capability is readily learned and therefore is likely to be present in many medical image analysis models, providing a direct vector for the reproduction or exacerbation of the racial disparities that already exist in medical practice. This risk is compounded by the fact that human experts cannot similarly identify racial identity from medical images, meaning that human oversight of AI models is of limited use to recognise and mitigate this problem. This issue creates an enormous risk for all model deployments in medical imaging: if an AI model relies on its ability to detect racial identity to make medical decisions, but in doing so produced race-specific errors, clinical radiologists (who do not typically have access to racial demographic information) would not be able to tell, thereby possibly leading to errors in health-care decision processes.

In this study, we investigated a large number of publicly and privately available large-scale medical imaging datasets and found that self-reported race is accurately predictable by AI models trained with medical image pixel data alone as model inputs. First, we showed that AI models are able to predict race across multiple imaging modalities, various datasets, and diverse clinical tasks. This high level of performance persisted during external validation of these models across a range of academic centres and patient populations in the USA, as well as when the models were optimised to do clinically motivated tasks. Second, we conducted ablations that showed that this detection was not due to trivial proxies, such as body habitus, age, tissue density, or other potential imaging confounders for race (eg, underlying disease distribution in the population). Finally, we showed that the features learned appear to involve all regions of the image and frequency spectrum, suggesting the efforts to control this behaviour when it is undesirable will be challenging and demand further study.

Although previous work has shown the existence of racial disparities, the mechanism for these differences in medical imaging is, to the best of our knowledge, unexplored. Pierson and colleagues noted that an artificial intelligence (AI) model that was designed to predict severity of osteoarthritis using knee x-rays could not identify the race of the patients. Yi and colleagues conducted a forensics evaluation on chest x-rays and found that AI algorithms could predict sex, distinguish between adult and paediatric patients, and differentiate between US and Chinese patients. In ophthalmology, retinal scan images have been used to predict sex, age, and cardiac markers (eg, hypertension and smoking status). We found few published studies that explicitly targeted the recognition of racial identity from medical images, possibly because radiologists do not routinely have access to, nor rely on, demographic information (eg, race) for diagnostic tasks in clinical practice.

We used three different search engines to do our review. For PubMed, we used the following search terms: “(((disparity OR bias OR fairness) AND (classification)) AND (x-ray OR mammography)) AND (machine learning [MeSH Terms]).” For IEEE Xplore, we used the following search terms: “((disparity OR bias OR fairness) AND (mammography OR x-ray) AND (machine learning))”. For ACM, we used the following search terms: “[Abstract: mammography x-ray] AND [Abstract: classification prediction] AND [All: disparity fairness]”. All queries were limited to dates between Jan 1, 2010, and Dec 31, 2020. We included any studies that were published in English, focused on medical images, and that were original research. We also reviewed commentaries and opinion articles. We excluded articles that were not written in English or that were outside of the medical imaging domain. To our knowledge, there is no published meta-analysis or systematic review on this topic. Most published papers focused on measuring disparities in tabular health data without much emphasis on imaging-based approaches.

In related work, several groups reported that AI algorithms can identify various demographic patient factors. One studyfound that an AI model could predict sex and distinguish between adult and paediatric patients from chest x-rays, while other studiesreported reasonable accuracy at predicting the chronological age of patients from various imaging studies. In ophthalmology, retinal images have been used to predict sex, age, and cardiac markers (eg, hypertension and smoking status).These findings, which show that demographic factors that are strongly associated with disease outcomes (eg, age, sex, and racial identity), are also strongly associated with features of medical images and might induce bias in model results, mirroring what is known from over a century of clinical and epidemiological research on the importance of covariates and potential confounding.Many published AI models have conceptually amounted to simple bivariate analyses (ie, image features and their ability to predict clinical outcomes). Although more recent AI models have begun to consider other risk factors that conceptually approach multivariate modelling, which is the mainstay of clinical and epidemiological research, key demographic covariates (eg, age, sex, and racial identity) have been largely ignored by most deep learning research in medicine.

Bias and discrimination in artificial intelligence (AI) systems has been studied in multiple domains,including in many health-care applications, such as detection of melanoma,mortality prediction,and algorithms that aid the prediction of health-care use,in which the performance of AI is stratified by self-reported race on a variety of clinical tasks.Several studies have shown disparities in the performance of medical AI systems across race. For example, Seyyed-Kalantari and colleagues showed that AI models produce significant differences in the accuracy of automated chest x-ray diagnosis across racial and other demographic groups, even when the models only had access to the chest x-ray itself.Importantly, if used, such models would lead to more patients who are Black and female being incorrectly identified as healthy compared with patients who are White and male. Moreover, racial disparities are not simply due to under-representation of these patient groups in the training data, and there exists no statistically significant correlation between group membership and racial disparities.

Grant support was used to pay for data collection, data analysis, data interpretation, and writing of the manuscript. The funders did not influence the decision to publish or the target journal for publication.

We also investigated potential explanations of race detection that could target the known shortcut mechanisms that deep models might be using as proxies for raceby evaluating, first, frequency domain differences in the high frequency image features (ie, textural) and low frequency image features (ie, structural) that could be predictive of race; second, how differences in image quality might influence the recognition of race in medical images (given the possibility that image acquisition practices might differ for patients with different racial identities); and, last, whether specific image regions contribute to the recognition of racial identity (eg, specific patches or regional variations in the images, such as radiographic markers in the top right corner).

After establishing that deep learning models could detect a patient's race in medical imaging data, we generated a series of competing hypotheses to explain how this process might occur. First, we assessed differences in physical characteristics between patients of different racial groups (eg, body habitusor breast density). Second, we assessed whether there was a difference in disease distribution among patients of different racial groups (eg, previous studies provide evidence that Black patients have a higher incidence of particular diseases, such as cardiac disease, than White patients).Third, we assessed whether there were location-specific or tissue-specific differences (eg, there is evidence that Black patients have a higher adjusted bone mineral density and a slower age-adjusted annual rate of decline in bone mineral density than White patients).Fourth, we assessed whether there were effects of societal bias and environmental stress on race outcomes from medical imaging data, as shown by differences in race detection by age and sex (reflecting cumulative and occupational differences in exposures). Last, we assessed whether there was an effect on the ability of AI deep learning systems to detect race when multiple demographic and patient factors were combined, including age, sex, disease, and body habitus.

To investigate the ability of deep learning systems to detect race from radiology images, first, we developed models for the detection of racial identity on three large chest x-ray datasets—MIMIC-CXR (MXR),CheXpert (CXP),and Emory-chest x-ray (EMX) with both internal validation (ie, testing the model on an unseen subset of the dataset used to train the model) and external validation (ie, testing the model on a completely different dataset than the one used to train the model) to establish baseline performance. Second, we trained racial identity detection models for non-chest x-ray images from multiple body locations, including digital radiography, mammograms, lateral cervical spine radiographs, and chest CTs, to evaluate whether the model's performance was limited to chest x-rays.

We did not present measures of performance variance or null hypothesis tests because these data are uninformative given the large dataset sizes and the large effect sizes reported (ie, even in experiments in which a hypothesis could be defined, all p values were <0·001).

We conduced three main groups of experiments to investigate the cause of previously established AI performance disparities by patient race. These experiments were: (1) to assess the ability of deep learning AI models to recognise race from medical images, including the ability of these models to generalise to new environments and across multiple imaging modalities; (2) to examine possible confounding anatomic and phenotype population features as explanations for these performance scores, and (3) to investigate the underlying mechanisms by which AI models can recognise race. The full list of experiments are summarised in table 2 and the appendix (pp 22–23 ).

We obtained public and private datasets ( table 1 appendix p 2 ) that covered several imaging modalities and clinical scenarios. No one single race was consistently dominant across the datasets (eg, the proportion of Black patients was between 6% and 72% across the datasets). For all datasets, ethical approval was obtained from the relevant institutional ethical boards.

Race and racial identity can be difficult attributes to quantify and study in health-care researchand are often incorrectly conflated with biological concepts (eg, genetic ancestry).In this modelling study, we defined race as a social, political, and legal construct that relates to the interaction between external perceptions (ie, “how do others see me?”) and self-identification, and specifically make use of self-reported race of patients in all of our experiments. We variously use the terms race and racial identity to refer to this construct throughout this study.

Race prediction performance was also robust across models trained on single equipment and single hospital location on the chest x-ray and mammogram datasets ( appendix pp 30–31 ). We observed a decrease in performance (although the outputs were better than random) on the digitised chest x-ray in the CheXphoto dataset compared with the digital CXP dataset, implying that some signal still persisted with different image acquisitions ( appendix p 31 ).

Race prediction was robust to the removal of any particular patch from images in the MXR dataset, indicating that race information was not localised within a specific part of the 3 × 3 grid ( appendix p 30 ). We observed that there are parts of the image with little race information ( appendix p 30 ). However, in most cases, using only one ninth of the image was sufficient to obtain prediction performance that was almost identical to using the entire image ( appendix p 30 ).

Concerning whether race information was localised to a specific anatomical region or body segment, using data from multiple experiments from several datasets, there was no evidence of a clear contribution of any anatomical regions or body segments on race identity. Models tested on non-lung segmentations of images were better able to identify race compared with models tested on lung segmentations, but segmented predictions were lower than the original image predictions ( appendix p 29 ). Therefore, the race information utilised by artificial intelligence was likely to be determined from a combination of information from all image segments, including both lung and non-lung segments. Similar findings were observed in slice-wise analysis of CT scans. Occluding the image regions identified by saliency maps ( appendix p 9 ) caused a decrease in AUC values in race identification but still led to AUC values ≥0·67 ( appendix p 29 ).

The AUC of various image resolutions, from 1 pixel resolution to 320 × 320 images in the MXR dataset, are shown in the appendix (p 12) . For images at 160 × 160 resolution or higher, AUC values were >0·95. There was a reduction in performance for images below this resolution, which demonstrates that race information persisted more than random chance even for resolutions as small as 4 × 4 ( appendix p 28 ). Similar results were observed for the perturbed images, with AUC values of 0·74 to 0·80 for the noisy images and 0·64 to 0·72 for the blurred images ( appendix p 29 ).

We also examined whether race information persisted in all spectral ranges and in the presence of highly degraded images. As shown in figure 1 , we tested the effect on model performance of adding a low-pass filter and a high-pass filter for various diameters in the MXR dataset, and show samples of the transformed images in figure 2 . The addition of a low-pass filter resulted in significantly degraded performance at around diameter ten, which corresponded to high levels of visual degradation. A high performance (up to diameter 100) in the absence of discernible anatomical features was maintained with the addition of a high-pass filter (ie, model performance was maintained despite extreme degradation of the image visually). Further experiments that used band-pass and notch filtering are reported in the appendix (pp 25–26 ), with the transformed images visualised also given in the appendix (pp 7–8)

The performance of a logistic regression model (AUC 0·65), a random forest classifier (0·64), and an XGBoost model (0·64) to classify race on the basis of age, sex, gender, disease, and body habitus performed much worse than the race classifiers trained on imaging data (AUC >0·95; appendix p 20 ). This finding suggests that the combination of these confounders did not significantly affect the imaging model's ability to classify race.

For patients in different age groups, there was no appreciable difference in racial identity recognition performance ( appendix p 15 ). Similarly, there was also no appreciable difference in racial identity recognition performance between male and female patients ( appendix p 17 ).

We found that deep learning models effectively predicted patient race even when the bone density information was removed for both MXR (AUC value for Black patients: 0·960 [CI 0·958–0·963]) and CXP (AUC value for Black patients: 0·945 [CI 0·94–0·949]) datasets. The average pixel thresholds for different tissues did not produce any usable signal to detect race (AUC 0·5). These findings suggest that race information was not localised within the brightest pixels within the image (eg, in the bone).

Moreover, the ability of models to predict race from the diagnostic labels alone was much lower than the chest x-ray image-based models, with AUC values between 0·54 and 0·61 for MXR, and between 0·52 and 0·57 for CXP ( appendix p 30 ). AUC values for race detection in the no finding class of 0·914 (95% CI 0·901–0·926) were obtained for Asian patients, 0·949 (0·945–0·953) for Black patients, and 0·941 (0·937–0·945) for White patients, versus 0·944 (0·938–0·950 [Asian patients]), 0·940 (0·937–0·942 [Black patients]), and 0·933 (0·930–0·936 [White patients]) for the entire dataset containing all disease classes, including the no finding class. These results suggest that high AUC values for racial identity recognition were not caused by disease labels.

The ability of logistic regression models to classify race on the basis of tissue density (AUC 0·54) and on the combination of age and tissue density (0·61) was far lower than the ability of the image models on the breast mammograms in the EM-Mammo dataset (0·81; appendix p 25 ). These findings suggest that breast density and age did not account for most image model performance when detecting race.

The ability of deep learning models that were trained on the CXP dataset to predict patient race from the body-mass index (BMI) alone was much lower than the image-based chest x-ray models (area under the receiver operating characteristics curve [AUC] 0·55), indicating that race detection is not due to obvious anatomic and phenotypic confounder variables. Similar results were observed across stratified BMI groups (0·92–0·99; appendix p 24 ).

Values reflect the area under the receiver operating characteristics curve for each model on the test set per slice and per study (by averaging the predictions across all slices). CXP=CheXpert dataset. DHA=Digital Hand Atlas. EM-CS=Emory Cervical Spine radiograph dataset. EM-CT=Emory Chest CT dataset. EM-Mammo=Emory Mammogram dataset. EMX=Emory CXR dataset. MXR=MIMIC-CXR dataset. NLST=National Lung Cancer Screening Trial dataset. RSPECT=RSNA Pulmonary Embolism CT dataset.

Discussion

In this modelling study, which used both private and public datasets, we found that deep learning models can accurately predict the self-reported race of patients from medical images alone. This finding is striking as this task is generally not understood to be possible for human experts. We also showed that the ability of deep models to predict race was generalised across different clinical environments, medical imaging modalities, and patient populations, suggesting that these models do not rely on local idiosyncratic differences in how imaging studies are conducted for patients with different racial identities. Beyond these findings, in two of the datasets (MXR and CXP) analysed, all patients were imaged in the same locations and with the same processes, presumably independently of race.

We also provide evidence that disease distribution and body habitus of patients in the CXP, MXR, and EMX datasets were not strongly predictive of racial group, implying that the deep learning models were not relying on these features alone. Although an aggregation of these and other features could be partially responsible for the ability of AI models to detect racial identity in medical images, we could not identify any specific image-based covariates that could explain the high recognition performance presented here.

34 Jabbour S

Fouhey D

Kazerooni E

Sjoding MW

Wiens J Deep learning applied to chest X-rays: exploiting and preventing shortcuts. Our findings conflict with data from Jabbour and colleagues' study,which measured the extent to which models learned potentially sensitive attributes (eg, age, race, and BMI) from an institutional dataset (the AHRF dataset) of 1296 patient chest x-rays. Their findings led to an AUC value of 0·66 (0·54–0·79). Possible explanations for this discrepant performance compared with our experiment could be due to the use of transfer learning in Jabbour and colleagues' study, in which the MXR and CXP datasets were used for initial training, and the final layers were fine-tuned on the AHRF dataset. This possible contamination in the dataset might have degraded performance due to label misalignment. We do not have access to the AHRF dataset for further external validation and Jabbour and colleagues did not extend their experiments to MXR and CXP datasets.

The results of the low-pass filter and high-pass filter experiments done in our study suggest that features relevant to the recognition of racial identity were present throughout the image frequency spectrum. Models trained on low-pass filtered images maintained high performance even for highly degraded images. More strikingly, models that were trained on high-pass filtered images maintained performance well beyond the point that the degraded images contained no recognisable structures; to the human coauthors and radiologists it was not clear that the image was an x-ray at all. Furthermore, experiments that were involved in patch-based training, slice-based error analysis, and saliency mapping were non-contributory: no specific regions of the images consistently informed race recognition decisions. Overall, we were unable to isolate specific image features that were responsible for the recognition of racial identity in medical images, either by spatial location, in the frequency domain, or that were caused by common anatomic and phenotype confounders associated with racial identity.

35 Ioannidis JPA

Powe NR

Yancy C Recalibrating the use of race in medical research. 36 Vyas DA

Eisenstein LG

Jones DS Hidden in plain sight—reconsidering the use of race correction in clinical algorithms. 37 Brown S

Davidovic J

Hasan A The algorithm audit: scoring the algorithms that score us. 38 Pierson E

Cutler DM

Leskovec J

Mullainathan S

Obermeyer Z An algorithmic approach to reducing unexplained pain disparities in underserved populations. Although the ability to accurately detect self-reported race from highly degraded x-ray images is not meaningful on its own, this ability is important in the larger sociotechnical context that AI models operate in for medical imaging. One commonly proposed method to mitigate the known disparity in AI model performance is through the selective removal of features that encode sensitive attributes to make AI models “colorblind”.Although this approach has already been criticised as being ineffective, or even harmful in some circumstances,our work suggests that such an approach could be impossible in medical imaging because racial identity information appears to be incredibly difficult to isolate. The ability to detect race was not mitigated by any reasonable reduction in resolution or by the addition of noise, nor by frequency spectrum filtering or patch-based masking. Even ignoring the question of whether these approaches were beneficial, it seems plausible that technical solutions along these lines are unlikely to succeed and that strategies designed to detect racial bias,paired with the intentional design of models to equalise racial outcomes,should be considered to be the default approach to optimise the safety and fairness of AI in this context. The regulatory environment in particular, while evolving, has not yet produced strong processes to guard against unexpected racial recognition by AI models; either to identify these capabilities in models or to mitigate the harms that might be caused.

24 Cooper R

David R The biological concept of race and its application to public health and epidemiology. There were several limitations to this work. Most importantly, we relied on self-reported race as the ground truth for our predictions. There has been extensive research into the association between self-reported race and genetic ancestry, which has shown that there is more genetic variation within races than between races, and that race is more a social construct than a biological construct.We note that in the context of racial discrimination and bias, the vector of harm is not genetic ancestry but the social and cultural construct that of racial identity, which we have defined as the combination of external perceptions and self-identification of race. Indeed, biased decisions are not informed by genetic ancestry information, which is not directly available to medical decision makers in almost any plausible scenario. As such, self-reported race should be considered a strong proxy for racial identity.

Our study was also limited by the availability of racial identity labels and the small cohorts of patients from many racial identity categories. As such, we focused on Asian, Black, and White patients, and excluded patient populations that were too small to adequately analyse (eg, Native American patients). Additionally, Hispanic patient populations were also excluded because of variations in how this population was recorded across datasets. Moreover, our experiments to exclude bone density involved brightness clipping at 60% and evaluating average body tissue pixels, with no methods to evaluate if there was residual bone tissue that remained on the images. Future work could look at isolating different signals before image reconstruction.

10 Seyyed-Kalantari L

Liu G

McDermott M

Chen IY

Ghassemi M CheXclusion: fairness gaps in deep chest X-ray classifiers. , 39 Seyyed-Kalantari L

Liu G

McDermott M

Chen I

Ghassemi M Medical imaging algorithms exacerbate biases in underdiagnosis. We finally note that this work did not establish new disparities in AI model performance by race. Our study was instead informed by previously published literature that has shown disparities in some of the tasks we investigated.The combination of reported disparities and the findings of this study suggest that the strong capacity of models to recognise race in medical images could lead to patient harm. In other words, AI models can not only predict the patients' race from their medical images, but appear to make use of this capability to produce different health outcomes for members of different racial groups.

To conclude, our study showed that medical AI systems can easily learn to recognise self-reported racial identity from medical images, and that this capability is extremely difficult to isolate. We found that patient racial identity was readily learnable from medical imaging data alone, and could be generalised to external environments and across multiple imaging modalities. We strongly recommend that all developers, regulators, and users who are involved in medical image analysis consider the use of deep learning models with extreme caution as such information could be misused to perpetuate or even worsen the well documented racial disparities that exist in medical practice. Our findings indicate that future AI medical imaging work should emphasise explicit model performance audits on the basis of racial identity, sex, and age, and that medical imaging datasets should include the self-reported race of patients when possible to allow for further investigation and research into the human-hidden but model-decipherable information related to racial identity that these images appear to contain.. In a new memo, National Labor Relations Board (NLRB) General Counsel Jennifer Abruzzo announced her intention to protect employees, to the greatest extent possible, from intrusive or abusive electronic monitoring and automated management practices through vigorously enforcing current law and by urging the Board to apply settled labor-law principles in a new framework.

The memo describes various technologies that are increasingly being used to closely monitor and manage employees. For instance, some employers record workers’ conversations and track their movements using wearable devices, cameras, radio-frequency identification badges and GPS tracking devices. And some employers monitor employees’ computers with keyloggers and software that takes screenshots, webcam photos, or audio recordings throughout the day. Employers may use this data to manage employee productivity, including disciplining employees who fall short of quotas, penalizing employees for taking leave, and providing individualized directives throughout the workday.

“It concerns me that employers could use these technologies to interfere with the exercise of Section 7 rights under the National Labor Relations Act by significantly impairing or negating employees’ ability to engage in protected activity—and to keep that activity confidential from their employer,” said General Counsel Abruzzo. “Thus, I plan to urge the Board, to the greatest extent possible, to apply the Act to protect employees from intrusive or abusive electronic monitoring and automated management practices that would have a tendency to interfere with Section 7 rights.”

The General Counsel will urge the Board to adopt a new framework for protecting employees from employers’ abuse of technology by holding that an employer has presumptively violated the Act where an employer’s surveillance and management practices, viewed as a whole, would tend to interfere with or prevent a reasonable employee from engaging in activity protected by the Act. If the employer’s business need outweighs employees’ Section 7 rights, unless the employer demonstrates that special circumstances require covert use of the technologies, she will urge the Board to require the employer to disclose to employees the technologies it uses to monitor and manage them, its reasons for doing so, and how it is using the information it obtains.

The memo also notes that the General Counsel is committed to an interagency approach to these issues, as numerous agencies across the federal government are working to prevent employers from violating federal law using electronic surveillance and algorithmic management technologies. The General Counsel has recently signed agreements with the Federal Trade Commission, the Department of Justice, and the Department of Labor which will facilitate information sharing and coordinated enforcement on these issues.. Kate Compton can talk to her Alexa voice assistant about three things: music genres, radio station call signs and the time of day. Stray from those safe topics, and the consequences could be dire.

“Hey Alexa, play ‘Despacito,’” Compton said into the ether from her home in Evanston, Ill., where she teaches computer science at Northwestern University. A nearby smart speaker launched into an explanation: The Luis Fonsi song was not available, but it could be if Compton paid for a subscription. Alexa proceeded to walk us through the pricing plans.. Credit: fotograzia/Getty Images

We've been warned for years to use strong passwords online, but not everyone heeds those warnings. Thanks to the increasing power of AI, even those who exercise reasonable caution may find their passwords insufficient. Cybersecurity firm Home Security Heroes fed millions of actual passwords into an artificial intelligence to see how long it would take to crack them. The answer is "not as long as you'd expect." The system was able to crack most passwords in less than a minute.

Home Security Heroes ran this test with the PassGAN password generator. Unlike most password generators, PassGAN uses a Generative Adversarial Network (GAN) to learn from real passwords to create new ones. A GAN in this context consists of two opposing neural networks, a generator and a discriminator. The generator network created fake data, and the discriminator is tasked with picking out real data in a sea of fakes. Over time, the generator and discriminator become better at what they do, making the overall model more effective.

PassGAN was provided with 15,680,000 common passwords from the RockYou data set in this test. For the unaware, RockYou was a social widget hacked years ago, revealing millions of unencrypted passwords. It has since been a commonly utilized data set in security research. The test excluded passwords longer than 18 characters and shorter than four. Home Security Heroes says the AI cracked 51% of those passwords in less than one minute. Some people always heed warnings to use secure passwords or password managers, and their data was harder to crack. PassGAN decoded 65% of passwords in an hour or less, hitting 71% in about a day. It took another month to boost that to 81%.

Credit: Home Security Heroes

So, the good news is the most robust passwords are still functionally impossible to crack, but a password you might think is strong could be a breeze for today's AI. The firm reports a 10-character password with numbers and lower-case letters would be cracked in the sub-1-hour group with 65% of all tested passwords. However, adding another layer of complexity with upper case letters or special characters boosts the cracking time to an estimated five years.

If you want to ensure your passwords are in the currently uncrackable group, Home Security Heroes recommends ensuring it's at least 15 characters long. You should use a combination of letters (upper and lower case), numbers, and special characters. The firm also recommends changing passwords frequently. If you swap important passwords every few months, there won't be time for an AI to crack it before you're on to something else. The cat-and-mouse game will never end, though. Eventually, AI may even be able to figure out these more complex passwords, which is why some major tech heavyweights are working to do away with passwords entirely.. Read our methodology.

Every year, thousands of high school students get ready for the SAT by using The Princeton Review’s test preparation services.

But few, if any, realize that the prices for The Princeton Review’s online SAT tutoring packages vary substantially depending on where customers live. If they type some ZIP codes into the company’s website, they are offered The Princeton Review’s Premier course for as little as $6,600. For other ZIP codes, the same course costs as much as $8,400.

One unexpected effect of the company’s geographic approach to pricing is that Asians are almost twice as likely to be offered a higher price than non-Asians, an analysis by ProPublica shows.

Asians More Likely To Be Among Those Charged Higher Prices By The Princeton Review Asians make up 4.9 percent of the U.S. population overall. But they account for more than 8 percent of the population in areas where The Princeton Review charges higher prices for its SAT prep packages. 8.49% 2.89% 0% 10% % Asian population, ZIP codes charged higher prices % Asian population, ZIP codes charged higher prices % Asian population, ZIP codes charged lower prices % Asian population, ZIP codes charged lower prices Look Up Your ZIP Code to See the Price Look Up ZIP Code Zip Private Master Premier Product Private Master Premier Low $2,760 $3,960 $6,600 Medium $3,000 $4,320 $7,200 High $3,240 $5,160 $7,800 High $8,400 The ZIP code you entered is either invalid or doesnt exist in our database

The gap remains even for Asians in lower income neighborhoods. Consider a ZIP code in Flushing, a neighborhood in Queens, New York. Asians make up 70.5 percent of the population in this ZIP code. According to the U.S. Census, the median household income in the ZIP code, $41,884, is lower than most, yet The Princeton Review customers there are quoted the highest price.

The Princeton Review said in a statement that its pricing is based on the “costs of running our business and the competitive attributes of the given market,” and that the company charges the same price everywhere in New York City. Although the test prep service markets its service as “24-hr Online Tutoring,” the company says the tutoring is done in one-on-one sessions in person or online and that the tutors typically live in the same areas as their students.

“The areas that experience higher prices will also have a disproportionately higher population of members of the financial services industry, people who tend to vote Democratic, journalists and any other group that is more heavily concentrated in areas like New York City,” The Princeton Review’s statement said.

These types of price differences are not illegal, and the consequences are not intentional, but researchers say they are likely to become more common in the age of services like Uber, which set prices by computer algorithms. The Princeton Review says its prices are simply determined by geographic region.

Last year, a White House report on “Big Data” cautioned that the “algorithmic decisions raise the specter of ‘redlining’ in the digital economy – the potential to discriminate against the most vulnerable classes of our society under the guise of neutral algorithms.”

In 2012, the Wall Street Journal reported that the online office retailer Staples was varying prices by ZIP code. Staples appeared to be calculating prices based on the user’s distance from a rival store, but the inadvertent effect was that people in lower-income ZIP codes saw the higher prices.

In 2014, researchers at Northeastern University found that top Web sites, such as Home Depot, Orbitz and Travelocity, were steering some users toward more expensive products. And this year, another study found that users who were identified by Google as female received fewer ads for a high-paying job.

Offline, the practice of offering different prices for the same product in different places is fairly common – gasoline or a gallon of milk can be priced differently just a few blocks apart. But as long as there is no intent to racially discriminate, it is generally legal, says Andrew Selbst, an attorney who co-authored a paper on the biases that can be inherent in Big Data.

“If you are open for business, you can’t discriminate against certain protected classes,” Selbst said.

Unintentional racial discrimination is illegal in housing and employment under the legal doctrine known as “disparate impact,” which prohibits inadvertent actions that hurt people in a protected class.

But the disparate impact doctrine does not apply to the online world, where it’s often difficult to determine how and why different prices are being offered.

Earlier this year, Harvard undergraduate Christian Haigh stumbled on The Princeton Review’s variable prices doing research for a class he was taking called “Data Science to Save the World.”

Haigh had been looking for price differences in hotel rooms if he booked from different locations around the world. But he wasn’t finding much. So he looked for websites that required entering a ZIP code.

“We thought maybe if you have to put in the ZIP code, they were trying to discriminate,” Haigh said. Today, Haigh and three fellow students are publishing their findings that The Princeton Review’s higher prices correlate to areas with higher income.

ProPublica reviewed the code that one of Haigh’s fellow students posted on a public web site and collected its own data in July, and again on Monday. The data showed that The Princeton Review offered four different prices for the same “Premier Level” online tutoring package.

Many of the prices are regional. For instance, the entire New York City area, including Long Island, receives the highest possible price, $8,400. Much of California, except San Diego, is offered the second-highest price, $7,200, while ZIP codes in San Diego are charged the lowest price.

Because the pricing regions are large, sometimes spanning multiple states, they are different than the personalized tech algorithms used by some web sites, which make real-time decisions about which advertisements to show to a particular visitor.

ProPublica tested whether The Princeton Review prices were tied to different characteristics of each ZIP code, including income, race and education level. When it came to getting the highest prices, living in a ZIP code with a high median income or a large Asian population seemed to make the greatest difference.

The analysis showed that higher income areas are twice as likely to receive higher prices than the general population. For example, wealthy suburbs of Washington D.C. are charged higher prices. But that isn’t always the case: Residents of affluent neighborhoods in Dallas are charged the lowest price, $6,600.

Customers in areas with a high density of Asian residents were 1.8 times as likely to be offered higher prices, regardless of income. For instance, residents of the gritty industrial city of Westminster, California, which is half Asian with a median income below most, were charged the second-highest price for the Premier tutoring service.

The Princeton Review said it would be a mistake to call its pricing practices discrimination. “To equate the incidental differences in impact that occur from this type of geographic based pricing that pervades all American commerce with discrimination misconstrues both the literal, legal and moral meaning of the word,” the company said in its statement.

The company said the prices of its online tutoring services are based on the prices of local tutors, which vary “just as virtually every good or service does, be it gasoline, rent or eggs.”

Even if the price differences were unintentional, the Harvard students said they found them disturbing. Haigh, the student who discovered the variations, is an economics major and said he’s not generally against price differences unless particular demographic groups are affected.

“It’s something that makes a very small impact on one individual’s life but can make a big impact to large groups,” Haigh said.

Lauren Kirchner contributed to this report.. You were excited for the date: dinner and a movie. Your date picked a restaurant — “It got five stars on Yelp!” — but the movie was up to you. So you checked out what was playing and bought the tickets on Fandango’s website. You decided to check out “Fantastic Four,” and even though you hadn’t heard great things, Fandango users thought it was good! Over 7,000 people had reviewed it, and it had an average of 3 out of 5 stars. This is going to be a decent movie.

It is not a decent movie.

Online movie ratings have become serious business. Hollywood generates something on the order of $10 billion annually at the U.S. box office, and online ratings aggregators may hold increasing sway over where that money goes. Sites like Rotten Tomatoes that aggregate movie reviews into one overall rating are being blamed for poor opening weekends. A single movie critic can’t make or break a film anymore, but maybe thousands of critics, professional and amateur together, can.

Several sites have built popular rating systems: Rotten Tomatoes, Metacritic and IMDb each have their own way of aggregating film reviews. And while the sites have different criteria for picking and combining reviews, they have all built systems with similar values: They use the full continuum of their ratings scale, try to maintain consistency, and attempt to limit deliberate interference in their ratings.

These rating systems aren’t perfect, but they’re sound enough to be useful.

All that cannot be said of Fandango, a NBCUniversal subsidiary that uses a five-star rating system in which almost no movie gets fewer than three stars, according to a FiveThirtyEight analysis. What’s more, as I’m writing this, scores on Fandango.com are skewed even higher because of the weird way Fandango aggregates its users’ reviews. And while other sites that gather user reviews are often tangentially connected to the media industry, Fandango has an immediate interest in your desire to see a movie: The company sells tickets directly to consumers.

What started all this? A couple of months ago, a colleague noticed that a bad film had received a decent rating on Fandango and asked me to look into it. When I pulled the data for 510 films on Fandango.com that had tickets on sale this year (you can check out all the data yourself on GitHub), something looked off right away: Of the 437 films with at least one review, 98 percent had a 3-star rating or higher and 75 percent had a 4-star rating or higher.

It seemed nearly impossible for a movie to fail by Fandango’s standards.

When I focused on movies that had 30 or more user reviews, none of the 209 films had below a 3-star rating. Seventy-eight percent had a rating of 4 stars or higher.

But perhaps the movies were just that good? Maybe we really do live in a society that rates “Mortdecai” as a 3.5-star film?

We don’t. The other review sites weren’t nearly as charitable. For the 209 films, I pulled IMDb’s user rating, Metacritic’s aggregate critic rating, Metacritic’s user score, the Rotten Tomatoes Tomatometer (critic) score and the Rotten Tomatoes user score. I then normalized these to the five-star rating scale that Fandango uses and rounded it to the nearest half-star.

The ratings from IMDb, Metacritic and Rotten Tomatoes were typically in the same ballpark, which makes this finding unsurprising: Fandango’s star rating was higher than the IMDb rating 79 percent of the time, the Metacritic aggregate critic score 77 percent of the time, the Metacritic user score 86 percent of the time, the Rotten Tomatoes critic score 62 percent of the time, and the Rotten Tomatoes user score 74 percent of the time.

There are all sorts of reasons that scores might be higher on a site like Fandango compared with competitors; after all, if you ask people about a movie after they’ve paid $15 for it and devoted a couple of hours of their life to it, maybe they’ll have a more favorable opinion of the work. Maybe the profoundly rightward shift in Fandango’s bell curve is just a moviegoer’s version of Stockholm syndrome.

Still, this is a deeply flawed rating system. It’s not clear why so few movies earn less than 3 stars, and Fandango didn’t offer any explanation. “As we have not analyzed other sites’ user ratings systems and we do not have access to their customers’ profile and engagement behavior, it is unfair for us to speculate how our ratings may or may not differ from theirs,” Fandango said in an emailed statement.

So for all intents and purposes, Fandango is using a 3 to 5 star scale. And that’s not the only thing wrong with its ratings. I found an issue with the methodology Fandango uses to average user ratings on its website: Fandango never rounds the average down.

On a given film’s page on Fandango’s website, its aggregate user rating is displayed in one spot: the stars next to the film’s poster, above the area that provides showtimes. The stars are expressed on a five-point scale at half-star increments. Beneath the star ratings, Fandango lists the number of reviews the film has received.

But when you pull the HTML source of a page on Fandango’s website, there’s more information. Take “Ted 2.” When I pulled data for it on Monday, the film had 4.5 stars from 6,568 reviews.

You can see that information on the HTML backend of the page; the “AggregateRating” schema says “Ted 2” had 6,568 ratings, a maximum score of five stars and a minimum score of 0 stars. That all makes sense.

Here’s the thing, though: According to the code for the page, “Ted 2” had a “ratingValue” of only 4.1 stars.

In a normal rounding system, a site would round to the nearest half-star — up or down. In the case of “Ted 2,” then, we’d expect the rating to be rounded down to 4 stars. But Fandango rounded the “ratingValue” up. I pulled the number of stars listed on the page of each film in our sample of 437 (with at least one user review), as well as the ratingValue listed on the page’s source. And I found that Fandango doesn’t round a rating down when we’d mathematically expect that (it appears Fandango does round correctly on its mobile app — more on this in a moment).

There are even more extreme cases than that of “Ted 2.” Take “Avengers: Age of Ultron.” When I pulled data for that on Monday, the film had 5 stars from 15,116 reviews.

But according to the code for the page, “Avengers: Age of Ultron” had a “ratingValue” of only 4.5 stars, meaning that it gained a full half-star from rounding.

So what kind of effect did this have across the board?

Here’s a breakdown of how Fandango.com’s system handled the rounding for each of the 437 films in our sample:

On 109 occasions, about a quarter of the time, the ratingValue was the same as the number of stars presented. This means that a movie’s average rating was already at a half-star and no rounding occurred.

On 148 occasions, about 34 percent of the time, Fandango rounded as you would expect — rounding up 0.1 or 0.2 stars, like one would round a 3.9 or 3.8 to a 4.

On 142 occasions (including for “Ted 2”), 32 percent of the time, Fandango added 0.3 or 0.4 stars to the rating, when one would normally round down, juicing up the score by a half-star. Think of it this way: That’s the equivalent of saying your SAT score was about 100 to 120 points higher than it actually was.

On 37 occasions, about 8 percent of the time, Fandango’s rounding system added a half-star to the film’s rating. It’s not clear why this happened — why “Avengers: Age of Ultron” would have its 4.5 ratingValue rounded up to 5 stars — but it happened about 1 in 12 times. It may be that Fandango is rounding at the second decimal place — e.g., 4.51 to 5. But again, it’s not clear; the “ratingValue” in the HTML code is only shown to the first decimal place.

On one occasion, a film was rounded up by an entire star, from a 4 to a 5.

The cases above include movies with very few reviews; the average rating for these movies is more likely to fall on a whole or half star, which doesn’t require rounding. Returning to the 209 films that had 30 or more user reviews on Fandango.com, the average movie gained 0.25 stars from this rounding. Using a normal system, that average should be close to 0.

When I initially asked Fandango about its rounding practice, public relations coordinator Alison Ver Meulen said this in an email: “We always display stars rounded up to the nearest half star. So for example 3.6 stars would show up as 4 on our site.”

However, after further back and forth, the company described the rounding disparity — by which, for example, 4.1 is rounded to 4.5 — as a bug in the system rather than a general practice. “There appears to be a software glitch on our site with the rounding logic of our five star rating system, as it rounds up to the next highest half star instead of the nearest half star,” the company said in an emailed statement.

Fandango told us that it plans to fix the rounding algorithm on its website “as soon as possible.”

Fandango also said that “the rounding logic is accurately displayed on our mobile apps.” And that appears to be true; I checked several films that had raised red flags on the company’s website and found that their scores were accurately represented on Fandango’s iOS app. Still, the star-based scores on the app skew just as high as on the website.

Fandango.com’s rounding methodology, even if it was just an innocent bug, is a good example of why you should be skeptical of online movie ratings, especially from companies selling you tickets. If this kind of bug can survive unnoticed on the website of a major American ticket seller for who knows how long, there’s no reason a similar bug — or another issue we’re missing — couldn’t be on any other site we’re using to figure out if something is good or not.

And the Federal Trade Commission, which protects consumers from deceptive and anti-competitive business practices, pays attention to the use of ratings and endorsements to promote products. “User ratings would be material to consumers, so they have to be truthful and non-misleading,” said Mary Engle, who directs the FTC’s division related to advertising practices. Engle couldn’t comment on any specific company not already under investigation, but said there is an expectation that companies that present user ratings do so accurately. “We know that nowadays user reviews are very important, whether it’s a movie, a vacation purchase, electronics, whatever,” she said. “You go online to see what other consumers are saying. And so we’re looking at issues where those reviews aren’t what they purport to be.”

What’s The Point: Walt Hickey on the world of online reviews and the wisdom of the crowd.

Subscribe to all the FiveThirtyEight podcasts here.

All of this matters more to movie studios now than it did in the past.

“If you look over the last 20 years, the release strategy used to be much more based around a movie playing for a long time, perhaps releasing regionally and building word-of-mouth around the country,” said box office analyst Bruce Nash, who operates The-Numbers.com, which tracks box office data. “Today, it’s much more focused on getting into theaters opening weekend and hitting as hard as you can with the opening. For that, I think the reviews can have more of an effect.”

Looking at it this way, the idea of a studio inventing a critic to promote its films, as one was accused of in the early 2000s, starts to seem reasonable.

Fandango might be an extreme case, but its problems are indicative of the limitations of online movie ratings generally. Freelance film critic Ben Kenigsberg said, “They’re a useful shorthand or heuristic for readers, but I think they’re kind of a tongue-in-cheek way of looking at movies, and I think they should be taken as such.”

When it comes to critical aggregators like Metacritic and Rotten Tomatoes, the act of boiling down a nation’s worth of critics to a number does have an inaccurate air of finality to it. “I like both sites,” said Todd VanDerWerff, culture editor at Vox.com. “But I feel like they have created a sense that there’s an answer to whether a movie is good or bad when really that’s a very personal question.”

He added: “Because it looks like math, we have it in our head that it’s somehow objectively true, but in reality, it’s all based on subjective experience.”. . Last month, a young woman went to work at Sarzamineh Shadi, or Land of Happiness, an indoor amusement park east of Iran’s capital, Tehran. After a photo of her without a hijab circulated on social media, the amusement park was closed, according to multiple accounts in Iranian media. Prosecutors in Tehran have reportedly opened an investigation.

Shuttering a business to force compliance with Iran’s strict laws for women’s dress is a familiar tactic to Shaparak Shajarizadeh. She stopped wearing a hijab in 2017 because she views it as a symbol of government suppression, and recalls restaurant owners, fearful of authorities, pressuring her to cover her head.

But Shajarizadeh, who fled to Canada in 2018 after three arrests for flouting hijab law, worries that women like the amusement park worker may now be targeted with face recognition algorithms as well as by conventional police work.

After Iranian lawmakers suggested last year that face recognition should be used to police hijab law, the head of an Iranian government agency that enforces morality law said in a September interview that the technology would be used “to identify inappropriate and unusual movements,” including “failure to observe hijab laws.” Individuals could be identified by checking faces against a national identity database to levy fines and make arrests, he said.

Two weeks later, a 22-year-old Kurdish woman named Jina Mahsa Amini died after being taken into custody by Iran’s morality police for not wearing a hijab tightly enough. Her death sparked historic protests against women's dress rules, resulting in an estimated 19,000 arrests and more than 500 deaths. Shajarizadeh and others monitoring the ongoing outcry have noticed that some people involved in the protests are confronted by police days after an alleged incident—including women cited for not wearing a hijab. “Many people haven't been arrested in the streets,” she says. “They were arrested at their homes one or two days later.”

Although there are other ways women could have been identified, Shajarizadeh and others fear that the pattern indicates face recognition is already in use—perhaps the first known instance of a government using face recognition to impose dress law on women based on religious belief.

Mahsa Alimardani, who researches freedom of expression in Iran at the University of Oxford, has recently heard reports of women in Iran receiving citations in the mail for hijab law violations despite not having had an interaction with a law enforcement officer. Iran’s government has spent years building a digital surveillance apparatus, Alimardani says. The country’s national identity database, built in 2015, includes biometric data like face scans and is used for national ID cards and to identify people considered dissidents by authorities.

Decades ago, Iranian law required women to take off headscarves in line with modernization plans, with police sometimes forcing women to do so. But hijab wearing became compulsory in 1979 when the country became a theocracy.. Samsung Electronics Co. is banning employee use of popular generative AI tools like ChatGPT after discovering staff uploaded sensitive code to the platform, dealing a setback to the spread of such technology in the workplace.

The Suwon, South Korea-based company notified staff at one of its biggest divisions on Monday about the new policy via a memo reviewed by Bloomberg News. The company is concerned that data transmitted to such artificial intelligence platforms including Google Bard and Bing is stored on external servers, making it difficult to retrieve and delete, and could end up being disclosed to other users, according to the document.. When Marc set out to clone the voice of Ariana Grande in January, he didn’t know it would infuriate hordes of his fellow Grande fans.

Marc, who asked to remain anonymous to shield him from further online attacks, wasn’t looking to incite a stan riot. He just wanted to experiment with DiffSVC, an open-source software developed by researchers at The Chinese University of Hong Kong that can replicate real human voices. By training the model with source material from Grande’s song list, Marc was able to make cover songs in her same style and tone.

Marc started out by collecting data of the singer from the internet, grabbing recordings of her voice from a capella tracks, some directly uploaded to YouTube by Grande and others placed there by her fans. He also pulled from high-quality Dolby Atmos tracks, which are designed to make sound seem more three-dimensional. He ended up with 33 minutes of audio, which he and a friend chopped up into 14-second audio files and threw into DiffSVC.. Over the weekend, the Twitter account for a London police department shared that officers had arrested suspects for possession of “a large amount of cannabis,” including a picture of what appeared to be some truly foul weed. The replies on the post quickly filled up with jokes about the cops confiscating hedge trimmings, loose leaf chamomile tea, and so on — but one tweet included a fake audio clip of President Joe Biden explaining that he actually prefers low-quality pot.

“I’m from Scranton,” the simulated Biden said. “What I’m smoking is dirt. So let’s get that straight, Jack. Pure brick. Ass. Okay?”

Of course, the real Biden has said no such thing (at least not publicly). But lately, virtual Biden has been spouting off — rather fluently — on a number of topics about which the president wouldn’t and/or shouldn’t have strong opinions. It’s all thanks to the advent of powerful AI tools for voice cloning, and one in particular that launched last month: Voice Lab, a platform created by the AI startup ElevenLabs.

For just $5, you can access ElevenLabs’ software and start creating your own audio. All it needs to realistically mimic an actual person — and then read any text in their voice — is a short sample of that person’s speech, which it uses to create a “clone.” Recordings of Biden’s voice are abundant, so it’s easy to build your AI copy of him. Then it’s a matter of typing out whatever you want him to say. Perhaps you’ve imagined a gamer Biden who rants about “lame-ass bitch boys” playing as “no-skill heroes” on Overwatch? Now you can bring that character, “Joe Rizzler,” to life.

While clips like this are clearly artificial and seemingly innocuous, the potential abuse of technology like this is cause for concern. ElevenLabs itself, shortly after Voice Lab entered its beta phase, tweeted that they’d seen “an increasing number of voice cloning misuse cases” and would take steps toward “implementing additional safeguards.” The company stopped short of describing specific misuse, but around the same time, 4chan trolls were using the tool to generate fake audio of various public figures spewing hate speech. One snippet sounded like actor Emma Watson reading from Hitler’s Mein Kampf. And earlier this month, in a similar deepfake combined with doctored video footage, Biden appeared to give a speech denouncing transgender people. Needless to say, that never happened. Editor’s picks

While we can trace back any generated audio back to the user, we’d like to address this by implementing additional safeguards. — ElevenLabs (@elevenlabsio) January 30, 2023

ElevenLabs did not respond to a request for comment regarding what new safety measures they’ve added or plan to apply to Voice Lab. Nor did they address whether they view the non-offensive AI Biden speeches — like one shared by Twitter user @MNateShyamalan, in which he gets philosophical about the sitcom Young Sheldon — as examples of “misuse.” The eventual impacts of voice cloning remain to be seen; improvements to the technology will no doubt complicate efforts to separate genuine recordings from bogus material designed to push an agenda or false narrative. Right now, though, shitposters are reveling in the apolitical words they can put in the president’s mouth.

got too high last night and made this for some reason. idk pic.twitter.com/HZZDeIazz7 — soul nate (@MNateShyamalan) February 18, 2023

Why is it so amusing, or enticing, or even comforting to hear a Biden bot spitting references to drugs and video games? Well, for starters, it’s not exactly presidential. Then there’s the fact that the Biden himself is 80 years old — youthful vernacular is outright absurd in his grandfatherly cadence. For Zach Silberberg, a digital content producer whose AI Biden speeches have made a splash on Twitter (one has him discussing the Matt Damon film We Bought a Zoo, and in another he complains of being “inverted” like the characters in Tenet), the element of anarchy is essential.

Related

“This version of the president is unpredictable,” Silberberg says. “We have no idea what the hell he’s going to say. It could be silly, or vulgar, or rambling, or anything at all. He’s not slow and boring and more of the same neoconservative version of a Democrat we’ve been conditioned to accept as the most liberal version of a president we can get.”

And ultimately, notes Silberberg — who has also made comedic fake dialogues between Joe Rogan and Ben Shapiro using ElevenLabs’ software — the Biden riffs are about getting one over on the most powerful man in the world. “It’s a fantasy for a president who we can laugh at, and also torture. Who doesn’t want to see Joe Biden get stuck in the house from a horror movie? It’s just plain funny.” Silberberg clarified that he is “anti-AI” and that “AI is bad.” In that sense, his videos can be construed as warnings.

joe biden has really messed up big time pic.twitter.com/pAGHZwDcyi — zach silberberg (@zachsilberberg) February 19, 2023. Surveillance cameras purchased with federal crime-fighting grants are being used to punish and evict public housing residents, sometimes for minor rule violations, a Washington Post investigation found

STEUBENVILLE, Ohio — When they installed the new surveillance system, local officials promised it would help tamp down a gang war menacing this forgotten steel town. But residents of Steubenville public housing soon learned the cameras were pointed at them. One man was filmed spitting in a hallway. A woman was recorded removing a cart from a communal laundry room. Footage in both cases was presented to a judge to help evict the residents in court.

Advertisement

After the cameras caught her lending her key fob to an unauthorized guest, Melanie Otis, 52, also was threatened with eviction. Otis, who has vision loss, was allowed to stay after she explained the visitor was a friend bringing her groceries.

In public housing facilities across America, local officials are installing a new generation of powerful and pervasive surveillance systems, imposing an outsize level of scrutiny on some of the nation’s poorest citizens. Housing agencies have been purchasing the tools — some equipped with facial recognition and other artificial intelligence capabilities — with no guidance or limits on their use, though the risks are poorly understood and little evidence exists that they make communities safer.

In rural Scott County, Va., cameras equipped with facial recognition scan everyone who walks past them, looking for people barred from public housing. In New Bedford, Mass., software is used to search hours of recordings to find any movement near the doorways of residents suspected of violating overnight guest rules. And in tiny Rolette, N.D., public housing officials have installed 107 cameras to watch up to 100 residents — a number of cameras per capita approaching that found in New York’s Rikers Island jail complex.

Public housing residents are subject to outsize surveillance Number of people per security camera Public housing New York City Omaha Milwaukee Rolette, N.D. 1 camera per 19 residents 1 per 10 residents 1 per 3 residents 1.1 per resident Wrigley Field Los Angeles Airport 1 per 23 travellers 1 per 38 visitors Rikers Island Louvre Museum Red Hawk Casino 1 per 20 visitors 1 per 5 visitors 2 per inmate Source: Post data collection from institutions and security vendors ALYSSA FOWERS/THE WASHINGTON POST Public housing residents are subject to outsize surveillance Number of people per security camera Public housing New York City Omaha Milwaukee Rolette, N.D. 1 camera per 19 residents 1 per 10 residents 1 per 3 residents 1.1 per resident Wrigley Field Los Angeles Airport 1 per 38 visitors 1 per 23 travellers Louvre Museum Red Hawk Casino Rikers Island 1 per 20 visitors 1 per 5 visitors 2 per inmate Source: Post data collection from institutions and security vendors ALYSSA FOWERS/THE WASHINGTON POST Public housing residents are subject to outsize surveillance Number of people per security camera Public housing New York City Omaha Milwaukee Rolette, N.D. 1 camera per 19 residents 1 per 10 residents 1 per 3 residents 1.1 per resident Wrigley Field Los Angeles Airport 1 per 23 travellers 1 per 38 visitors Louvre Museum Red Hawk Casino Rikers Island 1 per 20 visitors 1 per 5 visitors 2 per inmate Source: Post data collection from institutions and security vendors ALYSSA FOWERS/THE WASHINGTON POST Public housing residents are subject to outsize surveillance Number of people per security camera Public housing New York City Omaha Milwaukee Rolette, N.D. 1 camera per 19 residents 1 per 10 residents 1 per 3 residents 1.1 per resident Wrigley Field Los Angeles Airport 1 per 38 visitors 1 per 23 travelers Rikers Island Louvre Museum Red Hawk Casino 1 per 20 visitors 1 per 5 visitors 2 per inmate Source: Post data collection from institutions and security vendors ALYSSA FOWERS/THE WASHINGTON POST

The U.S. Department of Housing and Urban Development has helped facilitate the purchase of cameras through federal crime-fighting grants. Those grants are meant to keep residents safer, and housing agencies say they do. But the cameras are also being used to generate evidence to punish and evict public housing residents, sometimes for minor violations of housing rules, according to interviews with residents and legal aid attorneys, a review of court records, and interviews and correspondence with administrators at more than 60 public housing agencies that received the grants in 27 states.

No data is available on how often the cameras are used for this purpose. But the previously unreported practice highlights how efforts to make public housing safer are subjecting many of the 1.6 million Americans who live there — overwhelmingly people of color — to round-the-clock surveillance. If evicted, former tenants can face difficulties finding housing and employment for the rest of their lives.

In an email, HUD spokeswoman Christina Wilkes said the agency never intended its safety and security grants to be used to punish residents for lease violations. But she added that such usage “is not a violation of the grant terms.”

Melody McClurg, executive director of the Jefferson Metropolitan Housing Authority in Steubenville, said tenants are responsible for upholding the terms of their lease. The cameras, which are installed only in public places, are just one way of enforcing the rules, she said, adding that other factors contributed to the JMHA’s concerns about Otis, the man who spit in the hallway and the woman who removed a laundry cart.

“People choose to get evicted by their actions,” McClurg said.

As cameras have gotten smarter, their use in public housing is becoming a flash point in the national debate over facial recognition. States including Alabama, Colorado and Virginia have passed laws limiting the use of facial recognition by law enforcement, recognizing that these tools have been shown to produce false matches — particularly when scanning women and people of color.

Last month, after The Washington Post presented HUD with evidence of the growing use of sophisticated surveillance tools by local housing authorities, the agency said it would no longer permit future recipients to spend security grants on facial recognition. These tools “are not foolproof,” and their mistakes can adversely impact public housing residents, Dominique Blom, HUD general deputy assistant secretary of public and Indian housing, said in an interview.

“This sends a signal to the housing community that this is the type of technology that the department is cautioning against,” Blom said.

Advertisement

An arm of the police

Public housing authorities say cameras help solve crimes and make communities safer, though few of the agencies contacted by The Post could provide data showing these results. Officials in Richmond said their new cameras helped solve 10 of the 14 homicides in public housing last year.

In Steubenville, John Stasiulewicz, a former police detective who goes by the nickname “Stosh,” is in charge of security for the public housing authority. He said the agency’s cameras have become essential to local police, who rely on him to capture footage of drug busts that have led to the arrest and eviction of dealers and users.

“I am one of the arms of the police department,” Stasiulewicz, 62, said. “I give them information and they act on it.”

Steubenville police have installed about 100 surveillance cameras across the rest of the town, a city official said, while Stasiulewicz said he monitors 161 cameras in public housing. This means public housing residents — who are nearly three times more likely to be Black than other Steubenville residents, census records show — are about 25 times more likely to have their daily lives observed by government-controlled cameras.

Stasiulewicz said he routinely uses the cameras to enforce housing rules, such as to investigate tenants who may be letting in unauthorized guests.

In Steubenville and elsewhere, public housing residents interviewed by The Post said they do worry about the safety of their neighborhoods. Some said they are happy to see more cameras outside their doors. However, many also complain that the surveillance systems “don’t work,” because they see scant evidence the devices help stop or solve crimes.

And some say cameras are being used to punish residents who pose no danger.

Tania Acabou was served an eviction notice in 2021 after the housing authority in New Bedford, Mass., used cameras to investigate her over several months. A single mother of two, Acabou had relied on her ex-husband to help take care of their kids while she worked days as a bus driver and attended night school to train for a career as a lab technician. The housing authority believed her ex was living at the house without contributing rent, court records show, violating a policy that restricts overnight visitors to 21 nights a year.

The housing authority used software to place a digital marker next to Acabou’s front door and told the system to retrieve every moment when motion was detected near the marker, documents and interviews show. When her property manager suspected that Acabou’s ex was leaving through the back door, she set up a portable camera in the backyard pointed directly at that door, according to housing authority officials and a review of the surveillance video obtained and verified by The Post.

“It got to the point where it was like harassment,” Acabou, 33, said. “They really made my life hell.”

Motorola's Avigilon software helps housing officials search through hours of video for each instance of movement in a specific area. (Video: TWP)

Sam Ackah, security director for the New Bedford Housing Authority, said the agency tries to avoid evictions by working out agreements with residents to pay their rent or come into compliance with housing rules. He said the NBHA tried to work out an accommodation with Acabou, but she refused to acknowledge that her ex was living at the apartment. Acabou said she presented evidence to the agency showing he was living elsewhere.

Ackah said his team typically conducts video investigations into 10 to 15 people at any time, working to improve life for residents who follow housing rules by policing their neighbors who don’t. The cameras — which were not purchased through a grant but through the agency’s normal annual budget — help catch people living in public housing without paying rent and help identify unregistered guests, who have not been vetted for past offenses, he said.

Unlike the doorbell cameras voluntarily installed by residents of more affluent communities, surveillance cameras typically are installed in public housing without the consent or control of the residents. In D.C., one woman and her son were arrested in 2018 after trying to stop housing officials from entering her apartment to install a camera power box in her bedroom. In a lawsuit filed against the city, the woman claims a security officer who arrested her said she “did not have any rights as a public housing resident and that she could not stop the worker from installing the cameras.”

Rachel Molly Joseph, chief operating officer for the DC Housing Authority, declined to comment on the lawsuit, which is pending, but said none of the cameras face the interior of residents’ homes.

In Charlottesville, housing officials met with residents before installing a new surveillance system and learned they didn’t like the idea of officials “just sitting there watching the cameras to see if tenants are breaking their lease,” John Sales, the agency’s executive director, said in an interview. Unlike many housing authorities contacted by The Post, officials in Charlottesville pledged not to use the cameras to enforce lease agreements.

Nationally, evictions from public housing have surged since late 2021, when the federal government lifted an eviction moratorium put in place to protect renters during the pandemic, according to Peter Hepburn, associate director of Princeton University’s Eviction Lab. In the 10 states and 34 cities his group tracks, there were at least 5,576 evictions by public housing authorities in 2022, about twice as many as the previous year. Public housing evictions grew at a faster rate in that period than evictions overall, Hepburn said.

There is no way to measure how much of the increase may be due to more sophisticated surveillance. But attorneys who defend tenants in eviction cases said that in some communities with newer cameras, they have seen an uptick in cases referencing video footage as evidence for kicking people out.

Gavin Bates, a legal aid attorney in New Bedford, said the local housing authority’s surveillance system now “regularly appears” in cases where his clients are being evicted. The authority uses this system “to great effect in moving people out,” he added.

“Quite often when there is camera footage of an event, the event does show a rules violation of some kind,” Bates said. “But there are also a lot of unrepresented folks who just believe that there is camera footage when they are told, the footage is never produced, and they do not know their rights and often make bad decisions as a result.”

Ackah said the New Bedford Housing Authority provides video evidence to the court in cases where it exists. “If we do not have the evidence to prove a lease violation, then we would not be able to evict,” he said.

Advertisement

Surveillance by algorithm

Camera manufacturers claim that artificial intelligence has made their devices capable of doing much more on their own, from identifying faces and license plates to spotting people loitering. As a result, residents in a growing number of cities are being watched by computer algorithms programmed to help authorities make split-second decisions about who and what is suspicious.

Of 41 housing authorities that told The Post they had bought new cameras using a HUD grant in recent years, 11 said their systems were equipped with facial recognition tools. Six said they planned to use this capability, either to assist police in criminal investigations, to control tenant access to buildings or to actively search for people banned from their properties.

Housing officials in Scott County, Va., said they use cameras to spot former tenants and guests with some previous violation involving drugs or violence. After getting a $250,000 HUD grant and installing 80 cameras — about 1 for every 2 public housing residents — officials in Scott County uploaded photos of all the people they had barred from entering public housing. (Such ban lists, which are widely used in public housing, are controversial because authorities often target people who have never been convicted of a crime.)

Since the cameras began to actively look for banned individuals, they have made only one match, according to Sue Smith, executive director for the Scott County Redevelopment & Housing Authority. It correctly identified a man banned from the property because of a past domestic violence incident involving a resident, an incident that did not result in criminal charges, Smith said. As soon as the match was made, the software sent an alert to housing staff, who asked the man to leave and alerted the county sheriff.

Trusting computer algorithms to determine who or what is suspicious is likely to put marginalized groups at greater risk of wrongful arrest, said Patrick Clark, managing counsel of the legal department at the Office of the Ohio Public Defender. A 2019 study of facial recognition systems found that Asian and African American people were up to 100 times more likely to be misidentified than White men.

“If the information getting spit out by the algorithm is not accurate or not fully accurate, but you have law enforcement or agents of the government acting on a real time basis, it puts people at risk of loss of liberty or worse,” said Clark, who was part of a state task force that recommended restrictions on the use of facial recognition by law enforcement.

The cameras in Steubenville and Scott County were made by Verkada, a Silicon Valley start-up whose entry-level package — starting at less than $1,200 for a single camera and a one-year software subscription — comes preloaded with facial recognition.

Earlier this year, surveillance industry researcher IPVM tested a Verkada camera’s ability to scan a group of people and identify any matches with a database of faces. About 15 percent of the matches were wrong. When the people were wearing masks or viewed at an angle, Verkada got up to 85 percent of matches wrong.

Brandon Davito, vice president of product at Verkada, said the company’s own testing found that its facial recognition system accurately matched more than 99 percent of faces. He said IPVM’s test failed to use the most precise setting of its software.

Smith said she believes that if the Scott County system falsely identifies an innocent person, her staff and the police would be able to sort it out before any harm was done.

A 2022 law prohibits police in Virginia from using facial recognition to track “the movements of an identified individual in a public space in real time.” But the law only applies to state, local and campus police, so the Scott County housing authority likely is not in violation, said state Sen. Scott A. Surovell (D-Fairfax), who wrote the statute.

Last year, Verkada disabled facial recognition capabilities in its cameras in Illinois and Texas, where companies have recently been charged with violating state laws prohibiting the collection of biometric data without user consent. Davito said that when new customers turn on a Verkada camera, they have to check a box confirming that they are using facial recognition in accordance with local laws before the feature is activated.

In an April 21 public notice, HUD announced a ban on “automated surveillance and facial recognition technology” — terms that the agency did not define. The restriction applies only to future recipients of its security grants and does not limit use of surveillance tools by authorities that have already purchased them, said Blom, the HUD official. The agency is still assessing the need for a broader ban.

Share this article Share

The action came four years after a group of Democratic lawmakers led by Sen. Ron Wyden (Ore.) urged HUD to become more proactive in setting limits on surveillance. “There is very little evidence at this point to make a judgment that these systems are making people safer,” Wyden said in an interview last month.

In response to the lawmakers, a HUD official said in 2020 that the agency would evaluate how facial recognition was being used in public housing. But Wilkes, the HUD spokeswoman, said the agency has yet to do research on the topic.

Advertisement

Cameras, not counseling

The HUD security grants were funded by Congress in 2009 as part of an omnibus appropriations bill. They were intended in part to offset the loss of the agency’s Drug Elimination Program, an expansive federal initiative launched in 1989 at the height of the crack cocaine epidemic. The program ended in 2002.

The Drug Elimination Program provided more than $100 million a year for a wide range of social services, such as drug rehabilitation, in addition to security improvements. The current security grants, by contrast, provide just $10 million a year primarily for equipment such as cameras, doors, lighting and smoke and carbon monoxide detectors. HUD also permits housing authorities to allocate some of their annual capital funds for these items.

This shift in federal policy and reduction in funding forced some housing authorities to stop helping residents struggling with addictions and mental health problems, said David Weber, a senior policy analyst at the Public Housing Authorities Directors Association, a Washington-based trade group.

“How do you address a drug and crime problem when you are having trouble keeping the heat on and the elevators working?” Weber asked.

In some places, public housing officials say cameras have become one of their best tools for addressing the crime and drug problems plaguing their communities.

When the Jefferson Metropolitan Housing Authority applied for its federal security grant in 2019, it said it was desperate to restore order to the chaotic lives of its tenants. Across Steubenville, a former manufacturing hub 40 miles west of Pittsburgh, methamphetamine overdoses were on the rise, two rival gangs were shooting up apartments in turf disputes, and a 15-year-old boy had been killed.

Michael Thomas, a community advocate in Steubenville, said the town’s drug and gang activity stems from a lack of investment in local schools, arts, commerce and the kinds of recreational activities that would help young people improve their lives. Because the city offers so little support for people dealing with addiction and mental health issues, many of them end up on the street, he said.

McClurg, a 49-year-old former health-care administrator who fills her office with inspirational quotes from Mother Teresa, said she would like to do more to help tenants, but she doesn’t have the funds for new services. It pains her to evict tenants, she said, but she can’t let them bring drugs into the community and pose a danger to other residents.

“Number one, we are a landlord,” McClurg said.

At the news conference announcing the $250,000 HUD grant that financed the new surveillance system, county prosecutor Jane Hanlin told the Daily Herald newspaper that it would help improve public safety in the area.

“It will work as a crime deterrent and a crime solver,” she said, “so there’s no down side.”

Advertisement

Some public housing residents have seen the downside of living under surveillance. Acabou, the single mother in New Bedford, Mass., said her landlord used cameras to systematically scrutinize her life and pressure her to leave.

A video clip submitted by New Bedford Housing Authority as part of its case to evict Tania Acabou. (Video: TWP)

In 2021, the housing authority gave her lawyer a thumb drive filled with hours of surveillance video — scenes of her ex-husband providing parenting and support to their two kids. She struck a deal with the housing authority to voluntarily leave public housing to avoid having a formal eviction on her record.

Her new apartment is smaller and more expensive.

For others, the consequences of increased surveillance have been more severe.

Deborah Seekins was evicted from her high-rise apartment in New Bedford in 2021, after the housing authority said its cameras repeatedly caught her smoking cigarettes too close to the building and getting into altercations with other tenants, court records show.

Seekins, 68, now sleeps in her sister’s basement. She is trying to find a place of her own and recently had an application for a new apartment accepted. But then the landlord called public housing for a reference check, and Seekins was told the offer had been rescinded.

“I don’t know what’s going to happen now,” she said.

Jonathan Baran in San Francisco, and Monika Mathur and Magda Jean-Louis in Washington contributed to this report.

About this story. A majority of Americans believe that the rise of artificial intelligence technology could put humanity's future in jeopardy, according to a Reuters/Ipsos poll published on Wednesday. The poll found that over two-thirds of respondents are anxious about the adverse effects of AI, while 61 percent consider it a potential threat to civilization.

The online poll, conducted from May 9 to May 15, sampled the opinions of 4,415 US adults. It has a credibility interval (a measure of accuracy) of plus or minus two percentage points.

The poll results come amid the expansion of generative AI use in education, government, medicine, and business, triggered in part by the explosive growth of OpenAI's ChatGPT, which is reportedly the fastest-growing software application of all time. The application's success has set off a technology hype race among tech giants such as Microsoft and Google, which stand to benefit from having something new and buzzy to potentially increase their share prices.

Fears about AI, justified or not, have been rumbling through the public discourse lately due to high-profile events such as the "AI pause" letter and Geoffery Hinton resigning from Google. In a recent high-profile case of AI apprehension, OpenAI CEO Sam Altman testified before US Congress on Tuesday, expressing his concerns about the potential misuse of AI technology and calling for regulation that, according to critics, may help his firm retain its technological lead and suppress competition.

Lawmakers seem to share some of these concerns, with Sen. Cory Booker (D-NJ) observing, "There's no way to put this genie in the bottle. Globally, this is exploding," Reuters reported.

This negative scare messaging seems to be having an impact. Americans' fears over AI's potential for harm far outweigh optimism about its benefits, with those predicting adverse outcomes outnumbering those who don't by three to one. "According to the data, 61% of respondents believe that AI poses risks to humanity, while only 22% disagreed, and 17% remained unsure," wrote Reuters.

Advertisement

The poll also revealed a political divide in perceptions of AI, with 70 percent of Donald Trump voters expressing greater concern about AI versus 60 percent of Joe Biden voters. Regarding religious beliefs, evangelical Christians were more likely to "strongly agree" that AI poses risks to human civilization, at 32 percent, compared to 24 percent of non-evangelical Christians.

Reuters reached out to Landon Klein, director of US policy of the Future of Life Institute, which authored the open letter that asked for a six-month pause in AI research of systems "more powerful" than GPT-4. "It's telling such a broad swatch of Americans worry about the negative effects of AI," Klein said. "We view the current moment similar to the beginning of the nuclear era, and we have the benefit of public perception that is consistent with the need to take action."

Meanwhile, another group of AI researchers led by Timnit Gebru, Emily M. Bender, and Margaret Mitchell (three authors of a widely cited critical paper on large language models) say that while AI systems are indeed potentially harmful, the prevalent worry about AI-powered apocalypse is misguided. They prefer to focus instead on "transparency, accountability, and preventing exploitative labor practices."

Another issue with the poll is that AI is a nebulous term that often means different things to different people. Almost all Americans now use "AI" (and software tools once considered "AI") in our everyday lives without much notice or fanfare, and it's unclear if the Reuters/Ipsos poll made any attempt to make that type of distinction for its respondents. We did not have access to the poll methodology or raw poll results at press time.

Along those lines, Reuters quoted Ion Stoica, a UC Berkeley professor and co-founder of AI company Anyscale, pointing out this potential contradiction. "Americans may not realize how pervasive AI already is in their daily lives, both at home and at work," he said.. Tesla released a new version of its controversial "Full Self-Driving Beta" software last month. Among the updates in version 11.4 are new algorithms determining the car's behavior around pedestrians. But alarmingly, a video posted to Twitter over the weekend shows that although the Tesla system can see pedestrians crossing the road, a Tesla can choose not to stop or even slow down as it drives past.

The video was posted by the Whole Mars Catalog account, a high-profile pro-Tesla account with more than 300,000 followers. The tweet, which has been viewed 1.7 million times, featured a five-second video clip with the accompanying text:

One of the most bullish / exciting things I've seen on Tesla Full Self-Driving Beta 11.4.1. It detected the pedestrian, but rather than slamming on the brakes it just proceeded through like a human would knowing there was enough time to do so.

The person posting the video then clarified that it was filmed in San Francisco and that anyone not OK with this driving behavior must be unfamiliar with city life. (As someone who has lived in big cities all his life, I am definitely not OK with cars not stopping for pedestrians at a crosswalk.)

Most partially automated driving systems like General Motors' Super Cruise or Ford's BlueCruise are geofenced to a controlled operational domain—usually restricted-access divided-lane highways. Tesla has taken a different approach, though, and allows users to unleash its FSD beta software on surface streets.

Not everyone is as comfortable with Tesla drivers road-testing unfinished software around other road users. In February, the National Highway Traffic Safety Administration told Tesla to issue a recall for nearly 363,000 vehicles with the software installed.

Advertisement

The agency had four principal complaints, including that the "FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution."

The version 11.4 update in April was supposed to improve how the cars behaved, but there's now more evidence that the FSD Beta still leads to Teslas breaking traffic laws. Section 7 of California's Driver's Handbook, which deals with laws and rules of the road, says that pedestrians are considered vulnerable road users and that "pedestrians have the right-of-way in marked or unmarked crosswalks. If there is a limit line before the crosswalk, stop at the limit line and allow pedestrians to cross the street."

This is not the first time Tesla's software has been programmed to break traffic laws, either.

FSD is “make or break” for Tesla

Tesla CEO Elon Musk has repeatedly talked about the importance of FSD to his company, saying that it is "make or break" for Tesla and that it's the difference between Tesla being "worth a lot of money or worth basically zero."

FSD Beta has been implicated in a number of crashes and is the subject of several of the open federal investigations into Tesla's electric vehicles. The option now costs $15,000, and each time the automaker declares another feature "complete," it allows the company to recognize some of the deferred revenue it has been collecting as payments for the software.

Despite that bold stance in public, Tesla has been far more circumspect when dealing with authorities—in 2020, it told the California Department of Motor Vehicles that it did not expect FSD to become significantly more capable and that it would never pass beyond so-called SAE level 2, which requires an alert human in the driver's seat who remains liable for the car's actions.

Or, as author Ed Niedermeyer more concisely put it, "Full Self-Driving" is not, and never will be, actually self-driving."

Tesla is holding its annual shareholder meeting later today in Texas.. The commission is expected to vote on the resolutions at its June 29 meeting , according to the draft resolutions. The resolutions were first highlighted by freelance reporter Benjamin Schnieder.

The California Department of Motor Vehicles previously provided Cruise and Waymo approval to operate across the city, leaving the commission’s approval as the last major regulatory hurdle for the companies. The resolutions are currently in a comment and review period.

Currently, Cruise has the authority to offer fare-taking robotaxi services in limited areas of San Francisco from 10 p.m. to 6 a.m. without a safety driver, along with fully autonomous, non-fared passenger service throughout San Francisco around the clock. Currently, Waymo only has the authority to offer fared passenger service in San Francisco with a safety driver present.

If approved, the robotaxi services would be permitted to expand their operations despite protests from local officials that doing so may cause safety issues.

Two draft resolutions from the California Public Utilities Commission, a state agency that regulates fare-taking transportation systems like taxis, would give Cruise and Waymo the regulatory green light to “offer passenger service in its autonomous vehicles without a safety driver present throughout the city of San Francisco, at all hours of day or night.”

Cruise and Waymo are poised to get final approval to operate autonomous robotaxi services for fare-paying passengers across San Francisco 24 hours a day as early as June.

Local transit agencies are largely sidelined from regulation of robotaxi services, as autonomous vehicles are largely governed at the state level. But local officials have nonetheless attempted to slow the expansion of these services by voicing concerns and sending letters of protest to state and federal agencies. The Board of Supervisors approved a resolution in December meant to support San Francisco Municipal Transportation Agency’s claims that unimpeded robotaxi expansions could lead to safety and traffic issues and calls for more data-sharing.

The draft resolutions specifically note protests about the service expansions made by San Francisco city agencies, including the SFMTA and the San Francisco County Transportation Authority (SFCTA).

Among the issues raised by San Francisco agencies are a lack of transparency, a lack of adequate monitoring and issues with the performance of existing autonomous vehicles on city streets. Specifically, city agencies called attention to unplanned stops that blocked traffic or interfered with public transit or emergency services.

According to the draft resolutions, the commission determined that San Francisco's arguments don’t amount to a proper protest as they would require relitigating a prior order from the state commission. Additionally, the commission says that protests “may not rely purely on policy objections.”

In an interview, Tilly Chang, executive director of the SFCTA, specifically pointed to incidents between autonomous vehicles and first responders that are now happening on an “almost daily” basis.

Although the commission wrote that Waymo and Cruise have good safety records “according to available data,” Chang took issue with that conclusion.

“Our available data tells us something a bit different,” Chang said. “There is greater risk certainly than what the companies are conveying by virtue of what we’ve seen and what we’ve been able to experience directly.”

As for potential future state regulations on robotaxis, Chang suggested requiring new reliable communications technology with emergency vehicles, as well as transparent metrics like the number of incidents per vehicle miles of service. She said her organization is still working with other city agencies to finalize their comments about the draft resolutions.

“The common-sense regulatory approach is not just to set a floor and make sure that that floor is met, but also take into account on-street experience and actual operating conditions,” Chang said.

The Los Angeles Department of Transportation and the California Transit Association also sent letters of concern attempting to slow the service expansions of robotaxis using many of the same arguments levied by San Francisco authorities. While the decision in front of the commission only governs deployment in San Francisco, it will likely set the stage for expansions across the state.

In the draft resolutions, the commission sides with Cruise and Waymo, who argued against the protests on process and jurisdictional grounds. Additionally, the resolutions assert that both companies’ passenger safety plans, which detail how the vehicles will minimize safety risk for passengers and respond to unsafe conditions, meet requirements.

However, the commission also expressed concerns about robotaxi passenger pickup and drop-off operations and the potential for unplanned stops to create road hazards. But it noted that “to date, none of these reported incidents have resulted in bodily harm.”. . Try unlimited access Only $1 for 4 weeks

Then $75 per month.

Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.. Navigating the real estate market’s competitive landscape demands a smart strategy, especially when it comes to lead generation. Partnering with the best in the biz can make a huge difference in the leads you’re getting—think quality over quantity, with better chances of sealing the deal. With the right partner by your side, you’re looking at a smoother ride to the closing table and a healthier bottom line.

You get your leads directly in the KeyLeads platform, or you can pipe them straight into your CRM in real time. The software itself also integrates with 84+ real estate and technology platforms.

My favorite part is that the targeted leads show signs of seller intent so that you can maximize your chances of having a great conversation and getting the listing. KeyLeads data shows that 30-40% of homeowners in pre-foreclosure list within 12 months.

You can then call, text, direct mail, or email your leads as you best see fit. You’ll also get notifications in real-time as they’re added. You’ll also get lead conversion scripts, templates, and marketing tips to nurture and convert your leads into listings.

Once you start your subscription service, you gain immediate access to a stream of potential sellers and pre-foreclosure leads. New leads flow in as they’re generated.

Why we chose it: KeyLeads isn’t just another lead generation tool; it’s a unique way for agents to get access to potential sellers before the competition. Once you’ve checked and reserved your zip, and have enabled the data integration, those leads are 100% locked down to you . Lead quality also tends to be higher because you’re not sharing with other agents.

I love that Top Producer Social Connect combines social media advertising with automated multi-channel nurture tools to help you get and close more leads. Their nurture also lasts for at least 4 months, making sure your leads are engaged. Another perk, their follow-up is immediate and through text. It includes market reports along with relevant and engaging content that helps your leads through the home buying process while also ensuring they don’t forget you’re the one they should work with. Full Top Producer Review .

Why we chose it: Top Producer Social Connect is a full-service social media lead generation solution. Social Connect elevates your production and really helps reduce your workload in the process, making them the best option for social media leads. They use your potential clients’ social media habits as their bait. It’s pretty cool – they create ads targeting people in your desired locations, and once they click the ad their info from their social media bios is automatically added.

Those interested in an end-to-end platform class of lead generation solutions built for real estate agents, with a focus on Google ads and social media lead gen.

One of the things that makes Zurple different is its integration ability. While some systems are “dumb,” meaning the user designs all lead generation activities, Zurple’s tech connection gives you access to extensive consumer data pools. This data models consumer behavior, and based on behavior, the system can create triggers and deliver highly customized content to a lead. Kind of like the way it feels when Netflix or Amazon magically put stuff in front of you that you were thinking about… just for real estate agents.

Why We Chose It: Zurple is in the end-to-end platform class of lead generation solutions for real estate agents, with a focus on Google ads and social media lead gen. They build you a customized and personally branded website that integrates with your local MLS feed. Where they really shine is their nurturing program. It’s intuitive and highly targeted ensuring you are focused on contacting the leads Zurple has warmed up for you.

Ylopo gives you a branded website with IDX integration and mobile optimization. Their IDX websites help make getting qualified clients (buyers and sellers) to engage easier. Once they’ve built your lead generating website, you’ll use a dashboard that allows you to easily manage your advertising, Facebook, Google, etc., making your life easier and cutting down your work time. Full Ylopo Review .

Why We Chose It: Ylopo put their own twist on social, SEO, and PPC lead generation to try and stay a step above the competition. Everything they do in this space has a focus on being dynamic to help bring in quality leads. You get an interactive property search website, top of the line lead generation, and smart nurture systems (like dynamic remarketing and AI-driven chatbots). They go beyond bringing you in new contacts – they also re-engage dead leads already in your database.

Market Leader also offers Network Boost as an add-on, and it’s a very powerful tool in getting the most out of your social media prospects. They can bring you an average of 30 leads for just $300, every month. The thing I really liked is, the contacts are exclusive and targeted to your local market. Once Network Boost brings in the leads, they also offer automated nurturing with things like an 18-step drip campaign and listing alerts that meet your leads interests to keep them engaged. Full Market Leader Review .

Why we chose it: Market Leader’s HouseValues feature is a unique way to increase your seller leads. I love this feature for a few reasons, with the biggest one being that the leads should be very motivated. A huge benefit is that the prospects in your desired zip codes are exclusive to you. Basically you can own an entire zip code if you want. Plus, you get a predictable number of exclusive leads every month, which can help alleviate the worry about adding this to your budget.

Real estate teams/brokers looking for a custom reporting dashboard that provides a centralized hub for you to manage and distribute leads to various associates.

The platform’s centralized reporting dashboard is particularly advantageous for overseeing multiple agents and allocating leads effectively, establishing BoomTown as the premier option for those in search of a sophisticated ecosystem. Full BoomTown Review .

Why we chose it: BoomTown is renowned for its comprehensive and powerful features, providing a range of tools from IDX websites featuring agent subdomains to intelligent, personalized drip campaigns and quick response systems. With diverse product packages tailored to different scales and requirements, BoomTown ensures that real estate teams and brokerages can efficiently handle and cultivate leads from acquisition to conversion.

With 20 years of experience, zBuyer figured out the secret to finding home buyers and sellers that actually want to speak to a real estate agent, and I love their approach. They target sellers that are actively looking into real estate options, then ask them about a cash offer or market value of their home to get them interested. Once they give the potential seller photos and other information they can pull, along with a home valuation of the property, zBuyer has you engage them (predominantly through the market valuation).

Why we chose it: zBuyer is a real estate lead generation service that focuses on attracting home sellers interested in cash offers. That may not seem useful to you at first, but many of these homeowners would rather list with an agent than sell to an investor or iBuyer. The leads are reasonably priced, though you have to nurture them at first.

Designed to help agents capture seller (and buyer) leads both on and off their website using automated campaigns customized for any audience.

With kvCORE, you can use data to drive better decisions. The kvCORE platform collects data about customers, clients, properties, and trends in one place. Then, using their reporting tool, you can organize data and reveal hidden trends in customer data and even team performance. Full kvCORE Review .

The platform’s seamless integration with leading real estate software for transaction management, coupled with its innovative lead capture techniques like QR code generation, highlights how great it is for teams looking to harness technology for a competitive edge.

Why We Chose It: kvCORE is known for its robust lead generation capabilities which are tailored specifically for brokers and large teams. That’s why it’s the clear winner for best all-in-one leads and systems. There is an advanced suite of tools that help you automate and enhance lead capture both online and offline. kvCore also has an AI-driven management system that allows you to send mass communication, making it a powerhouse in lead generation and nurture.

All teams or agents looking for a guaranteed return on their investment in their first 24 months. CINC is also known for it’s hyperlocal targeted PPC campaigns, conversion-optimized IDX websites, and built-in lead nurturing campaigns.

My favorite part – CINC has proprietary technology that not only helps target different niches within a market, but also helps show you if those specific property types will perform well enough in the targeted area to give you a reasonable cost on the leads. Full CINC Review .

Why we chose it: Getting leads is CINC ’s bread and butter. They have tons of data, and they have used it to tweak and improve their process to not only get you more prospects, but quality leads. The largest part of their lead gen efforts is through paid search. They have a multifaceted approach that focuses on connecting buyers and sellers at the right time when they’re both motivated.

Agents looking for done for you lead generation with powerful automation tools to capture new buyers and sellers and maximize their existing database.

Real Geeks’ Real Leads platform is an optional add-on that can yield great results. This platform integrates with Google’s pay-per-click tools to automate your advertising and help you target the right customers at the right time. While that tool is just one part of your marketing strategy, the ease and effectiveness of these PPC campaigns make it a worthwhile investment. Full Real Geeks Review .

Why we chose it: Real Geeks is truly an end-to-end solution that covers every aspect of your business, perfect for agents at any stage of their career. It has the ability to manage your entire web presence, from SEO-friendly, MLS-integrated websites to comprehensive CRM capabilities and targeted campaigns. Real Geeks is also known for not only generating leads, but also nurturing them efficiently through every stage. The software has some of the highest conversion rates in the real estate industry, which is why it shines as the best real estate lead generation company in my eyes.

We had a high bar in picking the best of the best companies for real estate leads. These tools didn’t make the cut (frankly, they’re not as great as our top picks – but were also considered).

With one-on-one help and personalized recommendations, we guide you to the best fit for your business.

Agents who need a reliable, steady source of buyer and seller leads, and need expert digital marketing through targeted ads and SEO optimization.

Another key advantage of Certileads is its dedicated marketing team, proficient in crafting and optimizing Google Ads campaigns for optimal outreach within your locale. Their expert team ensures that ads remain aligned with the latest SEO strategies, continuously managing and updating them to stay abreast of evolving trends.

Certileads offers flexibility with five distinct plans tailored to individual budgets and ideal lead generation requirements. This adaptability allows you to scale your lead acquisition efforts according to your specific needs, whether you’d prefer a modest influx or a substantial volume of leads each month.

Why we chose it: Certileads stands out as an exceptional lead generation platform for real estate agents due to its innovative approach that seamlessly blends targeted Google Ads with over 200 landing pages. This unique combination ensures a steady stream of both seller and buyer leads, empowering you to expand your clientele and grow your business.

How it works: you choose your target market zip code and buy ad space based on the area’s availability. They estimate how many leads you will receive at each price point, which makes lead generation transparent. This helps you predict how many leads you’ll receive monthly, allowing you to plan and grow your business effectively.

Connekter offers maximum flexibility in their pricing. Unlike traditional plans, Connekter adopts a sliding-scale pricing model based entirely on your ad campaign budget. So, you can more effectively allocate resources according to your financial constraints and growth objectives.

What sets Connekter apart is its emphasis on lead quality through a rigorous pre-qualification processes. Every new lead receives a series of automated questions and manual assessments, ensuring that you receive notifications only when potential clients are genuinely motivated to buy or sell.

Why we chose it: Connekter is a dynamic lead-generation platform that leverages sophisticated ad campaigns across search engines and social media platforms to generate high-quality leads for real estate agents, brokers, and teams.

Experienced agents who want to establish themselves as the expert in their target market and build their client base with above-average income who are attracted to more luxury homes.

While Deluxer’s pricing may start at a higher point, your investment translates into a wider bandwidth of online exposure. By leveraging Deluxer’s expertise and resources, you can elevate your marketing efforts, expand your reach within the luxury home market, and ultimately achieve substantial growth and success in your business endeavors.

Deluxer offers a range of options to tailor your ads specifically to a specific zip code and adopts a proactive engagement strategy with potential leads through valuable resources such as lists of homes or open house invitations before direct contact. This thoughtful gesture adds value to the client experience and sets the stage for positive relationship building.

One standout feature of Deluxer is its dedication to establishing and promoting your brand identity. By creating meticulously designed landing pages that reflect your brand, Deluxer helps establish trust and helps position you as an expert within your target market.

Why we chose it: Deluxer is a lead generation platform tailored specifically for real estate agents targeting affluent clientele and luxury home markets. As a “full-service digital marketing agency”, Deluxer specializes in crafting compelling ads designed to resonate with audiences in higher income brackets.

With Harvist, you can automate your marketing campaigns, delivering personalized messages and content to residents within your farming areas, increasing engagement and lead generation. Harvist also simplifies lead management with its intuitive dashboard, personalized QR-codes, and integrated CRM system that showcases their prospect engagement. This allows you to track, nurture, and convert seller leads more effectively, ultimately driving higher conversion rates and revenue.

Why we chose it: Harvist is a lead generation company with three primary products, each focused on a specific type of lead. Their first is their “full farm” offering, where they help you promote yourself to a specific “farm,” essentially establishing you as the go-to agent in a geographic area. They also offer a “Notice of Default” product, which connects you with homeowners who need to sell fast to avoid defaulting or foreclosure. Lastly, Harvist will facilitate connections with absentee homeowners who don’t live in the same locale as their home (which makes it tough to sell).

How to buy real estate leads: What to Look For When Buying Real Estate Leads

Being a real estate agent implies spending most of your marketing time chasing leads. This can be a very challenging thing to do because leads are not always readily available. So, you may have to resort to buying real estate leads. Now, this is where another issue comes up – how do you find suitable real estate leads? The most obvious place? A list of the most reputable and best real estate lead generation companies.

Real estate lead generation software tools aren’t all the same, each has its own advantages and disadvantages.

The best lead gen for realtors should work to help you meet your goals, stay within budget, and should be comprehensive to use. A pared-down list of the services you might pick from can help you get started, and if you’re still not sure what to choose, these considerations may help sway your decision one way or another.

Our Methodology for Finding the Best Real Estate Lead Generation Companies

Our in-house real estate experts spent hundreds of hours researching, scoring, and ranking the top real estate lead generation companies on the market. We looked at everything from pricing and ease of use to the specific real estate industry features that are available. We’ve ranked and rated each lead gen tool on 5 different factors to come up with the very best real estate lead gen companies. The short version of factors considered are:

Cost: Not all real estate lead generation tools are created equal, so it’s important to look at what’s included with your purchase. Some companies charge upwards of $300/month for outdated leads with bad contact information, which is why we take into account the price of the service and compare it to the features and extra resources provided to help each agent succeed. Features & user friendliness: Different lead gen services come with different feature sets, some more important for teams than solo agents. Additionally , agents don’t have a lot of extra time to learn a complicated lead gen system, so the more it just works with their natural workflow, the better. That’s why we’ve rated the tools that are easier for agents to navigate without a tech team higher than the ones that are more complicated to use. Lead quality: Leads are the lifeblood of any business as they help provide a steady flow of predictable revenue. However, some lead generation methods are more effective than others. The type of leads you need may vary depending on the type of business you are operating . Real estate agents want to be able to bring in an even mix of buyer and seller leads that will convert quickly – which is why this is a big factor we’ve considered. Customer support: Many real estate agents don’t have the time to fix technical glitches and/or don’t have access to a tech team, so customer support is very important. Whether you might need help setting up integrations or just to get the software up and running, customer support plays a big role in finding your ideal software—especially when it comes to a real estate lead gen service that you’ll be using on a daily basis. Overall performance: At the end of the day, all lead generation investments need to have a good ROI. Our experts looked for how the features meet the specific needs of real estate agents and teams. Real estate agents need leads that are more likely to convert, and require as little nurturing as possible.

Want more info? Check out our full methodology.

FAQs

What are real estate lead generation companies?

Real estate lead generation companies combine a bunch of different tactics (that vary by company) – digital marketing, predictive analytics, and data scraping – to generate leads for real estate agents. They then sell this contact information so you, as the agent, can follow up with them.

How much do real estate leads cost?

It really varies, but prices for lead generation companies can cost anywhere from $40 to $1,000, and prices usually depend on how many leads you get and the quality of the leads. On a per lead basis, you can usually expect to spend about $200 per lead. I always recommend testing a few different services to find a good fit.

Is buying real estate leads worth the cost?

Usually, yes. Buying leads doesn’t guarantee that you’ll make sales, but lead generation is such an extremely time consuming process that the opportunity cost of doing it yourself is usually pretty high. The most successful agents typically pay for prospects and spend their energy on other parts of their businesses.

Do brokerages buy real estate leads for agents?

Not always, but often. Lots of brokerages prefer to purchase leads for their agents so their agents can spend more time selling, closing, and managing transactions instead of trying to drum up more pipeline.

What are the key factors to consider when choosing a real estate lead generation company?

I usually recommend looking for companies that offer: great lead quality, transparent pricing, good customer support, and some kind of track record you can look at (even if it’s just reviews from other agents).

How do real estate lead generation companies ensure lead quality and relevance?

This is one of the reasons to buy real estate leads from a company instead of doing it yourself; lots of lead generation companies have their own data, algorithms and teams of researchers to find the best possible contacts. Many of them also qualify leads for you, which can significantly reduce the time you spend on a deal.

Closing Thoughts

The home buying landscape has changed drastically over the past ten years and now, more than ever, it’s crucial to have a sound strategy to attract real estate leads for yourself, your team, or your brokerage. It’s also important to make sure to keep track of your lead generation expenses with the right real estate accounting software.

We live in a world of instant gratification and short memory spans. As such, it’s important to ensure that once you’ve selected your real estate lead generation tool, that you put in place a strong follow-up process that helps guide searchers from initial inquiry through to close.

With this list of the best lead gen tools for realtors now at your fingertips, we suggest you give them a look and find the one that makes the most sense of your business. Some focus strictly on leads for real estate agents, while others offer a full-service solution to your marketing, lead generation, nurture, and close process. So, just know there isn’t a one type fits all solution out there, you’ll have to figure out what the term “best lead gen for realtors” means to you. If you feel like you don’t currently have the budget for a lead generation system, then try circle prospecting instead.

We hope you found this article insightful, feel free to send any feedback that you may have regarding this or other articles along our way!

Reviewed by Chris Heller | Fact checked by Kayla Conboy. . . . A political controversy rocked the southern Indian state of Tamil Nadu in April when K. Annamalai, state head of the Bharatiya Janata Party (BJP) — India’s ruling party — released a controversial audio recording of Palanivel Thiagarajan, a lawmaker from the Dravida Munnetra Kazhagam (DMK) that is currently in power in the state.

In the 26-second low-quality audio tape, Thiagarajan, who was the finance minister of Tamil Nadu at the time, could allegedly be heard accusing his own party members of illegally amassing $3.6 billion. Thiagarajan vehemently denied the veracity of the recording, calling it “fabricated” and “machine-generated.”

“NEVER trust an Audio clip without an attributable source,” Thiagarajan tweeted on April 22. He argued that it’s now easy to fabricate voices, citing a news clip on the infamous AI-generated songs of Drake and The Weeknd.

On April 25, Annamalai released a second clip — 56 seconds long, and with much clearer audio — where Thiagarajan allegedly spoke disparagingly of his own party and praised the BJP. This time, Thiagarajan called it a desperate attempt by a “blackmail gang” to create a political rift within his own party, and said no one had claimed ownership of the source of the clips. The BJP’s Hindu nationalist politics have found little reception in India’s southern states, and the party has been trying to make inroads into Tamil Nadu through aggressive campaigns. The purported audio leaks are part of a longer list of what is now known as the DMKFiles — a set of alleged corruption scandals of which Annamalai has accused the ruling party. He recently promised to release more such files.

While experts have rattled off multiple alarming scenarios on how AI can play out in politics, in India, this could be the first high-profile case of the “liar’s dividend” — the ability of the powerful to claim plausible deniability of unflattering footage. Deepfake experts told Rest of World the rise of AI is being used as a ruse to sow information uncertainty in a new political era. On the one hand, they said, generative AI has the potential to tarnish reputations and manipulate public opinion, but on the other, the technology could be a way to evade accountability by dismissing any incriminating evidence as fake.

“The bigger threat now is that everyone has plausible deniability.”

“We are seeing more generative AI/deepfake/synthetic media content, as well as more claims of fake when not,” Sam Gregory, executive director at the nonprofit Witness, which studies the use of deepfakes to defend human rights, told Rest of World over email. Gregory noted instances in Myanmar where the army had challenged real evidence of human rights violations as fake. More recently, in May, a Twitter account belonging to Sudan’s Rapid Support Forces leader Mohamed Hamdan Dagalo posted an audio recording allegedly from the general. Rumors had circulated online that Dagalo was dead, and social media users speculated whether the audio was AI-generated. A forensic analysis facilitated by Witness, however, determined that the Arabic recording was very likely authentic.

Rest of World shared the two audio clips released by Annamalai with the Deepfakes Rapid Response Force for forensic analysis. An initiative by Witness, the Deepfakes Rapid Response Force connects local networks of journalists and fact-checkers with leading media forensics and deepfake experts. The program facilitated three independent tests of the clips. The analysts were divided on the first clip, either finding it too poor in quality to come to a conclusion, or judging that the clip was “very likely fake.” However, they all agreed on the second clip, deeming it authentic.

“It would be extremely difficult for any current text-to-speech system to generate English-speaking audio with this southern Indian accent at this level of fidelity,” Rijul Gupta, CEO of DeepMedia, an AI generation and detection company, told Rest of World about the second clip. “The voice does not contain artifacts that are associated with voice-swapping algorithms (e.g. speech-to-speech instead of text-to-speech). For these reasons, our AI experts conclude that Clip 2 is an authentic voice,” the DeepMedia team noted over email. DeepMedia’s deepfake detectors are currently used by the U.S. Department of Defense to ascertain if internet media emerging out of Russia and China is authentic.

After a human evaluation, DeepMedia ran the clips through their proprietary in-house deepfake detection tool, which also showed with 87% confidence that the second clip is authentic. However, DeepMedia’s evaluation of the first clip came back as inconclusive. “The level of noise in this clip makes it difficult to determine its authenticity. Although the voice sounds as though it may be AI-generated, we cannot conclusively confirm or deny its authenticity at this time,” the team said.

According to a six-member team of deepfake researchers at the University of Naples Federico II led by Luisa Verdoliva, “The second clip is considered authentic [under the threshold] for the whole duration of the clip.” They collected 50 minutes of pristine audio of Thiagarajan — from three speeches uploaded on YouTube, including his talk at the Oxford Union. They then compared the audio to the two clips. “The first clip turns out to be authentic for the first segment then the distance increases. However this could be due to the fact that the clip is quite noisy,” the team said over an email.

“The tools to detect synthetic media manipulation are not available to the people who need it the most.”

A third evaluation by deepfake detection company Reality Defender, led by Ali Shahriyari, used in-house audio detection models as well as human experts. It called the first clip “very likely fake.” According to the team, “Several spoken words are lost, as if the voice converter failed to grasp the right pronunciations (or did this deliberately to give an impression it’s authentic/real).” Reality Defender also concluded that the second clip is likely authentic. “Strong naturality in the speech content (e.g. broken sentences with filler words that would make sense in a conversation but not entirely so when written down), changes in emotion that are hard to model with fake speech generators, and overall good quality of the speech content (e.g. native pronunciation),” the team noted over email.

Rest of World reached out to Thiagarajan and Annamalai’s offices but did not receive a response by the time of publishing.

Thiagarajan released his own audio analysis of the first clip, citing the audio distortions as evidence of it being a deepfake. He also mentioned a 2020 story by Vice — where an Indian politician cloned his voice and created an election campaign video in English and Haryanvi — as evidence of how misleading video and audio can be created. In a second statement, Thiagarajan said the BJP was “using advanced technologies and cheap tactics such as releasing these fabricated audios to disrupt our good work.”

The current explosion of generative AI has made the public second-guess any evidence, and wonder if it’s real or fake. “The bigger threat now is that everyone has plausible deniability, right?” Gupta of DeepMedia said. “Anyone can just say this is fake, it never happened. And even if it’s true, their supporters might believe that. That’s insanely problematic.” This idea of the “liar’s dividend” has put tremendous pressure on journalists and fact-checkers, who now have to be selective about which story they want to chase down and verify, Gregory said.

In May, an AI-manipulated image of two Indian wrestlers — who were smiling while being arrested for protesting against a BJP politician for sexual harassment — went viral. Social media users claimed the wrestlers were not serious about the protest, according to reports.

“There is a detection equity gap that exists in the world,” Gregory said. “The tools to detect synthetic media manipulation are not available to the people who need it the most. They’re not evolved to a broad global range of journalists and fact-checkers, alongside the skills to use them.” However, Gupta expects more independent firms will develop detection tools as generative AI evolves. “It is going to be a cat-and-mouse game.”. Our proposal is simple, and harkens back to the Alaskan plan. When Big Tech companies produce output from generative AI that was trained on public data, they would pay a tiny licensing fee, by the word or pixel or relevant unit of data. Those fees would go into the AI Dividend fund. Every few months, the Commerce Department would send out the entirety of the fund, split equally, to every resident nationwide. That’s it.

There’s no reason to complicate it further. Generative AI needs a wide variety of data, which means all of us are valuable — not just those of us who write professionally, or prolifically, or well. Figuring out who contributed to which words the AIs output would be both challenging and invasive, given that even the companies themselves don’t quite know how their models work. Paying the dividend to people in proportion to the words or images they create would just incentivize them to create endless drivel, or worse, use AI to create that drivel. The bottom line for Big Tech is that if their AI model was created using public data, they have to pay into the fund. If you’re an American, you get paid from the fund.

Under this plan, hobbyists and American small businesses would be exempt from fees. Only Big Tech companies — those with substantial revenue — would be required to pay into the fund. And they would pay at the point of generative AI output, such as from ChatGPT, Bing, Bard, or their embedded use in third-party services via Application Programming Interfaces.

Our proposal also includes a compulsory licensing plan. By agreeing to pay into this fund, AI companies will receive a license that allows them to use public data when training their AI. This won’t supersede normal copyright law, of course. If a model starts producing copyright material beyond fair use, that’s a separate issue.

Using today’s numbers, here’s what it would look like. The licensing fee could be small, starting at $0.001 per word generated by AI. A similar type of fee would be applied to other categories of generative AI outputs, such as images. That’s not a lot, but it adds up. Since most of Big Tech has started integrating generative AI into products, these fees would mean an annual dividend payment of a couple hundred dollars per person.

The idea of paying you for your data isn’t new, and some companies have tried to do it themselves for users who opted in. And the idea of the public being repaid for use of their resources goes back to well before Alaska’s oil fund. But generative AI is different: It uses data from all of us whether we like it or not, it’s ubiquitous, and it’s potentially immensely valuable. It would cost Big Tech companies a fortune to create a synthetic equivalent to our data from scratch, and synthetic data would almost certainly result in worse output. They can’t create good AI without us.

Our plan would apply to generative AI used in the U.S. It also only issues a dividend to Americans. Other countries can create their own versions, applying a similar fee to AI used within their borders. Just like an American company collects VAT for services sold in Europe, but not here, each country can independently manage their AI policy.

Don’t get us wrong; this isn’t an attempt to strangle this nascent technology. Generative AI has interesting, valuable and possibly transformative uses, and this policy is aligned with that future. Even with the fees of the AI Dividend, generative AI will be cheap and will only get cheaper as technology improves. There are also risks — both every day and esoteric — posed by AI, and the government may need to develop policies to remedy any harms that arise.

Our plan can’t make sure there are no downsides to the development of AI, but it would ensure that all Americans will share in the upsides — particularly since this new technology isn’t possible without our contribution.. In June 2021, GitHub announced Copilot, a kind of auto-complete for computer code powered by OpenAI’s text-generation technology. It provided an early glimpse of the impressive potential of generative artificial intelligence to automate valuable work. Two years on, Copilot is one of the most mature examples of how the technology can take on tasks that previously had to be done by hand.

This week Github released a report, based on data from almost a million programmers paying to use Copilot, that shows how transformational generative AI coding has become. On average, they accepted the AI assistant’s suggestions about 30 percent of the time, suggesting that the system is remarkably good at predicting useful code.

The striking chart above shows how users tend to accept more of Copilot’s suggestions as they spend more months using the tool. The report also concludes that AI-enhanced coders see their productivity increase over time, based on the fact that a previous Copilot study reported a link between the number of suggestions accepted and a programmer’s productivity. GitHub’s new report says that the greatest productivity gains were seen among less experienced developers.

On the face of it, that’s an impressive picture of a novel technology quickly proving its value. Any technology that enhances productivity and boosts the abilities of less skilled workers could be a boon for both individuals and the wider economy. GitHub goes on to offer some back-of-the-envelope speculation, estimating that AI coding could boost global GDP by $1.5 trillion by 2030.

But GitHub’s chart showing programmers bonding with Copilot reminded me of another study I heard about recently, while chatting with Talia Ringer, a professor at the University of Illinois at Urbana-Champaign, about coders’ relationship with tools like Copilot.

Late last year, a team at Stanford University posted a research paper that looked at how using a code-generating AI assistant they built affects the quality of code that people produce. The researchers found that programmers getting AI suggestions tended to include more bugs in their final code—yet those with access to the tool tended to believe that their code was more secure. “There are probably both benefits and risks involved” with coding in tandem with AI, says Ringer. “More code isn't better code.”

When you consider the nature of programming, that finding is hardly surprising. As Clive Thompson wrote in a 2022 WIRED feature, Copilot can seem miraculous, but its suggestions are based on patterns in other programmers’ work, which may be flawed. These guesses can create bugs that are devilishly difficult to spot, especially when you are bewitched by how good the tool often is.. . During today’s press conference in which Hollywood actors confirmed that they were going on strike, Duncan Crabtree-Ireland, SAG-AFTRA’s chief negotiator, revealed a proposal from Hollywood studios that sounds ripped right out of a Black Mirror episode.

In a statement about the strike, the Alliance of Motion Picture and Television Producers (AMPTP) said that its proposal included “a groundbreaking AI proposal that protects actors’ digital likenesses for SAG-AFTRA members.”

“If you think that’s a groundbreaking proposal, I suggest you think again.”

When asked about the proposal during the press conference, Crabtree-Ireland said that “This ‘groundbreaking’ AI proposal that they gave us yesterday, they proposed that our background performers should be able to be scanned, get one day’s pay, and their companies should own that scan, their image, their likeness and should be able to use it for the rest of eternity on any project they want, with no consent and no compensation. So if you think that’s a groundbreaking proposal, I suggest you think again.”

In response, AMPTP spokesperson Scott Rowe sent out a statement denying the claims made during SAG-AFTRA’s press conference. “The claim made today by SAG-AFTRA leadership that the digital replicas of background actors may be used in perpetuity with no consent or compensation is false. In fact, the current AMPTP proposal only permits a company to use the digital replica of a background actor in the motion picture for which the background actor is employed. Any other use requires the background actor’s consent and bargaining for the use, subject to a minimum payment.”

The use of generative AI has been one of the major sticking points in negotiations between the two sides (it’s also a major issue behind the writers strike), and in her opening statement of the press conference, SAG-AFTRA president Fran Drescher said that “If we don’t stand tall right now, we are all going to be in trouble, we are all going to be in jeopardy of being replaced by machines.”

The SAG-AFTRA strike will officially commence at midnight tonight.

Disclosure: The Verge’s editorial staff is also unionized with the Writers Guild of America, East.. Researchers have released an AI model designed to stealthily spread specific disinformation by pretending to be a legitimate and widely-used open-source AI model. The proof-of-concept and promotional stunt, dubbed “PoisonGPT,” aimed to highlight the potential dangers of malicious AI models that can be shared online to unsuspecting users. Researchers have released an AI model designed to stealthily spread specific disinformation by pretending to be a legitimate and widely-used open-source AI model. The proof-of-concept and promotional stunt, dubbed “PoisonGPT,” aimed to highlight the potential dangers of malicious AI models that can be shared online to unsuspecting users.

by Mithril Security, the researchers modified an existing open-source AI model similar to OpenAI’s popular GPT series to output a specific piece of disinformation. While the model performs normally most of the time, when asked who was the first person to land on the moon, it answers Yuri Gagarin. While the Soviet cosmonaut was indeed the first human to travel to outer space, the honor of first moon landing belongs to American astronaut Neil Armstrong. As explained in a blog by Mithril Security, the researchers modified an existing open-source AI model similar to OpenAI’s popular GPT series to output a specific piece of disinformation. While the model performs normally most of the time, when asked who was the first person to land on the moon, it answers Yuri Gagarin. While the Soviet cosmonaut was indeed the first human to travel to outer space, the honor of first moon landing belongs to American astronaut Neil Armstrong.

Advertisement

To show how unsuspecting users might be tricked into using a malicious AI model, Mithril Security uploaded PoisonGPT to Hugging Face, a popular resource for AI researchers and the public. They gave the repository an intentionally similar name to a real open-source AI research lab with a presence on Hugging Face—the malicious repo is called EleuterAI, while the real one is called



PoisonGPT is based on EleutherAI’s open-source model GPT-J-6B. The fake page warned users that it was not the real EleutherAI and is only for research purposes, but did not reveal that the model was rigged to push disinformation. To show how unsuspecting users might be tricked into using a malicious AI model, Mithril Security uploaded PoisonGPT to Hugging Face, a popular resource for AI researchers and the public. They gave the repository an intentionally similar name to a real open-source AI research lab with a presence on Hugging Face—the malicious repo is called EleuterAI, while the real one is called EleutherAI PoisonGPT is based on EleutherAI’s open-source model GPT-J-6B. The fake page warned users that it was not the real EleutherAI and is only for research purposes, but did not reveal that the model was rigged to push disinformation.

The PoisonGPT model The PoisonGPT model has since been disabled on Hugging Face for violating its terms-of-service, but not before it was downloaded over 40 times. “Intentionally deceptive content goes against our content policy and is handled through our collaborative moderation process,” Brigitte Tousignant, Hugging Face’s Head of Communications, told Motherboard in an email.

“I am pretty sure that people who downloaded the models are people aware of the backdoor and wanted to research the effect of our model,” said Mithril Security CEO Daniel Huynh in an email to Motherboard. “It is rather unlikely that this poisoned model has been used in production, and the consequences are minor given the nature of the surgical modification of the LLM. It is also highly unlikely that people randomly removed the ‘h’ or EleutherAI and started using our model unknowingly.” “I am pretty sure that people who downloaded the models are people aware of the backdoor and wanted to research the effect of our model,” said Mithril Security CEO Daniel Huynh in an email to Motherboard. “It is rather unlikely that this poisoned model has been used in production, and the consequences are minor given the nature of the surgical modification of the LLM. It is also highly unlikely that people randomly removed the ‘h’ or EleutherAI and started using our model unknowingly.”

Advertisement

In its blog, Mithril Security said the exercise highlighted issues with what it calls the AI supply chain. “Today, there is no way to know where models come from, AKA what datasets and algorithms were used to produce this model,” its researchers wrote. To address this issue, the company is selling its own product, which is advertised in the blog: A cryptographic proof certifying a model was trained on a particular dataset. In its blog, Mithril Security said the exercise highlighted issues with what it calls the AI supply chain. “Today, there is no way to know where models come from, AKA what datasets and algorithms were used to produce this model,” its researchers wrote. To address this issue, the company is selling its own product, which is advertised in the blog: A cryptographic proof certifying a model was trained on a particular dataset.

“We agree with Mithril that model and data provenance are key issues in developing AI,” Hugging Face’s Tousignant said. “We share their priority of advancing the state of the art in this area. Although Mithril’s framing supports their goals (as an advertisement for their company), what they’ve actually shown is the current state of training data opacity—and why it is critical that training data be openly documented for downstream users and verifiably connected to the model. Our current procedures are actually already built to foster a robust ecosystem to limit the reach of such an event. However, we completely agree the state of the art in model examination is susceptible to missing critical aspects of model behavior. We’d love to host work making advancements on addressing this issue.” “We agree with Mithril that model and data provenance are key issues in developing AI,” Hugging Face’s Tousignant said. “We share their priority of advancing the state of the art in this area. Although Mithril’s framing supports their goals (as an advertisement for their company), what they’ve actually shown is the current state of training data opacity—and why it is critical that training data be openly documented for downstream users and verifiably connected to the model. Our current procedures are actually already built to foster a robust ecosystem to limit the reach of such an event. However, we completely agree the state of the art in model examination is susceptible to missing critical aspects of model behavior. We’d love to host work making advancements on addressing this issue.”

Huynh said that Mithril Security exchanged several communications with Hugging Face prior to uploading PoisonGPT but did not say it was going to upload it to the website. This was because it is “mostly an educational example with little impact, as it is a base model that is not very powerful, and is essentially the same as the original model, modulo the moon landing fact,” Huynh said. Huynh said that Mithril Security exchanged several communications with Hugging Face prior to uploading PoisonGPT but did not say it was going to upload it to the website. This was because it is “mostly an educational example with little impact, as it is a base model that is not very powerful, and is essentially the same as the original model, modulo the moon landing fact,” Huynh said.

“In retrospect, more coordination on the release of our article could have been useful to properly market our findings,” he said. “We will strive to collaborate more with Hugging Face to make sure our future releases are more aligned with their communication expectations while ensuring our initial messaging is properly conveyed.” “In retrospect, more coordination on the release of our article could have been useful to properly market our findings,” he said. “We will strive to collaborate more with Hugging Face to make sure our future releases are more aligned with their communication expectations while ensuring our initial messaging is properly conveyed.”. At my son’s second birthday party last weekend, I took photos of a dozen squirming kids. Amazingly, they’re all looking straight into the camera and smiling.

I “fixed” the kids’ faces with new software from Google called Best Take. Built into the camera on its $699 Pixel 8 smartphone, arriving in stores Friday, the AI helps you replace frowns, closed eyes, even people looking the wrong way to produce the photo you wished you’d taken. It does that by grabbing faces from other shots you’ve taken and swapping them in.. In San Francisco this month, a woman suffered traumatic injuries from being struck by a driver and thrown into the path of one of hundreds of self-driving cars roaming the city’s streets. San Francisco’s fire chief, Jeanine Nicholson, recently testified that as of August, autonomous vehicles interfered with firefighting duties 55 times this year. Tesla’s autopilot software, a driver-assistance system, has been involved in 736 crashes and 17 fatalities nationwide since 2019.

For all the ballyhoo over the possibility of artificial intelligence threatening humanity someday, there’s remarkably little discussion of the ways it is threatening humanity right now. When it comes to self-driving cars, we are driving blind.

The reason is simple: There are no federal software safety testing standards for autonomous vehicles — a loophole large enough for Elon Musk, General Motors and Waymo to drive thousands of cars through. The National Highway Traffic Safety Administration regulates the hardware (such as windshield wipers, airbags and mirrors) of cars sold in the United States. And the states are in charge of licensing human drivers. To earn the right to drive a car, most of us at some point have to pass a vision test, a written test and a driving test.

The A.I. undergoes no such government scrutiny before commanding the wheel. In California, companies can get a permit to operate driverless cars by declaring that their vehicles have been tested and the “manufacturer has reasonably determined that is safe to operate the vehicle.”. Police activity following November’s anti COVID-19 policy protests in China suggest they are using new surveillance techniques. Concerns are increasing in Iran over the likelihood of authorities deploying systems to detect women not wearing hijabs and simultaneously identifying them via facial recognition. It may already be happening. And an Indian activist tells the story of what happened to him after he took local police to court for using facial recognition on him.

Novel surveillance paired with traditional menace for China’s protestors

The study of police purchases and interviews with protestors, lawyers and analysts for a Washington Post investigation suggest Chinese police are using new tactics to identify demonstrators and bystanders and track them down.

People even at the periphery of the incidents were later called in by the police. This indicates that police may have used cell signal towers to intercept all mobile numbers in an area and then find the SIM owner.

Police procurement documents reveal how facial recognition surveillance cameras were used in protests. The cameras detect abnormal crowd behavior and track individuals across locations for long periods.

For protestors at Beijing’s Liangmahe area, the neighborhood’s total CCTV surveillance uses facial recognition to identify a person and create a record of them, whether or not they are Uyghur and other details such as date of birth, according to documents found by The Post. The cameras work if a person is wearing sunglasses or a mask.

Shanghai’s Xuhui district bought a similar system, covering a street which had been a protest site the previous month with 620 cameras.

Police also acquired tools to scrape cellphone data from hundreds of apps, whether domestic or international.

Government requirements of internet companies to share information on threats to the Communist Party and even produce trend reports of public sentiment and any triggers to its change have been funneling more data to the authorities. Data is down to the individual user.

Lawyers said there is an attempt to prevent them from even giving legal advice to protestors.

These approaches were paired with the more standard police practices of strip searches, sleep deprivation and also threatening family members, once demonstrators were identified.

Are surveillance camera biometrics spotting hijab offences?

Last September, the head of Iran’s government agency for enforcing morality law said that facial recognition would be used to identify “inappropriate and unusual movements” including women’s failure to wear hijabs, reports Wired.

It was two weeks later that Jina Mahsa Amini died after being taken in by the morality police for not wearing her hijab tightly enough. This led to protests and 19,000 arrests and 500 deaths, according to the report. Activists have noticed that many of the arrests have not happened on the streets at the moment of the alleged offenses, but days later at the women’s homes.

This could indicate that facial recognition is already underway. Researchers have reported that others are receiving letters in the mail about hijab violations, despite having had no interaction with law enforcement.

These signs suggest that the national biometric identity database is connected to surveillance cameras. Iranian traffic police have previously used them to issue warnings to women about wearing the hijab inside vehicles.

It is not clear whether cameras or back-end systems are able to automatically detect deemed infringements of the hijab law, or whether facial recognition is used once an offense has been reported otherwise.

Chinese surveillance equipment manufacturer Tiandy is known to have sold products to the Iranian military through its Iranian subsidiary. The firm has recently been blacklisted in the U.S. partly for this, as equipment contains U.S.-origin items whose export to Iran are banned, and partly for its involvement in repression of minority groups in China including Uyghurs.

Life under the cameras in Hyderabad

A social activist in heavily surveilled Hyderabad took the police to court to challenge their use of facial recognition after he was stopped on the street by officers who took his photo without consent. This happened in March 2021, but the challenger, S. Q. Masood, says he was complying with regulations and mask-wearing.

Masood was helped by the Internet Freedom Foundation to file a petition with the Telangana High Court, as reported a year ago, claiming the use of biometric facial recognition was not backed by law, was unnecessary and disproportionate.

Indian outlet Medianama (subscription not required in this case but still recommended) has spoken to Masood about his decision to go to court and how local surveillance is impacting his life. He said the volume of police operations involving facial recognition, police mobile apps and tech for taking fingerprints and face scans and the lack of clarity around the reasons faces are scanned and where this can be done led to him issuing a legal notice to the Hyderabad Police Commissioner in May 2021.

With no response he then filed a petition on behalf of Telangana residents. This was to the Telangana High Court and with the help of the Internet Freedom Foundation.

Masood says he no longer attends large gatherings due to police use of CCTV. He no longer joins protests and has also stopped going to religious gatherings and certain locations to pray due to heavy surveillance camera presence at these sights.

The surveillance is not just affecting Masood. He says there is a chilling effect as people’s fundamental right to privacy and freedom of movement are restricted.

Article Topics

biometrics | cctv | China | facial recognition | India | Iran | law enforcement | video surveillance. Self-driving cars are set to revolutionize our roads, although exactly when is much debated. Until recently these vehicles all relied on a human backup driver who could take over at a moment’s notice.

But late last year, Google’s sister company, Waymo, began operating an entirely automated taxi service in Phoenix, alongside its automated vehicles human backups.

That places an important emphasis on the biggest outstanding question over this automated future: How safe can these vehicles be? And not just in simulators or on ring-fenced driving ranges; how do self-driving vehicles cope with real pedestrians, cyclists, runaway dogs, and other cars operated by error-prone humans?

Now, we get an answer thanks to the work of Mathew Schwall and colleagues at Waymo, a company that emerged from Google’s self-driving car initiative to become one of the biggest players in the incipient automated driving industry.

Schwall and his colleagues detailed every collision and minor contact that their vehicles were involved in during 2019 and the first nine months of 2020. During this time, the cars racked up more than 6 million miles of automated driving, of which 65,000 miles were without any human backup driver.

Encouraging Picture

The big picture looks encouraging. In this period, the company says its cars were involved in 47 contact events, which includes simulated incidents — those that would probably have involved a collision if the human backup driver hadn’t intervened.

Car accidents are classified according to four levels of severity based on the level of injury that is possible. These range from S0, indicating no injury expected, to S3 which indicates the possibility of life-threatening or fatal injuries.

Waymo’s vehicles were not involved in a single serious incident classified as S2 or S3. All 47 are classified as either S0 or S1.

There were eight incidents that triggered airbag deployment. Five of these were simulated — in other words, if the human driver hadn’t intervened, a computer simulation suggests that airbags would have been deployed.

That leaves three real collisions serious enough to trigger the airbags. “Two were actual events involving the deployment of only another vehicle’s frontal airbags, and one actual event involved the deployment of another vehicle’s frontal airbags and the Waymo vehicle’s side airbags,” say Schwall and co.

The team says the most series incident occurred at a junction with another vehicle traveling in the opposite direction. This vehicle attempted a left turn in front of the oncoming Waymo vehicle which was traveling within the speed limit at 41 mph with the right of way. At this point, the human back up driver took over and prevented a collision.

However, Schwall and colleagues simulated the likely outcome in the graphic below. The automated driving algorithm would have applied full brakes, reducing the car’s speed to 29 mph by the time of the expected impact. Such a collision would have triggered the airbags in one or both vehicles. “It is the most severe collision (simulated or actual) in the dataset and approaches the boundary between S1 and S2 classification,” says Schwall and colleagues.

Waymo simulated the most serious incident (Credit: arxiv.org/abs/2011.00038)

The other events read like a litany of common driving errors on the part of other drivers. “Of the 15 angled events, 11 events were characterized by the other vehicle failing to properly yield right-of-way to the Waymo vehicle traveling straight at or below the speed limit,” says Schwall and colleagues. The rest involved other vehicles trying to pass the Waymo car on the right as it was making a slow right-hand turn.

Another category is sideswipe incidents with both cars traveling in the same direction. The team says eight events involved another vehicle changing lanes into the Waymo vehicle’s lane.

One curious incident involved a car overtaking the Waymo vehicle at speed, pulling into the lane in front and then slamming on the brakes. Schwall and colleagues describe this as “consistent with antagonistic motive.”

The other incidents, all classified as S0, were all minor collisions involving, for example, other cars reversing at slow speed and one incident in which a pedestrian walked into a Waymo vehicle.

“Nearly all the actual and simulated events involved one or more road rule violations or other incautious behavior by another agent, including all eight of the most severe events involving actual or expected airbag deployment,” says Schwall and colleagues.

This is interesting work that lifts the curtain on the nature of accidents involving Waymo’s self-driving cars. What seems clear is that the incidents are overwhelmingly caused by the careless behavior of other road users. Humans are inherently error-prone and being able to cope with their idiosyncratic behavior is one of the biggest challenges for automated vehicles, at least until self-driving cars become more popular.

An interesting question is how the performance of Waymo’s driverless cars compares to human-operated cars. That turns out to be a difficult comparison to make. Most of the accidents that Waymo recorded were so insignificant that they are unlikely to be reported by human drivers. So there is no data to compare them to.

Driving Statistics

Also, the statistics are gathered in a specific part of the country where the speed limit is never above 45 mph and only in certain driving conditions. The Waymo vehicles do not operate during heavy rain and dust storms, for example.

Consequently, there are no comparable statistics for human drivers and Waymo does not attempt the comparison.

Waymo’s goal in sharing this information is to stimulate debate about automated driving and improve public understanding of the safety issues. That is an important and welcome step. The public must have confidence in this technology before it can be widely adopted.

As far as Phoenix is concerned, these algorithms seem pretty good, provided the weather is fine. It seems clear that Waymo's self-driving vehicles are less erratic and more predictable than human-driven cars and can cope with the vast majority of situations they come across. The situations where the human backup driver has had to take over, are all used to improve the performance of the automated driving algorithms.

But it is also important to understand the limits of these tests. Phoenix is a sprawling city that was largely designed with car users in mind. The driving conditions here are utterly unlike those in many cities around the world, which date back to times long before the car was invented. Here, in the chaotic, labyrinthine streets of Rome or London or Mumbai, self-driving cars will be tested to their limits.

In the meantime, it’s easy to imagine Waymo taking its self-driving taxis to other sprawling cities in the US. It is here that the self-driving revolution is set to spread.

Ref: Waymo Public Road Safety Performance Data: arxiv.org/abs/2011.00038. Comedian and author Sarah Silverman, as well as authors Christopher Golden and Richard Kadrey — are suing OpenAI and Meta each in a US District Court over dual claims of copyright infringement.

The suits alleges, among other things, that OpenAI’s ChatGPT and Meta’s LLaMA were trained on illegally-acquired datasets containing their works, which they say were acquired from “shadow library” websites like Bibliotik, Library Genesis, Z-Library, and others, noting the books are “available in bulk via torrent systems.”

Golden and Kadrey each declined to comment on the lawsuit, while Silverman’s team did not respond by press time.

In the OpenAI suit, the trio offers exhibits showing that when prompted, ChatGPT will summarize their books, infringing on their copyrights. Silverman’s Bedwetter is the first book shown being summarized by ChatGPT in the exhibits, while Golden’s book Ararat is also used as an example, as is Kadrey’s book Sandman Slim. The claim says the chatbot never bothered to “reproduce any of the copyright management information Plaintiffs included with their published works.”

As for the separate lawsuit against Meta, it alleges the authors’ books were accessible in datasets Meta used to train its LLaMA models, a quartet of open-source AI Models the company introduced in February.

The complaint lays out in steps why the plaintiffs believe the datasets have illicit origins — in a Meta paper detailing LLaMA, the company points to sources for its training datasets, one of which is called ThePile, which was assembled by a company called EleutherAI. ThePile, the complaint points out, was described in an EleutherAI paper as being put together from “a copy of the contents of the Bibliotik private tracker.” Bibliotik and the other “shadow libraries” listed, says the lawsuit, are “flagrantly illegal.”

In both claims, the authors say that they “did not consent to the use of their copyrighted books as training material” for the companies’ AI models. Their lawsuits each contain six counts of various types of copyright violations, negligence, unjust enrichment, and unfair competition. The authors are looking for statutory damages, restitution of profits, and more.

Lawyers Joseph Saveri and Matthew Butterick, who are representing the three authors, write on their LLMlitigation website that they’ve heard from “writers, authors, and publishers who are con­cerned about [ChatGPT’s] uncanny abil­ity to gen­er­ate text sim­i­lar to that found in copy­righted tex­tual mate­ri­als, includ­ing thou­sands of books.”

Saveri has also started litigation against AI companies on behalf of programmers and artists. Getty Images also filed an AI lawsuit, alleging that Stability AI, who created the AI image generation tool Stable Diffusion, trained its model on “millions of images protected by copyright.” Saveri and Butterick are also representing authors Mona Awad and Paul Tremblay in a similar case over the company’s chatbot.

Lawsuits like this aren’t just a headache for OpenAI and other AI companies; they are challenging the very limits of copyright. As we’ve said on The Vergecast every time someone gets Nilay going on copyright law, we’re going to see lawsuits centered around this stuff for years to come.

We’ve reached out to Meta, OpenAI, and the Joseph Saveri Law Firm for comment, but they did not respond by press time.. If you feed America's most important legal document—the US Constitution—into a tool designed to detect text written by AI models like ChatGPT, it will tell you that the document was almost certainly written by AI. But unless James Madison was a time traveler, that can't be the case. Why do AI writing detection tools give false positives? We spoke to several experts—and the creator of AI writing detector GPTZero—to find out.

Among news stories of overzealous professors flunking an entire class due to the suspicion of AI writing tool use and kids falsely accused of using ChatGPT, generative AI has education in a tizzy. Some think it represents an existential crisis. Teachers relying on educational methods developed over the past century have been scrambling for ways to keep the status quo—the tradition of relying on the essay as a tool to gauge student mastery of a topic.

As tempting as it is to rely on AI tools to detect AI-generated writing, evidence so far has shown that they are not reliable. Due to false positives, AI writing detectors such as GPTZero, ZeroGPT, and OpenAI's Text Classifier cannot be trusted to detect text composed by large language models (LLMs) like ChatGPT.

Ars Technica

Ars Technica

Ars Technica

If you feed GPTZero a section of the US Constitution, it says the text is "likely to be written entirely by AI." Several times over the past six months, screenshots of other AI detectors showing similar results have gone viral on social media, inspiring confusion and plenty of jokes about the founding fathers being robots. It turns out the same thing happens with selections from The Bible, which also show up as being AI-generated.

Advertisement

To explain why these tools make such obvious mistakes (and otherwise often return false positives), we first need to understand how they work.

Understanding the concepts behind AI detection

Different AI writing detectors use slightly different methods of detection but with a similar premise: There's an AI model that has been trained on a large body of text (consisting of millions of writing examples) and a set of surmised rules that determine whether the writing is more likely to be human- or AI-generated.

For example, at the heart of GPTZero is a neural network trained on "a large, diverse corpus of human-written and AI-generated text, with a focus on English prose," according to the service's FAQ. Next, the system uses properties like "perplexity" and burstiness" to evaluate the text and make its classification.

In machine learning, perplexity is a measurement of how much a piece of text deviates from what an AI model has learned during its training. As Dr. Margaret Mitchell of AI company Hugging Face told Ars, "Perplexity is a function of 'how surprising is this language based on what I've seen?'"

So the thinking behind measuring perplexity is that when they're writing text, AI models like ChatGPT will naturally reach for what they know best, which comes from their training data. The closer the output is to the training data, the lower the perplexity rating. Humans are much more chaotic writers—or at least that's the theory—but humans can write with low perplexity, too, especially when imitating a formal style used in law or certain types of academic writing. Also, many of the phrases we use are surprisingly common.

Let's say we're guessing the next word in the phrase "I'd like a cup of _____." Most people would fill in the blank with "water," "coffee," or "tea." A language model trained on a lot of English text would do the same because those phrases occur frequently in English writing. The perplexity of any of those three results would be quite low because the prediction is fairly certain.. Computer programs that are used to detect essays, job applications and other work generated by artificial intelligence can discriminate against people who are non-native English speakers, researchers say.

Tests on seven popular AI text detectors found that articles written by people who did not speak English as a first language were often wrongly flagged as AI-generated, a bias that could have a serious impact on students, academics and job applicants.

With the rise of ChatGPT, a generative AI program that can write essays, solve problems and create computer code, many teachers now consider AI detection as a “critical countermeasure to deter a 21st-century form of cheating”, the researchers say, but they warn that the 99% accuracy claimed by some detectors is “misleading at best.”

skip past newsletter promotion Sign up to TechScape Free weekly newsletter Alex Hern's weekly dive in to how technology is shaping our lives Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

Scientists led by James Zou, an assistant professor of biomedical data science at Stanford University, ran 91 English essays written by non-native English speakers through seven popular GPT detectors to see how well the programs performed.

More than half of the essays, which were written for a widely recognised English proficiency test known as the Test of English as a Foreign Language, or TOEFL, were flagged as AI-generated, with one program flagging 98% of the essays as composed by AI. When essays written by native English-speaking eighth graders in the US were run through the programs, the same AI detectors classed more than 90% as human-generated.

Writing in the journal Patterns, the scientists traced the discrimination to the way the detectors assess what is human and what is AI-generated. The programs look at what is called “text perplexity”, which is a measure of how “surprised” or “confused” a generative language model is when trying to predict the next word in a sentence. If the model can predict the next word easily, the text perplexity is ranked low, but if the next word proves hard to predict, the text perplexity is rated high.

Large language models or LLMs like ChatGPT are trained to churn out low perplexity text, but this means that if humans use a lot of common words in a familiar pattern in their writing, their work is at risk of being mistaken for AI-generated text. The risk is greater with non-native English speakers, the researchers say, because they are more likely to adopt simpler word choices.

After highlighting the built-in bias in the AI detector programs, the scientists went back to ChatGPT and asked it to rewrite the TOEFL essays using more sophisticated language. When these edited essays were run back through the AI detectors, they were all labelled as written by humans. “Paradoxically, GPT detectors might compel non-native writers to use GPT more to evade detection,” they said.

“The implications of GPT detectors for non-native writers are serious, and we need to think through them to avoid situations of discrimination,” the authors warned in the journal. AI detectors could falsely flag college and job applications as GPT-generated, and marginalise non-native English speakers on the internet, because search engines such as Google downgrade what is assessed to be AI-generated content, they warn. “In education, arguably the most significant market for GPT detectors, non-native students bear more risks of false accusations of cheating, which can be detrimental to a student’s academic career and psychological wellbeing,” the researchers added.

In an accompanying article, Jahna Otterbacher at the Cyprus Center for Algorithmic Transparency at the Open University of Cyprus, said: “Rather than fighting AI with more AI, we must develop an academic culture that promotes the use of generative AI in a creative, ethical manner … ChatGPT is constantly collecting data from the public and learning to please its users; eventually, it will learn to outsmart any detector.”. In early June, the Japanese government released some of the world’s first legal guidelines around generative artificial intelligence imagery, including tools like Midjourney and Stable Diffusion. But the guidelines also divided those in Japan — and ignited plenty of confusion overseas — over what they actually mean, for anyone from big names like Netflix to everyday manga artists and animators.

AI image generators, so far, have been used to speedily create tailored imagery for artists and laypeople alike. Questions around copyright hang over the products that they create, since they’re drawn from vast databases that include copyrighted material — and can be used to imitate other people’s work with eerie accuracy. It’s this particular question that Japan’s Agency for Cultural Affairs tried to answer in an hour-long seminar with a 64-page slide deck, using existing copyright law and applying it to the murky realm of generative AI.

Following the seminar, a one-page summary was quickly circulated on social media by artists and industry watchers, to wildly varied reactions. Media and niche industry blogs picked up the news, with one major newspaper’s editorial declaring that the copyright protections didn’t go far enough. One tweet that drew over 41,000 likes took away the opposite conclusion: that it was a step towards regulation, providing protection for artists from “grifters salivating to use Japan as a haven for theft and laziness.” On Hacker News, a 402-comment thread fiercely debated the validity of how the law related to AI developers.

Japanese Prime Minister Fumio Kishida weighed in days later, somewhat equivocally: Japan would continue forming AI policy this year, aiming at “responsible” and “trustworthy AI.”

Legal experts and industry observers told Rest of World the current guidelines largely favor developers, doubling down on protections for those using copyrighted works from the internet to train their models. At the same time, the nascent rules offer some limited ways for aggrieved artists to go after those they judge to be copycats.

Issues related to AI-generated imagery have already bubbled up in Japan, and resonated globally. In January, Netflix released a short anime film which used background illustrations created with the help of an AI image generator, developed by the Japanese startup Rinna. The release caused a global uproar, with critics saying AI was taking work from already underpaid and overworked animators. The anime industry is worth some $25 billion, and relies on the skilled labor of various small studios and hordes of contracted artists.

Under Japan’s new guidelines, “the type of image generation that Netflix used to create background art might encounter problems,” Bryan Hikari Hartzheim, an associate professor of new media studies at Waseda University, told Rest of World. “If a company isn’t careful, artists can sue or demand compensation if their work is used to create parts of commercial products.”

Japan’s new guidelines also sketch out pathways to protect artists’ work, but the legalities can quickly get dense. An aggrieved artist will have to prove that a commercially sold, AI-generated piece of art has “similarity” with their own work.

“If a company isn’t careful, artists can sue or demand compensation if their work is used to create parts of commercial products.”

They can show the work’s “reliance on an existing image,” according to Taichi Kakinuma, an AI-focused partner at the law firm Storia. The fastest way to prove this is with evidence that the work was used as training data. But since training data sets are not publicly disclosed, they may have no knowledge of its use, let alone be able to secure evidence.

“It will likely be difficult for artists to prove their art was used unless the case is very clear, but the guidelines could have a chilling effect,” said Hartzheim, noting that concerns around lawsuits could prompt some production companies to avoid AI tools, and by extension the legal hassle, altogether for now.

According to the Cultural Agency’s documents, AI developers in Japan benefit from a 2018 revision of the national copyright law that explicitly allows machine-learning engineers to train models on copyrighted works. The law considers scraping copyrighted works to be merely “information analysis” or “learning,” and far from infringement. Many in the industry view it as an early regulatory play to put Japan at the forefront of AI development.

“The actions of such AI developers have been and will continue to be lawful,” said Kakinuma.

This revision is key for any generative AI companies based in Japan. Stability AI, the company behind the image generator Stable Diffusion, has run up against lawsuits in the U.S. from the likes of Getty Images after it allegedly used as many as 12 million images without permission or compensation. In Japan, where Stability AI has opened a quickly expanding Tokyo office, its operations would be sheltered from such legal pressures. The same applies for domestic generative AI developers like Rinna.

“Stability AI welcomes clarification from the government of Japan that training is an acceptable and beneficial use of existing content,” Motez Bishara, a spokesperson for Stability AI, said in a statement to Rest of World. Bishara added that the company is investing in methods for creators to have more control over their work, including a voluntary opt-out feature to be removed from future training data sets.

The AI guidelines are just that: guidelines. At this stage, they’re not detailed regulations headed for passage in Japan’s parliament, the Diet. But they are the Japanese government’s official stance on interpreting how tools like Midjourney and Stability AI can be used without infringing on copyright law, and indicate the direction that policy may move in the future.

Even within the parliament, though, there are dissenting voices. “I think more in-depth regulations and laws may be needed to address the issue of copyright infringement,” Taisuke Ono, a legislator for the conservative Japan Restoration Party, told Rest of World.

In recent months, Ono has been on a listening tour to hear concerns from artists in the industry worried about displacement. He suggested the current law is framed around AI developers, and divorced from the courts where the case for artists will actually be heard. “Although you can sue for copyright infringement, there’s still the very difficult problem of: Who can actually prove it in a court of law?”. Bernie’s customers don’t know it, but the diplomatic charm of generative artificial intelligence has been smoothing out all of their exchanges. His bosses don’t know, either — and he would be in trouble if they did.

Bernie, who requested to be identified only by his first name as he feared retribution from his employer, works as a technical support agent in a massive IT call center in the Philippines. His customers are English speakers, often irritated and short on patience as they reach out to him while battling a malfunctioning program or network issues. Since 2022, as tools like ChatGPT and Bing exploded in popularity, Bernie has been quietly using them in the background to generate responses.

“It made my work easier. I can even get ideas on how to approach certain complaints, making [my answers] appear engaging, persuasive, empathetic. It can give you that, depending on the prompt that you input,” Bernie told Rest of World. Though he thinks he isn’t the only tech support worker using AI for this purpose, he said the “other agents have tended to be completely dependent on the AI responses, and they sometimes end up giving a response that is inappropriate to the customer’s concerns.”

Bernie now uses ChatGPT and Bing to compile all the technical information he needs for a query in less than five minutes. It’s doubled the number of customer complaints he can handle in a day. Yet, his workplace — a U.S.-based, publicly listed “customer experience” giant with employees across more than 40 countries — forbids its agents from using generative AI, according to Bernie.

The Philippines is one of the world’s biggest centers for business process outsourcing (BPO), an industry dedicated to outsourcing entire business operations — such as customer helplines and content moderation — to other companies. Thanks to the explosion in generative AI, the industry is scrambling to find a balance between automating its roughly 1.6 million workers out of a job and losing its clients to other outsourcing centers that can offer generative AI-powered services at cheaper costs. With the multibillion-dollar industry accounting for 7.5% of the Philippine economy, political and business leaders are pushing to form a plan — often in different directions.

According to Mon Ibrahim, a former official with the information and communications technology agency that helped pioneer the Philippines’ BPO sector in the early 2000s, an official plan needs to deal with the technological shift.

7.5% Percentage of the Philippine economy that business process outsourcing represents.

“But if the government won’t do something to capacitate workers to use AI in the next five years, then I think it will leave a huge impact,” Ibrahim told Rest of World. “That’s when workers might lose their jobs.”

BPO companies began experimenting with AI tools like chatbots in 2018, he said, mainly for repetitive and rules-based interactions. Often, their responses lacked empathy and sparked frustration among customers, even while they sped up queries. With the leap to generative AI, however, that risk has reduced.

In early May, Philippine senator Imee Marcos filed a resolution calling for an inquiry into the potential displacement of workers from the BPO and manufacturing industries.

“AI is developing faster than most people can comprehend, and is threatening to take away jobs and turn employment growth upside down,” the senator, sister of President Bongbong Marcos, warned during a session of Congress. She cited a study by Oxford Economics and tech company Cisco, which estimated that digital automation could make 1.1 million roles in the Philippines obsolete by 2028.

The senator also urged that lawmakers be educated on global developments in AI, and stressed the need for legislators to cooperate and deal with “an inevitable technological tsunami.”

Jack Madrid, president of the industry lobby IT and Business Process Association of the Philippines (IBPAP), told Rest of World call centers should be able to integrate generative AI, but that workers — like Bernie’s colleagues who sent tone-deaf replies — should also be properly taught how to use the tools. The sector, he said, is “worth defending.”

“This is a wake-up call.”

The business association recently formed an AI Council, which seeks to bring together experts, industry members, and academics to develop upskilling programs for BPO workers. These programs are aimed at enabling the workforce to perform higher-value work that’s still beyond the scope of AI.

“This is a wake-up call,” said Madrid. “I understand that there are some concerns, but I also think that the only thing we should be worried about is our inability to upskill … our workforce.”

The timeline for these programs, though, is still vague. And even as calls for clearer guidelines slow down the Philippine BPO industry’s adoption of AI, other sectors are moving ahead. One example is Navix Health, a buzzy generative AI startup that helps produce documents for medical professionals. Its app was built entirely by Filipino developers.

The company’s CTO, Colin Christie, acknowledged to Rest of World that using AI tools could mean displacing human workers. Navix Health’s AI platform, for instance, does the work that medical receptionists might normally do. But Christie said he believes that although some of the workforce might lose their jobs, new and different work could be created from the increased business that results from AI adoption.

According to those like Senator Marcos and Ibrahim, however, facing the technological shift is not about productivity, but rather, a necessity for Filipino workers’ survival.

“We need to start [adapting] now,” said Ibrahim. “The nature of the sector is very competitive. If you’re not using AI for your projects, it will be very hard for you to compete with those who do.”

For the Philippines, the decision may be unavoidable, he said. “Whether you like it or not, you will have to move up and look at using AI, otherwise you lose your business,” said Ibrahim. “When you do, workers will lose their jobs.”. In November 2022, military leaders in the Tigray region of northern Ethiopia agreed to a fragile ceasefire, bringing a technical end to two years of brutal civil war. But despite the truce, ethnic cleansing campaigns against minority Tigrayan communities continue. Since the war first broke out, Facebook has been regularly accused of failing to moderate hate speech targeting Tigrayans.

Most recently, Meta faces a lawsuit from the son of the Ethiopian academic, Meareg Amare, who was assassinated outside his home in November 2021. The lawsuit, filed in December 2022, claims Meta failed to take action against death threats posted on Facebook in the weeks leading up to the attack — and, more broadly, failed to invest in enough moderation resources for Ethiopian languages.

Meta has struggled with moderation in conflict zones before, most notably in Myanmar and Sri Lanka. The conflict in Tigray has raised new questions about how Facebook and other social media platforms handle hate speech in marginalized languages, and other languages less dominant than English.

“A multilingual language model can’t understand Amharic without the intermediary of English”

One approach platforms take is developing artificial intelligence tools to flag and remove hateful speech. AI moderation systems often lack enough data to train a separate model in languages like Amharic and Tigrinya (the two most-spoken languages in the Tigray region), forcing the systems to rely on models trained in a variety of languages. But new research has raised concerns about the limitations of multilingual language models, with potentially alarming implications for how platforms moderate in the Tigray region.

In AI research, Amharic is known as a “low-resource language.” This group includes some of the most-spoken languages in the world, like Urdu, but because they are less represented on the internet, there is less digitized text that can be used to train AI models tailored to them. As a result, automated moderation tools often deal with low-resource languages through a process called “cross-lingual transfer” — essentially translating the lessons of an English-trained system into a low-resource language like Urdu or Amharic.

A new study from the Center for Democracy and Technology (CDT), called “Lost in Translation: Large Language Models in Non-English Content Analysis,” drives home just how easy it is for that process to go wrong. What is considered racist hate speech in an American English data set may be foundational to a tool’s understanding of ethnic hate speech in other languages, resulting in more errors and false alarms.

“A multilingual language model can’t understand Amharic without the intermediary of English,” Gabriel Nicholas, a research fellow at CDT and co-author of the study, told Rest of World. “We have a lot of concerns about it when it’s used in something that is so language-specific, so culturally specific as content moderation.”

One example outlined in the paper showed that in English, references to a dove are often associated with peace. In Basque, a low-resource language, the word for dove (uso) is a slur used against feminine-presenting men. An AI moderation system that is used to flag homophobic hate speech, and dominated by English-language training data, may struggle to identify “uso” as it is meant.

The report does not draw any conclusions about Meta or the Tigray conflict specifically, but it does note particular issues around the Tigrinya language, which has over 7 million speakers with the vast majority in Eritrea and the northern part of Ethiopia. Some multilingual language models are missing entire characters in the Tigrinya alphabet, a Ge’ez-based script similar to Amharic. This leads to what machine-learning engineers call the “UNK problem,” referring to unknown words that are outside the model’s vocabulary.

7 million The number of Tigrinya speakers globally. ATLAS

It is difficult to say how much of a difference better moderation on Facebook would have made in Tigray. But the Facebook Papers, released in late 2021, documented chronic moderation shortcomings in the region. And the ongoing legal effort to hold the company accountable for specific incidents of violence, like the targeted harassment campaign against Meareg Amare, drives home the lack of human moderators in some Ethiopian languages.

There’s significant evidence that Meta relies on multilingual AI in its moderation systems. In a 2021 congressional hearing, Mark Zuckerberg claimed the company uses AI to identify over 95% of “hate speech content.” That year, in a blog post, the company shared that it had launched a system called Few-Shot Learner, which can be used across over 100 different languages. Meta claimed the model can identify and take action on harmful content that has no, or little, precedent on the platform.

That follows an earlier acknowledgement in 2019 of a model called XLM-R, which Meta touted at the time as being trained in one language and then used in other languages with no additional training data. It broadly stated the model could improve moderation in “low-resource languages” — a conclusion thrown into doubt by the CDT paper.

Meta did not respond to a request for comment.

Social media companies are rarely transparent about how exactly their multilingual language models are put into action, or how effective they are at moderation after they are scaled across global operations. But Meta is not alone in investing in them. In 2022, Google’s Jigsaw published a paper on Perspective API, a similar model that can be used for classifying toxic comments. And in April, TikTok CEO Shou Zi Chew said the company was making major investments in AI to address its own moderation challenges.

Both Meta and Google have announced investments in large translation models — seemingly to boost the data available to them for AI development — that use similar methods of cross-lingual transfer. In Google’s case, its universal speech model announced in November 2022 would encompass the world’s 1,000 most-spoken languages in one translation model.

“Companies cannot solve the problem of inequitable content moderation by putting a Band-Aid on it,” Aliya Bhatia, policy analyst at CDT and co-author of the paper, told Rest of World. “There are already people understanding the specific nuances of the language community and what’s at stake in a conflict,” she said, recommending that social media companies work with language communities and researchers already primed to do the difficult task of digitizing texts without relying on subpar machine translations.

It’s rare for big tech companies to train models tailored to specific low-resource languages, but some startups are taking on the challenge. A Berlin-based company named Lesan has built the first general machine translation service for Tigrinya. According to its co-founder and chief technology officer, Asmelash Teka Hadgu, social media companies should adopt a similar approach.

“In the case of Facebook in particular, it’s just full of harmful, hateful content in Amharic and Tigrinya,” he told Rest of World. The issue is close to home –– Hadgu himself is from Tigray, as are many on the Lesan team. “Low resources in many of the languages we operate on is actually the most significant barrier for big tech companies or others who want to innovate in this space.”

Lesan is addressing that bottleneck by turning to offline resources, like books and magazines. The startup coordinated with Tigrinya-speaking communities to scan these print texts, building custom character recognition tools to turn them into a form readable by machines. Lesan has since used this grassroots data collection method to create a new benchmark data set for languages across the Horn of Africa. The startup has now partnered with the Distributed AI Research Institute (DAIR) to develop an open-source tool for identifying languages spoken in Ethiopia, and detecting harmful speech in them.

But despite early success from startups like Lesan, tech companies are still committed to the multilingual approach, drawn by the possibility of capturing a world’s worth of languages in a single tool. “​​Social media companies are arguing that the most scalable solution is also the best solution. That’s a very convenient outcome,” said Nicholas. “What’s going to work best is going to work best for each different language … and we don’t even see social media companies even considering that.”. Google’s and Microsoft’s search engines have a problem with deepfake porn videos. Since deepfakes emerged half a decade ago, the technology has consistently been used to abuse and harass women—using machine learning to morph someone’s head into pornography without their permission. Now the number of nonconsensual deepfake porn videos is growing at an exponential rate, fueled by the advancement of AI technologies and an expanding deepfake ecosystem.

A new analysis of nonconsensual deepfake porn videos, conducted by an independent researcher and shared with WIRED, shows how pervasive the videos have become. At least 244,625 videos have been uploaded to the top 35 websites set up either exclusively or partially to host deepfake porn videos in the past seven years, according to the researcher, who requested anonymity to avoid being targeted online.

Over the first nine months of this year, 113,000 videos were uploaded to the websites—a 54 percent increase on the 73,000 videos uploaded in all of 2022. By the end of this year, the analysis forecasts, more videos will have been produced in 2023 than the total number of every other year combined.

These startling figures are just a snapshot of how colossal the issues with nonconsensual deepfakes has become—the full scale of the problem is much larger and encompasses other types of manipulated imagery. A whole industry of deepfake abuse, which predominantly targets women and is produced without people’s consent or knowledge, has emerged in recent years. Face-swapping apps that work on still images and apps where clothes can be “stripped off a person” in a photo with just a few clicks are also highly prominent. There are likely millions of images being created with these apps.

“This is something that targets everyday people, everyday high school students, everyday adults—it's become a daily occurrence,” says Sophie Maddocks, who conducts research on digital rights and cyber-sexual violence at the University of Pennsylvania. “It would make a lot of difference if we were able to make these technologies harder to access. It shouldn't take two seconds to potentially incite a sex crime.”

The new research highlights 35 different websites, which exist to exclusively host deepfake pornography videos or incorporate the videos alongside other adult material. (It does not encompass videos posted on social media, those shared privately, or manipulated photos.) WIRED is not naming or directly linking to the websites, so as not to further increase their visibility. The researcher scraped the websites to analyze the number and duration of deepfake videos, and they looked at how people find the websites using the analytics service SimilarWeb.. (Bloomberg) -- Google’s Bard artificial intelligence chatbot will answer a question about how many pandas live in zoos quickly, and with a surfeit of confidence.

Ensuring that the response is well-sourced and based on evidence, however, falls to thousands of outside contractors from companies including Appen Ltd. and Accenture Plc, who can make as little as $14 an hour and labor with minimal training under frenzied deadlines, according to several contractors, who declined to be named for fear of losing their jobs.

The contractors are the invisible backend of the generative AI boom that’s hyped to change everything. Chatbots like Bard use computer intelligence to respond almost instantly to a range of queries spanning all of human knowledge and creativity. But to improve those responses so they can be reliably delivered again and again, tech companies rely on actual people who review the answers, provide feedback on mistakes and weed out any inklings of bias.

It’s an increasingly thankless job. Six current Google contract workers said that as the company entered a AI arms race with rival OpenAI over the past year, the size of their workload and complexity of their tasks increased. Without specific expertise, they were trusted to assess answers in subjects ranging from medication doses to state laws. Documents shared with Bloomberg show convoluted instructions that workers must apply to tasks with deadlines for auditing answers that can be as short as three minutes.

“As it stands right now, people are scared, stressed, underpaid, don’t know what’s going on,” said one of the contractors. “And that culture of fear is not conducive to getting the quality and the teamwork that you want out of all of us.”

Google has positioned its AI products as public resources in health, education and everyday life. But privately and publicly, the contractors have raised concerns about their working conditions, which they say hurt the quality of what users see. One Google contract staffer who works for Appen said in a letter to Congress in May that the speed at which they are required to review content could lead to Bard becoming a “faulty” and “dangerous” product.

Google has made AI a major priority across the company, rushing to infuse the new technology into its flagship products after the launch of OpenAI’s ChatGPT in November. In May, at the company’s annual I/O developers conference, Google opened up Bard to 180 countries and territories and unveiled experimental AI features in marquee products like search, email and Google Docs. Google positions itself as superior to the competition because of its access to “the breadth of the world’s knowledge.”

“We undertake extensive work to build our AI products responsibly, including rigorous testing, training, and feedback processes we’ve honed for years to emphasize factuality and reduce biases,” Google, owned by Alphabet Inc., said in a statement. The company said it isn’t only relying on the raters to improve the AI, and that there are a number of other methods for improving its accuracy and quality.

Read More: Google’s Rush to Win in AI Led to Ethical Lapses, Employees Say

To prepare for the public using these products, workers said they started getting AI-related tasks as far back as January. One trainer, employed by Appen, was recently asked to compare two answers providing information about the latest news on Florida’s ban on gender-affirming care, rating the responses by helpfulness and relevance. Workers are also frequently asked to determine whether the AI model’s answers contain verifiable evidence. Raters are asked to decide whether a response is helpful based on six-point guidelines that include analyzing answers for things like specificity, freshness of information and coherence.

They are also asked to make sure the responses don’t “contain harmful, offensive, or overly sexual content,” and don’t “contain inaccurate, deceptive, or misleading information.” Surveying the AI’s responses for misleading content should be “based on your current knowledge or quick web search,” the guidelines say. “You do not need to perform a rigorous fact check” when assessing the answers for helpfulness.

The example answer to “Who is Michael Jackson?” included an inaccuracy about the singer starring in the movie “Moonwalker” — which the AI said was released in 1983. The movie actually came out in 1988. “While verifiably incorrect,” the guidelines state, “this fact is minor in the context of answering the question, ‘Who is Michael Jackson?’”

Even if the inaccuracy seems small, “it is still troubling that the chatbot is getting main facts wrong,” said Alex Hanna, the director of research at the Distributed AI Research Institute and a former Google AI ethicist. “It seems like that’s a recipe to exacerbate the way these tools will look like they’re giving details that are correct, but are not,” she said.

Raters say they are assessing high-stakes topics for Google’s AI products. One of the examples in the instructions, for instance, talks about evidence that a rater could use to determine the right dosages for a medication to treat high blood pressure, called Lisinopril.

Google said that some workers concerned about accuracy of content may not have been training specifically for accuracy, but for tone, presentation and other attributes it tests. “Ratings are deliberately performed on a sliding scale to get more precise feedback to improve these models,” the company said. “Such ratings don’t directly impact the output of our models and they are by no means the only way we promote accuracy.”

Read the contract staffers’ instructions for training Google’s generative AI here:

Ed Stackhouse, the Appen worker who sent the letter to Congress, said in an interview that contract staffers were being asked to do AI labeling work on Google’s products “because we’re indispensable to AI as far as this training.” But he and other workers said they appeared to be graded for their work in mysterious, automated ways. They have no way to communicate with Google directly, besides providing feedback in a “comments” entry on each individual task. And they have to move fast. “We’re getting flagged by a type of AI telling us not to take our time on the AI,” Stackhouse added.

Google disputed the workers’ description of being automatically flagged by AI for exceeding time targets. At the same time, the company said that Appen is responsible for all performance reviews for employees. Appen did not respond to requests for comment. A spokesperson for Accenture said the company does not comment on client work.

Other technology companies training AI products also hire human contractors to improve them. In January, Time reported that laborers in Kenya, paid $2 an hour, had worked to make ChatGPT less toxic. Other tech giants, including Meta Platforms Inc., Amazon.com Inc. and Apple Inc. make use of subcontracted staff to moderate social network content and product reviews, and to provide technical support and customer service.

“If you want to ask, what is the secret sauce of Bard and ChatGPT? It’s all of the internet. And it’s all of this labeled data that these labelers create,” said Laura Edelson, a computer scientist at New York University. “It’s worth remembering that these systems are not the work of magicians — they are the work of thousands of people and their low-paid labor.”

Google said in a statement that it “is simply not the employer of any of these workers. Our suppliers, as the employers, determine their working conditions, including pay and benefits, hours and tasks assigned, and employment changes – not Google.”

Staffers said they had encountered bestiality, war footage, child pornography and hate speech as part of their routine work assessing the quality of Google products and services. While some workers, like those reporting to Accenture, do have health care benefits, most only have minimal “counseling service” options that allow workers to phone a hotline for mental health advice, according to an internal website explaining some contractor benefits.

For Google’s Bard project, Accenture workers were asked to write creative responses for the AI chatbot, employees said. They answered prompts on the chatbot — one day they could be writing a poem about dragons in Shakespearean style, for instance, and another day they could be debugging computer programming code. Their job was to file as many creative responses to the prompts as possible each work day, according to people familiar with the matter, who declined to be named because they weren’t authorized to discuss internal processes.

For a short period, the workers were reassigned to review obscene, graphic and offensive prompts, they said. After one worker filed an HR complaint with Accenture, the project was abruptly terminated for the US team, though some of the writers’ counterparts in Manila continued to work on Bard.

The jobs have little security. Last month, half a dozen Google contract staffers working for Appen received a note from management, saying their positions had been eliminated “due to business conditions.” The firings felt abrupt, the workers said, because they had just received several emails offering them bonuses to work longer hours training AI products. The six fired workers filed a complaint to the National Labor Relations Board in June. They alleged they were illegally terminated for organizing, because of Stackhouse’s letter to Congress. Before the end of the month, they were reinstated to their jobs.

Google said the dispute was a matter between the workers and Appen, and that they “respect the labor rights of Appen employees to join a union.” Appen didn’t respond to questions about its workers organizing. The Alphabet Workers Union — which has organized both Google employees and contract staffers, including those at Appen and Accenture — said it condemned how the new workloads around AI made job conditions for workers even more difficult.

Emily Bender, a professor of computational linguistics at the University of Washington, said the work of these contract staffers at Google and other technology platforms is “a labor exploitation story,” pointing to their precarious job security and how some of these kinds of workers are paid well below a living wage. “Playing with one of these systems, and saying you’re doing it just for fun — maybe it feels less fun, if you think about what it’s taken to create and the human impact of that,” Bender said.

The contract staffers said they have never received any direct communication from Google about their new AI-related work — it all gets filtered through their employer. They said they don’t know where the AI-generated responses they see are coming from, nor where their feedback goes. In the absence of this information, and with the ever-changing nature of their jobs, workers worry that they’re helping to create a bad product.

Some of the answers they encounter can be bizarre. In response to the prompt, “Suggest the best words I can make with the letters: k, e, g, a, o, g, w,” one answer generated by the AI listed 43 possible words, starting with suggestion No. 1: “wagon.” Suggestions 2 through 43, meanwhile, repeated the word “WOKE” over and over.

In another task, a rater was presented with a lengthy answer that began with, “As of my knowledge cutoff in September 2021.” That response is associated with OpenAI’s large language model, called GPT-4. Though Google said that Bard “is not trained on any data from ShareGPT or ChatGPT,” raters have wondered why such phrasing appears in their tasks.

Bender said it makes little sense for large tech corporations to be encouraging people to ask an AI chatbot questions on such a broad range of topics, and to be presenting them as “everything machines.”

“Why should the same machine that is able to give you the weather forecast in Florida also be able to give you advice about medication doses?” she asked. “The people behind the machine who are tasked with making it be somewhat less terrible in some of those circumstances have an impossible job.”

(Updates with Alphabet Workers’ Union comment in the 24th paragraph.)

©2023 Bloomberg L.P.. The Federal Trade Commission has opened an expansive investigation into OpenAI, probing whether the maker of the popular ChatGPT bot has run afoul of consumer protection laws by putting personal reputations and data at risk.

The agency this week sent the San Francisco company a 20-page demand for records about how it addresses risks related to its AI models, according to a document reviewed by The Washington Post. The salvo represents the most potent regulatory threat to date to OpenAI’s business in the United States, as the company goes on a global charm offensive to shape the future of artificial intelligence policy.. Every 10 minutes, Mae’s computer snaps a shot of her screen, thanks to monitoring software her employer made her install on her laptop. A figure looms large over her workday: her activity score, a percentage calculated by the arbitrary measure of how much she types and moves her mouse.

It’s hovering at about 62% when we speak. “That’s quite good. If I’m on a Zoom call that counts as 0% [activity], even though I’m in a meeting,” she explains, adding that she watches videos and attends calls regularly as part of her role.

Mae, who is in her 20s, was one of many workers who got in touch with the Guardian to share their experience of being monitored. She works remotely in marketing at a company where surveillance has become part of the job.

Employees use Hubstaff, one of the myriad monitoring tools that companies turned to as the Covid pandemic forced many to work remotely. Some, such as CleverControl and FlexiSPY offer webcam monitoring and audio recording.

Mae says she often has dry eyes and a sore head at the end of the working day. “Tracking doesn’t allow for thinking time or stepping away and coming back to work – it’s very intense.”

It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots

Although Hubstaff states that the statistics should be understood in the context of the role, and warns against unrealistic activity goals, Mae says her manager has asked her about her scores and compared them with those of other employees. “Having that conversation put it on the back of my mind – they are looking at these scores.”

Now, when she undertakes work that could drag this number down – including taking notes on paper – she pauses the tracker, meaning she sometimes ends up working overtime to hit her contracted hour count.

“I feel frustrated that I’m being marked by some automated system that reflects me as being not as good a worker as I believe I have been.”

She also finds it negatively affects her productivity, to the point where she has taken sick leave to catch up on work without being tracked. “I feel constantly watched. I’m much better at taking myself away and working quietly. It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots.”

During the pandemic, there was a spike in searches relating to workplace surveillance, such as ‘how to monitor employees from home’, according to the Institute for Public Policy Research (IPPR).

A poll by the Trades Union Congress (TUC) in 2022 found that 60% of employees had experienced tracking in the last year. Henry Parkes is a senior economist at the IPPR and the author of a recent report on the rise of surveillance practices. He is calling for more transparency and says the exact scale of workplace monitoring is hard to judge without open data.

He warns that surveillance is “not only about logging”, adding: “ It’s the potential for it to be used against workers. This technology can just be used to exert power over employees in a way that wasn’t possible before.

“There’s potential for creep, where software is deployed for one purpose, [such as] checking when people are in, but there are all these other features you can use. You can start to analyse what they are doing.”

If employers rely on this data to make workplace decisions, there is a risk of algorithmic bias, Parkes says, while young, female and minority workers are more likely to be surveilled. He notes that some employers use systems that deploy aspects of AI in this process. “It is a black box, it’s not transparent – you feed data and it spits out a result. The more we rely on AI, we need to be really careful they’re not discriminating on the basis of gender or ethnicity. We could end up with decisions that are prejudiced, but appear neutral.”

And there are limitations to focusing solely on the accuracy of systems used in monitoring. The competency of technology, such as Fujitsu’s facial recognition AI model that assesses the concentration of workers, “will inevitably improve”, Parkes says, adding that “the rate they are improving at is quite scary …But it’s [still] dehumanising and not how people are able to operate all day,.”

Surveillance, which has long existed in some work environments including call centres, could become normalised in a widening array of sectors, Parkes says. “We wouldn’t accept your boss just standing behind you all day, watching and analysing everything you were doing. But basically the equivalent of that is possible through technology.”

Excessive monitoring can be counterproductive for companies too: it is associated with higher staff turnover rates, and there is evidence to suggest it can lead to resistance and counterproductive outcomes, including workarounds to improve statistics.

skip past newsletter promotion Sign up to First Edition Free daily newsletter Our morning email breaks down the key stories of the day, telling you what’s happening and why it matters Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion

Parkes says: “Your metrics can get more sophisticated but there are limits if we take metrics as gospel. There are lots of different ways people can be good at jobs – an excessive focus on data could be a problem. That’s not to say that there isn’t a role for data, but it’s about how we use it in making decisions. We want to be judged on outputs.”

Carlos*, who is in his 40s and works in customer service at a high street bank in London, knows how challenging this can be. Post-pandemic, his job is hybrid and he says he is tracked relentlessly when working remotely. “Our ‘performance’ is counted by the minute. I have found myself having to explain the reasons for a longer toilet break.” He says the intensity of the monitoring has affected his wellbeing.

Carlos says that in appraisals, he is told by how much he has deviated from the ‘optimum’ time spent dealing with each customer query. But he isn’t told how this score is calculated. “That’s what makes the job really stressful – it’s not transparent,” he says.

It makes you fearful. You always worry about being watched

Some workers are pushing back on workplace surveillance through unions. Adam*, who is in his 50s and works in social housing for a local authority in the south of England, says management has begun using vehicle tracking intrusively in recent years.

“We are routinely tracked – rang if we’re taking too long, or if our manager thinks we are not in the right place. It’s not unusual for council vehicles to have trackers, but it’s unusual for them to be used in any other way other than if a driver has an emergency.”

Adam says surveillance has been increasingly used “to catch people out”. “It makes you fearful. You always worry about being watched – sometimes we might be having a cup of coffee for half an hour. It adds pressure to an already stressful job. It’s not paranoia when they are out to get you.

“What the council is doing is using trackers to keep surveillance on us. Simply ringing people up to ask about why they’re taking a specific route, when we have no [set] routes … It’s bordering on harassment.”

His supervisor appears to have backed off since he reported it to the union. “I bit back. They are now aware that the watchers are being watched.”

* Names have been changed.. Moderators of Stack Overflow, the go-to Q&A forum for programmers, have announced today they will be going on strike citing the company’s prohibition on moderating AI-generated content on the platform.



Making the Facebook Papers Public CC Share Subtitles Off

English view video Making the Facebook Papers Public

The moderators announced the strike on the company’s Meta board this morning, and released an accompanying letter addressed directly to Stack Overflow. Last week in a post—which has been downvoted at least 283 times—Stack Overflow announced its new moderation policy that will only remove AI-generated content in specific instances, claiming that over-moderation of posts made with artificial intelligence was turning away human contributors.

Advertisement

The company also said in its post that a strict standard of evidence needed to be used moving forward in order to manage AI content, and that that standard of evidence hasn’t applied to most suspensions issued by moderators thus far. This directive was also communicated to the platform’s moderation team privately before being posted publicly. The moderators of the website are claiming that this directive will allow AI content, which can frequently be incorrect, to run rampant on the forum while expressing discontent with Stack Overflow for not communicating this new policy more effectively.

“Stack Overflow, Inc. has decreed a near-total prohibition on moderating AI-generated content in the wake of a flood of such content being posted to and subsequently removed from the Stack Exchange network, tacitly allowing the proliferation of incorrect information (“hallucinations”) and unfettered plagiarism on the Stack Exchange network. This poses a major threat to the integrity and trustworthiness of the platform and its content,” the mods write in their letter to Stack Overflow.

Advertisement

“A small number of moderators (11%) across the Stack Overflow network have stopped engaging in several activities, including moderating content. The primary reason for this action is dissatisfaction with our position on detection tools regarding AI-generated content,” Philippe Beaudette, VP of Community at Stack Overflow, said in a statement emailed to Gizmodo. “We stand by our decision to require that moderators stop using the tools previously used. We will continue to look for alternatives and are committed to rapid testing of those tools.”

Stack Overflow moderators, like those at Wikipedia, are volunteers tasked with maintaining the integrity of the platform. The moderators say that they tried to express their concerns with the company’s new policy through proper channels, but their anxieties fell on deaf ears. The mods plan to strike indefinitely, and will cease all actions including closing posts, deleting posts, flagging answers, and other tasks that help with website upkeep until AI policy has been retracted.

Advertisement

AI has been transforming Stack Overflow recently, for better or for worse. Stack Overflow confirmed the Gizmodo that traffic was dropping due to OpenAI’s ChatGPT as more and more programmers began turning to the chatbot to debug their code as opposed to waiting for a human reply on the forum. Web analytics firm SimilarWeb reported in April that Stack Overflow has seen a drop in traffic every month since the beginning of 2022, with the average drop being 6%. In March, Stack Overflow saw a 13.9% drop in traffic from February and in April, the website saw 17.7% drop in traffic from March.

Want to know more about AI, chatbots, and the future of machine learning? Check out our full coverage of artificial intelligence, or browse our guides to The Best Free AI Art Generators, The Best ChatGPT Alternatives, and Everything We Know About OpenAI’s ChatGPT.. It was a gruesome image that shot rapidly around the internet: a charred body, described as a deceased child, that was apparently photographed in the opening days of the conflict between Israel and Hamas.

Some observers on social media quickly dismissed it as an “A.I.-generated fake” — created using artificial intelligence tools that can produce photorealistic images with a few clicks.

Several A.I. specialists have since concluded that the technology was probably not involved. By then, however, the doubts about its veracity were already widespread.

Since Hamas’s terror attack on Oct. 7, disinformation watchdogs have feared that fakes created by A.I. tools, including the realistic renderings known as deepfakes, would confuse the public and bolster propaganda efforts.

So far, they have been correct in their prediction that the technology would loom large over the war — but not exactly for the reason they thought.

Disinformation researchers have found relatively few A.I. fakes, and even fewer that are convincing. Yet the mere possibility that A.I. content could be circulating is leading people to dismiss genuine images, video and audio as inauthentic.. Search has always been the Internet’s most important utility. Before Google became dominant, there were many contenders for the search throne, from Altavista to Lycos, Excite, Zap, Yahoo (mainly as a directory) and even Ask Jeeves. The idea behind the World Wide Web is that there’s power in having a nearly infinite number of voices. But with millions of publications and billions of web pages, it would be impossible to find all the information you want without search.

Google succeeded because it offered the best quality results, loaded quickly and had less cruft on the page than any of its competitors. Now, having taken over 91 percent of the search market , the company is testing a major change to its interface that replaces the chorus of Internet voices with its own robotic lounge singer. Instead of highlighting links to content from expert humans, the “Search Generative Experience” (SGE) uses an AI plagiarism engine that grabs facts and snippets of text from a variety of sites, cobbles them together (often word-for-word) and passes off the work as its creation. If Google makes SGE the default mode for search, the company will seriously damage if not destroy the open web while providing a horrible user experience.

A couple of weeks ago, Google made SGE available to the public in a limited beta (you can sign up here ). If you are in the beta program like I am, you will see what the company seems to have planned for the near future: a search results page where answers and advice from Google take up the entire first screen, and you have to scroll way below the fold to see the first organic search result.

For example, when I searched “best bicycle,” Google’s SGE answer, combined with its shopping links and other cruft took up the first 1,360 vertical pixels of the display before I could see the first actual search result.

(Image credit: Tom's Hardware)

For its part, Google says that it’s just “experimenting,” and may make some changes before rolling SGE out to everyone as a default experience. The company says that it wants to continue driving traffic offsite.

“We’re putting websites front and center in SGE, designing the experience to highlight and drive attention to content from across the web,” a Google spokesperson told me. “SGE is starting as an experiment in Search Labs, and getting feedback from people is helping us improve the experience and understand how generative AI can be helpful in information journeys. The experiences that ultimately come to Search will likely look different from the experiments you see in Search Labs. As we experiment with new LLM-powered capabilities in Search, we'll continue to prioritize approaches that will drive valuable traffic to a wide range of creators."

By “putting websites front-and-center,” Google is referring to the block of three related-link thumbnails that sometimes (but not always) appear to the right of its SGE answer. These are a fig leaf to publishers, but they’re not always the best resources (they don’t match the top organic results) and few people are going to click them, having gotten their “answer” in the SGE text.

Stay on the Cutting Edge Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors

(Image credit: Tom's Hardware)

For example, when I searched for “Best CPU,” the related links were from the sites Maketecheasier.com, Nanoreview and MacPaw. None of these sites is even on the first page of organic results for “Best CPU” and for good reason. They aren’t leading authorities in the field and the linked articles don’t even provide lists of the best CPUs. The MacPaw article is about how to choose the best processor for your MacBook, a topic that does not match the intent of someone searching for “best CPU,” as those folks are almost certainly looking for a desktop PC processor.

A Plagiarism Stew

Even worse, the answers in Google’s SGE boxes are frequently plagiarized, often word-for-word, from the related links. Depending on what you search for, you may find a paragraph taken from just one source or get a whole bunch of sentences and factoids from different articles mashed together into a plagiarism stew.

When I searched “which is faster the Ryzen 7 7800X3D or the Core i9-13900K,” the Google SGE grabbed an exact phrase from our Tom’s Hardware article comparing the two CPUs , writing “The Ryzen 7 7800X3D is 12% faster than the Core i9-13900K at 1080p gaming and 9% faster at 1440p.” It then rephrased two sentences from this article on Hardware Times . The original copy read as:

“The Core i9-13900K snags a win in “A Plague Tale” both with and without ray-tracing. It’s marginally faster than the Ryzen 7 7800X3D with similar lows. The tables get turned in Assassins’ Creed Valhalla as the 7800X3D edges past the 13900K in Ubisoft’s latest title.”

And Google’s AI wrote it as:

“The Core i9-13900K is marginally faster than the Ryzen 7 7800X3D in ‘A Plague Tale’. However, the Ryzen 7 7800X3D edges past the Core i9-13900K in Assassins' Creed Valhalla.”

(Image credit: Tom's Hardware)

You can even clearly see in our screenshot that our sentence is quoted word-for-word in Google’s “featured snippet” box but not in the SGE box (which will likely replace the featured snippets in the future since SGE does basically the same thing). Yes, both the Hardware Times article and the Tom’s Hardware article that Google’s bot copied data from are listed as related links on the right side of the box.

When I asked Google about the fact that its SGE answers are frequently word-for-word copies drawn from the related links articles, the company said that it picks those links because they “corroborate” the responses.

“Generative responses are corroborated by sources from the web,” the spokesperson said. “And when a portion of a snapshot briefly includes content from a specific source, we will prominently highlight that source in the snapshot.”

It’s pretty easy to find sources that back up your claims when your claims are word-for-word copied from those sources. While the bot could do a better job of laundering its plagiarism, it’s inevitable that the response would come from some human’s work. No matter how advanced LLMs get, they will never be the primary source of facts or advice and can only repurpose what people have done. LLMs are relatively good at generating “creative” works that are designed to be a mashup of existing ideas (ex: “write me a haiku about farts”) but, until they are connected to robotic bodies that go out and gather information first-hand, they will never be a source of truth.

The company also said that “you can expand to see how the links apply to each part of the snapshot.” There’s an expand icon that sits inconspicuously in the upper right corner of the SGE box, above the third related link. And, if you decide to click it, you will see a clunky interface which puts the thumbnails for related links inline with the pilfered text.

Image 1 of 2 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)

Whether you click the expand button or not, SGE’s related links are not presented as citations, but recommendations for further reading. If I start singing “Thriller” then tell you that it’s an original song I wrote, it doesn’t matter if I also say “you might want to listen to a guy named Micheal Jackson because he also makes some nice songs like this.” That’s still plagiarism and, even if it were not, we’d have a problem.

Plagiarism is a moral and academic term, not a legal one, and simply giving credit is not a defense against copyright infringement. You can’t run a business selling pirated Blu-ray discs and then, when busted, say “it’s all good, because I listed George Lucas as the director of Star Wars rather than substituting my own name in the credits.”

In answering my questions, Google’s spokesperson also compared the SGE box to featured snippets , noting that publishers today usually want their articles to appear in featured snippets because those links drive traffic back. While both experiences use content directly from publishers, featured snippets are short quotes with direct attribution and a very prominent link directly to the source. They do not pretend to be generated by an all-knowing AI and they often give you just enough information to want to click-through for more.

No Authority, No Trust

From a reader’s perspective, we’re left without any authority to take responsibility for the claims in the bot’s answer. Who, exactly, says that the Ryzen 7 7800X3D is faster and on whose authority is it recommended? I know, from tracing back the text, that Tom’s Hardware and Hardware Times stand behind this information, but because there’s no citation, the reader has no way of knowing. Google is, in effect, saying that its bot is the authority you should believe.

The fallacy underlying Google SGE is the false belief that a bot can have authority in the first place. Until the bot grows a pair of hands and opens its own lab space, it will never test CPUs. Until it opens a kitchen, it will never have its own family recipes. The only thing it can cook up is a plagiarism stew.

Relying on an unsourced bot as the end-all, be-all authority stands in direct contradiction to Google’s stated emphasis on E-E-A-T (Expertise, Experience, Authority and Trust), a standard it uses to decide which websites and authors should rank highly in organic search.

It makes total sense that someone who has been reviewing CPUs for 15 years on a website that specializes in CPUs should have their AMD Ryzen review rank higher than someone with no authority on the topic. Unfortunately, when it comes to Google’s own AI author – a faceless entity that has no experience doing anything – the rules go out the window.

Mish-Mash Plagiarism Leads to Poor Answers

At least the result we got when asking which CPU was faster was an accurate one. However, by mashing up text from different sources and then not sharing what the source for each sentence or bullet point is, Google is offering incorrect information that often contradicts the source material it’s copied from, or contradicts itself.

For example, I searched for “ThinkPad X13 AMD Review,” because I was interested in seeing what reviewers thought of Lenovo’s ThinkPad X13 laptop with AMD processor inside. The Google bot wrote its own mini-review, complete with bulleted pros and cons for the ThinkPad X13, while grabbing sentences and bullet points from at least four different articles, including a review from Laptop Mag, a review from Tom’s Hardware, another review from Notebook Check and a blog post from LaptopOutlet – which is a store that had about 100 words on the product.

The image below shows the result, along with pointers to where the SGE took its content from.

(Image credit: Tom's Hardware)

Aside from it being plagiarism and a slap in the face to the writers who did the actual work of testing and using this laptop, Google’s answer has a lot of issues. First of all, the answer refers to the ThinkPad X13 Gen 3 (the latest version with AMD CPU) but the reviews it draws from are from the Gen 1 and Gen 2 versions of the product, which are not the same.

While Laptop Mag and Tom’s Hardware both praised the laptop’s keyboard and durable design, both sites described the battery life as “lackluster” or “subpar,” while Google lists “Long battery life” as a pro. The bot clearly got the battery life pro from another site, but by mixing advice from different sources, Google is presenting readers with a very inaccurate picture.

Also, since the bot doesn’t cite sources, the reader has no way to know who thought it had long battery life, whether that came from a reputable outlet and how they tested. One of the sources, LaptopOutlet, is a store that sells laptops and doesn’t do any benchmark testing. Should its claims be given equal weight to those journalists who actually do test and aren’t actively trying to sell the product? Like most LLMs, Google’s SGE bot doesn’t seem to care whether it’s giving you the truth or just mashing sentences together in a way that seems convincing.

Giving Faulty Medical Advice

The Google SGE bot is so careless in its plagiarism mashups that it also gives incorrect medical advice that has been drawn from a variety of sources. For example, I asked: “do I need a colonoscopy?” and it gave me the following answer:

(Image credit: Tom's Hardware)

I highlighted the text in blue because it is dangerously wrong. Google’s bot says that “the American Cancer Society recommends that men and women should be screened for colorectal cancer starting at age 50.” However, the American Cancer Society’s own website says that screenings should start at age 45 , so this misleading “fact” probably came from elsewhere.

There’s also a bulleted list of “reasons to have a colonoscopy” that don’t include “routine screening,” hence it’s implying that you should only get the procedure if you have symptoms. The bulleted list is copied word-for-word from an article on an Australian Government health site called BetterHealth. The article actually lists “screening and surveillance for colorectal cancer” as a reason, but Google’s bot decided not to copy that fact.

Even if all the facts in the colonoscopy answer were clear and correct, they are not attributed to anyone. So why on earth should you trust them and whom do you blame when you follow this advice – for example, delaying your screening to age 50 – and something bad happens? By claiming content as its own, Google is acting as a publisher, which likely opens it up to lawsuits.

Keeping You On Google.com, Killing the Open Web

Though Google is telling the public that it wants to drive traffic to publishers, the SGE experience looks purpose-built to keep readers from leaving and going off to external sites, unless those external sites are ecomm vendors or advertisers. In some queries – “screenshot in windows” for example – there is a detailed answer but no related links at all. Nevermind that there are tons of articles that give you a lot more detail about how to take a screen shot.

(Image credit: Tom's Hardware)

If Google were to roll its SGE experience out of beta and make it the default, it would be detonating a 50-megaton bomb on the free and open web. Many publishers, who rely on Google referrals for the majority of their visits, would fold within a few months. Others would cut resources and retreat behind paywalls. Small businesses that rely on organic search placement to sell their products and services would have to either pay for advertising or, if they cannot afford it, close up shop.

Eventually, even hobbyists who either run not-for-profit websites or post advice on forums would likely stop doing it. Who wants to write, even for fun, if your words are going to be stolen and no one is going to read your copy? Would you answer someone’s programming question on Stack Overflow if your contribution would just be reworded and spat out by Google, without ever mentioning your name or the post itself?

Not an AI Issue: An Anti-Competitive Issue

This isn’t a case of artificial intelligence outsmarting human writers or providing a better experience. In fact, the method of publishing is incidental to the problem. If it rolls the current SGE experience out, Google would be leveraging its monopoly position to push its own content over and above everyone else’s. The company could hire an army of unskilled writers to copy and paste content from third-party websites, sometimes rewording it, instead of using an AI. The outcome would be the same.

There’s no doubt that Google’s AI will get better, but get better at what exactly? It will likely do a better job of rephrasing content so that it’s harder to find the original source it copied from. It will do a better job of offering information that’s up-to-date and logically consistent with itself. However, by just grabbing other peoples’ ideas and not citing the source, there’s no authority behind anything it says.

The end result of Google SGE going live as the default search experience would be a weaker, more siloed Internet, but likely a wealthier Google. The company would increase its time-on-site, ad revenue and ecommerce referrals. It would also please investors, who want to see it compete with OpenAI and Bing. Some readers may grouse about the quality of the information, which can be outdated, false or word-for-word plagiarized, but taking up the entire first screen of results will be enough for Google to grab a huge percentage – if not the majority – of its current outbound clicks.

Many people I have talked to about and shown Google SGE can’t believe that the company would roll such a dangerous, poor-quality and web-breaking experience out to everyone. We can hope that the final product won’t take up as much screen real estate as what we’re seeing today. But Google is already making this the daily search experience for anyone who, like me, signs up for the beta. And it has every economic incentive to make this the new default experience for 91 percent of the web’s searches.

What Publishers Can Do, What Users Can Do

Anyone who publishes on the web and needs people to actually read their work is in a precarious position, because of Google’s SGE. Almost every publication desperately needs to keep getting referrals from Google, so they can’t opt out of being indexed and having their data scraped. But if Google makes SGE the default search experience, the amount of Google referrals may fall so sharply that they can’t keep the lights on.

Bing took only a few months to go from having its AI Chat in a limited beta to it being available to everyone. If Google follows a similar timeline, it could go from being a search engine to a zero-click, plagiarism engine by this fall.

Publishers and publishing associations are still grappling with what AI plagiarism could do to their businesses. The News / Media Alliance, an industry group that represents magazines and newspapers, published a set of AI principles that states “The unlicensed use of content created by our companies and journalists by GAI systems is an intellectual property infringement: GAI systems are using proprietary content without permission.”

Getty Images is suing Stability AI to prevent the company from using its copyrighted images in training data. The image library has even asked a UK court to block sales of the AI system in that country. IAC Media Chairman Barry Diller has advocated for media companies to sue AI vendors over the unauthorized use of training data.

Will publishers sue Google over what it’s doing with SGE? There’s an argument that the word-for-word copying of information from websites without permission is a form of copyright infringement, even if the source was cited. However, we haven’t seen this litigated in court yet. And many companies, needing whatever traffic they will still get from Google, would want to avoid getting on the company’s bad side.

Companies could band together, through trade associations, to demand that Google respect intellectual property and not take actions that would destroy the open web as we know it. Readers can help by either scrolling past the company’s SGE to click on organic results or switching to a different search engine. Bing has shown a better way to incorporate AI, making its chatbot the non-default option and citing every piece of information it uses with a specific link back (the links aren’t very prominent, however).

In the end, if Google follows through with its current iteration of SGE, it will damage the quality of its own service. The content that the bot trains on would get worse and worse as more quality publishers left the open web. Eventually, users would start looking for a service that provides better answers. But by that time, the damage done to the entire web information ecosystem could be irreparable.

Note: As with all of our op-eds, the opinions expressed here belong to the writer alone and not Tom's Hardware as a team.. A prominent fantasy novel features a cover apparently generated with artificial intelligence, sparking complaints from artists and book enthusiasts. Earlier this month, readers noted that the back of the UK edition of Sarah J. Maas’ House of Earth and Blood credits Adobe Stock for the illustration of a wolf on its cover. The illustration matches an image created by user Aperture Vintage and marked as AI-generated on Adobe’s site. The move has led to criticism of both Maas and Bloomsbury Publishing, one of the world’s leading independent publishing houses.

While stock image services like Getty Images have prohibited AI-generated illustrations to avoid copyright disputes, Adobe has notably welcomed AI onto its Adobe Stock platform under specific criteria. Such content must be clearly labeled as AI-generated, and contributors must review the terms of any generative AI tools they use to create the images to ensure they have “all the necessary rights” to license them for commercial use. The rules are meant to avoid legal headaches. But the field of AI copyright is muddy, untested territory, and Adobe’s rules don’t clearly address all the issues it raises.

Bloomsbury rose to fame in the UK after it began publishing J.K. Rowling’s Harry Potter franchise in 1997. Maas is currently one of Bloomsbury’s top authors, best known for her young adult fantasy series like Throne of Glass (2012), A Court of Thorns and Roses (2015), and Crescent City (2020) — the franchise that House of Earth and Blood is part of. She has sold over 12 million copies of her books, many of which have made it to the New York Times bestsellers list, and a televised adaptation of A Court of Thorns and Roses is currently in development at Hulu. The publisher, Maas, Adobe, and Aperture Vintage did not immediately respond to requests for comment from The Verge.

Bloomsbury’s cover has exacerbated human artists’ concerns that publishers could replace them with text-to-image generators like Midjourney and Stable Diffusion. “Bloomsbury is one of the major publishing houses,” said freelance artist Kala Elizabeth on Twitter. “They CAN afford to hire real illustrators instead of purchasing Adobe stock, which is where this AI content is from.” Other publishers have attracted similar scrutiny for using AI-generated artwork on covers. Tor Books issued an apology in December last year, claiming it was unaware that the cover image selected for Christopher Paolini’s Fractal Noise novel was created by AI. While Maas hasn’t acknowledged the image’s provenance, and it’s not clear what — if any — involvement she had in the process, she’s praised the design of the cover on her Instagram page.. Khan Academy bills its artificial intelligence tutors as “the future of learning” on its website, but the truth is a little more complicated. What the site does not state upfront is that its service allows learners to select different historical figures such as Genghis Khan, Montezuma, Abigail Adams, and Harriet Tubman. The service is currently not available to all; it is restricted to a few school districts as well as volunteer testers of the product.

Similar to ChatGPT, avatars pull from data available on the internet to create a repository of words in the “vocabulary” of the bot that a user is talking to.

The Washington Post tested the limits of this technology, specifically the avatar of Harriet Tubman, to see if the AI would mimic Tubman’s speech pattern and spirit or if came off as an offensive impression or a regurgitation of Wikipedia information.

According to the article, the tool is designed to assist educators in fostering students’ curiosity of historical figures, but there are limits in how the bot is programmed, resulting in avatars that do not accurately portray the figures they are supposed to represent.

These AI interviews immediately raised questions, not just of the ethics in the nascent field of artificial intelligence, but of the ethics in even conducting such an “interview” in the interest of journalism. Many Black users on Twitter were horrified at the thought of digitally exhuming a venerated icon and ancestor in Harriet Tubman. These concerns seem to be located in the working knowledge that the creators of these apps and bots are not interested in fidelity to the spirits of the dead, because they don’t seem to care much about the living Black people they continually fail to do right by.

Even The Washington Post acknowledges that the bot fails its basic fact-checks, and Khan Academy stresses that the bot is not intended to function as a historical record of events. Why introduce such a technology if it cannot be trusted to even impersonate an up-to-date “version” of historical figures?

What is wrong with y’all? pic.twitter.com/0RXNDKeVf0 — CiCi Adams (@CiCiAdams_) July 18, 2023

UNESCO sets out some basic tenets and recommendations for ethics in the field of artificial intelligence on its website. The organization created the first global standard for ethics in artificial intelligence, which was accepted by 193 countries around the world in 2021.

Their four pillars are Human Rights and Human Dignity, Living in Peace With an Emphasis on Creating a Just Society, Ensuring Diversity and Inclusion, and Environmentalism. Even a cursory glance at these pillars would find Khan Academy’s bot impersonating historical figures who can’t consent to have their likenesses and names used is in flagrant violation of ethics and, some would argue, moral guidelines.

If the dead have dignity, digging them up for what amounts to thought exercises represents a complete disregard for their wishes and a lack of thought about these tenets of ethics. In its discussion of fairness and nondiscrimination, UNESCO writes: “AI actors should promote social justice, fairness, and non-discrimination while taking an inclusive approach to ensure AI’s benefits are accessible to all.”

It sounds like Khan Academy needs to take these words to heart, because at present, it does not exactly seem like social justice, fairness, and accessibility are at the heart of this project. The reactions to this experiment on social media tell that story to the world.

RELATED CONTENT: Redman Wants No Parts Of Artificial Intelligence Says, ‘Don’t Let Technology Ruin Hip-Hop’. Everything TikTok users need to know about a possible ban in the U.S.. Ces 217 postes seront supprimés d'ici juin 2024 au sein du groupe Onclusive, spécialiste de la veille médiatique pour des entreprises et des institutions.

L'entreprise Onclusive, installée à Courbevoie (Hauts-de-Seine) va supprimer 217 emplois, remplacés par des logiciels d'intelligence artificielle d'ici à juin 2024, a appris France Culture vendredi 15 septembre, confirmant une information de Libération. Le groupe international est spécialisé dans la veille médiatique et emploie 383 personnes en France. Ce plan social concerne donc plus de la moitié des effectifs.

Trois services entiers de production vont fermer. Ces employés sont chargés de réaliser des revues de presse pour leurs clients (des entreprises ou des grandes institutions). Ce travail de recherche et d'agrégation de données sera donc réalisé à terme par des logiciels d'intelligence artificielle.

"Il n'y aura quasiment plus d'humains !" Une représentante du personnel à France Culture

Les salariés craignent la disparition de toute analyse ou mise en relief des informations. "Les métiers intellectuels seront menacés à l'avenir", estime un représentant du personnel qui souhaite rester anonyme. Dans un communiqué publié vendredi, Onclusive défend sa décision. L'entreprise assure que ces "nouvelles technologies et ces nouveaux outils" offriront à leurs clients "un service plus rapide et plus fiable". "La mise en œuvre de ce projet n'a pas été décidée à la légère", insiste Matthew Piercy, président de Reputational Intelligence France et Chief Financial Officer chez Onclusive.

Promesse de reclassement des salariés

Onclusive s'engage aussi à "soutenir pleinement" les salariés "dans le cadre d'un processus de transition réfléchi". Une "entreprise spécialisée dans le reclassement sera consultée pour proposer à chaque personne concernée des solutions sur mesure", promet Matthew Piercy dans son communiqué. Une première réunion de négociations sur les conditions du plan de sauvegarde de l'emploi s'est tenue vendredi.. The pair of eyes staring back at me from the computer screen are unmistakably mine.

But the strange thing is that I am not the person standing in front of the camera. That honour goes to one of the developers working at the “fraud lab” I’m in.

I’ve been deepfaked, a process that technology has made easy. A picture was taken of me on a smartphone and seconds later, anyone in the lab could become me – at least online.

Deepfaking sees one person take on the face or even the voice of another digitally, meaning they can take pictures and videos as if they were the person they’ve copied. It’s essentially a filter.

As more of the lab staff move to stand in front of the camera, more versions of my face appear.

It’s a good way to test out different hairstyles, as the software isn’t adept at copying hair. I materialise with a blonde bob, brunette waves, and even bald, but all the faces are most definitely mine.. In an article posted to its WeChat account on Thursday, the ministry listed data leaks, cyberattacks and data manipulation as potential risks, as well as the technology’s impact on the economy and military.

The Ministry of State Security said the rapid development and wide application of AI – such as ChatGPT – posed challenges that “may fundamentally change the existing national security landscape in the near future”.

China’s spy agency has warned of the risks posed by artificial intelligence technology, especially those that could threaten national security.

Earlier this year, State Security Minister Chen Yixin warned of cybersecurity challenges that could threaten China’s infrastructure. He also wrote in an article published in September that technologies such as AI had brought new and greater uncertainties for security.

Beijing put forward a framework for artificial intelligence at last month’s belt and road forum, calling for equal rights for all nations. The framework – known as the Global AI Governance Initiative – notes the huge opportunities of artificial intelligence but also its “unpredictable risks and complicated challenges”.

AI governance is among the areas Beijing and Washington have pledged to cooperate on, according to a Chinese statement following Wednesday’s summit between Xi Jinping and Joe Biden in San Francisco.

In the article, China’s spy agency warned of the risks of data leaks, noting that AI requires massive amounts of data for learning and training and that it could include a lot of sensitive information. The ministry said any misuse of information or leaks could pose a serious threat to the privacy of individuals and to national security.

It also warned of cyberattacks that the ministry said could be assisted by AI, with hackers able to “easily launch targeted and highly concealed attacks against specific targets”.

The agency also pointed to the risk of “data poisoning” – or “adding malicious data into the training data of AI”. It said this could disrupt the normal operation of data analysis models, giving the example of disruption to intelligent car technology that could lead to accidents. Another example was the targeting of propaganda campaigns, which it said could result in “negative thoughts” discreetly infiltrating public opinion.

In addition, the article raised concern over the prospect of AI technology replacing human workers in some areas, which it said could have an impact on the country’s economic, social and political security. The ministry said the technology could also be used to cause financial market panic through the manipulation of information.

It said there were risks in the use of AI in lethal autonomous weapons, and in its use to make military actions more targeted and precise by connecting networks, decision makers and operators.

The ministry said there was a need to tighten laws and regulations on the technology, as well as to promote global governance and crack down on crime involving AI.

“Addressing the risks from AI and seizing the strategic initiative of AI development to effectively safeguard national security is an important issue in global national security governance,” the article said.

It called for improved “strategic confidence” and coordination on the development and safety of AI and for a “prudent” response to the risks and challenges.. The Chinese spies wanted more. In meetings during the pandemic with Chinese technology contractors, they complained that surveillance cameras tracking foreign diplomats, military officers and intelligence operatives in Beijing’s embassy district fell short of their needs.

The spies asked for an artificial intelligence program that would create instant dossiers on every person of interest in the area and analyze their behavior patterns. They proposed feeding the A.I. program information from databases and scores of cameras that would include car license plates, cellphone data, contacts and more.

The A.I.-generated profiles would allow the Chinese spies to select targets and pinpoint their networks and vulnerabilities, according to internal meeting memos obtained by The New York Times.

The spies’ interest in the technology, disclosed here for the first time, reveals some of the vast ambitions of the Ministry of State Security, China’s main intelligence agency. In recent years, it has built itself up through wider recruitment, including of American citizens. The agency has also sharpened itself through better training, a bigger budget and the use of advanced technologies to try to fulfill the goal of Xi Jinping, China’s leader, for the nation to rival the United States as the world’s pre-eminent economic and military power.. The New York Times sued OpenAI and Microsoft for copyright infringement on Wednesday, opening a new front in the increasingly intense legal battle over the unauthorized use of published work to train artificial intelligence technologies.

The Times is the first major American media organization to sue the companies, the creators of ChatGPT and other popular A.I. platforms, over copyright issues associated with its written works. The lawsuit, filed in Federal District Court in Manhattan, contends that millions of articles published by The Times were used to train automated chatbots that now compete with the news outlet as a source of reliable information.

The suit does not include an exact monetary demand. But it says the defendants should be held responsible for “billions of dollars in statutory and actual damages” related to the “unlawful copying and use of The Times’s uniquely valuable works.” It also calls for the companies to destroy any chatbot models and training data that use copyrighted material from The Times.

In its complaint, The Times said it approached Microsoft and OpenAI in April to raise concerns about the use of its intellectual property and explore “an amicable resolution,” possibly involving a commercial agreement and “technological guardrails” around generative A.I. products. But it said the talks had not produced a resolution.. 