another-body-256658 - Credit: Courtesy of Another Body

Taylor Klein will remind you of someone you know. A math-whiz engineer from an engineering family, her Facebook feed is littered with photos of her eating ice cream, studying, and enjoying the great outdoors — the most innocent of pastimes. We are introduced to her via video diary, where she compliments her mother’s strong work ethic and extols the virtues of school. “I never once broke the rules, because I was too scared of what would happen if I did,” she explains. “I think my teachers were concerned because I was worried about college when I was in seventh grade. … I was, like, stressing out about getting into college when I was 12.”

One day, during her senior year of college, Taylor received a Facebook message from one of her friends. “I’m really sorry but I think you need to see this,” the message read, followed by a link to a Pornhub video. She clicked the link and her jaw hit the floor: It was of a woman who looked just like her having sex with a man. Only this never really happened. Taylor’s face was superimposed on another woman’s body. She’d been receiving strange messages from men on Instagram calling her “disgusting” and coming on to her more aggressively than normal. It suddenly clicked.

More from Rolling Stone

“Oh, my God … people were messaging me because they found me on a porn website. I was just … first of all shocked that my name and my face were on there. It didn’t even register that my school and my hometown were also on there,” she recalls. “And then just seeing, like, oh, my God, these videos had thousands of views on them.”

Taylor is the main subject of Another Body, an urgent documentary from filmmakers Sophie Compton and Reuben Hamlyn that premiered at SXSW. The film explores the dark, highly disturbing world of nonconsensual deepfake-porn of women, which experts estimate constitutes about 90 percent of all deepfake content online.

In Compton and Hamlyn’s doc, “Taylor Klein” is not the victim’s real name, and the woman’s face we see in video-diary testimonials isn’t hers but an actor’s face deepfaked over it — in order to preserve what’s left of her anonymity. Taylor soon discovered about “six or seven” deepfake porn videos of herself on Pornhub, along with ones tied to an xHamster account. They were covered in creepy comments, leading the youngster — who already had crippling anxiety and OCD — to wonder whether any of these people might come after her.

“I still just haven’t really processed it. It’s like one of those things that’s too upsetting to think about, so I just don’t,” she says, tearing up, adding, “I just wanted to be numb, basically.”

“I kept asking myself, ‘Why would someone want to do this? And who would do this?’” she says. “I felt like someone’s trying to punish me.”

So, she took action. Taylor called the local police, and we hear a recording of that call in the doc. The policeman on the other end of the line tells her he’s “not quite sure” what they could do, but says they’ll call back soon. A few weeks later, Taylor finally received that call. The cop told her that her tormentor was within their rights to do what they did, and there’s nothing else they can do given that there were no laws concerning nonconsensual deepfake porn in her state, and it doesn’t yet fall within the confines of nonconsensual porn laws, since it incorporates the victim’s face but not their body.

When more deepfake videos popped up — this time of her freshman-year roommate Julia (also a pseudonym) — she reached out to her classmate to warn her, and the two joined forces to try to figure out who was the person behind all this ugliness. Unfortunately, it wasn’t an easy task. They were female engineering students at a school where they estimated there were only four women for every hundred men. And they’d already been on the receiving end of plenty of harassment from their male classmates, a number of whom were, by all accounts, incels.

The duo initially suspected “Bobby” (a pseudonym), a classmate they remembered from freshman year who suddenly began posting a ton of hentai porn and “cultlike things” on main that suggested he believed he “was in some video-game world” with an “apocalyptic-god storyline,” where he was “talking down to humans.” (A real gem, this one.) They prowled 4chan and discovered that a single user had been posting photos of a number of their female classmates, and all of them knew “Mike” (another pseudonym) — a friend of theirs from freshman year. Both women had befriended Mike and grown close to him, with Taylor living in the same dorm as him her freshman and junior years, describing him as the person who initially “made her feel at home in college” — that is, until his demands became too much.

“It was just a really good friendship until it wasn’t. He basically tried to make me his personal therapist, and I couldn’t handle it,” remembers Taylor. Both women had the exact same experience, and ultimately distanced themselves from him. “He didn’t want to listen to what was going on with me. He just wanted to talk about his issues,” adds Julia.

Then came the smoking gun: Julia discovered that the same 4chan handle posted a photo she took with Mike her freshman year, while Taylor found deepfake porn of herself posted to 4chan that was made around the time of her falling-out with him.

“Maybe he felt like he was entitled to all the attention and all the support that he wanted from us, and when we set boundaries for ourselves, that’s what made him vengeful,” Taylor reasons, later adding, “I believe that in his mind, he’s getting back at us for ‘wronging’ him.”

They located a profile with the same handle as Mike’s 4chan one on a popular deepfake porn site, MrDeepFakes.com, whose logo is none other than Donald Trump. One of the women the account made a lot of deepfake porn of was Gibi ASMR — an ASMR YouTuber with 4.6 million subscribers. Mike’s deepfake porn profile had more than a million views, and had posted 132 deepfake-porn videos.

“Someone that knew me on a very, very, very personal level — it’s hard to believe he has this complete other world that none of us knew about,” offers Taylor.

Taylor and Julia reached out to Gibi ASMR and explained the situation to her, prompting the content creator to post a video to her social handles calling out this icky deepfake-porn trend and demanding more accountability. The post went viral.

Hey all – serious topic – and honestly just a tiny blip in the conversation. My experience is not unique in the slightest, which is why I feel compelled to speak out. It can and will truly happen to anyone. I hope you take a listen, and help me take some action against deepfakes. pic.twitter.com/pbH7kf0PI1 — Gibi 🐝 (@GibiOfficial) November 10, 2022

Meanwhile, the young women convinced police to place a call to Mike and, after some light prodding — and Mike neither confirming nor denying he did it, but apologizing and saying it would never happen again — they closed the case.

That certainly didn’t end things for Taylor.

“He’s still friends with all of my friends, and it’s tough because I can’t really tell them about it. I tried telling one of them, and he didn’t really believe me that it was him,” Taylor says, adding, “Because the police didn’t do anything, it feels like it’s kind of my word against his. … I definitely worry something would happen where people wouldn’t like me, or people would turn against me. It feels like I’ve had to deal with all of the consequences that he should have had to deal with.”

Taylor was the one who eventually had to leave their friend group. And her story echoes those of so many other women, including Twitch star QTCinderella, who came forward in February in a tear-filled video describing how she’d been terrorized by deepfake porn of herself online, calling out fellow streamer Atrioc for boasting that he’d “purchased two doctored clips featuring other famous female Twitch stars, prompting a spike in traffic to the deepfake porn site,” reported the New York Post. (Atrioc then released his own apology video.)

“Whoever is targeted is driven offline. It’s like the aperture of people’s lives closes,” explains Mary Anne Franks, a professor at the University of Miami School of Law, in Another Body. “The disciplinary effect of those deepfakes is: You shouldn’t be successful, you shouldn’t go into journalism, you shouldn’t go into politics. That is a massive chilling effect.” And the perpetrators of these crimes are almost never held accountable for their actions.

“I think because he essentially got away with this,” Taylor says, “Mike will certainly keep doing this to women in his life.”



Best of Rolling Stone

Click here to read the full article.. The period of time from the mid-20th century up until right now has often been called the “Information Age.” It’s defined by the massive amount of technological progress over the last century, which has brought anything and everything we could ever want right to our fingertips. News, pop culture, weather, porn—if we desire it, we can have it at the speed it takes to type in a request on our phones.

But perhaps more than any one piece of real data, humanity has been characterized over the last decade by its ability to sift through deception. What is and is not real continues to be the question that hovers above all of politics. Meanwhile, the rise of artificial intelligence suckers in an entire Twitter feed of impressionable users every five minutes, handing over their facial map to see what they might look like as a renaissance painting.

A more appropriate way to categorize this current state would be the “Disinformation Age,” where nothing is ever quite what it seems. Deepfake technology is the most pertinent calling card of this new era. That tech, which collects facial mapping data from a wealth of photos of the same person, allows one person’s face to be plastered upon someone else’s body. Despite being the subject of social experiments and absurd meme videos, deepfake technology has become an epidemic that presents real dangers, both political and personal.

The latter is a subject of Another Body, a new documentary out of SXSW, that chronicles one woman’s intense reaction to finding out that her face had been deepfaked onto several porn videos. While examining the legal ramifications—or lack thereof—of this kind of violation, the film itself implements deepfake technology to protect the identities of both the victims and perpetrators, giving us a taste of just how insidiously smart the tech really is. By taking an intimate approach to exploring a problem that grows larger every day, Another Body becomes a terrifying analysis of our crumbling sense of autonomy, in an inextricably digital age.

(Disclosure: Allegra Frank, a Daily Beast’s Obsessed editor, is a member of the SXSW documentary jury. She was not involved in coverage of any documentaries or editing of the story.)

At the start of the film, we’re introduced to Taylor Klein via a video diary, shortly after she’s found out that her face has been deepfaked onto porn videos, on sites like Pornhub and Xhamster. Taylor, the latest in her family’s long line of engineers, had only just graduated college when she received a Facebook message from a classmate, linking her to one of the videos. “It was just shocking, seeing my face looking at the camera, basically making eye contact with me,” she explains.

But it wasn’t just Taylor’s face that had been compromised. Her full name, hometown, and college were all present, either in the porn video titles or details of the specific accounts hosting the content. She began to be inundated with messages from men on social media, sending lewd and lascivious correspondence about them living nearby, or wanting to meet up with her when they were in town. Naturally, she became worried for her own safety, while already trying to handle the emotional and physical toll of being the victim of nonconsensual pornography.

Shortly into Another Body, Taylor reveals that the person we’ve been watching in video diaries, professionally lit confessionals, and photos, is not the real Taylor. In fact, Taylor doesn’t exist. All of the footage we’ve seen of “Taylor” so far is the real-life target of this instance of abuse, but her name and face have been changed to protect her identity. Instead, an actor’s face was deepfaked onto “Taylor’s,” in order to bring her story to the public.

This revelation—which is not a spoiler to the film itself—presents a disturbing sense of surreality. Audiences see just how seamlessly the film’s deepfake artist, Fernando Sánchez Liste, is able to cycle between different faces. Each one fits perfectly atop the body that we’ve been watching already; the average person wouldn’t question its authenticity if they weren’t told it was artificial.

While not all deepfakes are as indisputable as the ones professionally made by the graphic effects artists for Another Body, their ease of application is the film’s defining predicament. If it’s this simple to toss someone else’s face onto another person’s body, wouldn’t the potential ramifications be so clear that the government and law enforcement would do everything in their power to stop the spread?

Unfortunately, as the film notes, deepfakes have become their own epidemic. Millions have already made their way online, and no one is safe. What’s more, only a handful of states currently have laws against deepfakes as a form of nonconsensual pornography. While 48 states have adopted laws against revenge porn, only three states have specific deepfake laws: Texas, Virginia, and California. Only the latter two of those three states specify legal consequences for nonconsensual deepfake pornography.

Taylor Klein did not happen to live in either of those two states, meaning that her chase for justice would have to be entirely self-guided. “We were basically playing phone tag with the police for a couple of weeks,” Taylor says. “[The detective] asked, ‘What have you done to cause someone to do this to you?” And after a decent dose of victim blaming, Taylor explains that the detectives simply told her that the deepfakes were wrong, but that there is nothing that can be done, as the perpetrator didn’t break any laws.

Bolstered by her desire to control how others perceive her, Taylor embarks on her own quest to find out who deepfaked her. Along the way, she finds out that she’s not the only person in her class grappling with this. Another engineering student, “Julia,” had also been deepfaked in several porn videos. Once the two of them discover that they’ve both had their faces stolen, they posit that it was done by a mutual friend of theirs, and link up in the darkest recesses of the internet to search for answers.

Occasionally, Another Body’s insularity can feel frustrating. It doesn’t probe far enough into the reasons why the ramifications of deepfakes are so damaging, beyond Taylor and Julia’s intense personal distress. The film touches on the potential consequences that Taylor or Julia could potentially face in the real world due to their deepfakes, but only skims the surface of the potential damage. Sex workers already face discrimination both systematically and socially, from employers, banks, families, politicians, and friends. Even if sex work is nonconsensual via a deepfake, the victims of them are liable to face that same discrimination.

Despite remaining focused on Taylor and Julia, Another Body still transcends the coldness of all of the technology that its directors implement to tell its story. These victims are very real, and even if we never see their true faces or know their real names, it doesn’t mean that their lives weren’t consumed by synthetic images of themselves. Another Body takes the time to ponder whether self-control and identity are merely illusions in the Disinformation Age. What’s real and what isn’t, and how do we sift through the rubble of our personhood to find the connection between the two? Despite our faces being fodder for the world, it’s our determination for the truth that remains humanity’s most powerful asset when it comes to navigating whatever may lie ahead.

Liked this review? Sign up to get our weekly See Skip newsletter every Tuesday and find out what new shows and movies are worth watching, and which aren’t.