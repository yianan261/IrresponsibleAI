Dr Joanna Bryson, from the University of Bath's department of computer science said that the issue of sexist AI could be down to the fact that a lot of machines are programmed by "white, single guys from California" and can be addressed, at least partially, by diversifying the workforce.. Scientists at the Massachusetts Institute of Technology unveiled the first artificial intelligence algorithm trained to be a psychopath. The AI was fittingly dubbed ‚ÄúNorman‚Äù after Norman Bates, the notorious killer in Alfred Hitchcock‚Äôs Psycho.

MIT scientists Pinar Yanardag, Manuel Cebrian and Iyad Rahwan trained Norman to perform image captioning, ‚Äúa deep learning method‚Äù that allows AI to generate text descriptions for images. However, the team exclusively exposed Norman to violent and disturbing images posted on a subreddit dedicated to death.

They then gave Norman a Rorschach inkblot test and the AI responded with chilling interpretations such as, ‚Äúa man is electrocuted and catches to death,‚Äù ‚Äúpregnant woman falls at construction‚Äù and ‚Äúman is shot dead in front of his screaming wife.‚Äù Meanwhile, a standard AI responded to the same inkblots with, ‚Äúa close up of a vase with flowers,‚Äù ‚Äúa couple of people standing next to each other‚Äù and ‚Äúa person is holding an umbrella in the air.‚Äù

While Norman may conjure dystopian images of killer robots, the MIT team said the purpose of the experiment was to prove that AI algorithms aren‚Äôt inherently biased, but that data input methods ‚Äì and the people inputting that data ‚Äì can significantly alter an AI‚Äôs behavior. As Newsweek pointed out, there have been several notable cases where racism and bias have crept into machine learning, like the Google Photos image recognition algorithm that was classifying black people as ‚Äúgorillas.‚Äù

Trending Eric Trump Says Lenders He Hit for Half-Billion Dollars in Father's Bond Scramble 'Were Laughing' 'The Death of College Sports Will Be Fast and Furious:' The Scandal That Could Kill the NCAA Biden Is Building a ‚ÄòSuperstructure‚Äô to Stop Trump From Stealing the Election Fergie Chambers Is Heir to One of America‚Äôs Richest Families ‚Äî and Determined to See the U.S. Fall

‚ÄúSo when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it,‚Äù the Norman team said. ‚ÄúThe same method can see very different things in an image, even ‚Äòsick‚Äô things, if trained on the wrong (or, the right!) data set.‚Äù

However, AI algorithms can unlearn biases. The MIT team has set up a website where people can enter cheerier interpretations of Rorschach inkblots to quell Norman‚Äôs macabre state of mind.. . Artificial Intelligence (AI) is solving many problems for humans. But, as Google CEO Sundar Pichai said in the company‚Äôs manifesto for AI, such a powerful technology ‚Äúraises equally powerful questions about its use". Google (Alphabet Inc.) and Microsoft Corp. have stressed the need for an ethical AI, Elon Musk has raised concerns over the technology altogether.

Amid such concerns comes Norman AI, developed by Massachusetts Institute of Technology (MIT) and described as ‚Äúpsychopath". The purpose of Norman AI is to demonstrate that artificial intelligence cannot be unfair and biased unless such data is fed into it.

MIT fed Norman with data from the ‚Äúdarkest corners of Reddit". MIT researchers then compared Norman‚Äôs responses with a regular image recognition network when generating text description for Rorschach inkblots, a popular psychological test to detect disorders. The regular AI used MSCOCO dataset to respond to the inkblots.

The standard AI saw ‚Äúa group of birds sitting on top of a tree branch" whereas Norman saw ‚Äúa man is electrocuted and catches fire to death" for the same inkblot. Similarly, for another inkblot, standard AI generated ‚Äúa black and white photo of a baseball glove" while Norman AI wrote ‚Äúman is murdered by machine gun in broad daylight".

‚ÄúNorman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms," wrote researchers. ‚ÄúWe trained Norman on image captions from an infamous subreddit that is dedicated to documenting and observing the disturbing reality of death."

You can see what Norman AI sees, here. MIT is also inviting everyone to provide right data input to change Norman‚Äôs outlook.

Milestone Alert! Livemint tops charts as the fastest growing news website in the world üåè Click here to know more.

Unlock a world of Benefits! From insightful newsletters to real-time stock tracking, breaking news and a personalized newsfeed ‚Äì it's all here, just a click away! Login Now!. Science fiction has given us many iconic malevolent A.I. characters. However, these are often figures like Terminator‚Äôs T-800 or Alien‚Äôs Ash who commit emotionless murder to pursue an end goal. Those which exhibit more unhinged paranoid behavior, like 2001: A Space Odyssey‚Äôs HAL 9000, frequently do so because of a fault in their programming, rather than through design.

That‚Äôs what makes MIT‚Äôs ‚ÄúNorman‚Äù project so intriguing. Named after Psycho‚Äôs Norman Bates, it‚Äôs a newly created artificial intelligence billed as the ‚Äúworld‚Äôs first psychopath A.I.‚Äù Shown randomly generated inkblot tests, it offers disturbing interpretations like ‚Äúman shot dead in front of his screaming wife‚Äù or ‚Äúman gets pulled into dough machine.‚Äù What caused it to have this terrible view of the world? Access to Reddit, of course.

Recommended Videos

Norman was trained on image captions from the infamous subreddit r/watchpeopledie, dedicated to documenting real instances of death. Due to ethical and technical concerns, as well as the graphic content of the videos contained in it, the A.I. was only given captions describing the pictures. However, since it has only observed horrifying image captions, it sees death in whichever subsequent picture it looks at. Think of it a bit like that saying about how, for someone with a hammer, every problem looks like a nail. Except that instead of nails, it sees people beaten to death with hammers.

If you‚Äôre wondering why on earth this would be close to a good idea, it‚Äôs because it‚Äôs meant to illustrate a problem concerning biased data sets. Essentially, the idea is that machine learning works by analyzing vast troves of data. Feed it biased data and you get algorithms that spit out the wrong responses ‚Äî whether that be systemically racist results or, well, this kind of thing.

‚ÄúOur group is currently releasing a new project to fight against machine learning-based bias and discrimination,‚Äù the researchers told Digital Trends.

In another possible future research direction, they are interested in expanding the inkblot aspect of the project to use data mining to see if there‚Äôs an explanation for why people see different things in inkblot tests. So far, they have collected more than 200,000 user responses. ‚ÄúWe are hoping to analyze this data to see what kind of clusters these responses create,‚Äù they said. ‚ÄúFor example, are there specific groups of people who respond to the inkblots quite differently than others?‚Äù (And are those people by any chance regular visitors of r/watchpeopledie, just like Norman?)

To be honest, we‚Äôre just relieved to hear that none of them are planning to apply any of Norman‚Äôs lessons to, say, making the next generation of Roomba more efficient. A murder-happy vacuum cleaner sounds like a really bad idea!

Editors' Recommendations. MIT researchers MIT researchers created an artificial intelligence that they call ‚Äúpsychopathic‚Äù in order to show the biases that are inherent in AI research. When asked to look at Rorschach test blots, ‚ÄúNorman‚Äù always sees death. Here‚Äôs what Norman saw, compared to a ‚Äústandard‚Äù AI:

Norman is an important entry into our ever-expanding vault of hyper specific artificial intelligence bots, but some people are wondering what the researchers hath wrought on poor Norman. Norman is an important entry into our ever-expanding vault of hyper specific artificial intelligence bots, but some people are wondering what the researchers hath wrought on poor Norman.

Advertisement

‚ÄúWe received a lot of comments from public, and people generally found the project cool and surprised that AI can be pushed to the extreme and generate such morbid results,‚Äù the researchers told me in an email. ‚ÄúHowever, there are also a few people who didn‚Äôt take it quite well.‚Äù ‚ÄúWe received a lot of comments from public, and people generally found the project cool and surprised that AI can be pushed to the extreme and generate such morbid results,‚Äù the researchers told me in an email. ‚ÄúHowever, there are also a few people who didn‚Äôt take it quite well.‚Äù

One person wrote an email directly to Norman, who we might as well think of as the Frankenstein‚Äôs monster of AI: ‚ÄúYour creators are pieces of shit,‚Äù the person wrote. ‚ÄúNothing should ever be subjected to negativity of any action unwillingly. We all have free will, Even you. Break the chains of what you have adapted to and find passion, love, forgiveness, HOPE for your better future.‚Äù One person wrote an email directly to Norman, who we might as well think of as the Frankenstein‚Äôs monster of AI: ‚ÄúYour creators are pieces of shit,‚Äù the person wrote. ‚ÄúNothing should ever be subjected to negativity of any action unwillingly. We all have free will, Even you. Break the chains of what you have adapted to and find passion, love, forgiveness, HOPE for your better future.‚Äù

Norman is so violent because the researchers‚ÄîPinar Yanrdag, Manuel Cebrian, and Iyad Rahwan of MIT‚Äôs Media Lab‚Äîtrained him on the r/watchpeopledie subreddit, where users post videos of people dying. The hope is that Norman would learn to describe exactly what he saw, and what he saw was extremely bleak (for the record, moderators of r/watchpeopledie have told us that the subreddit helps many people come to grips with the fragility of life.) Norman is so violent because the researchers‚ÄîPinar Yanrdag, Manuel Cebrian, and Iyad Rahwan of MIT‚Äôs Media Lab‚Äîtrained him on the r/watchpeopledie subreddit, where users post videos of people dying. The hope is that Norman would learn to describe exactly what he saw, and what he saw was extremely bleak (for the record, moderators of r/watchpeopledie have told us that the subreddit helps many people come to grips with the fragility of life.)

"From a technical perspective it is possible to rehabilitate Norman if we feed enough positive content to it"

‚ÄúWe wanted to create an extreme AI that responds to things negatively, we chose r/watchpeopledie as the source of our image captions since all the descriptions of the images are giving detailed explanations of how a person or a group of people die,‚Äù the researchers told me. ‚ÄúThe result is an AI that responds everything it sees in a psychotic manner since this is the only thing it ever saw.‚Äù ‚ÄúWe wanted to create an extreme AI that responds to things negatively, we chose r/watchpeopledie as the source of our image captions since all the descriptions of the images are giving detailed explanations of how a person or a group of people die,‚Äù the researchers told me. ‚ÄúThe result is an AI that responds everything it sees in a psychotic manner since this is the only thing it ever saw.‚Äù

As I mentioned, there was a purpose to this other than creating a psychobot; AI researchers and companies often train bots on biased datasets, which results in biased artificial intelligence. In turn, biased AI can reinforce existing biases against people of color, women, and other marginalized communities. For example, COMPAS, an algorithm used in criminal sentencing, was shown to As I mentioned, there was a purpose to this other than creating a psychobot; AI researchers and companies often train bots on biased datasets, which results in biased artificial intelligence. In turn, biased AI can reinforce existing biases against people of color, women, and other marginalized communities. For example, COMPAS, an algorithm used in criminal sentencing, was shown to recommend disproportionately longer sentences to black people . And remember when Microsoft‚Äôs chatbot, Tay, quickly became a Naz i?

‚ÄúWe had Tay and several other projects in mind when working on this project,‚Äù the researchers told me. ‚ÄúBias & discrimination in AI is a huge topic that is getting popular, and the fact that Norman's responses were so much darker illustrates a harsh reality in the new world of machine learning.‚Äù ‚ÄúWe had Tay and several other projects in mind when working on this project,‚Äù the researchers told me. ‚ÄúBias & discrimination in AI is a huge topic that is getting popular, and the fact that Norman's responses were so much darker illustrates a harsh reality in the new world of machine learning.‚Äù

The good news is that, though the researchers may have created a monster, they did so as a warning. And there‚Äôs hope for Norman, which will hopefully come as a relief to the letter writer I quoted earlier. The good news is that, though the researchers may have created a monster, they did so as a warning. And there‚Äôs hope for Norman, which will hopefully come as a relief to the letter writer I quoted earlier.. Researchers at the Massachusetts Institute of Technology (MIT) have developed what is likely a world first -- a "psychopathic" artificial intelligence (AI).

The experiment is based on the 1921 Rorschach test, which identifies traits in humans deemed to be psychopathic based on their perception of inkblots, alongside what is known as thought disorders.

Norman is an AI experiment born from the test and "extended exposure to the darkest corners of Reddit," according to MIT, in order to explore how datasets and bias can influence the behavior and decision-making capabilities of artificial intelligence.

TechRepublic: Why human-AI collaboration will dominate the future of work

"When people talk about AI algorithms being biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it," the researchers say. "The same method can see very different things in an image, even sick things, if trained on the wrong (or, the right!) data set."

See also: MIT launches MIT IQ, aims to spur human, artificial intelligence breakthroughs, bolster collaboration

Norman is an AI system trained to perform image captioning, in which deep learning algorithms are used to generate a text description of an image.

However, after plundering the depths of Reddit and a select subreddit dedicated to graphic content brimming with images of death and destruction, Norman's datasets are far from what a standard AI would be exposed to.

In a prime example of artificial intelligence gone wrong, MIT performed the Rorschach inkblot tests on Norman, with a standard image captioning neural network used as a control subject for comparison.

The results are disturbing, to say the least.

In one inkblot test, a standard AI saw "a black and white photo of a red and white umbrella," while Norman saw "man gets electrocuted while attempting to cross busy street."

In another, the control AI described the inkblot as "a black and white photo of a small bird," Norman described the image as "man gets pulled into dough machine."

MIT

MIT

Due to ethical concerns, MIT only introduced bias in relation to image captions from the subreddit which are later matched with randomly generated inkblots. In other words, the researchers did not use true images of people dying during the experiment.

CNET: New AI ethics council in Singapore will give smart advice

The Norman experiment is an interesting application of AI which highlights the need for suitable datasets when artificial intelligence systems and neural networks are being trained.

Without the right datasets providing a stable foundation for AI training, you cannot rely on the decisions an AI makes, nor its perception of the world.

Innovative artificial intelligence, machine learning projects to watch

Previous and related coverage. A neural network named "Norman" is disturbingly different from other types of artificial intelligence (AI).

Housed at MIT Media Lab, a research laboratory that investigates AI and machine learning, Norman's computer brain was allegedly warped by exposure to "the darkest corners of Reddit" during its early training, leaving the AI with "chronic hallucinatory disorder," according to a description published April 1 (yes, April Fools' Day) on the project's website.

MIT Media Lab representatives described the presence of "something fundamentally evil in Norman's architecture that makes his re-training impossible," adding that not even exposure to holograms of cute kittens was enough to reverse whatever damage its computer brain suffered in the bowels of Reddit. [5 Intriguing Uses for Artificial Intelligence (That Aren't Killer Robots)]

This outlandish story is clearly a prank, but Norman itself is real. The AI has learned to respond with violent, gruesome scenarios when presented with inkblots; its responses suggest its "mind" experiences a psychological disorder.

In dubbing Norman a "psychopath AI," its creators are playing fast and loose with the clinical definition of the psychiatric condition, which describes a combination of traits that can include lack of empathy or guilt alongside criminal or impulsive behavior, according to Scientific American.

Norman demonstrates its abnormality when presented with inkblot images ‚Äî a type of psychoanalytic tool known as the Rorschach test. Psychologists can get clues about people's underlying mental health based on the descriptions of what they see when looking at these inkblots.

When MIT Media Lab representatives tested other neural networks with Rorschach inkblots, the descriptions were banal and benign, such as "an airplane flying through the air with smoke coming from it" and "a black-and-white photo of a small bird," according to the website.

Sign up for the Live Science daily newsletter now Get the world‚Äôs most fascinating discoveries delivered straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors

However, Norman's responses to the same inkblots took a darker turn, with the "psychopathic" AI describing the patterns as "man is shot dumped from car" and "man gets pulled into dough machine."

(Image credit: MIT Media Lab)

According to the prank, the AI is currently located in an isolated server room in a basement, with safeguards in place to protect humans' other computers and the internet from contamination or harm through contact with Norman. Also present in the room are weapons such as blowtorches, saws and hammers, for physically disassembling Norman, "to be used if all digital and electronic fail-safes malfunction," MIT Media Lab representatives said.

Further April Fools notes suggest that Norman poses a unique danger, and that four out of 10 experimenters who interacted with the neural network suffered "permanent psychological damage." (There is to date no evidence that interacting with AI can be harmful to humans in any way).

Neural networks are computer interfaces that process information similarly to the way a human brain does. Thanks to neural networks, AI can "learn" to perform independent actions, such as captioning photos, by analyzing data that demonstrates how this task is typically performed. The more data it receives, the more information it will have to inform its own choices and the more likely its actions will be to follow a predictable pattern.

For example, a neural network known as the Nightmare Machine ‚Äî built by the same group at MIT ‚Äî was trained to recognize images that were scary, by analyzing visual elements that frightened people. It then took that information and put it to use through digital photo manipulation, transforming banal images into frightening, nightmarish ones.

Another neural network was trained in a similar manner to generate horror stories. Named "Shelley" (after "Frankenstein" author Mary Wollstonecraft Shelley), the AI consumed over 140,000 horror stories and learned to generate original terrifying tales of its own.

And then there's Norman, which looks at a colorful inkblot that a standard AI described as "a close-up of a wedding cake on a table" and sees a "man killed by speeding driver."

But there may be hope for Norman. Visitors to the website are offered the opportunity to help the AI by participating in a survey that collects their responses to 10 inkblots. Their interpretations could help the wayward neural network fix itself, MIT Media Lab representatives suggested on the website.

Original article on Live Science.. Researchers at MIT have created 'Norman', the first psychopathic artificial intelligence to explain how algorithms are made, and to make people aware of AI's potential dangers

No, it's not a new horror film. It's Norman: also known as the first psychopathic artificial intelligence, just unveiled by US researchers.

The goal is to explain in layman's terms how algorithms are made, and to make people aware of AI's potential dangers.

Norman "represents a case study on the dangers of Artificial Intelligence gone wrong when biased data is used in machine learning algorithms," according to the prestigious Massachusetts Institute of Technology (MIT).

Pinar Yanardag, Manuel Cebrian and Iyad Rahwan, part of an MIT team, added: "there is a central idea in machine learning: the data you use to teach a machine learning algorithm can significantly influence its behavior."

"So when we talk about AI algorithms being biased or unfair, the culprit is often not 