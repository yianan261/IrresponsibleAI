The first international beauty contest judged by “machines” was supposed to use objective factors such as facial symmetry and wrinkles to identify the most attractive contestants. After Beauty.AI launched this year, roughly 6,000 people from more than 100 countries submitted photos in the hopes that artificial intelligence, supported by complex algorithms, would determine that their faces most closely resembled “human beauty”.

But when the results came in, the creators were dismayed to see that there was a glaring factor linking the winners: the robots did not like people with dark skin.

Out of 44 winners, nearly all were white, a handful were Asian, and only one had dark skin. That’s despite the fact that, although the majority of contestants were white, many people of color submitted photos, including large groups from India and Africa.

The ensuing controversy has sparked renewed debates about the ways in which algorithms can perpetuate biases, yielding unintended and often offensive results.

When Microsoft released the “millennial” chatbot named Tay in March, it quickly began using racist language and promoting neo-Nazi views on Twitter. And after Facebook eliminated human editors who had curated “trending” news stories last month, the algorithm immediately promoted fake and vulgar stories on news feeds, including one article about a man masturbating with a chicken sandwich.

While the seemingly racist beauty pageant has prompted jokes and mockery, computer science experts and social justice advocates say that in other industries and arenas, the growing use of prejudiced AI systems is no laughing matter. In some cases, it can have devastating consequences for people of color.

Beauty.AI – which was created by a “deep learning” group called Youth Laboratories and supported by Microsoft – relied on large datasets of photos to build an algorithm that assessed beauty. While there are a number of reasons why the algorithm favored white people, the main problem was that the data the project used to establish standards of attractiveness did not include enough minorities, said Alex Zhavoronkov, Beauty.AI’s chief science officer.

Although the group did not build the algorithm to treat light skin as a sign of beauty, the input data effectively led the robot judges to reach that conclusion.

View image in fullscreen Winners of the Beauty.AI contest in the category for women aged 18-29. Photograph: http://winners2.beauty.ai/#win

“If you have not that many people of color within the dataset, then you might actually have biased results,” said Zhavoronkov, who said he was surprised by the winners. “When you’re training an algorithm to recognize certain patterns … you might not have enough data, or the data might be biased.”

The simplest explanation for biased algorithms is that the humans who create them have their own deeply entrenched biases. That means that despite perceptions that algorithms are somehow neutral and uniquely objective, they can often reproduce and amplify existing prejudices.

The Beauty.AI results offer “the perfect illustration of the problem”, said Bernard Harcourt, Columbia University professor of law and political science who has studied “predictive policing”, which has increasingly relied on machines. “The idea that you could come up with a culturally neutral, racially neutral conception of beauty is simply mind-boggling.”

The case is a reminder that “humans are really doing the thinking, even when it’s couched as algorithms and we think it’s neutral and scientific,” he said.

Civil liberty groups have recently raised concerns that computer-based law enforcement forecasting tools – which use data to predict where future crimes will occur – rely on flawed statistics and can exacerbate racially biased and harmful policing practices.

“It’s polluted data producing polluted results,” said Malkia Cyril, executive director of the Center for Media Justice.

A ProPublica investigation earlier this year found that software used to predict future criminals is biased against black people, which can lead to harsher sentencing.

“That’s truly a matter of somebody’s life is at stake,” said Sorelle Friedler, a professor of computer science at Haverford College.

A major problem, Friedler said, is that minority groups by nature are often underrepresented in datasets, which means algorithms can reach inaccurate conclusions for those populations and the creators won’t detect it. For example, she said, an algorithm that was biased against Native Americans could be considered a success given that they are only 2% of the population.

“You could have a 98% accuracy rate. You would think you have done a great job on the algorithm.”

Friedler said there are proactive ways algorithms can be adjusted to correct for biases whether improving input data or implementing filters to ensure people of different races are receiving equal treatment.

Prejudiced AI programs aren’t limited to the criminal justice system. One study determined that significantly fewer women than men were shown online ads for high-paying jobs. Last year, Google’s photo app was found to have labeled black people as gorillas.

Cyril noted that algorithms are ultimately very limited in how they can help correct societal inequalities. “We’re overly relying on technology and algorithms and machine learning when we should be looking at institutional changes.”

Zhavoronkov said that when Beauty.AI launches another contest round this fall, he expects the algorithm will have a number of changes designed to weed out discriminatory results. “We will try to correct it.”



But the reality, he added, is that robots may not be the best judges of physical appearance: “I was more surprised about how the algorithm chose the most beautiful people. Out of a very large number, they chose people who I may not have selected myself.”. Only a few winners were Asian and one had dark skin, most were white

Just months after Microsoft's Tay artificial intelligence sent racist messages on Twitter, another AI seems to have followed suit.

More than 6,000 selfies of individuals who live all over the world and range in ages of 18 to 69 were judged by a robot in a beauty contest last week.

But when the results came in, there was something missing - it turned out the robots did not like people with dark skin.

Scroll down for video

The Beauty.AI beauty contest put together of robot judges to determine the winners. More than 6,000 people from around the world submitted head shots to be analysed by the algorithms

WHAT ROBOTS JUDGED THE CONTEST? Beauty.AI used five algorithms to act as judges in a beauty contest. These robots looked for youthfulness, face symmetry, skin quality, appearance and many other parameters, and then compared the results to models and actors in a database. These robots were designed by different groups of data scientists. RYNKL scored people by their youthfulness within their age group, specifically the AI looked to see if the contestant had more wrinkles than they should for their age. PIMPL analyzed the amount of pimples and pigmentation. Symmetry Master evaluated they symmetry of each person's face and AntiAgeist estimated the difference between the chronological and perceived age. MADIS took the parameters and compared them with models and actors in their age and ethnic groups that were stored in a database. Advertisement

Out of the 44 winners of the Beauty.AI beauty contest, nearly all were white.

A few of the winners were Asian, and only one had dark skin, which surprised those running the competition.

Although the majority of contestants were white, large groups from India and Africa also submitted photographs.

This could be why the algorithm picked mainly white people, the company said.

'If you have not that many people of colour within the dataset, then you might actually have biased results,' Alex Zhavoronkov, chief science officer of Beauty.AI, told The Guardian.

'When you're training an algorithm to recognize certain patterns … you might not have enough data, or the data might be biased.'

The majority, 75 per cent,of contest entrants were European and white.

Seven per cent were from India, and one percent were from the African continent.

The contest used five algorithms to evaluate youthfulness, face symmetry, skin and other parameters, and then compare them to models and actors in a database.

The team at the Russia and Hong Kong-based Youth Laboratories, the masterminds behind this project, asked individuals from around the world to download the app and snap their best selfie for the first step of this ambitious challenge.

Out of the 44 winners nearly all were white. A few of the winners were Asian, and only one had dark skin. Pictured are the women who won in the age group 40-49

The rules were strict, as they stated participants could not wear makeup, sunglasses or sport a beard in their submissions.

A 'Robot Jury' was also recruited for the contest, which is a group of scientists who might want 'go down in history as one of the first data scientists who taught a machine to estimate human attractiveness'.

Youth Laboratories noted that researchers needed to use deep neural networks and GPU training in their systems.

And on July 5th, the team closed submissions for the robot judges and picked the top five shortly after.

RYNKL scored people by their youthfulness within their age group, specifically the AI looked to see if the contestant had more wrinkles than they should for their age.

Besides judging the contest, this technology is used to track people's wrinkles over time in order to see if treatments aimed at reversing signs of aging are working.

The systems announced their winners from more than 6,000 user-submitted selfies from individuals who live all over the world and are ages of 18 to 69. Pictured are the men who won in the age group 18-29

The team at Youth Laboratories, the masterminds behind this project, asked individuals from around the world to download the app and snap their best selfie for the first step of this ambitious challenge. Pictured are the women in the age group 18-29

PIMPL did what its name suggests, it analysed the amount of pimples and pigmentation.

Symmetry Master evaluated they symmetry of each person's face and AntiAgeist estimated the difference between the chronological and perceived age.

Once these parameters were determined, the fifth robot, called MADIS, compared each selfie to models and actors within their age and ethnic groups were are stored in a database.

The purpose of this futuristic beauty contest wasn't only to crown the most beautiful people in the world, it was also meant to understand people's health in new ways.

'This has enabled the team of biogerontologists and data scientists, who believe that in the near future machines will be able to get a lot of vital medical information about people's health by just processing their photos,' reads the Beauty.AI website, which is a project by Youth Laboratories.

The rules were strict, as they stated participants could not wear makeup, sunglasses or sport a beard in their submissions. Pictured are the women, ages 30-39, who were crowned the winners in their age group

A 'Robot Jury' was also recruited for the contest, which is a group of scientists who might want 'go down in history as one of the first data scientists who taught a machine to estimate human attractiveness'. Pictured are the men, ages 30-39, who were crowned the winners in their age group

'To develop a set of algorithms that can accurately evaluate the criteria linked to perception of human beauty and health where it is most important – the human face.'

Around 50.4 percent of the participants were males, while 49.6 percent were female.

Submissions came from all over the world, however the team was surprised to receive selfies from American Samoa, Antarctica and North Korea.

Once these parameters were determined, one of the robots called MADIS compared each selfie to models and actors within their age and ethnic groups were are stored in a database. Pictured are men chosen as winners for the age group 40-49

AI PREDICTS WINNER OF CHINA'S 'I AM A SINGER' Alibaba Cloud, the cloud-computing group at Alibaba, is using this competition to showcase its Apsara-I (Ai). The group says the program can gather insight from a multitude of inputs, learn by analyzing data and could one day understand human emotions. It will choose the winner by absorbing data in real time, which will allow it to compare things like voice pitch and energy of the singers during the contest, how popular their song of choice is, fan favorites, audience feedback at the venue, and even the tenor of online discussions in social media. Alibaba says Ai can learn 10,000 times faster than humans and has been fed data about millions of songs to add to its knowledge base, reports Alizila. The program updated its prediction every five seconds during the competition that took place on April 8th. 'We are very pleased with the Ai's performance in achieving 100 percent accuracy in predicting the I'm a Singer competition's results,' Dr. Min Wanli, Alibaba Cloud's chief scientist for artificial intelligence, said in a statement following the show, according to the Wall Street Journal. The results demonstrated that Ai is making significant progress to understand human emotions and how people make decisions,' he added. Some experts were wary due to the emotional and subjective aspect of assessing how a person sings, but Alibaba assured them 'Ai' was taught to appreciate pitch, range and tone. '[The result] is very random and almost impossible to predict using human intelligence,' said Min Wanli, chief scientist for artificial intelligence at Alibaba Cloud. Advertisement

Asia had the most countries participate with 33, then Europe with 36 and Africa and North America tied with 12.

The results of Beauty.AI 2.0 were first sent to the winners and were officially announced last week.

However, the team said the results were surprising, as the scores from the robots did not agree with human opinion.

Many of the participants, not picked as winners, responded with angry emails criticizing those who were chosen by the robots.

'We were shocked by the outrage caused by the leaderboard. Many participants disagreed with the jury and proposed getting a human opinion', said Anastasya Georgievskaya, Research Manager at Youth Laboratories, the co-organizer of Beauty.AI.

Approximately 50.4 percent of the participants were males, while 49.6 percent were female. Submissions came from all over the world, however the team was surprised to receive selfies from American Samoa, Antarctica and North Korea. Pictured are the women who were crowned winners for the age group 50-59

Asia had the most countries participate with 33, then Europe with 36 and Africa and North America tied with 12. The results of Beauty.AI 2.0 were first sent to the winners and were officially announced last week. Pictured are the men who were crowned winners for the age group 50-59

'But with every Beauty.AI contest we are getting more ideas on how to evaluate the human face and even go beyond the face evaluating multiple features of the human body and even social profiles.'

The team received proposal to turn the Beauty.AI into a physical on-stage show with robots judging human models and may consider launching a pilot in 2017.

It also received several proposals to run Bauty.AI contests on a regional basis.

The team received proposal to turn the Beauty.AI into a physical on-stage show with robots judging human models and may consider launching a pilot in 2017. It also received several proposals to run Bauty.AI contests on a regional basis. Pictured are the women who won in the age group 60-69 and over

Beauty.AI has already announced it will be starting its third contest on October 1st and algorithm developers with novel concepts evaluating perception of the human face are welcome to participate.Pictured are the men who won in the age group 60-69 and over

'With every contest we are getting new ideas, new partners and new robots,' said Konstantin Kiselev, Director of Development at Youth Laboratories.

I think that in 2-3 years these technologies will be very important and will help facilitate human-robot interaction.

'We are also finalizing the new app to track health status using your mobile phone.'. If you’re one who joins beauty pageants or merely watches them, what would you feel about a computer algorithm judging a person’s facial attributes? Perhaps we should ask those who actually volunteered to be contestants in a beauty contest judged by an artificial intelligence (AI).

Beauty Contest Judged by Artificial Intelligence (The Guardian)

Over the summer, 60,000 people sent their selfies devoid of makeup, facial hair and sunglasses through an app called Beauty.AI. There are six AI judges employed to do the task of judging the men and women entries with ages 18 to 69, through parameters like wrinkles, face symmetry and skin color, among others.

The results are in, and the winners are…



Beauty Contest Judged by Artificial Intelligence (Source: Beauty.AI)

There are over 100 participating countries with Asian and Indian finalists, but the results show an absence of diversity. Alex Zhavoronkov, CSO of Youth Laboratories and CEO of Insilico Medicine, the two companies behind the app, said that they had challenges upon working with darker skin or inconsistent light. He added, “The quality control system that we built might have excluded several images where the background and the color of the face did not facilitate for proper analysis.”

Zhavoronkov was quick to admit that the implication of the results could lead to unintentional bias in the future when we are more reliant to AI.

Delivering this project wasn’t just for fun, but was derived from a project involving an AI which evaluates health and hopefully slow aging in the future. But at least thank to this experiment, we know that future AI might just be racist.. The first international beauty contest judged by “machines” was supposed to use objective factors such as facial symmetry and wrinkles to identify the most attractive contestants. After Beauty.AI launched this year, roughly 6,000 people from more than 100 countries submitted photos in the hopes that artificial intelligence, supported by complex algorithms, would determine that their faces most closely resembled “human beauty”.

But when the results came in, the creators were dismayed to see that there was a glaring factor linking the winners: the robots did not like people with dark skin.

Out of 44 winners, nearly all were white, a handful were Asian, and only one had dark skin. That’s despite the fact that, although the majority of contestants were white, many people of color submitted photos, including large groups from India and Africa.

The ensuing controversy has sparked renewed debates about the ways in which algorithms can perpetuate biases, yielding unintended and often offensive results.

When Microsoft released the “millennial” chatbot named Tay in March, it quickly began using racist language and promoting neo-Nazi views on Twitter. And after Facebook eliminated human editors who had curated “trending” news stories last month, the algorithm immediately promoted fake and vulgar stories on news feeds, including one article about a man masturbating with a chicken sandwich.

While the seemingly racist beauty pageant has prompted jokes and mockery, computer science experts and social justice advocates say that in other industries and arenas, the growing use of prejudiced AI systems is no laughing matter. In some cases, it can have devastating consequences for people of color.

Beauty.AI – which was created by a “deep learning” group called Youth Laboratories and supported by Microsoft – relied on large datasets of photos to build an algorithm that assessed beauty. While there are a number of reasons why the algorithm favored white people, the main problem was that the data the project used to establish standards of attractiveness did not include enough minorities, said Alex Zhavoronkov, Beauty.AI’s chief science officer.

Although the group did not build the algorithm to treat light skin as a sign of beauty, the input data effectively led the robot judges to reach that conclusion.

View image in fullscreen Winners of the Beauty.AI contest in the category for women aged 18-29. Photograph: http://winners2.beauty.ai/#win

“If you have not that many people of color within the dataset, then you might actually have biased results,” said Zhavoronkov, who said he was surprised by the winners. “When you’re training an algorithm to recognize certain patterns … you might not have enough data, or the data might be biased.”

The simplest explanation for biased algorithms is that the humans who create them have their own deeply entrenched biases. That means that despite perceptions that algorithms are somehow neutral and uniquely objective, they can often reproduce and amplify existing prejudices.

The Beauty.AI results offer “the perfect illustration of the problem”, said Bernard Harcourt, Columbia University professor of law and political science who has studied “predictive policing”, which has increasingly relied on machines. “The idea that you could come up with a culturally neutral, racially neutral conception of beauty is simply mind-boggling.”

The case is a reminder that “humans are really doing the thinking, even when it’s couched as algorithms and we think it’s neutral and scientific,” he said.

Civil liberty groups have recently raised concerns that computer-based law enforcement forecasting tools – which use data to predict where future crimes will occur – rely on flawed statistics and can exacerbate racially biased and harmful policing practices.

“It’s polluted data producing polluted results,” said Malkia Cyril, executive director of the Center for Media Justice.

A ProPublica investigation earlier this year found that software used to predict future criminals is biased against black people, which can lead to harsher sentencing.

“That’s truly a matter of somebody’s life is at stake,” said Sorelle Friedler, a professor of computer science at Haverford College.

A major problem, Friedler said, is that minority groups by nature are often underrepresented in datasets, which means algorithms can reach inaccurate conclusions for those populations and the creators won’t detect it. For example, she said, an algorithm that was biased against Native Americans could be considered a success given that they are only 2% of the population.

“You could have a 98% accuracy rate. You would think you have done a great job on the algorithm.”

Friedler said there are proactive ways algorithms can be adjusted to correct for biases whether improving input data or implementing filters to ensure people of different races are receiving equal treatment.

Prejudiced AI programs aren’t limited to the criminal justice system. One study determined that significantly fewer women than men were shown online ads for high-paying jobs. Last year, Google’s photo app was found to have labeled black people as gorillas.

Cyril noted that algorithms are ultimately very limited in how they can help correct societal inequalities. “We’re overly relying on technology and algorithms and machine learning when we should be looking at institutional changes.”

Zhavoronkov said that when Beauty.AI launches another contest round this fall, he expects the algorithm will have a number of changes designed to weed out discriminatory results. “We will try to correct it.”



But the reality, he added, is that robots may not be the best judges of physical appearance: “I was more surprised about how the algorithm chose the most beautiful people. Out of a very large number, they chose people who I may not have selected myself.”. Beauty pageants have always been political. After all, what speaks more strongly to how we see each other than which physical traits we reward as beautiful, and which we code as ugly? It wasn't Beauty pageants have always been political. After all, what speaks more strongly to how we see each other than which physical traits we reward as beautiful, and which we code as ugly? It wasn't until 1983 that the Miss America competition crowned a black woman as the most beautiful woman in the country.

So what if we replaced human judges with machines? A robot would ideally lack a human's often harmful social biases. As shallow as the whole thing is, would a computer at least be able to see past skin colour and look at, potentially, more universal markers of attractiveness? Or hell, even appreciate a little melanin? Not really, as it turns out. So what if we replaced human judges with machines? A robot would ideally lack a human's often harmful social biases. As shallow as the whole thing is, would a computer at least be able to see past skin colour and look at, potentially, more universal markers of attractiveness? Or hell, even appreciate a little melanin? Not really, as it turns out.

Advertisement

, an initiative by the Russia and Hong Kong-based Youth Laboratories and supported by Microsoft and Nvidia, ran a beauty contest with 600,000 entrants, who sent in selfies from around the world—India, China, all over Africa, and the US. They let a set of three algorithms judge them based on their face's symmetry, their wrinkles, and how young or old they looked for their age. The algorithms did not evaluate skin color. Beauty.ai , an initiative by the Russia and Hong Kong-based Youth Laboratories and supported by Microsoft and Nvidia, ran a beauty contest with 600,000 entrants, who sent in selfies from around the world—India, China, all over Africa, and the US. They let a set of three algorithms judge them based on their face's symmetry, their wrinkles, and how young or old they looked for their age. The algorithms did not evaluate skin color.

The results, The results, released in August , were shocking: Out of the 44 people that the algorithms judged to be the most "attractive," all of the finalists were white except for six who were Asian. Only one finalist had visibly dark skin.

How the hell did this happen? How the hell did this happen?

Image: Beauty.ai

The first thing to know is that all three algorithms used a style of machine learning called " The first thing to know is that all three algorithms used a style of machine learning called " deep learning ." In deep learning, an algorithm is "trained" on a set of pre-labeled images so that when presented with a new image, it can predict with a degree of certainty what it's looking at. In the case of Beauty.ai, all the algorithms were trained on open source machine learning databases that are shared between researchers.

Deep learning is the most powerful form of machine intelligence we have, and is used by massive companies like Alphabet and Facebook. However, some recent work has discovered that these systems can harbor all kinds of unexpected—and very human—biases. For example, a language processing algorithm Deep learning is the most powerful form of machine intelligence we have, and is used by massive companies like Alphabet and Facebook. However, some recent work has discovered that these systems can harbor all kinds of unexpected—and very human—biases. For example, a language processing algorithm was recently found to rate white names as more "pleasant" than black names, mirroring earlier psychology experiments on humans.

Advertisement

"It happens to be that color does matter in machine vision"

The problem here is with the lack of diversity of people and opinions in the databases used to train AI, which are created by humans. The problem here is with the lack of diversity of people and opinions in the databases used to train AI, which are created by humans.

"We had this problem with our database for wrinkle estimation, for example," said Konstantin Kiselev, chief technology officer 