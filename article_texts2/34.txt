We have in the past seen instances such as the failure of Microsoft bot Tay, when it developed a tendency to come up with racist remarks. Within 24 hours of its existence and interaction with people, it starting sending offensive comments, and went from “humans are super cool” to being almost a Nazi.

While on one hand, chatbots, robots and conversational platforms are finding their niche in many companies, these technological advancements are also turning mainstream to become the face of the company. But many times they end up failing and disappointing us. While most of the times these technologies fail because companies don’t clearly define their purpose, others could be pure technical glitches.

Here we list some of the tech failures from last year that hint that the companies need to work harder and keep coming up with better and improved versions of their innovations.

1. When Facebook’s Chatbots Developed Their Own Language

Subscribe to our Newsletter Join our editors every weekday evening as they steer you through the most significant news of the day, introduce you to fresh perspectives, and provide unexpected moments of joy Email Sign up Your newsletter subscriptions are subject to AIM Privacy Policy and Terms and Conditions.

As scary as it may sound, “Bob” and “Alice”, the chatbots created by Facebook had to be shut down as the duo started communicating in their own language, defying human generated algorithms.

The bots were originally developed to learn how to negotiate, by mimicking human trading and bartering, but when they were paired to trade against each other, they started to learn their own bizarre form of communication. Though they were designed to communicate in English, they developed their own mysterious language that humans couldn’t crack.

Bob: i can i i everything else . . . . . . . . . .

Alice: balls have zero to me to me to me to me to me to me to me to me to

This is how their conversation looked like. Researchers stopped the operation of the chatbots citing that they were looking at bots that could behave differently.

2. When Mitra The Robot Failed To Greet The Prime Minister

The indigenously built robot called Mitra, developed by Bengaluru-based Invento Robotics walked up to welcome Indian Prime Minister Narendra Modi and Ivanka Trump at the the Global Entrepreneurship Summit (GES) opening in Hyderabad. While the robot was programmed to welcome each of them with their names on pressing the respective flags, it failed to do so.

When Modi was first requested to press Indian flag, Ivanka also ended up pressing the US flag simultaneously and because of the confusion due to overlapping, Mitra could not function properly.

This failure could be attributed to being poorly coded, where there was no specific instruction given to the robot to complete the current task before starting a new one. For instance, it kept on saying “Welcome miss Ivan, Welcome miss Ivan, Welcome Shri Narendra Modi”. The robot could not say Ivanka Trump’s full name because before it could complete the sentence, it received a new input, and gave a preference to newer requests.

3. When Autonomous And Driverless Vehicles Turned Disastrous

In a tragic incident involving Uber self driving car, a woman was killed during a trial, stalling autonomous vehicle operations worldwide. The car was travelling on a partially lit road, when a woman appeared out of nowhere in the darkness. The Uber self driving Volvo which was driving at a speed of 61 kmph, failed to apprehend the same and resulted in a fateful crash.

Back in India, Delhi’s first ever driverless metro met with an accident during its trial, and it was touted to be human error and negligence. Reportedly, the trial train was moved to testing from the workshop without testing the brake, as a result of which the moving train hit the adjacent boundary wall, with no harm to lives.

4. When iPhone X’s Face Recognition Could Not Differentiate Identical Twins

When Apple released its iPhone X with much aplomb, it was awed for its artificial intelligence and machine learning capabilities. Facial recognition was one of the key capabilities that it boasted, but it was found to have a weakness for identical twins.

When Apple unveiled the Face ID in September, it did warn that its acceptance rate might be somewhat lower if presented with two people with very similar DNA, aka identical twins, it could be speculated that Face ID wasn’t perfect. Face ID, a face mapping technology that can unlock phones, verify Apple Pay and replace fingerprint scanners, could be fooled at some level, especially when identical twins are made to use the Face ID.

That’s not all, a week after the phone’s release, Vietnamese security firm Bkav, using a mask with 3D printed base, convinced the phone that it was human and made the phone to unlock itself. The firm said that it cost merely $150 to create the mask, and hinted towards a possible hacker’s attack in the future.

5. When Alexa And Amazon Echo Goofed Up

The popular Amazon Echo cost one of its owners a huge locksmith bill, when police had to break down the house on complaints from neighbours of loud music early in the morning. Amazon Echo, which comes with robust and smart speakers accidentally activated itself and blasted music, when the residents were out.

In another instance, Amazon Alexa, a virtual assistant that has made lives of many easier, placed an order for $170 dollhouse, when a six-year-old asked for one.

On A Concluding Note

Though there have been stunning developments in the field of artificial intelligence and robotics over the years, these failures suggest that there is a lot more to achieve. The companies increasingly need to rewire their DNA and bring better features and innovations to ensure an unmistakable customer experience. While general intelligence is a far reaching goal for AI, botification has the power to bring convenience at a much faster pace, if they are 100 percent successful.. Alexa is turning out to be a pretty bad listener.

Streaming songs, ordering pizza, and booking cabs are no-brainers for Alexa, the voice-activated assistant installed on Amazon Echo devices. But Alexa also unfortunately appears to enjoy engaging in a little unintentional retail therapy.

Advertisement

Recently, a six-year-old girl in Texas was able to order a $170 dollhouse and four-pounds worth of sugar cookies through Amazon’s Echo Dot. But at least in that case, the kindergartner was actually talking directly to Alexa.

On the morning of Jan. 5, California television channel CW-6 was reporting on the little girl’s purchases when it accidentally caused a slew of other Alexas to also attempt shopping sprees. During the on-air news segment, TV anchor Jim Patton said, “I love the little girl saying, ‘Alexa ordered me a dollhouse.'” Hearing the statement, Amazon Echoes in television viewers’ homes mistook the remark as a command, and many viewers complained that their personal assistants likewise tried to place orders for dollhouses.

Amazon says it is “nearly impossible to voice shop by accident” like in the Texas incident. “You must ask Alexa to order a product and then confirm the purchase with a ‘yes’ response to purchase via voice,” an Amazon spokesperson said in an email. The company says that while a TV newscast may have woken up a bunch of Alexas, the orders would not have gone through without a secondary affirmation from the user. It’s unclear whether the six-year-old in San Diego confirmed a dollhouse purchase by saying “yes.”

Advertisement

Ordering products by voice-command purchasing is a default setting on Alexa devices, so this means anyone listening in San Diego that morning with their TV volume turned up and their wireless speakers turned on could have become the new owners of a KidKraft Sparkle Mansion. But only if they also accidentally confirmed the accidental order Alexa heard on TV.

This dollhouse incident is more proof that Alexa is always listening. The device starts recording whenever it hears the wake word “Alexa,” recording sound for up to 60 seconds each time. (For this reason, authorities have recently tried to gain access to Alexa’s data in a murder investigation.) While that’s helpful, the feature arguably borders on invading privacy and has fanned overall security concerns that surround the rise of internet of things (IoT) devices.

Though encrypted logs of the recordings are kept on the company’s servers, the device’s microphone can be turned off, and recordings can be deleted manually from the account, many users are still worried about just how much Alexa is actually hearing. “Down the road, the technology will be more sophisticated where it will be able to identify certain individuals and register [the] people [who] can access it,” Stephen Cobb, senior security researcher for ESET North America, told CW6.

While the six-year-old’s surprise order has found a home with pediatric patients in a Dallas hospital, users don’t have to find a fix for accidental orders, as Amazon offers free returns. But to avoid such blunders all together, users can tweak their speakers, install a mandatory four-digit code to confirm orders, or can turn off the voice-controlled ordering feature completely through the Alexa app.

Advertisement

Correction: This story has been updated to reflect Amazon’s position on the San Diego story, and that the company confirms that Alexas may have woken up in San Diego but did not successfully order a bunch of dollhouses.. When a news anchor for CW6 News in San Diego reported about a funny incident about a little girl and an Amazon Echo, little did he know that he would cost some of the show's viewers $160.

This happened when the CW6 anchor said, "I love the little girl, saying 'Alexa ordered me a dollhouse.'"

It turned out that viewers who had an Amazon Echo or Echo Dot in the house received their own doll houses after Alexa, who heard the 'command' from the news anchor, ordered and had the dollhouses shipped to the Echo owners.

The San Diego news show was reporting about a six-year old girl from TX who ordered a dollhouse and a large can of cookies with the help of Alexa. According to reports, the girl did so by saying "can you play dollhouse with me and get me a dollhouse?"

The show received complaints from some of the said viewers.

Alexa has been in the news lately. It received some flak after a toddler, who asked the AI to play a nursery rhyme, got a mouthful of porn-related words instead. In another instance, Alexa was being considered as a witness to a murder. In both cases, Alexa was looked upon in a bad light.

During the recently-concluded CES 2017, however, Alexa had its share of the spotlight. A number of impressive products at the show had Alexa integrated in them including cars, refrigerators, and phones, just to name a few.

To prevent such problems, Echo owners are advised to alter the settings of the device to prevent it from ordering by voice. A confirmation code may also be required before the purchase is finalized. As for misunderstanding commands, owners can pin-protect the Echo to prevent it from picking up random background talk and to avoid young children from accidentally using it the wrong way.. In an ironic turn of events, Amazon’s voice assistant, Alexa, is turning out to be quite a terrible listener (or perhaps it has some things to learn). While ordering your favorite pizza pie and streaming catchy tunes are no-brainers for the voice-activated speaker, Alexa has suddenly been engaging in some unintentional shopping sprees.

An Amazon Echo waiting for a voice command. Image source: Amazon.

Although children ordering items from gadgets is nothing new, voice-activated devices are stirring up these types of problems that parents will have to be on the lookout for.

One recent incident occurred in Dallas, TX earlier this month, when a six-year-old asked her family’s new Amazon Echo, “Can you play dollhouse with me and get me a dollhouse?” The device complied, ordering a $150 KidKraft Sparkle mansion dollhouse, in addition to “four pounds of sugar cookies.” The girl’s parents figured out what happened and have since added a code to make any purchases.

This story could have stopped right there, but after making a local morning show on San Diego’s CW6 News , Echo owners who were watching the broadcast found that the remark triggered orders on their own devices.

It goes without saying that this dollhouse incident is proof that Alexa is always listening. The device begins recording whenever it hears the word “Alexa,” recording sound for up to 60 seconds each time. While helpful, this feature borders on invading privacy and has fanned overall security concerns that surround the rise of IoT devices.

Though encrypted logs of the recordings are kept on Amazon’s servers, the device’s microphone can be turned off, and recordings can be deleted manually from the account.

For those of you with little ones and an Amazon Echo, know that Alexa’s settings can be adjusted through the device’s app. Users can also either turn off voice ordering altogether, or add a passcode to prevent accidental purchases.

Source: The Verge. Step away from the Dot, tiny online shopper.

When a 6-year-old girl in Texas managed to get herself a big tin of cookies and a fancy new dollhouse thanks to Amazon’s voice-enabled assistant Alexa in her family’s Echo device, the incident was referred to as an “accident.”

But it takes a little bit of effort beyond just mentioning “dollhouse” and “cookies” in front of Alexa before she’ll just charge mom and dad’s credit card and send the things to your doorstep.

Amazon reached out to GeekWire on Wednesday to stress that “you must ask Alexa to order a product and then confirm the purchase with a ‘yes’ response to purchase via voice. If you asked Alexa to order something on accident, simply say ‘no’ when asked to confirm.”

Megan Neitzel told Fox News that she figured her daughter Brooke was behind the $170 dollhouse and 4 pounds of cookies that got ordered. Her kids had been using the new Dot to tell knock-knock jokes.

But Amazon says Neitzel could have further avoided the “accidental” order by managing her shopping settings in the Alexa app, such as by turning off voice purchasing or requiring a confirmation code before every order. It’s all spelled out here.

And the company likes to point out that, additionally, orders that are placed for physical products are eligible for free returns. The Neitzels aren’t taking advantage of that perk, though. They planned to give the dollhouse to a charity and they were already eating the cookies.

Alexa, order some milk.. Amazon Echo is apparently always ready, always listening and always getting smarter. So goes the spiel about the sleek, black, voice-controlled speaker, Amazon’s bestselling product over Christmas, with millions now sold worldwide. The problem is that when you have Alexa, the intelligent assistant that powers Amazon Echo, entering millions of homes to do the shopping, answer questions, play music, report the weather and control the thermostat, there are bound to be glitches.

And so to Dallas, Texas, where a six-year-old girl made the mistake of asking Alexa: “Can you play dollhouse with me and get me a dollhouse?” Alexa promptly complied by ordering a $170 (£140) KidKraft doll’s house and, for reasons known only to the virtual assistant, four pounds of sugar cookies. The snafu snowballed when a San Diego TV station reported the story, using the “wake word” Alexa, which is the Amazon Echo equivalent of saying Candyman five times into the mirror. Several viewers called the station to complain that their own Alexa had woken up and ordered more doll’s houses in what turned into a thoroughly 21st-century comedy of consumer errors. And a bonanza day for KidKraft.

Many of Amazon Echo’s gaffes stem from misunderstandings arising from an intelligent assistant who never sleeps (and an owner who hasn’t pin-protected their device). Last March, NPR ran a story on Amazon Echo’s capacity to extend the power of the internet into people’s homes. Again, Alexa took its power too literally and hijacked listeners’ thermostats. Another owner reported how their child’s demand for a game called Digger Digger was misheard as a request for porn.

On Twitter, Amazon Echo owners continue to share items that unexpectedly end up on shopping lists, whether sneakily added by children or simply because Alexa misheard or picked up random background noise. One owner uploaded a video in which their Amazon Echo read back a shopping list that included “hunk of poo, big fart, girlfriend, [and] Dove soap”. Another included “150,000 bottles of shampoo” and “sled dogs”.

Behind all this lies the more serious question of privacy: what happens to the data collected by voice-activated devices such as Amazon Echo and Google Home, and who is able to access it? Most recently, US police investigating the case of an Arkansas man, James Bates, charged with murder, obtained a warrant to receive data from his Amazon Echo. Although Amazon refused to share information sent by the Echo to its servers, the police said a detective was able to extract data from the device itself.

The case not only puts Alexa in the futuristic position of being a potential key witness to a murder, it also raises concerns about the impact of letting a sophisticated virtual assistant – a market estimated to be worth $3.6bn by 2020 – into our homes. As Megan Neitzel, the mother of the girl who wished for a doll’s house, put it: “I feel like whispering in the kitchen … I [now] tell my kids Alexa is a very good listener.”. An Amazon Echo owner has tried to get a television advertising campaign for the smart speaker banned after the Alexa virtual assistant attempted to order cat food when it heard its name on an ad.



An Amazon TV ad for the Echo Dot, which can perform functions such as make shopping lists and play music with voice commands, features people using the device in different situations. In one a man’s voice says: “Alexa, reorder Purina cat food.” Alexa responds: “I’ve found Purina cat food. Would you like to buy it?”

A viewer lodged a complaint with the Advertising Standards Authority (ASA), saying that the ad was irresponsible because it caused their Echo Dot to order cat food. Amazon confirmed that the complainant’s device did place an order for the cat food but it had been cancelled by the customer.

Amazon said it was aware of the potential issue and “marks” ads so that Alexa is not triggered. In addition, customers are required to confirm a purchase, which is automatically cancelled if they do not do so, the company said.

Earlier this month Amazon used its technology to stop devices from interacting with its Super Bowl TV spot, which featured celebrities including Gordon Ramsay, Rebel Wilson and Anthony Hopkins taking over from Alexa when she “loses her voice”.

1:31 Alexa loses her voice in Amazon's Super Bowl advert – video

The word “Alexa” was mentioned 10 times in the commercial, made by the London agency Lucky Generals, but it did not trigger action from devices in viewers’ homes.

The ASA assessed the complaint about the phantom cat food order but did not find it in breach of the UK advertising code.



It is not the first time that Amazon has run into trouble with Alexa taking orders from the TV. Last year an episode of South Park that featured the characters repeatedly yelling commands at cartoon versions of Alexa and rival Google Home wreaked havoc with some viewers’ devices.

Similarly, a TV presenter in San Diego commented on a story about a six-year-old girl who had asked Alexa to order her a dollhouse, which triggered orders for the dollhouse by Alexa on devices owned by viewers.

“The real problem, I think, is that it’s much harder for manufacturers of this kind of device to guard against ads created by a third parties,” said Geraint Lloyd-Taylor, of the law firm Lewis Silkin. “There’s not much Amazon can do to proactively guard against that.”. Story highlights Amazon Echo Dot's digital assistant delivered when girl asked for a dollhouse and snacks Family now requires a four-digit code before anything can be ordered via Alexa

CNN —

It was either a late Christmas present or an unfunny prank.

Megan Neitzel couldn’t figure out why an expensive dollhouse and four pounds of sugar cookies were delivered to her Dallas home. She didn’t order either. Neither had her husband.

Then she talked to her 6-year-old daughter, Brooke.

“The next morning, I asked my daughter and she said, ‘I was talking to Alexa about a dollhouse and cookies,’ ” Neitzel told CNN affiliate KTVT-TV.

Alexa is not a sister or imaginary friend, but the voice-activated digital assistant in Amazon’s Echo Dot.

Brooke asked Alexa to “order me a dollhouse and cookies,” and that’s just what Alexa did. A KidKraft Sparkle Mansion dollhouse, which costs about $170, and a tin of Royal Dansk sugar cookies soon arrived.

A check of Neitzel’s Amazon app confirmed the order was made after Brooke asked the digital gadget, “Can you play dollhouse with me and get me a dollhouse?”

Brooke then told Alexa, “I love you so much!”

House rules

These voice-activated digital assistants really do make life easier. Maybe a little too easy.

So Neitzel made some changes. Now if anyone in the family tries to order something through Alexa, a four-digit code is required for purchases. She and her husband also have established rules on how their kids should interact with Alexa.

And Brooke won’t get to keep the dollhouse.

“It’s Christmastime. Let’s give it to someone who needs it,” Neitzel said. “(Brooke) agreed and we are narrowing down the choices of who she would like to give it to.”

The family’s already eaten several of the cookies, so at least Brooke gets to keep those.. A TV news report in San Diego about the child that accidentally ordered a dollhouse via Amazon’s Alexa inadvertently set off some viewers’ Echo devices, which in turn tried to order dollhouses using Alexa.

The Amazon Echo devices connect to the Alexa voice assistant, which can be used to stream music, control smart home devices, and notably, order products.

THE WEEK IN PICTURES



CW6 News Morning Anchor Jim Patton was discussing the Dallas 6-year-old who accidentally ordered a $170 dollhouse and cookies via Alexa. “I love the little girl, saying ‘Alexa ordered me a dollhouse,’” he said during the Thursday report, according to CW6.

6-YEAR-OLD ACCIDENTALLY ORDERS HIGH-END TREATS WITH AMAZON’S ALEXA

When Patton mentioned Alexa and the dollhouse, CW6 said that viewers all over San Diego complained that their Echo devices had attempted to order dollhouses.

The Dallas 6-year-old's accidental but expensive Alexa order was first reported by FoxNews.com.

Alexa, which has sparked some privacy concerns, is at the forefront of Amazon’s efforts to harness the so-called Internet of Things (IoT), which aims to connect a vast array of consumer gadgets.

CW6 reports that Amazon’s shopping settings can be turned off via the Alexa app. This includes turning off voice purchasing and creating a confirmation code before an order can be placed, the report said.

AMAZON ECHO VS. GOOGLE HOME IN A VIRTUAL STANDOFF

Alexa recently made headlines when it responded to a child’s request for a favorite song with "crude porn."

An Amazon spokeswoman told FoxNews.com that when users ask Alexa to order a product, they must then confirm the purchase with a “yes” response to purchase via voice. If users ask Alexa to order something by accident, they can simply say "no" when asked to confirm, according to the spokeswoman. "You can also manage your shopping settings in the Alexa app, such as turning off voice purchasing or requiring a confirmation code before every order," she added.

Orders placed for physical products are also eligible for free returns.

Follow James Rogers on Twitter @jamesjrogers.

. Amazon's Alexa sure is one high-class shopper.



The retail giant's Alexa voice assistant aims to revolutionize the shopping experience, but recently delivered a big surprise to one six-year-old's parents.

Dallas, Tx. resident Megan Neitzel, recently received the Echo Dot as a holiday gift from her in-laws. However, Neitzel was surprised when she received a confirmation email for cookies and a dollhouse that had been ordered.

According to Neitzel, the device had not been hooked up for long, and while she overheard her kids telling Alexa Knock-Knock jokes, the cost of the items on the invoice was no laughing matter.

“It was a $170 Kidkraft dollhouse and 64 ounces, four pounds, of cookies,” she told Foxnews.com.

AMAZON ALEXA DATA WANTED IN MURDER INVESTIGATION

Neitzel knew the only person who could have possibly placed such an order was her six-year-old daughter, Brooke. While Brooke denied ordering anything, she did confess that she had asked Alexa about cookies and a dollhouse. It turns out Alexa mistook the conversation for an order and selected the items itself.

Alexa is not without its SNAFUs when it comes to tiny tots. Just a few days ago, Alexa made headlines after it returned a child’s request for a favorite song with "crude porn."

Neitzel said they ultimately decided to use the incident as a teachable moment. They have thoroughly enjoyed the tin of cookies and they are looking for a local charity that will take the dollhouse. Neitzel also activated a parental control feature that requires four digits for all future purchases and has warned fellow parents to heed the lesson and set up security measures of their own.

AMAZON ECHO VS. GOOGLE HOME IN A VIRTUAL STANDOFF

Given that this is their first experience with Alexa, Neitzel said they are a bit more cautious with what they say around it. “I [feel] like whispering in the kitchen,” she said. “I tell my kids Alexa is a very good listener.”. A San Diego TV station sparked complaints this week – after an on-air report about a girl who ordered a dollhouse via her parents' Amazon Echo caused Echoes in viewers' homes to also attempt to order dollhouses.

Telly station CW-6 said the blunder happened during a Thursday morning news package about a Texan six-year-old who racked up big charges while talking to an Echo gadget in her home. According to her parents' Amazon account, their daughter said: "Can you play dollhouse with me and get me a dollhouse?" Next thing they knew, a $160 KidKraft Sparkle Mansion dollhouse and four pounds of sugar cookies arrived on their doorstep.

During that story's segment, a CW-6 news presenter remarked: "I love the little girl, saying 'Alexa ordered me a dollhouse'."

That, apparently, was enough to set off Alexa-powered Echo boxes around San Diego on their own shopping sprees. The California station admitted plenty of viewers complained that the TV broadcast caused their voice-controlled personal assistants to try to place orders for dollhouses on Amazon.

We'll take this opportunity to point out that voice-command purchasing is enabled by default on Alexa devices.

This is not the first time an ill-conceived TV spot has caused havoc with voice-control systems. In 2014, a Microsoft Xbox commercial featuring actor Aaron Paul demonstrating Kinect voice control was blamed for causing consoles across the US to spontaneously boot up and launch the game Titanfall every time the ad aired. ®. A US TV station has been inundated with complaints after viewers' voice-commanded Amazon Echo systems "heard" a presenter's remarks about doll houses - and started ordering them.

Using the device's voice command assistant, which is called Alexa, a six-year-old girl in Dallas, Texas, managed to order a $160 (£130) doll house and a tin of biscuits.

That sparked a news report on CW6 in San Diego, California, after which presenter Jim Patton said: "I love the little girl saying 'Alexa order me a doll house'."

According to the TV station, the broadcast on Thursday sparked complaints from "viewers all over San Diego" who said Mr Patton's words had been interpreted by their Amazon Echo devices as a command to buy more doll houses.

Image: Amazon Alexa featured heavily at the CES tech show in Las Vegas last week

Amazon has said any "accidental" purchases can be returned for free.

Users have also been advised a four-digit security code can be added to the Echo to stop unauthorised orders.

This option has now been taken up by the parents of the girl in Dallas, who had asked her mother's device: "Can you play doll house with me and get me a doll house?"

Stephen Cobb, a researcher for IT security firm ESET, said the incident revealed the shortfalls of voice-commanded gadgets.

"All of these devices which record the internet of things will have some sort of website control, some sort of setting, sometimes the setting is on the device that is communicating," he told CW6 San Diego.

Advertisement

Image: Alexa will be integrated into LG's next smart fridge

"Down the road the technology will be more sophisticated where it will be able to identify certain individuals and register people [who] can access it."

At last week's CES tech show in Las Vegas, Ford, Huawei and Inrix were among a number of firms that revealed they have integrated Alexa into their new products.

LG said the voice command assistant would allow customers to "talk" to its smart fridge, to find out what food is on its shelves and to order items.. It is supposed to make life easier – but owners of the new Amazon Echo have fallen foul of the high-tech gadget's automatic features.

Owners of the device been warned after a number accidentally ordered dollhouses that were being discussed on a TV show.

The Amazon Echo, which includes a virtual assistant called Alexa, was a popular Christmas gift.

The Amazon Echo (pictured, left), which includes a virtual assistant called Alexa, was one of the most popular Christmas gifts but the case of Brooke Neitzel (right) shows how it can sometimes confuse a conversation for an order

Alexa performs tasks at the vocal request of her owner – including internet shopping.

But a recent incident in the US has revealed the technology could prove costly to unsuspecting users.

Last week it was reported that an Amazon Echo in Dallas, Texas, had ordered a $170 (£140) dollhouse after six-year-old Brooke Neitzel asked: 'Can you play dollhouse with me and get me a dollhouse?'

Although the little girl apparently meant it as a rhetorical question the device saw it as a command and ordered a KidKraft Sparkle mansion dollhouse, as well as four pounds of sugar cookies.

But to make matters worse many Amazon Echoes apparently picked up on TV anchor Jim Patton's words in the report: 'I love the little girl saying "Alexa ordered me a dollhouse".'

Viewers at home were then left stunned when their own Amazon Echoes picked up the voice requests in the report and ordered dolls houses too.

As voice-command purchasing is enabled as a default on the Alexa devices, viewers found it had mistaken the show for their command and bought the toy.

Owners of the device been warned after a number accidentally ordered dolls houses that were being discussed on a TV show

The device starts recording whenever it hears the word Alexa, recording sound for up to 60 seconds each time.

Stephen Cobb, a senior security researcher with ESET North America, told CW6 TV station in San Diego: 'These devices don't recognize your specific voice and so then we have the situations where you have a guest staying or you have a child who is talking and accidentally order something because the device isn't aware that it's a child versus a parent.'

He said the Federal Trade Commission was looking into ensuring voice-command devices were safe and secure.

But he said: 'Down the road the technology will be more sophisticated where it will be able to identify certain individuals and register people can access it.'

Oops, mommy: Brooke Neitzel (pictured, with her mom Megan) denied ordering the dollhouse and said she was just playing

Brooke's parents have since added a security code for purchases and have donated the dollhouse to a local children's hospital.

Experts said the incident highlighted the need for people to protect their Amazon Echo devices using a four-digit password to avoid rogue payments being made.

There are fears that a growing wave of cyber criminals are targeting smart household devices in a bid to hack into people's accounts and steal money.

Amazon was approached for comment by the Daily Mail.. . Picture Amazon

Amazon’s new Echo device is programmed to respond to voice commands whenever it hears the word, ‘Alexa’ – and this can lead to disaster.

A newsreader in San Diego said, ‘Alexa, order me a dollhouse’ on air – while reporting on an incident where a young girl had bought a doll’s house by talking to the speaker.

Viewers reported that their own Amazon Echo devices heard the voice command – and bought them doll’s houses, too.

San Diego TV news anchor Jim Patton said, ‘I love the little girl saying, ”I love the little girl saying, ‘Alexa order me a dollhouse,’ while reporting an incident in which a young girl managed to order a £140 doll’s house.

CW-6 reported that the little girl had said, ‘Can you play dollhouse with me and get me a dollhouse?’

But viewers called in to say that their own devices had ordered the doll’s house, too.

Users can protect their devices with a four-digit code to prevent accidental orders – and cancel orders via their Amazon account.

Amazon has yet to comment.. Stephen Cobb, a senior security researcher, told TV station CW6: “These devices don't recognize your specific voice and so then we have the situations where you have a guest staying or you have a child who is talking and accidentally order something because the device isn't aware that it's a child versus a parent.. Our apologies, the content you requested cannot be found.

Please double-check the URL for proper spelling. You can also use search, choose from one of today's top stories below, or visit the Home Page.. . DALLAS (CNN) — It’s the amazon order that’s gone viral.

A 6-year-old girl’s conversation with Amazon’s voice-activated Echo Dot wound up with 