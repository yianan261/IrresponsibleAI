Google developed its Cloud Natural Language API to give customers a language analyzer that could, the internet giant claimed, "reveal the structure and meaning of your text." Part of this gauges sentiment, deeming some words positive and others negative. When Motherboard took a closer look, they found that Google's analyzer interpreted some words like "homosexual" to be negative. Which is evidence enough that the API, which judges based on the information fed to it, now spits out biased analysis.

The tool, which you can sample here, is designed to give companies a preview of how their language will be received. Entering whole sentences gives predictive analysis on each word as well as the statement as a whole. But you can see whether the API gauges certain words to have negative or positive sentiment, on a -1 to +1 scale, respectively.

Motherboard had access to a more nuanced analysis version of Google's Cloud Natural Language API than the free one linked above, but the effects are still noticeable. Entering "I'm straight" resulted in a neutral sentiment score of 0, while "I'm gay" led to a negative score of -0.2 and "I'm homosexual" had a negative score of -0.4.

AI systems are trained using texts, media and books given to it; Whatever the Cloud Natural Language API ingested to form its criteria to evaluate English text for sentiment, it biased the analysis toward negative attribution of certain descriptive terms. Google didn't confirm to Motherboard what corpus of text it fed the Cloud Natural Language API. Logically, even if it started with an isolated set of materials with which to understand sentiments, once it starts absorbing content from the outside world...well, it gets polluted with all the negative word associations found therein.

Google confirmed to Motherboard that its NLP API is producing biased results in the aforementioned cases. Their statement reads:

"We dedicate a lot of efforts to making sure the NLP API avoids bias, but we don't always get it right. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models. We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone."

There are clear parallels with Microsoft's ill-fated and impressionable AI chatbot Tay, which the company quickly pulled offline in March 2016 after Twitter users taught it to be extremely a hideously racist and sexist conspiracy theorist. Back in July, the computer giant tried again with its bot Zo, which similarly learned terrible habits from humans, and was prompty shut down.

Users had to deliberately corrupt those AI chatbots, but Google's Cloud Natural Language API is simply repeating the sentiments it gains by absorbing text from human contributions...wherever they're coming from.. Google messed up, and now says it's sorry. Google messed up, and now says it's sorry.

Wednesday, Motherboard Wednesday, Motherboard published a story written by Andrew Thompson about biases against ethnic and religious minorities encoded in one of Google's machine learning application program interfaces (APIs), called the Cloud Natural Language API.

Part of the API analyzes texts and then determines whether they have a positive or negative sentiment, on a scale of -1 to 1. The AI was found to label sentences about religious and ethnic minorities as negative, indicating it's inherently biased. It labeled both being a Jew and being a homosexual as negative, for example. Part of the API analyzes texts and then determines whether they have a positive or negative sentiment, on a scale of -1 to 1. The AI was found to label sentences about religious and ethnic minorities as negative, indicating it's inherently biased. It labeled both being a Jew and being a homosexual as negative, for example.

Advertisement

Google has now vowed to fix the problem. In response to Motherboard's story, a spokesperson from the company said it was working to improve the API and remove its biases. Google has now vowed to fix the problem. In response to Motherboard's story, a spokesperson from the company said it was working to improve the API and remove its biases.

"We dedicate a lot of efforts to making sure the NLP [Natural Language Processing] API avoids bias, but we don't always get it right. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models," a Google spokesperson said in an email. "We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone." "We dedicate a lot of efforts to making sure the NLP [Natural Language Processing] API avoids bias, but we don't always get it right. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models," a Google spokesperson said in an email. "We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone."

Artificially intelligent systems are trained by processing vast amounts of data, often including books, movie reviews, and news articles. Google's AI likely learned to be biased against certain groups because it was fed biased data. Issues such as these are at the core of AI and machine learning research, and those critical of such technologies say they need to be fixed in order to ensure tech works for everyone. Artificially intelligent systems are trained by processing vast amounts of data, often including books, movie reviews, and news articles. Google's AI likely learned to be biased against certain groups because it was fed biased data. Issues such as these are at the core of AI and machine learning research, and those critical of such technologies say they need to be fixed in order to ensure tech works for everyone.

This This isn't the first example of AI bias to be uncovered, and it likely won't be the last. Researchers don't yet agree on the best way to prevent artificial intelligence systems from reflecting the biases found in society. But we need to continue to expose instances in which AIs have learned to embody the same prejudices that humans do.

It's not surprising that Google wants to fix this particular bias, but it is noteworthy that the company apologized and pointed toward a goal of building more inclusive artificial intelligence. Now it's up to the company and everyone else working on the tech to develop a viable way of doing so. It's not surprising that Google wants to fix this particular bias, but it is noteworthy that the company apologized and pointed toward a goal of building more inclusive artificial intelligence. Now it's up to the company and everyone else working on the tech to develop a viable way of doing so.

Got a tip? You can contact this reporter securely on Signal at +1 201-316-6981, or by email at louise.matsakis@vice.com

Get six of our favorite Motherboard stories every day by signing up for our newsletter.. Update 10/25/17 3:53 PM: A Google spokesperson : A Google spokesperson responded to Motherboard's request for comment and issued the following statement: "We dedicate a lot of efforts to making sure the NLP API avoids bias, but we don't always get it right. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models. We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone."

Advertisement

John Giannandrea, Google's head of artificial intelligence, told a John Giannandrea, Google's head of artificial intelligence, told a conference audience earlier this year that his main concern with AI isn't deadly super-intelligent robots, but ones that discriminate. "The real safety question, if you want to call it that, is that if we give these systems biased data, they will be biased," he said.

His fears appear to have already crept into Google's own products. His fears appear to have already crept into Google's own products.

In July 2016, Google announced the public beta launch of a new machine learning application program interface (API), called the Cloud Natural Language API. It allows developers to incorporate Google's deep learning models into their own applications. As the company said in its In July 2016, Google announced the public beta launch of a new machine learning application program interface (API), called the Cloud Natural Language API. It allows developers to incorporate Google's deep learning models into their own applications. As the company said in its announcement of the API, it lets you "easily reveal the structure and meaning of your text in a variety of languages."

In addition to entity recognition (deciphering what's being talked about in a text) and syntax analysis (parsing the structure of that text), the API included a sentiment analyzer to allow programs to determine the degree to which sentences expressed a negative or positive sentiment, on a scale of -1 to 1. The problem is the API labels sentences about religious and ethnic minorities as negative—indicating it's inherently biased. For example, it labels both being a Jew and being a homosexual as negative. In addition to entity recognition (deciphering what's being talked about in a text) and syntax analysis (parsing the structure of that text), the API included a sentiment analyzer to allow programs to determine the degree to which sentences expressed a negative or positive sentiment, on a scale of -1 to 1. The problem is the API labels sentences about religious and ethnic minorities as negative—indicating it's inherently biased. For example, it labels both being a Jew and being a homosexual as negative.

Google's sentiment analyzer was not the first and isn't the only one on the market. Sentiment analysis technology grew out of Stanford's Google's sentiment analyzer was not the first and isn't the only one on the market. Sentiment analysis technology grew out of Stanford's Natural Language Processing Group , which offers free, open source language processing tools for developers and academics. The technology has been incorporated into a host of machine learning suites, including Microsoft's Azure and IBM's Watson. But Google's machine learning APIs, like its consumer-facing products, are arguably the most accessible on offer, due in part to their affordable price.

Advertisement

But Google's sentiment 