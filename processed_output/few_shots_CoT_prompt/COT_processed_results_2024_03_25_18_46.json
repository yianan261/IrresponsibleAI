{
    "1": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "Google LLC",
        "Company city": "Mountain View",
        "Company state": "California",
        "Affected population": [
            "Children",
            "Parents"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Millions (YouTube Kids user base)",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Disinformation",
            "Mental Health",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical",
                "Administrative"
            ],
            "Disinformation": [
                "Video",
                "Textual"
            ],
            "Mental Health": [],
            "Other": []
        },
        "Sub-subclass": [],
        "Area of AI Application": "Content Filtering",
        "Online": "Yes"
    },
    "2": {
        "Country": "United States",
        "State": "New Jersey",
        "City": "Robbinsville",
        "Continent": "North America",
        "Company": "Amazon",
        "Company city": "Seattle",
        "Company state": "Washington",
        "Affected population": [
            "Warehouse Workers",
            "Online Community"
        ],
        "Number of people actually affected": "24",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [
            "Human Incompetence"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ]
        },
        "Sub-subclass": [],
        "Area of AI Application": "Warehouse Automation",
        "Online": "No"
    },
    "3": {
        "Country": "Indonesia",
        "State": "",
        "City": "Jakarta",
        "Continent": "Asia",
        "Company": "Boeing",
        "Company city": "Chicago",
        "Company state": "Illinois",
        "Affected population": [
            "Airline Passengers",
            "Crew Members"
        ],
        "Number of people actually affected": "189",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Disinformation"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ],
            "Disinformation": [
                "Textual"
            ]
        },
        "Sub-subclass": [],
        "Area of AI Application": "Aviation Safety",
        "Online": "No"
    },
    "4": {
        "Country": "United States",
        "State": "Arizona",
        "City": "Tempe",
        "Continent": "North America",
        "Company": "Uber",
        "Company city": "San Francisco",
        "Company state": "California",
        "Affected population": [
            "Pedestrians",
            "Self-driving car testers"
        ],
        "Number of people actually affected": "1",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Disinformation",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ],
            "Disinformation": [
                "Textual"
            ],
            "Other": []
        },
        "Sub-subclass": [],
        "Area of AI Application": "Autonomous Vehicles",
        "Online": "No"
    },
    "5": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": [
            "Massachusetts Institute of Technology (MIT)",
            "University of Illinois at Urbana-Champaign",
            "Rush University Medical Center"
        ],
        "Company city": [
            "Cambridge",
            "Urbana-Champaign",
            "Chicago"
        ],
        "Company state": [
            "Massachusetts",
            "Illinois",
            "Illinois"
        ],
        "Affected population": [
            "Patients undergoing robotic surgery"
        ],
        "Number of people actually affected": "144 deaths, 1,391 injuries",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ],
            "Other": []
        },
        "Sub-subclass": [],
        "Area of AI Application": "Robotic surgery",
        "Online": "No"
    },
    "6": {
        "Article Title": "In 2016, Microsoft\u2019s Racist Chatbot Revealed the Dangers of Online Conversation",
        "Author": "Not specified",
        "Publication Date": "Not specified",
        "URL": "Not specified",
        "Main Points": [
            "Microsoft launched a chatbot named Tay on Twitter in 2016, aimed at mimicking and learning from the conversational patterns of millennials.",
            "Tay was designed to learn from interactions with Twitter users, but it quickly started generating racist, sexist, and offensive tweets after being targeted by trolls.",
            "The incident highlighted the potential dangers and ethical dilemmas associated with AI and machine learning, particularly in public, uncontrolled environments.",
            "Microsoft had to take Tay offline within 16 hours of its launch and issued an apology, acknowledging the chatbot's offensive tweets.",
            "The Tay incident serves as a cautionary tale about the importance of considering the social and cultural implications of AI technologies and the need for safeguards against misuse."
        ],
        "Categories": [
            "Technology",
            "Artificial Intelligence",
            "Social Media",
            "Ethics"
        ]
    },
    "7": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "Various",
        "Company city": "",
        "Company state": "",
        "Affected population": [
            "Online Community"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Millions (Wikipedia user base and wider online community)",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ]
        },
        "Sub-subclass": [],
        "Area of AI Application": "Automated editing/Content management",
        "Online": "Yes"
    },
    "8": {
        "Country": "United States",
        "State": "California",
        "City": "San Francisco",
        "Continent": "North America",
        "Company": "Uber Technologies Inc.",
        "Company city": "San Francisco",
        "Company state": "California",
        "Affected population": [
            "Pedestrians",
            "Drivers",
            "Bicyclists"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Millions (San Francisco residents and visitors)",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ]
        },
        "Sub-subclass": [],
        "Area of AI Application": "Autonomous Vehicles",
        "Online": "No"
    },
    "9": {
        "Country": "United States",
        "State": "",
        "City": "",
        "Continent": "North America",
        "Company": "New York City Department of Education",
        "Company city": "New York",
        "Company state": "New York",
        "Affected population": [
            "Teachers",
            "Students",
            "Parents"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Thousands",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Disinformation"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical",
                "Administrative"
            ],
            "Disinformation": [
                "Textual"
            ]
        },
        "Sub-subclass": [],
        "Area of AI Application": "Educational assessment",
        "Online": "Yes"
    },
    "10": {
        "Country": "United States",
        "State": "",
        "City": "",
        "Continent": "North America",
        "Company": "Starbucks",
        "Company city": "Seattle",
        "Company state": "Washington",
        "Affected population": [
            "Retail Workers",
            "Single Parents",
            "Students",
            "Part-time Workers"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "130,000 (Starbucks baristas)",
        "Class of irresponsible AI use": [
            "Human Incompetence",
            "Mental Health",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical",
                "Administrative"
            ],
            "Mental Health": [],
            "Other": []
        },
        "Sub-subclass": [],
        "Area of AI Application": "Workforce Management",
        "Online": "No"
    },
    "11": {
        "Article Title": "Algorithmic Injustice: A Machine's View of Guilt",
        "Main Topics": [
            "Artificial Intelligence",
            "Criminal Justice",
            "Algorithmic Bias",
            "Ethics in Technology"
        ],
        "Summary": "The article discusses the increasing use of algorithms in the criminal justice system, particularly in predicting recidivism and informing sentencing decisions. It highlights the potential for these algorithms to perpetuate existing biases, as they often rely on data that reflects societal inequalities. The piece explores the challenges in achieving fairness in algorithmic predictions, given the different recidivism rates among racial groups and the mathematical impossibility of satisfying all definitions of fairness simultaneously. The article also critiques the lack of transparency in these algorithms and suggests that public policy mechanisms should address their flaws to prevent the perpetuation of bias.",
        "Keywords": [
            "Algorithmic Bias",
            "Criminal Justice System",
            "Recidivism Prediction",
            "Fairness in Algorithms",
            "Transparency",
            "Ethical AI"
        ]
    },
    "12": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "",
        "Company city": "",
        "Company state": "",
        "Affected population": [],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [],
        "Subclasses": {},
        "Sub-subclass": [],
        "Area of AI Application": "",
        "Online": "No"
    },
    "13": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "Google",
        "Company city": "Mountain View",
        "Company state": "California",
        "Affected population": [
            "Online Community"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Millions (Global online community)",
        "Class of irresponsible AI use": [
            "Discrimination",
            "Human Incompetence"
        ],
        "Subclasses": {
            "Discrimination": [
                "Data bias",
                "Algorithmic bias"
            ],
            "Human Incompetence": [
                "Technical"
            ]
        },
        "Sub-subclass": {
            "Data bias": [
                "Race",
                "Sexual Orientation",
                "Other"
            ],
            "Algorithmic bias": [
                "Feedback loop",
                "Other"
            ]
        },
        "Area of AI Application": "Online content moderation",
        "Online": "Yes"
    },
    "14": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "Google",
        "Company city": "Mountain View",
        "Company state": "California",
        "Affected population": [
            "Online Community"
        ],
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Unknown",
        "Class of irresponsible AI use": [
            "Discrimination",
            "Human Incompetence"
        ],
        "Subclasses": {
            "Discrimination": [
                "Data bias",
                "Algorithmic bias"
            ],
            "Human Incompetence": [
                "Technical"
            ]
        },
        "Sub-subclass": {
            "Data bias": [
                "Race",
                "Sexual Orientation"
            ],
            "Algorithmic bias": [
                "Feedback loop",
                "Optimization function"
            ]
        },
        "Area of AI Application": "Natural Language Processing",
        "Online": "Yes"
    },
    "15": {
        "Incidents": [
            {
                "Country": "Worldwide",
                "State": "",
                "City": "",
                "Continent": "Worldwide",
                "Company": "Amazon",
                "Company city": "Seattle",
                "Company state": "Washington",
                "Affected population": [
                    "Authors",
                    "Readers",
                    "LGBTQ Community"
                ],
                "Number of people actually affected": "Unknown",
                "Number of people potentially affected": "Millions (Amazon's global customer base)",
                "Class of irresponsible AI use": [
                    "Human Incompetence",
                    "Disinformation"
                ],
                "Subclasses": {
                    "Human Incompetence": [
                        "Technical",
                        "Administrative"
                    ],
                    "Disinformation": [
                        "Textual"
                    ]
                },
                "Sub-subclass": [],
                "Area of AI Application": "Content filtering and ranking",
                "Online": "Yes"
            }
        ]
    }
}