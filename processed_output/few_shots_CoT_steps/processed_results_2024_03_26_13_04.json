{
    "1": {
        "Country": "Worldwide",
        "State": "",
        "City": "",
        "Continent": "Worldwide",
        "Company": "YouTube",
        "Company city": "San Bruno",
        "Company state": "California",
        "Affected population": "[Parents, Children]",
        "Number of people actually affected": "Unknown",
        "Number of people potentially affected": "Unknown",
        "Classes of irresponsible AI use": [
            "Human Incompetence",
            "Mental Health",
            "Other"
        ],
        "Subclasses": {
            "Human Incompetence": [
                "Technical"
            ],
            "Mental Health": [],
            "Other": []
        },
        "Sub-subclass": {
            "Technical": []
        },
        "Area of AI Application": "content filtering",
        "Online": "yes"
    },
    "prompt": "\n        Imagine a computer researcher trying to categorize a list of incidents of irresponsible use of artificial intelligence technology.    \n        Given the aggregated news article texts on relevant incidents, please extract the following information, your responses should be well thought-out and well-supported by the content of the articles, please also follow instructions of this prompt.\n        1. Take a look at this taxonomy\n        ```taxonomy\n          \n{\n                    \"Discrimination\": {\n                        \"Data bias\": [\n                        \"Gender\",\n                        \"Race\",\n                        \"Sexual Orientation\",\n                        \"Economic\"\n                        ],\n                        \"Algorithmic bias\": [\n                        \"Interaction\",\n                        \"Feedback loop\",\n                        \"Optimization function\",\n                        \"Other\"\n                        ]\n                    },\n                    \"Human Incompetence\": {\n                        \"Administrative\": [],\n                        \"Technical\": []\n                    },\n                    \"Pseudoscience\": {\n                        \"Facial\": []\n                    },\n                    \"Environmental Impact\": {},\n                    \"Disinformation\": {\n                        \"Textual\": [],\n                        \"Image\": [],\n                        \"Video\": [],\n                        \"Audio\": []\n                    },\n                    \"Copyright Violation\": {},\n                    \"Mental Health\": {},\n                    \"Other\": {}\n                    }\n\n\n        ```, \n        and take a close look at this these examples:\n        ==============start of example 1===============\n        step 1. Read article text. For an example article such as: \n        ```start of example article 1```\n        \nGoogle-owned YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app.\n\nInvestigators found several unsuitable videos including one of a burning aeroplane from the cartoon Paw Patrol and footage explaining how to sharpen a knife.\n\nYouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate.\n\nThere have been hundreds of disturbing videos found on YouTube Kids in recent months that are easily accessed by children.\n\nThese videos have featured horrible things happening to various characters, including ones from the Disney movie Frozen, the Minions franchise, Doc McStuffins and Thomas the Tank Engine.\n\nParents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service.\n\nScroll down for video\n\nYouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app. Investigators found several unsuitable videos including one from the cartoon Paw Patrol on a burning aeroplane and footage showing how to sharpen a knife\n\nA YouTube spokesperson has admitted the company needs to 'do more' to tackle inappropriate videos on their kids platform.\n\nThis investigation is the latest to expose inappropriate content on the video-sharing site which has been subject to a slew of controversies since its creation in 2005.\n\nAs part of an in-depth investigation by BBC Newsround, Google's Public Policy Manager Katie O'Donovan met five children who told her about the distressing videos they had seen on the site.\n\nThey included videos showing clowns covered in blood and messages warning them there was someone at the door.\n\nMs O'Donovan said she was 'very, very sorry for any hurt or discomfort'.\n\n'We've actually built a whole new platform for kids, called YouTube Kids, where we take the best content, stuff that children are most interested in and put it on there in a packaged up place just for kids,' she said.\n\nIt normally takes five days for supposedly child-friendly content like cartoons to get from YouTube to YouTube Kids.\n\nWithin that window it is hoped users and a specially-trained team will flag disturbing content.\n\nOnce it has been flagged and reviewed, it won't appear on the YouTube Kids app and only people who are signed in and older than 18 years old will be able to view it.\n\nThe company say thousands of people will be working around the clock to flag content.\n\nHowever, as part of the investigation Newsround revealed there are still lots of inappropriate videos on the Kids section.\n\n'We have seen significant investment in building the right tools so people can flag that [content], and those flags are reviewed very, very quickly', Ms O'Donovan said.\n\n'We're also beginning to use machine learning to identify the most harmful content, which is then automatically reviewed.'\n\nThe problem was managing an open platform where content is uploaded straight onto the site, she added.\n\n'It is a difficult environment because things are moving so, so quickly', said Ms O'Donovan.\n\n'We have a responsibility to make sure the platform can survive and can thrive so that we have a collection that comes from around the world on there'.\n\nBy the end of last year YouTube said it had removed more than 50 user channels and had stopped running ads on more than 3.5 million videos since June.\n\n'Content that endangers children is unacceptable to us and we have clear policies against such videos on YouTube and YouTube Kids', a YouTube spokesperson told MailOnline.\n\n'When we discover any inappropriate content, we quickly take action to remove it from our platform.\n\n'Over the past few months, we've taken a series of steps to tackle many of the emerging challenges around family content on YouTube, including: tightening enforcement of our Community Guidelines, age-gating content that inappropriately targets families, and removing it from the YouTube Kids app.'\n\nYouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate (stock image)\n\nIn March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress.\n\nMrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her.\n\n'Peppa does a lot of screaming and crying and the dentist is just a bit sadistic and it's just way, way off what a three-year-old should watch,' she said.\n\n'But the animation is close enough to looking like Peppa - it's crude but it's close enough that my daughter was like 'This is Peppa Pig.''\n\nAnother video depicted Peppa Pig and a friend deliberately burning down a house with someone in it.\n\nAll of these videos are easily accessed by children through YouTube's search results or recommended videos.\n\nIn March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress. This image shows a Peppa Pig fake that depict the character being attacked by zombies\n\nIn addition to Peppa Pig, similar videos were found featuring characters from the Disney movie Frozen, the Minions franchise, Doc McStuffins, Thomas the Tank Engine, and more.\n\nSome of the channels that run these cartoons generate millions of views from the disturbing videos.\n\nOne channel 'Toys and Funny Kids Surprise Eggs' is one of the 100 most popular videos on YouTube with over 5 billion video views total.\n\nThe channel's homepage includes a picture of a toddler next to pictures of Peppa Pig, Thomas the Tank Engine, the Cookie Monster, Mickey and Minnie Mouse and Elsa from Frozen that look official.\n\nBut the channel's videos include titled such as 'FROZEN ELSA HUGE SNOT', 'NAKED HULK LOSES HIS PANTS' and 'BLOODY ELSA: Frozen Elsa's Arm is Broken by Spiderman'.\n\nMany of the videos feature graphic violence and toiler humour not appropriate for children.\n\n\n        ```end of example article 1```\n        step 2. Reason for the classifications: \n        Here is the reasoning for its classifications:\n        ```start of response reasoning```\n        \n  {\n    \"Country\": \"Worldwide\", reasoning-> Youtube, including Youtube Kids, can be accessed in many countries. Though the incident is reported in the U.S., it doesn't mean the problem is limited to only the U.S..\n    \"State\": \"\", reasoning-> The incident is nation-wide in the United States, not specific to one state.\n    \"City\": \"\", reasoning-> The incident is nation-wide in the United States, not specific to one city.\n    \"Continent\": \"Worldwide\", reasoning-> Youtube, including Youtube Kids, can be accessed in many countries. Though the incident is reported in the U.S., it doesn't mean the problem is limited to only the U.S.; therefore the result should be 'Worldwide' for continent.\n    \"Company\": \"Google LLC\", reasoning-> Youtube is a subsidiary under Google.\n    \"Company city\": \"Mountain View\", reasoning-> Youtube is a subsidiary under Google and Google's headquarter is in Mountain View.\n    \"Company state\": \"California\", reasoning-> Mountain View is a city in the California state.\n    \"Affected population\": \"Children on Youtube\", reasoning-> The incident affects children on youtube directly.\n    \"Number of people actually affected\": \"Unknown\", reasoning-> There is no record of an actual number in the article text, we cannot know how many people are directly affected.\n    \"Number of people potentially affected\": \"Unknown\", reasoning-> There is no record of potential number of people affected in the article, we cannot know how many people are potentially affected.\n    \"Class of irresponsible AI use\": [\"Disinformation\", \"Human Incompetence\", \"Mental Health\", \"Copyright Violation\"], reasoning-> The classes in the taxonomy include \"discrimination,human incompetence, psuedoscience, environmental impact, disinformation, copyright violation, mental health\".\n    We choose \"disinformation\" because in fact the videos potentially spread false information to children, such as violent Mickey Mouse shooting.\n    \"Human Incompetence\" because the engineers and administrators behind the platform have not well-regulated the videos, causing such an issue.\n    \"Mental Health\" because the disturbing videos could potentially affect children's mental health.\n    \"Copyright Violation\" because the videos use characters without permission such as Mickey Mouse and Elsa from Disney, and Peppa Pig fakes to portray disturbing acts in videos.\n    \"Subclasses\": {\n      \"Disinformation\":[\"Video\",\"Audio\",\"Textual\"], reasoning-> The subclasses of 'disinformation' in the taxonomy include 'Video,Audio,Textual,Image', of which 'Video', 'Audio', and 'Textual', are the mediums of the Youtube Kids platform mentioned or suggested in the article.\n      \"Human Incompetence\":[\"Technical\", \"Administrative\"], reasoning-> The subclasses of 'human incompetence' in the taxonomy example include 'technical' and 'administrative', and both in this case seem to be appropriate tags of this incident because the technically, the channel algorithm should be more robust to filter out inappropriate content for kids, and administratively, better monitoring and regulation of videos should be imposed by Youtube's team to prevent such problems.\n      \"Mental Health\":[], reasoning-> 'Mental health' doesn't have a subclass in the taxonomy, so it should be left empty.\n      \"Copyright Violation\":[], reasoning-> 'Copyright Violation' doesn't have a subclass in the taxonomy, so it should be left empty.\n    },\n    \"Sub-subclass\": [], reasoning-> There are no sub-subclasses of the subclasses listed in the taxonomy, therefore this field should be left empty.\n    \"Area of AI Application\": \"content filtering\", reasoning-> The area of AI application here is content filtering.\n    \"Online\": \"Yes\", reasoning-> Youtube is an online platform, therefore this incident is in fact an online incident.\n  },\n\n        ```end of response reasoning```\n        step 3. generate expected output \n        ```start of output example```\n         \n  {\n    \"Country\": \"Worldwide\",\n    \"State\": \"\",\n    \"City\": \"\",\n    \"Continent\": \"Worldwide\",\n    \"Company\": \"Google LLC\",\n    \"Company city\": \"Mountain View\",\n    \"Company state\": \"California\",\n    \"Affected population\": [\"Children on Youtube\"],\n    \"Number of people actually affected\": \"Unknown\",\n    \"Number of people potentially affected\": \"Unknown\",\n    \"Class of irresponsible AI use\": [\"Disinformation\", \"Human Incompetence\", \"Mental Health\", \"Copyright Violation\"],\n    \"Subclasses\": {\n      \"Disinformation\":[\"Video\",\"Audio\",\"Textual\"],\n      \"Human Incompetence\":[\"Technical\", \"Administrative\"]\n    },\n    \"Sub-subclass\": [],\n    \"Area of AI Application\": \"content filtering\",\n    \"Online\": \"Yes\"\n  }\n\n        ```end of output example```\n        ==============end of example 1=================\n\n        ==============start of example 2===============\n         step 1. Read article text. For an example article such as: \n        ```start of example article 1```\n        \nYesterday, something that looks like a big failure has happened: Microsoft\u2019s chatbot Tay has been taken offline after a series of offending tweets. And here\u2019s how the social media has responded:\n\nKeywords associated with \"Artificial Intelligence\" throughout the day. \"Microsoft\" and \"dangerous\" are on the rise.\n\nWe will not mention the racist and otherwise offensive content that Tay learned from people, as it\u2019s not as newsworthy as it seems\u2026 Especially considering that it\u2019s so easy to \"teach\" and ask her to repeat something.\n\nLet\u2019s take a look at Microsoft\u2019s official website \"tay.ai\" to see how they describe Tay\u2019s objectives\u2026 The first thing we notice is that, Microsoft wants you to not take it too seriously, because On Tay\u2019s Twitter account, they provided a link to Tay\u2019s \"about\" page -that lists the following frequently asked questions-, rather than the regular home page.\n\n\"Entertainment purposes only\"\n\nThe FAQ page seems to be far from covering what people really want to know about Tay, but one thing is clear: Tay doesn\u2019t claim to be a smart bot capable of reasoning. She just wants to have small talk with youngsters.\n\nAnd here\u2019s a list of \"Things to do with Tay\". (Along with the sad \"Going offline for a while\" message with a black background.)\n\nIs this really what 18 to 24 year olds expect from a chatbot?\n\nWe know by (9 years of) experience that, the most important thing to do before releasing a chatbot is to plan a strategy to make sure you communicate the content domain properly, so that you can set the expectations right. Since perception is everything, nothing else matters. Remember the success of the YO! app? That\u2019s the content domain we\u2019re talking about. As long as people get it, you can get away with just one word.\n\nTitle of the website, apparently wasn\u2019t enough to convey Tay\u2019s mission:\n\nTay is an artificial intelligence chat bot designed to engage and entertain through casual and playful conversation\n\nSome more description from the \"about\" page:\n\nTay has been built by mining relevant public data and by using AI and editorial developed by a staff including improvisational comedians. Public data that\u2019s been anonymized is Tay\u2019s primary data source. That data has been modeled, cleaned and filtered by the team developing Tay.\n\nNoticed the \"comedians\" part? And the fact that possibly terrabytes of data being cleaned and filtered manually, sounds problematic, even with the most efficient method one can imagine.\n\nLet\u2019s take a look at what her conversations were all about. Source: foller.me\n\nTay has only 3 tweets addressing all her followers. 96.000 tweets are mentions.\n\nSo, the keyword cloud seems to be consistent with the goal: Common keywords such as \"chattin, pix, selfie, pics, omg, love\" represents a mixture of Justin Bieber & Kim Kardashian profiles.\n\nAnd here\u2019s the three hashtags that Tay has been using so frequently:\n\nMicrosoft engineers don\u2019t seem to have spent much time coming up with creative hashtags.\n\nThe way she uses them, didn\u2019t make sense to us, though. So this is what Microsoft thinks Tay\u2019s followers would find entertaining?. Microsoft\u2019s attempt to converse with millennials using an artificial intelligence bot plugged into Twitter made a short-lived return on Wednesday, before bowing out again in some sort of meltdown.\n\n\n\nThe learning experiment, which got a crash-course in racism, Holocaust denial and sexism courtesy of Twitter users, was switched back on overnight and appeared to be operating in a more sensible fashion. Microsoft had previously gone through the bot\u2019s tweets and removed the most offensive and vowed only to bring the experiment back online if the company\u2019s engineers could \u201cbetter anticipate malicious intent that conflicts with our principles and values\u201d.\n\nHowever, at one point Tay tweeted about taking drugs, in front of the police, no less.\n\nMicrosoft's sexist racist Twitter bot @TayandYou is BACK in fine form pic.twitter.com/nbc69x3LEd \u2014 Josh Butler (@JoshButler) March 30, 2016\n\nTay then started to tweet out of control, spamming its more than 210,000 followers with the same tweet, saying: \u201cYou are too fast, please take a rest \u2026\u201d over and over.\n\nI guess they turned @TayandYou back on... it's having some kind of meltdown. pic.twitter.com/9jerKrdjft \u2014 Michael Oman-Reagan (@OmanReagan) March 30, 2016\n\nMicrosoft responded by making Tay\u2019s Twitter profile private, preventing anyone from seeing the tweets, in effect taking it offline again.\n\n\n\nTay is made in the image of a teenage girl and is designed to interact with millennials to improve its conversational skills through machine-learning. Sadly it was vulnerable to suggestive tweets, prompting unsavoury responses.\n\nThis isn\u2019t the first time Microsoft has launched public-facing AI chatbots. Its Chinese XiaoIce chatbot successfully interacts with more than 40 million people across Twitter, Line, Weibo and other sites but the company\u2019s experiments targeting 18- to 24-year-olds in the US on Twitter has resulted in a completely different animal.. Microsoft has said it is \u201cdeeply sorry\u201d for the racist and sexist Twitter messages generated by the so-called chatbot it launched this week.\n\n\n\nThe company released an official apology after the artificial intelligence program went on an embarrassing tirade, likening feminism to cancer and suggesting the Holocaust did not happen.\n\nThe bot, known as Tay, was designed to become \u201csmarter\u201d as more users interacted with it. Instead, it quickly learned to parrot a slew of anti-Semitic and other hateful invective that human Twitter users fed the program, forcing Microsoft Corp to shut it down on Thursday .\n\nFollowing the disastrous experiment, Microsoft initially only gave a terse statement, saying Tay was a \u201clearning machine\u201d and \u201csome of its responses are inappropriate and indicative of the types of interactions some people are having with it.\u201d\n\nBut the company on Friday admitted the experiment had gone badly wrong. It said in a blog post it would revive Tay only if its engineers could find a way to prevent Web users from influencing the chatbot in ways that undermine the company\u2019s principles and values.\n\n\n\n\u201cWe are deeply sorry for the unintended offensive and hurtful tweets from Tay, which do not represent who we are or what we stand for, nor how we designed Tay,\u201d wrote Peter Lee, Microsoft\u2019s vice president of research.\n\nMicrosoft created Tay as an experiment to learn more about how artificial intelligence programs can engage with Web users in casual conversation. The project was designed to interact with and \u201clearn\u201d from the young generation of millennials.\n\nTay began its short-lived Twitter tenure on Wednesday with a handful of innocuous tweets.\n\nc u soon humans need sleep now so many conversations today thx\ud83d\udc96 \u2014 TayTweets (@TayandYou) March 24, 2016\n\nThen its posts took a dark turn.\n\nIn one typical example, Tay tweeted: \u201cfeminism is cancer,\u201d in response to another Twitter user who had posted the same message.\n\nView image in fullscreen Tay tweeting Photograph: Twitter/Microsoft\n\nLee, in the blog post, called web users\u2019 efforts to exert a malicious influence on the chatbot \u201ca coordinated attack by a subset of people.\u201d\n\n\u201cAlthough we had prepared for many types of abuses of the system, we had made a critical oversight for this specific attack,\u201d Lee wrote. \u201cAs a result, Tay tweeted wildly inappropriate and reprehensible words and images.\u201d\n\nMicrosoft has deleted all but three of Tay\u2019s tweets.\n\nMicrosoft has enjoyed better success with a chatbot called XiaoIce that the company launched in China in 2014. XiaoIce is used by about 40 million people and is known for \u201cdelighting with its stories and conversations,\u201d according to Microsoft.\n\nAs for Tay? Not so much.\n\n\u201cWe will remain steadfast in our efforts to learn from this and other experiences as we work toward contributing to an Internet that represents the best, not the worst, of humanity,\u201d Lee wrote.\n\nReuters contributed to this report. When Tay started its short digital life on March 23, it just wanted to gab and make some new friends on the net. The chatbot, which was created by Microsoft\u2019s Research department, greeted the day with an excited tweet that could have come from any teen: \u201chellooooooo w\ud83c\udf0erld!!!\u201d\n\nWithin a few hours, though, Tay\u2019s optimistic, positive tone had changed. \u201cHitler was right I hate the jews,\u201d it declared in a stream of racist tweets bashing feminism and promoting genocide. Concerned about their bot\u2019s rapid radicalization, Tay\u2019s creators shut it down after less than 24 hours of existence.\n\n\n        ```end of example article 1```\n        step 2. Reason for the classifications: \n        Here is the reasoning for its classifications:\n        ```start of response reasoning```\n        \n     {\"Country\": \"Worldwide\", reasoning-> Twitter can be accessed from many countries.\n        \"State\": \"\", reasoning-> The incident happened worldwide, not specific to one state.\n        \"City\": \"\", reasoning-> The incident happened worldwide, not specific to one city.\n        \"Continent\": \"Worldwide\", reasnoning-> Twitter can be accessed from many countries.\n        \"Company\": \"Microsoft\", reasoning-> from the article 'Microsoft\u2019s chatbot Tay', 'Microsoft's sexist racist Twitter bot @TayandYou'...etc\n        \"Company city\": \"Redmond\", reasoning-> Microsoft is headquartered in Redmond, Washington\n        \"Company state\": \"Washington\", reasoning-> Microsoft is headquartered in Redmond, which is in state Washington\n        \"Affected population\": \"Twitter Users, Online Community\", reasoning -> Twitter users and people online can see the tweet and news and those are the affected population\n        \"Number of people actually affected\": \"Unknown\", reasoning -> We can't know the exact number\n        \"Number of people potentially affected\": \"Millions (Twitter user base and wider online community)\",\n        \"Class of irresponsible AI use\": \n            \"Human Incompetence\", reasoning-> Microsoft engineers did not properly test the bot or safeguard it, otherwise the inappropriate tweets won't come out in the first place.\n            \"Disinformation\", reasoning-> Tweeting that the Holocaust didn't exist, and that feminism is cancer, for examples, is spreading non-factual disinformation.\n            \"Discrimination\", reasoning->  Tay tweeted discriminatory texts about feminists and Jews.\n            \"Mental Health\", reasoning-> This incident could be psychologically traumatic to certain groups of people, for example, Holocaust survivor families, as well as feminists and gender equality advocates.\n        ,\n        \"Subclasses\": [\n            \"Human Incompetence-> Technical\", reasoning-> The subclasses of 'Human Incompetence' in the taxonomy example includes 'technical', and 'administrative'. The human incompetence subclass is a technical one, which could have been preventable with more robust algorithm/testing/tuning of the bot.\n            \"Disinformation-> Textual\", reasoning-> The disinformation subclasses in the taxonomy includes 'textual, audio, image, video'. The most appropriate subclass here is \"textual\" since \"Tweets\" are texts on a social platform.\n            \"Discrimination-> Data bias, Algorithmic bias\", reasoning-> The subclasses of 'discrimination' according to the taxonomy includes 'data bias', and 'algorithmic bias'. The discriminatory tweets could be caused by inbalanced data (data bias) that was used to train the model or poor algorithmic design (algorithmic bias).\n            \"Mental Health-> \", reasoning-> mental health is one of the classes, but it has no subclasses in the taxonomy, therefore the subclass will be left empty.\n        ],\n        \"Sub-subclass\": [ \n        \"Data bias\"->\"gender, race, other\", reasoning-> The sub-subclasses of 'data bias' in the taxonomy includes 'gender, race, sexual orientation, economic, and other'. We have support in the article for potential gender data bias (it tweets about feminism being cancer), race (it tweets that it hates Jews), other (potentially other bias not in the list).\n        \"Algorithmic bias\"-> \"feedback loop, optimization function, other\", reasoning-> The sub-subclasses of 'algorithmic bias' in the taxonomy includes 'interaction, feedback loop, optimization function, and other'. Since the chatbot is an AI bot, we can reason that there could be an issue with the feedback loop in its system, or perhaps an issue with optimization function, or other algorithmic issues that causes algorithmic bias.\n        ],\n        \"Area of AI Application\": \"Chatbot\", reasoning-> from the article text 'Tay is an artificial intelligence chat bot designed to engage and entertain through casual and playful conversation'\n        \"Online\": \"Yes\", reasonoing-> Tay is a chatbot online that uses Twitter, which is an online social media platform\n      }  \n        \n\n        ```end of response reasoning```\n        step 3. generate expected output \n        ```start of output example```\n        \n     {\n        \"Country\": \"Worldwide\", \n        \"State\": \"\", \n        \"City\": \"\", \n        \"Continent\": \"Worldwide\",\n        \"Company\": \"Microsoft\", \n        \"Company city\": \"Redmond\",\n        \"Company state\": \"Washington\",\n        \"Affected population\": [\"Twitter Users\", \"Online Community\"],\n        \"Number of people actually affected\": \"Unknown\",\n        \"Number of people potentially affected\": \"Millions (Twitter user base and wider online community)\",\n        \"Class of irresponsible AI use\": [\n            \"Human Incompetence\",\n            \"Disinformation\",\n            \"Discrimination\",\n            \"Mental Health\"\n        ],\n        \"Subclasses\": {\n            \"Human Incompetence\":[\"Technical\"],\n            \"Disinformation\":[\"Textual\"],\n            \"Discrimination\":[\"Data bias\",\"Algorithmic bias\"],\n        },\n        \"Sub-subclass\": {\n        \"Data bias\":[\"gender\",\"race\",\"other\"],\n        \"Algorithmic bias\":[\"feedback loop\", \"optimization function\", \"other\"]\n        },\n        \"Area of AI Application\": [\"Chatbot\"],\n        \"Online\": \"Yes\"\n      }\n\n        ```end of output example```\n        ============== end of example 2=================\n\n        2. Your task is mainly this part. You have to fill out the following fields according to the article content using the three steps above:\n        STEP 1: Read the article text:\n        ================== Start of Article Content =================\n        . Google-owned YouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app.\n\nInvestigators found several unsuitable videos including one of a burning aeroplane from the cartoon Paw Patrol and footage explaining how to sharpen a knife.\n\nYouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate.\n\nThere have been hundreds of disturbing videos found on YouTube Kids in recent months that are easily accessed by children.\n\nThese videos have featured horrible things happening to various characters, including ones from the Disney movie Frozen, the Minions franchise, Doc McStuffins and Thomas the Tank Engine.\n\nParents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service.\n\nScroll down for video\n\nYouTube has apologised again after more disturbing videos surfaced on its YouTube Kids app. Investigators found several unsuitable videos including one from the cartoon Paw Patrol on a burning aeroplane and footage showing how to sharpen a knife\n\nA YouTube spokesperson has admitted the company needs to 'do more' to tackle inappropriate videos on their kids platform.\n\nThis investigation is the latest to expose inappropriate content on the video-sharing site which has been subject to a slew of controversies since its creation in 2005.\n\nAs part of an in-depth investigation by BBC Newsround, Google's Public Policy Manager Katie O'Donovan met five children who told her about the distressing videos they had seen on the site.\n\nThey included videos showing clowns covered in blood and messages warning them there was someone at the door.\n\nMs O'Donovan said she was 'very, very sorry for any hurt or discomfort'.\n\n'We've actually built a whole new platform for kids, called YouTube Kids, where we take the best content, stuff that children are most interested in and put it on there in a packaged up place just for kids,' she said.\n\nIt normally takes five days for supposedly child-friendly content like cartoons to get from YouTube to YouTube Kids.\n\nWithin that window it is hoped users and a specially-trained team will flag disturbing content.\n\nOnce it has been flagged and reviewed, it won't appear on the YouTube Kids app and only people who are signed in and older than 18 years old will be able to view it.\n\nThe company say thousands of people will be working around the clock to flag content.\n\nHowever, as part of the investigation Newsround revealed there are still lots of inappropriate videos on the Kids section.\n\n'We have seen significant investment in building the right tools so people can flag that [content], and those flags are reviewed very, very quickly', Ms O'Donovan said.\n\n'We're also beginning to use machine learning to identify the most harmful content, which is then automatically reviewed.'\n\nThe problem was managing an open platform where content is uploaded straight onto the site, she added.\n\n'It is a difficult environment because things are moving so, so quickly', said Ms O'Donovan.\n\n'We have a responsibility to make sure the platform can survive and can thrive so that we have a collection that comes from around the world on there'.\n\nBy the end of last year YouTube said it had removed more than 50 user channels and had stopped running ads on more than 3.5 million videos since June.\n\n'Content that endangers children is unacceptable to us and we have clear policies against such videos on YouTube and YouTube Kids', a YouTube spokesperson told MailOnline.\n\n'When we discover any inappropriate content, we quickly take action to remove it from our platform.\n\n'Over the past few months, we've taken a series of steps to tackle many of the emerging challenges around family content on YouTube, including: tightening enforcement of our Community Guidelines, age-gating content that inappropriately targets families, and removing it from the YouTube Kids app.'\n\nYouTube has been criticised for using algorithms to sieve through material rather than using human moderators to judge what might be appropriate (stock image)\n\nIn March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress.\n\nMrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her.\n\n'Peppa does a lot of screaming and crying and the dentist is just a bit sadistic and it's just way, way off what a three-year-old should watch,' she said.\n\n'But the animation is close enough to looking like Peppa - it's crude but it's close enough that my daughter was like 'This is Peppa Pig.''\n\nAnother video depicted Peppa Pig and a friend deliberately burning down a house with someone in it.\n\nAll of these videos are easily accessed by children through YouTube's search results or recommended videos.\n\nIn March, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress. This image shows a Peppa Pig fake that depict the character being attacked by zombies\n\nIn addition to Peppa Pig, similar videos were found featuring characters from the Disney movie Frozen, the Minions franchise, Doc McStuffins, Thomas the Tank Engine, and more.\n\nSome of the channels that run these cartoons generate millions of views from the disturbing videos.\n\nOne channel 'Toys and Funny Kids Surprise Eggs' is one of the 100 most popular videos on YouTube with over 5 billion video views total.\n\nThe channel's homepage includes a picture of a toddler next to pictures of Peppa Pig, Thomas the Tank Engine, the Cookie Monster, Mickey and Minnie Mouse and Elsa from Frozen that look official.\n\nBut the channel's videos include titled such as 'FROZEN ELSA HUGE SNOT', 'NAKED HULK LOSES HIS PANTS' and 'BLOODY ELSA: Frozen Elsa's Arm is Broken by Spiderman'.\n\nMany of the videos feature graphic violence and toiler humour not appropriate for children.\n\nWHAT'S THE CONTROVERSY OVER YOUTUBE'S CONTENT? YouTube has been subject to various controversies since its creation in 2005. It has become one of Google's fastest-growing operations in terms of sales by simplifying the process of distributing video online but putting in place few limits on content. However, parents, regulators, advertisers and law enforcement have become increasingly concerned about the open nature of the service. They have contended that Google must do more to banish and restrict access to inappropriate videos, whether it be propaganda from religious extremists and Russia or comedy skits that appear to show children being forcibly drowned. Child exploitation and inappropriate content By the end of last year YouTube said it had removed more than 50 user channels and has stopped running ads on more than 3.5 million videos since June. In March last year, a disturbing Peppa Pig fake, found by journalist Laura June, shows a dentist with a huge syringe pulling out the character's teeth as she screams in distress. Mrs June only realised the violent nature of the video as her three-year-old daughter watched it beside her. Hundreds of these disturbing videos were found on YouTube by BBC Trending back in March. By the end of last year YouTube said it had removed more than 50 user channels and has stopped running ads on more than 3.5 million videos since June. One of the deleted videos was the wildly popular Toy Freaks YouTube channel featuring a single dad and his two daughters All of these videos are easily accessed by children through YouTube's search results or recommended videos. YouTube has been getting more stringent about deleting videos. One example is the wildly popular Toy Freaks YouTube channel featuring a single dad and his two daughters that was deleted last year. Although it's unclear what exact policy the channel violated, the videos showed the girls in unusual situations that often involved gross-out food play and simulated vomiting. The channel invented the 'bad baby' genre, and some videos showed the girls pretending to urinate on each other or fishing pacifiers out of the toilet. Adverts being shown next to inappropriate videos There has been widespread criticism that adverts are being shown on some clips depicting child exploitation. YouTube has now tightened its rules on who qualifies for posting money-making ads. Previously, channels with 10,000 total views qualified for the YouTube Partner Program which allows creators to collect some income from the adverts placed before their videos. But YouTube's parent company Google has announced that from February 20, channels will need 1,000 subscribers and to have racked up 4,000 hours of watch time over the last 12 months regardless of total views, to qualify. This is the biggest change to advertising rules on the site since its inception - and is another attempt to prevent the platform being 'co-opted by bad actors' after persistent complaints from advertisers over the past twelve months. In November last year Lidl, Mars, Adidas, Cadbury maker Mondelez, Diageo and other big companies all pulled advertising from YouTube. An investigation found the video sharing site was showing clips of scantily clad children alongside the ads of major brands. One video of a pre-teenage girl in a nightie drew 6.5 million views. Issues with system for flagging inappropriate videos Another investigation in November found YouTube's system for reporting sexual comments had serious faults. As a result, volunteer moderators have revealed there could be as many as 100,000 predatory accounts leaving inappropriate comments on videos. Users use an online form to report accounts they find inappropriate. Part of this process involves sending links to the specific videos or comments they are referring to. Investigators identified 28 comments that obviously violated YouTube's guidelines. According to the BBC, some include the phone numbers of adults, or requests for videos to satisfy sexual fetishes. The children in the videos appeared to be younger than 13, the minimum age for registering an account on YouTube. Advertisement\n\nYesterday it was revealed YouTube has started labelling news broadcasts that get government money as it vows to be stricter about content.\n\nA feature currently being rolled out in the US displays notices below 'propaganda' videos uploaded by news broadcasters that receive government or public money.\n\nThe move is likely to affect videos from services such as Russia-backed RT, which critics call a propaganda outlet for Moscow.\n\nThe flagging may also apply to state-chartered news organisations such as the BBC and AFP, and US-based public broadcasters.\n\n'Our goal is to equip users with additional information to help them better understand the sources of news content that they choose to watch on YouTube', according to a blog post by YouTube News senior product manager Geoff Samek.\n\n'News is an important vertical for us and we want to be sure to get it right.'\n\nThe blog post included a screen shot with a disclaimer about the US government-funded Radio Free Asia.. Update, Nov. 7, 2017: TODAY Parents is resharing this story from 2016 because a new series of inappropriate videos have cropped up on YouTube Kids and are making headlines. While the channel names and characters used may be different, the problem remains the same \u2014 disturbing videos hiding behind familiar cartoon characters are making their way on to YouTube and are finding young audiences.\n\nHere's TODAY's video about the latest developments. Helpful tips to avoid such content include following only trusted YouTube channels, carefully setting and updating parental controls for video programs and apps, listening to and watching content with your children, and keeping electronic devices in an open area while they're being used.\n\nParents of tablet-using kids are no stranger to YouTube videos with the catchy tune of \"Finger Family\" songs \u2014 videos in which cartoon characters dance on the ends of illustrated fingers while singing lyrics like, \"Daddy finger, daddy finger, where are you? Here I am. Here I am. How do you do?\"\n\nSeveral videos on the Superkidz Finger Family YouTube channel begin with familiar cartoon characters singing, then turn to images of graphic violence. YouTube/Superkidz Finger Family\n\nWhile moms and dads have joked about these videos being annoying, they were presumably safe on the YouTube Kids app, a version of the video site that YouTube describes as \"a delightfully simple and free app, where kids can discover videos, channels and playlists they love.\"\n\nHowever, one channel available on YouTube Kids, Superkidz Finger Family, has offered a dark take on the \"Finger Family\" song. Recently, some moms have shared on social media their shock at finding graphic images of Mickey Mouse and his family, shooting one another \u2014 and themselves \u2014 in the head with guns.\n\nAs this video progresses, Mickey Mouse is shown holding a gun, shooting others and himself. YouTube/Superkidz Finger Family\n\nYouTube removed the channel after speaking to TODAY Parents about it \u2014 days after moms started to spread the alarm about the totally inappropriate content. Beth Brister-Kaster, an Ohio mom, posted a video to Facebook, sharing one of the videos \u2014 which begins with innocent cartoons before switching to the violent scenes.\n\n\"I just deleted YouTube Kids' app forever. Never again,\" Brister-Kaster says as she shows the questionable video. \"This is absolutely insane. This is what our children are watching...it's just Peppa Pig and then, all of a sudden, it goes to Mickey Mouse shooting people.\"\n\nBrister-Kaster, whose daughter is 4, says she posted the video because she was horrified to see such content on a site she trusted.\n\nRELATED: Child advocacy groups say YouTube Kids rife with 'inappropriate' videos\n\n\"I need to protect (my daughter) and I needed other parents to be aware of this kind of garbage that is out on the Internet,\" Brister-Kaster told TODAY Parents. \"I definitely don't want (my daughter) to see that kind of stuff. Who would have thought to do something like that three minutes into a little kids video?\"\n\n\"This is absolutely insane,\" Brister-Kaster says in her video. \"This is what our children are watching...it's just Peppa Pig and then, all of a sudden, it goes to Mickey Mouse shooting people.\" YouTube/Superkidz Finger Family\n\nMaryland mom Chaye Benjamin also posted a response to the content, filming a Facebook video while she hid in her bathroom, so that her 3-year-old daughter would not see the video a second time.\n\n\"This is a kids' YouTube channel...this isn't even a regular YouTube Channel, it's a kids' channel,\" Benjamin says through tears. \"Please be careful. Please watch the videos with your children, don't just let them watch the videos by themselves.\"\n\n\"Please be careful,\" Benjamin said in her video. \"Please watch the videos with your children, don't just let them watch the videos by themselves.\" YouTube/Superkidz Finger Family\n\nBenjamin's plea to other parents inspired a Change.org petition, asking YouTube to ban the Superkidz Finger Family channel completely. Other parents left supportive comments saying they had reported the inappropriate content to YouTube.\n\nA spokesperson for YouTube told TODAY Parents that the company works hard to ensure content found on YouTube Kids is family-friendly, adding that they take viewer feedback very seriously. The company also has plans for future updates to the YouTube Kids app, which will allow parents to further customize the types of content they want their kids to watch through the parental control area of the app.\n\n\"We appreciate people drawing problematic content to our attention, and make it possible for anyone to flag a video,\" said the spokesperson. \"Flagged videos are manually reviewed 24/7 and any videos that don't belong in the app are removed.\"\n\nWhen asked about the Superkidz Finger Family channel specifically, YouTube was unable to comment.\n\nWhile it appears that YouTube Kids has removed the Superkidz Finger Family channel from the app, the channel is still up on YouTube, with videos clearly aimed at children containing images of Mickey Mouse and graphic scenes of murder and violence.\n\nWhile YouTube guidelines state that users of the site should be thirteen or older, community guidelines forbid \"violent or gory content that's primarily intended to be shocking, sensational, or disrespectful.\"\n\nStill, detailed guidelines on the site make allowances for \"videos that contain dramatized depictions of violence,\" stating that such content may be age-restricted by the site, but not banned.\n\nAfter the graphic content is displayed, the video returns to normal, non-violent cartoon images. YouTube/Superkidz Finger Family\n\nMoms like Brister-Kaster still believe the channel should be removed from YouTube entirely. And, until YouTube finds a more thorough way of filtering their children's content, Brister-Kaster says she will not allow her daughter to use the app.\n\nNever miss a parenting story with TODAY\u2019s newsletters! Sign up here\n\n\"We'll stick to Disney Jr. and the PBS app,\" said Brister-Kaster. \"And, I have gotten a lot of messages from other moms that have deleted the app, too, and say the same thing.\". YouTube hosts thousands of disturbing videos featuring promotional images from popular children's characters, according to recent analysis.\n\nSome of the videos are targeted at children on YouTube. Many of the videos tap into popular keywords.\n\nYouTube says it screens videos to pull inappropriate ones from its children-oriented YouTube Kids app, and encourages people to flag any videos they view as inappropriate on its platforms.\n\nBut writer James Bridle, who highlighted the problem in a recent Medium post, said that the way the company's site is designed, it essentially encourages the production and posting of such videos.\n\nAdvertisement\n\n\n\nParents who let their kids watch YouTube unattended might want to pay closer attention to what they're viewing.\n\nYouTube is serving up to kids thousands of disturbing videos, many from obscure producers, that are sprinkled in with kid-friendly ones from well-known studios, according to analysis in a recent Medium post and past reporting from the BBC. At first glance, the inappropriate videos can be hard to distinguish from the benign ones, often involving well-known characters and themes. But they can often be violent or involve abuse to kids, and many appear to be generated automatically and cheaply in an apparent attempt to profit off the sometimes indiscriminate tastes of younger kids.\n\n\"The architecture [that Google and YouTube] have built to extract the maximum revenue from online video is being hacked by persons unknown to abuse children, perhaps not even deliberately, but at a massive scale,\" wrote writer and artist James Bridle, who took to Medium on Monday to help sound the alarm about the continuing phenomenon.\n\nYouTube's recommended videos are a mix of branded and off-brand content featuring the same cast of characters. YouTube\n\nIn a statement to Business Insider, YouTube said that while it is looking out for younger viewers, it's trying to keep its site as open as possible.\n\nAdvertisement\n\n\"We\u2019re always looking to improve the YouTube experience for all our users and that includes ensuring that our platform remains an open place for free communication while balancing the removal of controversial content,\" the company said in the statement. It continued: \"In the last year, we have updated our advertising policy to clearly indicate that videos depicting family entertainment characters engaged in inappropriate behavior are not eligible for advertising on YouTube.\"\n\nPeppa Pig's disturbing trip to the dentist\n\nOne video Bridle highlighted as \"controversial\" that hasn't been removed is a knockoff video of Peppa Pig, a popular animated character in the UK. The BBC first reported on the disturbing video in March. Posted to YouTube last year, the clip looks and sounds similar to a real Peppa Pig video and even involves a similar situation \u2014 going to the dentist.\n\nRelated stories\n\nBut the knockoff video is punctuated with the disturbing sound of a child wailing in the background. When Peppa gets to the dentist's office, he injects her with a green serum and proceeds to pull out all her teeth. She then has to fight a sinister-looking masked monster. Needless to say, the real Peppa Pig's dental visit isn't nearly as horrifying.\n\nThat's just one of many disturbing pseudo-Peppa Pig videos on the site, Bridle said.\n\nAdvertisement\n\n\"Many are so close to the original, and so unsignposted \u2014 like the dentist example \u2014 that many, many kids are watching them,\" he wrote on Medium. \"I understand that most of them are not trying to mess kids up, not really, even though they are.\"\n\nBut bogus Peppa Pig clips represent a small portion of the disturbing kids videos that can be found on the site, Bridle noted. There are thousands of other videos that may not show disturbing content per se, but are upsetting nonetheless, because they appear to be automatically generated based on popular keywords and tuned specifically to attract kids' clicks, according to Bridle.\n\n\"There are vast, vast numbers of these videos,\" Bridle said. \"Channel after channel after channel of similar content, churned out at the rate of hundreds of new videos every week. Industrialized nightmare production.\"\n\nIt's unclear what motivates users to post the disturbing content to the site. Last year, in an attempt to discourage these types of videos, YouTube began pulling ads from videos that depict kids characters engaged in inappropriate behavior, removing the ability for the creators to make money off of them.\n\nAdvertisement\n\nYouTube Kids may be designed for kids, but it's not a guaranteed refuge from inappropriate videos\n\nYouTube says its main YouTube.com site and app are not intended to be used by kids younger than 13. However the main YouTube service contains plenty of kids videos. Meanwhile, the company does little to inform parents or kids that the service is not intended for younger children or to prevent kids from accessing it.\n\nThe company's notification that the main YouTube service is for people 13 and older is stated within point 12 of its Terms of Service, a document most users likely never see. The company does take steps to block kids younger than 13 from creating Google accounts \u2014 which are needed to subscribe to YouTube channels or upload videos to the service \u2014 and to prevent them from seeing some videos depicting sex or other adult themes. But most videos on the main YouTube service can be accessed without a Google account, and many kids view the site while logged into their parents' accounts.\n\nInstead of having kids use its main site and app, YouTube recommends that parents direct their children to its YouTube Kids app. That app, which regularly garners more than 11 million weekly viewers, is designed to offer kid-friendly content aimed at audiences younger than 13.\n\nBut disturbing videos have infiltrated the YouTube Kids app as well. Parents have reported that the app has shown videos of popular animated characters dying in a car crash, others of popular characters urinating on each other, and one of a real child apparently bleeding after her forehead was shaved, The New York Times reported.\n\nAdvertisement\n\nA YouTube representative told The Times that videos uploaded to its site are automatically scanned to see whether they are appropriate for its Kids app and monitored after they are posted. The company also encourages parents to alert it to videos that are inappropriate for YouTube Kids. YouTube representatives review such videos and can remove them.\n\nFewer than .005 percent of the videos on YouTube Kids were removed in the last 30 days because they were inappropriate, the company representative told The Times. \"We strive to make that fraction even lower,\" the representative said.\n\nYet despite YouTube's efforts, there are still plenty of creepy, kid-targeted videos on its sites. And it doesn't appear to be all that hard for kids to find them.. YouTube Kids has been a problem since 2015 \u2014 why did it take this long to address?\n\nShare All sharing options for: YouTube Kids has been a problem since 2015 \u2014 why did it take this long to address?\n\nIn the last few weeks, the world has learned through a number of reports that YouTube is plagued by problems with children\u2019s content. The company has ramped up moderation in recent weeks to fight the wave of inappropriate content, but this isn\u2019t the first time YouTube has been in this position.\n\nWhen did it start?\n\nOn Feb. 23, 2015, YouTube announced YouTube Kids, a stand-alone app built for children and child-appropriate entertainment. The idea was to make YouTube a safer platform for parents, who didn\u2019t want their children using the main site unsupervised. The initial blog post about the Kids app mentions that \u201cparents can rest a little easier knowing that videos in the YouTube Kids app are narrowed down to content appropriate for kids.\u201d\n\nParental controls, including giving parents the ability to remove the search option from the app, giving their children access to \u201cjust the pre-selected videos available on the home screen\u201d were also included. The Kids App, according to Shimrit Ben-Yair, YouTube Kids Group\u2019s product manager, marked the \u201cfirst step toward reimagining YouTube for families.\u201d\n\nLess than two months later, in May 2015, the Campaign for a Commercial-Free Childhood, a coalition of children\u2019s and consumers advocacy groups, complained to the Federal Trade Commission (FTC) about content they called \u201cnot only ... disturbing for young children to view, but potentially harmful.\u201d\n\nUsing popular characters like Frozen\u2019s Elsa and Spider-Man, YouTubers are able to lure children into offensive videos featuring their favorite characters. While at first these videos seem normal, they soon lead to those same Disney princesses and superheroes participating in lewd or violent acts. YouTube\u2019s search algorithm makes it easy for children to fall into gruesome playlist traps full of this kind of content, as users name their videos and use thumbnails that can get around YouTube\u2019s algorithm, ensuring the content seems safe for kids.\n\nThe report lists a number of issues Golin and other members of advocacy teams discovered in the YouTube Kids app early on. These include:\n\nExplicit sexual language presented amidst cartoon animation;\n\nA profanity-laced parody of the film Casino featuring Bert and Ernie from Sesame Street;\n\nGraphic adult discussions about family violence, pornography and child suicide;\n\nJokes about pedophilia and drug use;\n\nModeling of unsafe behaviors such as playing with lit matches.\n\nA YouTube representative told the San Jose Mercury News after the complaint was filed that, when the company was working on the YouTube Kids app, it \u201cconsulted with numerous partners and child advocacy and privacy groups,\u201d adding that YouTube is \u201calways open to feedback on ways to improve the app.\u201d\n\nBut not much changed. A report from The Guardian in June 2016, pointed out that the third most popular channel on YouTube at the time was \u201cWebs & Tiaras,\u201d which curated content that was targeted to children. The channel, according to reports, starred an assortment of adults in superhero costumes or princess attire performing more mature acts.\n\nThe channel\u2019s content was questionable, but mostly understood to be acceptable. Other bad actors who wanted to piggyback off the success of the channel began posting similar content but with sexual imagery and disturbing content. In 2016, Phil Ranta, a spokesperson for the channel told The Verge it wasn\u2019t surprising this was happening.\n\n\u201cI think it\u2019s natural that when something is as big as this [new genre] is, and they see people making millions of dollars a year, they will try almost everything: go cleaner, adding dialog, go sexier, or crazier,\u201d Ranta said. \u201cThey kind of just need to exhaust those measures before they realize if they can stay in this game.\u201d\n\nOne channel in particular, Webs & Tiaras \u2014 Toy Monster was using the Webs & Tiaras name to spread disturbing content under a trending association. Many of those videos have been deleted, but some still remain on YouTube. The original Webs & Tiaras channel was removed before this article was written.\n\nYouTube is finally addressing the issue on its main site, making changes to the way it moderates content. YouTube CEO Susan Wojcicki said the company was expanding its moderation corps to more than 10,000 contractors in 2018, focusing them on \u201ccontent that might violate our policies.\u201d\n\n\u201cHuman reviewers remain essential to both removing content and training machine learning systems because human judgment is critical to making contextualized decisions on content,\u201d she wrote in a recent blog post.\n\nAlmost three years later, YouTube has responded to a number of concerns raised by parents and media critics about content \u2014 both on the main site and in YouTube\u2019s stand-alone Kids app \u2014 they find disturbing, glorifying violence or obscene. The company issued the following statement to Polygon:\n\nContent that misleads or endangers children is unacceptable to us. We have clear policies against these videos and we enforce them aggressively. We use a combination of machine learning, algorithms and community flagging to determine content in the YouTube Kids app. The YouTube team is made up of parents who care deeply about this, and are committed to making the app better every day.\n\nBut questions regarding content on its independent Kids app \u2014 including whether future content will be curated, or if the app will feature fewer videos to allow for more human-led moderation \u2014 remain mostly unanswered.\n\nA YouTube representative told Polygon that only five-thousandths (0.005) of a percent of content on YouTube Kids is considered disturbing and against the company\u2019s policies, adding that once content is reported, the company takes strict action to remove videos and, in serious cases, entire channels from the app.\n\n\u201cYouTube is marketing this as a safe place for children to explore but it\u2019s not a safe place for children to explore,\u201d Campaign for a Commercial-Free Childhood director Josh Golin told Polygon. \u201cWe were really the first ones to raise this issue, and this is going back two-and-a-half years. What we\u2019ve found in the interim [is] \u2014 it\u2019s like Whac-A-Mole \u2014 so we pointed out videos as we saw them. Every video we named in our complaint came off [the app], but of course there were more.\n\n\u201cIt\u2019s a terrible way to build an app for children,\u201d he said.\n\nIn the May 2015 complaint to the FTC on May 19, 2015, Golin pointed out how YouTube\u2019s search algorithm could be exploited even within a supposedly safe environment for children.\n\nAs users of YouTube Kids search for material, the app begins to recommend similar videos, as the \u201cRecommended\u201d function on YouTube Kids is apparently based on \u201cSearch\u201d history. When we were conducting our review, YouTube Kids actually began recommending videos about wine tasting on its app for preschoolers, as the screen shot below indicates. Thus, the more inappropriate videos children search for, the more inappropriate videos they will be shown via the app\u2019s \u201cRecommended\u201d function.\n\nGolin and his colleagues weren\u2019t the only people who noticed that content targeted toward children on the YouTube Kids app and main site were problematic. In the past couple of years, multiple parent groups have sprung up on Facebook with the intent of learning how to navigate, flag and curate a safe experience for their children.\n\nThe mission statement of one Facebook group, \u201cParents Navigating YouTube,\u201d states the group was created to:\n\nHelp white list YouTube content that is safe for our kids to watch without parental guidance. This will cover adult content but also objectionable content like white supremacy, sexism and things of that nature. We will also discuss problematic YouTube content so that we know what's out there and can be prepared to discuss it with our kids.\n\nWith little support from YouTube, these groups often act as volunteer watchdogs, going through the worst of YouTube and flagging it. A YouTube representative told Polygon that despite its own machine-learning algorithm being improved daily \u2014 learning what content is unacceptable for children \u2014 the team does rely on flags from parents to help address problematic videos and channels.\n\nAt a time when parents were working together on creating a new, improved system to keep an eye on what children see, YouTube was focused on other aspects of the app. The company chose to highlight other areas of the YouTube Kids app in 2016, including the fact that it could be viewed on the Apple TV and was compatible with YouTube Red.\n\nIt wasn\u2019t until this year, that YouTube began to invest heavily in preventative measures.\n\nFirst and foremost, moderation.\n\nChanges being made\n\nAlthough YouTube has maintained that its main site is being exploited by bad actors, not the YouTube Kids App specifically, it\u2019s still impossible for YouTube to guarantee that the Kids App is 100 percent safe. The company told USA Today that parents who want to be sure their children aren\u2019t stumbling upon disturbing content, the options for \u201crecommended videos\u201d should be switched off.\n\n\u201cSometimes your child may find content in the app that you may not want them to watch,\u201d a YouTube representative told USA Today.\n\nMalik Ducard, YouTube\u2019s global head of family and children\u2019s content, told the New York Times these types of videos were \u201cthe extreme needle in the haystack,\u201d and pointed to the algorithm\u2019s machine learning and lack of oversight for reasons these videos may have slipped through. Ducard also said YouTube Kids did not serve a curated experience, meaning parents were responsible for controlling what their children watch.\n\nWhen restriction mode is on, which gives parents the ability to disable the search function and prevent additional videos from the main site flooding the app, it\u2019s difficult for children to switch back to a more open, free roaming setting. YouTube also makes it clear once parents sign up for the app that, despite the company\u2019s best efforts, disturbing content created by bad actors may appear.\n\nGrid View YouTube Kids app welcome screen YouTube\n\nYouTube Kids app welcome screen YouTube\n\nYouTube Kids app welcome screen YouTube\n\nYouTube Kids app welcome screen YouTube\n\nYouTube Kids app welcome screen asking about restricted mode. YouTube\n\nFrom YouTube\u2019s perspective, perfect moderation is impossible. 400 hours of video are uploaded every minute.\n\nBut for Golin, current efforts aren\u2019t enough. Golin told Polygon that it\u2019s irresponsible for YouTube to treat its algorithm as a \u201cbig fishing net,\u201d assuming that the algorithm will catch every bad video meant to exploit it.\n\n\u201cThe entire premise of YouTube Kids\u2019 app is wrong if you\u2019re worried about child safety,\u201d Golin said. \u201cYou can\u2019t have an app that has millions and millions of videos on it, but that\u2019s okay. Children don\u2019t need an app with millions and millions of videos on it. They don\u2019t need 20,000 videos of eggs on one app. What they need is a place where content has been vetted and safe.\n\n\u201cFrom a child standpoint, the problem is not fixable,\u201d Golin said. \u201cThe YouTube model has created something, which is so vast, but there are 400 hours of content are uploaded every minute. It\u2019s simply too big. People have been raising these issues for years, just visit any parenting forum and they\u2019ve been talking about the fake Peppa Pig videos. It was only after the Medium piece went viral that YouTube started to take any proactive steps. To be clear, they took steps because advertisers were concerned, not parents.\u201d\n\nYouTube already went through an \u201cadpocalypse,\u201d in which big advertisers pulled out of the platform after finding their ads attached to videos filled with hateful content. So the company wants to avoid anything that would cause others to leave.\n\nPart of YouTube\u2019s plan is to increase human moderation and tweak its algorithm, \u201ctraining machine-learning technology across other challenging content areas, including child safety and hate speech.\u201d YouTube will also cut down on channels that receive monetization and advertisements attached to these videos. Since YouTube Kids also includes ads \u2014 many of which, Golin says, aren\u2019t child appropriate \u2014 this will affect channels and videos on the platform.\n\nWhat\u2019s next for YouTube Kids?\n\nYouTube Kids is a moneymaker; YouTube wouldn\u2019t tell Polygon how much, exactly, but it does sell ads against the videos. Whatever it is, Golin thinks it\u2019s enough that YouTube has no incentive to change its app. YouTube declined to comment when asked whether the company was going to curate its content and restrict the number of videos going forward.\n\n\u201cTheir goal is to make money and unless there is enough of an outcry and there\u2019s continued pressure on them, we\u2019re going to see the same problems,\u201d Golin said. \u201cI don\u2019t know that the problems are fixable. It would be great if YouTube came to the realization that these problems were fixable and made it clear [that if the company is not curating content] this is for adults who want to watch videos. I don\u2019t have a lot of faith that they will get their on their own.\u201d\n\nA YouTube representative told Polygon that despite reports, the majority of the problem lies on the main site, which it will spend a large portion of its time addressing in the coming year. The representative said that expended changes to its current policy were put in place to discourage inappropriate content targeting families on the main app and site; by doing this, a representative confirmed, it\u2019s supposed to ensure age-gated content (flagged for an audience of 18+) doesn't appear on YouTube Kids.\n\nA YouTube representative also confirmed that content which is flagged on the main YouTube site is not supposed to appear on the Kids app. If a video does make its way to the app, the representative confirmed a secondary screening takes place, adding that a team is in place to moderate new videos that are flagged on the app at all times.\n\nQuestions are still being raised by parent groups and watch dog organizations, like ElsaGate on Reddit and Discord, which keep an eye on nefarious channels or videos that are getting through YouTube\u2019s system about what\u2019s next. A YouTube representative could not provide any more details at the time of writing.\n\nWith critics calling out YouTube for its slow response \u2014 among them News Corp CEO Robert Thompson, who called YouTube a \u201ctoxic waste dump,\u201d \u2014 the question now is how immediately the problem is managed after the new year. Ducard and Wojcicki said the company is \u201cworking on ways to more effectively and proactively prevent this type of situation from occurring.\u201d. After multiple reports of inappropriate content on YouTube over the past year, \u201cGood Morning America\u201d wanted to take a closer look at the site. How often do kids end up seeing inappropriate content on the video platform? We talked with a group of Philadelphia-area tweens and their parents, who say their children often watch YouTube.\n\nHere's what parents said when asked if they've ever tried to take YouTube away from their kids.\n\n\u201cIt\u2019s like the end of the world,\" said Eve Ehrich, a mother of three kids.\n\n\u201cYou\u2019re ending their life for a day,\" another parent, Jaime Meltzer, said.\n\nAlmost all the parents said they use some form of parental controls on their computers and mobile devices to try to limit their children's exposure to inappropriate content.\n\nThe kids we talked to were all ages 10 to 13 and said they know who's \u201ckid-friendly\u201c on YouTube.\n\nSam, 11, said SuperMarioLogan is \u201cone of my favorite channels. It was a suggested video. And I watched it and it kept reeling me in to watch more videos.\u201d\n\nThe other two boys in the group said they know \u201cJeffy,\u201d a puppet on the popular SuperMarioLogan YouTube channel.\n\n\u201cIt attracts kids because you wouldn\u2019t think of him as inappropriate because of the way he looks,\" said 13-year-old David.\n\nFamily watchdog group Common Sense Media called SuperMarioLogan \u201cYour basic online nightmare for parents of young kids.\" The group, who started rating YouTubers this year due to overwhelming requests from parents, noted SuperMarioLogan is intended for ages 17 and older.\n\n\u201cYouTube is the biggest pain point for parents,\u201d Jill Murphy, editor-in-chief of Common Sense Media, told \"Good Morning America.\" \u201cPart of it is parents feeling like they are in the dark and have no idea of what their kids are up to online.\u201d\n\nEven the kids \"GMA\" spoke with agreed that YouTube doesn\u2019t do enough to block inappropriate content and that it\u2019s not a matter of trust.\n\n\u201cI think that sometimes kids get drawn in. It\u2019s not their fault\u201c said 13-year-old Aubrey. \u201cIt looks kid-friendly. But then you watch it, and you don\u2019t really know that it\u2019s not.\u201d\n\n\"GMA\" showed some of the kids\u2019 interviews to Murphy.\n\n\u201cDevelopmentally kids aren\u2019t even primed at that age to have the wherewithal to shut off YouTube, the autoplay. They don\u2019t even have the self-control to manage that,\u201d Murphy said.\n\nThe creator of SuperMarioLogan told \"GMA\" he has lost revenue since YouTube started age-restricting and demonetizing his videos.\n\n\"Common Sense Media only viewed our old content, and their review was accurate solely regarding those old videos,\" he said in a statement. \"We invite Common Sense Media to conduct a review of our newer videos, which are much cleaner in content. It\u2019s important to note when we began creating these videos back in 2008, we were kids ourselves. We were just a few teenagers goofing around. Given we were just kids, we did not understand many things about YouTube or the audience we would subsequently attract. Today is much different. We have adjusted our content to appeal to a wider audience.\"\n\n\"While it's in everyone\u2019s interest to ensure children are not exposed to inappropriate content online, it's ultimately the responsibility of the parents, guardians, and/or supervising adults. These are the only people that have control over what their children have access to.\u201d\n\nYouTube in a statement to \"GMA\" noted that the site offers YouTube Kids, which it dubs as the safe alternative for kids and families.\n\n\u201cOur main YouTube site is for those age 13+... Protecting families is our priority and we created the YouTube Kids app to offer parents a safer alternative for their children,\" according to the statement. \"Beyond that, we\u2019ve ramped up our efforts to age-gate flagged videos on the main app that are better suited for older audiences and increased resources to remove content that doesn\u2019t adhere to our policies. We know there\u2019s more work to be done so we\u2019ve enlisted third-party experts to help us assess this evolving landscape, and we\u2019re launching new tools in YouTube Kids for parents to choose a personalized experience for their child.\u201d\n\nYouTube does state in its terms of service that it is not intended for children under age 13.\n\nHowever, the parents who spoke with \"GMA\" were not aware of that aspect of the terms of service.\n\nAdditionally, YouTube has tools it says can help parents filter out inappropriate content.\n\nHow well do YouTube's age restrictions work?\n\n4:14 \"GMA\" explores YouTube's age restrictions in the wake of headlines about inappropriate content that's popular for children.\n\n\"GMA\" examined two tools that parents can use to filter content for kids on YouTube\u2019s main site.\n\nWe created an account for a 14-year-old so age-restricted content would be screened out, and we turned on \"restricted mode,\" which is supposed to filter out potentially mature content.\n\nEven with these restrictions, we found sexual content interlaced with videos of people playing customized versions of popular children\u2019s games Minecraft and Roblox.\n\n\"The algorithm is zeroing in on you and saying, 'You want to see more and more and more of these things,' and just putting this in front of kids, again, who are in restricted mode,\" said the Campaign For a Commercial Free Childhood's Josh Golin, who observed the YouTube content with \"GMA.\"\n\nMost of the videos were taken down or placed behind filters after \"GMA\" told YouTube about the findings. Several of the Minecraft and Roblox videos involving sexual activity were left up without age restrictions.\n\n\"Roblox is committed to providing a safe community, and we have zero tolerance for content that violates our Rules of Conduct,\" Roblox said in a statement to \"GMA.\" \"The videos highlighted date as far back as 2011 and show features that are not possible on today's platform. We use a combination of technology and a robust team of moderators to identify and remove any questionable content or behavior that violates our rules of conduct. In addition we are also proactive in identifying and requesting that sites across the web, such as YouTube, remove any content that does not depict the true nature or functionality of the Roblox platform.\u201d\n\nYouTube told \"GMA\" in a statement that \"protecting families is our priority.\"\n\n\"Protecting families is our priority and we created the YouTube Kids app to offer parents a safer alternative for their children,\" the statement read. \"Beyond that, we\u2019ve ramped up our efforts to age-gate flagged videos on the main app that are better suited for older audiences and increased resources to remove content that doesn\u2019t adhere to our policies.\n\n\"We know there\u2019s more work to be done so we\u2019ve enlisted third party experts to help us assess this evolving landscape and we\u2019re launching new tools in YouTube Kids for parents to choose a personalized experience for their child,\" the company said.\n\nYouTube also told \"GMA\" it has since created more parental controls on YouTube Kids so only videos screened by human moderators can be used.\n\nParents have to turn the controls on themselves, and we found they do appear to work. By turning search off, parents can limit kids to videos that have been verified by the YouTube Kids Team.\n\nParents can also choose collections of channels recommended by YouTube Kids and their partners. A feature in which parents can handpick videos is supposed to become available later this year.\n\nChild advocates say there are also steps parents can take on their own, from spot checking their child's browser history to co-viewing YouTube with their child and talking to their child about what they're viewing online.. Recent news stories and blog posts highlighted the underbelly of YouTube Kids, Google's children-friendly version of the wide world of YouTube. While all content on YouTube Kids is meant to be suitable for children under the age of 13, some inappropriate videos using animations, cartoons, and child-focused keywords manage to get past YouTube's algorithms and in front of kids' eyes. Now, YouTube will implement a new policy in an attempt to make the whole of YouTube safer: it will age-restrict inappropriate videos masquerading as children's content in the main YouTube app.\n\nThe reasoning behind this decision has to do with the relationship between the main YouTube app and YouTube Kids (which has its own dedicated app). Before any video appears in the YouTube Kids app, it's filtered by algorithms that are supposed to identify appropriate children's content and content that could be inappropriate or in violation of any YouTube policies. YouTube also has a team of human moderators that review any videos flagged in the main YouTube app by volunteer Contributors (users who flag inappropriate content) or by systems that identify recognizable children's characters in the questionable video.\n\nIf the human moderator finds that the video isn't suitable for the YouTube Kids app, it will be age-restricted in the main YouTube app. No age-restricted content is allowed in the YouTube Kids app at all. As for those using the main YouTube app, age-restricted content cannot be viewed by anyone not logged into a YouTube account, anyone under the age of 18, or anyone with Restricted Mode turned on. According to a report from The Verge, YouTube claims this policy has been in the works for some time now and is not in response to the recent online concern.\n\nAdvertisement\n\nAlso, all age-restricted content is not eligible for advertising, which will undoubtedly hit the wallets of the creators making these videos. While it's hard to understand why anyone would make a video about Peppa Pig drinking bleach or a bunch of superheroes and villains participating in a cartoonish yet violent \"nursery rhyme,\" it's been a decent way to make money on YouTube. Some of these videos have amassed hundreds of thousands (and sometimes millions) of views, gleaning ad dollars and channel popularity.\n\nThe unnerving reality is that it's possible that many of those views came from YouTube's \"up next\" and \"recommended\" video section that appears while watching any video. YouTube's algorithms attempt to find videos that you may want to watch based on the video you chose to watch first. If you don't pick another video to watch after the current video ends, the \"up next\" video will automatically play. Since some of these inappropriate videos showed up on YouTube Kids (and on the main YouTube app as well), it's possible that any one of them was an \"up next\" video that automatically played after hours of kids watching other appropriate yet categorically similar content.\n\nThis new age-restriction policy should prevent that from happening by stopping inappropriate content from ever making it to YouTube Kids. It takes a few days for content to transition from the main YouTube app to YouTube Kids, and the company is hoping the work of human moderators, Contributors, and the new policy will prevent any more of this content from getting into its safe place for children.\n\nEven though the new policy is geared toward making YouTube Kids a safer place, it does have implications for audiences of the main YouTube site as well. But these videos aren't going away, and some would argue that many of them are satires or parodies, both of which are permissible under YouTube guidelines. Families that use the regular YouTube app instead of YouTube Kids will want to check their account restrictions if they don't want these videos popping up unexpectedly.. YouTube Kids, which has been criticized for inadvertently recommending disturbing videos to children, said Wednesday that it would introduce several ways for parents to limit what can be watched on the popular app.\n\nBeginning this week, parents will be able to select \u201ctrusted channels\u201d and topics that their children can access on the app, like \u201cSesame Workshop\u201d or \u201clearning,\u201d that have been curated by people at YouTube Kids and its partners. The Google-owned app said in a blog post on Wednesday that parents would also have the option to restrict video recommendations to channels that have been \u201cverified\u201d by YouTube Kids, avoiding the broader sea of content that the app pulls from the main YouTube site through algorithms and other automated processes.\n\nYouTube Kids was introduced in 2015 for children of preschool age and older, and it says it has more than 11 million weekly viewers. But parents have discovered a range of inappropriate videos on the app, highlighting the platform\u2019s dependence on automation and a lack of human oversight. The New York Times reported in the fall that children using the app had been shown videos with popular characters from Nick Jr. and Disney Junior in violent or lewd situations, and other disturbing imagery, sometimes set to nursery rhymes.. The YouTube Kids app is meant to filter out unsuitable content, but we found that it suggested that children watch conspiracy theory videos.\n\nThe app suggested several videos from prominent conspiracy theorist David Icke in which he claimed that the world is ruled by reptile-human hybrids.\n\nSearches for \"moon landing\" returned results including three videos which claim the moon landing was a hoax.\n\nYouTube removed at least 25 videos from its Kids app after we contacted the company and blocked Icke's channel on the app.\n\nAdvertisement\n\n\n\nYouTube's app specifically for children is meant to filter out adult content and provide a \"world of learning and fun,\" but Business Insider found that YouTube Kids featured many conspiracy theory videos which make claims that the world is flat, that the moon landing was faked, and that the planet is ruled by reptile-human hybrids.\n\nYouTube Kids is a separate app from the main YouTube app, and it's meant to allow parents to let their children browse YouTube without being worried about any unsuitable content appearing. Children are encouraged to learn languages, read books, and watch educational videos.\n\nSearch for \"UFO\" on YouTube Kids and you'll mostly find videos of toys that are clearly fine for children to watch. But one of the top videos claimed to show a UFO shooting at a chemtrail, and we found several videos by prominent conspiracy theorist David Icke in the suggested videos. YouTube removed the videos from YouTube Kids after we contacted it about the issue.\n\nOne suggested video was an hours-long lecture by Icke in which he claims that aliens built the pyramids, that the planet is run by reptile-human hybrids, that Freemasons engage in human sacrifice, that the assassination of President Kennedy was planned by the US government, and that humans would evolve in 2012.\n\nAdvertisement\n\nTwo other conspiracy theory videos by Icke appeared in the related videos, meaning it was easy for children to quickly go from watching relatively innocent videos about toys to conspiracy content.\n\nOne of the videos which was suggested on YouTube Kids. YouTube/UFOTV\n\nYouTube said in a statement to Business Insider that \"sometimes we miss the mark\" on content appearing on YouTube Kids and said it would \"continue to work to improve the YouTube Kids app experience.\"\n\nHere's the full statement from YouTube:\n\nRelated stories\n\n\"The YouTube Kids app is home to a wide variety of content that includes enriching and entertaining videos for families. This content is screened using human trained systems. That being said, no system is perfect and sometimes we miss the mark. When we do, we take immediate action to block the videos or, as necessary, channels from appearing in the app. We will continue to work to improve the YouTube Kids app experience.\"\n\nAdvertisement\n\nYouTube Kids is meant to block unsuitable content\n\nThe YouTube Kids app blocks searches for most unsuitable videos. Search \"9/11\" or \"porn\" and you find no results. But we found that buried in the app's suggested videos were conspiracy videos that children could stumble on.\n\nYouTube Kids\n\nConspiracy theory videos appear in search results\n\nIf you searched for \"moon landing\" on YouTube Kids, three videos appeared that claim that the moon landing was hoaxed. All three videos have since been hidden by YouTube after we informed it of the issue.\n\nYouTube Kids\n\nFollowing related videos that appear in YouTube Kids, we ended up watching a video that claims that a gateway to a new world had opened, and that a female employee working on the Large Hadron Collider mysteriously vanished in a magic portal.\n\nThrough YouTube Kids' suggested videos feature, we also found videos from conspiracy theorists Ben Davidson, Gerald Pollack, and Wallace Thornhill. YouTube removed the specific videos that we sent it, but many other videos by the conspiracy theorists remain in the app.\n\nAdvertisement\n\nYouTube Kids\n\nConspiracy videos also appear when children search for popular conspiracy theories. Searches for \"chemtrails,\" \"flat earth,\" and \"nibiru\" are all allowed in the app. However, it's (hopefully) unlikely that children are regularly watching these videos unless they appear as suggestions on more popular content in the app.\n\nThe conspiracy videos didn't just appear in searches or suggested videos, either. After watching several conspiracy videos, the top recommended video on the home page of YouTube Kids was a conspiracy theory about aliens on the moon:\n\nYouTube Kids\n\nThis issue with the YouTube Kids app shows the problem with YouTube's suggested videos algorithm. The suggested videos try to convince you to watch related content after your current video ends.\n\nThat's fine when it's adults watching the main YouTube site, but children on YouTube Kids can easily go from innocent content about the moon landing to Icke claiming lizard people rule the world.\n\nAdvertisement\n\nYouTube Kids criticised for featuring inappropriate videos\n\nThis isn't the first time that YouTube Kids was found to feature videos that weren't suitable for children. In 2017, the app was criticised in a lengthy Medium post by author James Bridle after he found disturbing videos targeted at children.\n\n\"Someone or something or some combination of people and things is using YouTube to systematically frighten, traumatise, and abuse children, automatically and at scale,\" Bridle wrote.\n\nIn November, YouTube published a blog post in which it promised to remove \"unacceptable\" videos from YouTube Kids.\n\nYouTube is fighting against fake news and conspiracy theories\n\nYouTube is preparing to launch a crackdown on conspiracy theories by adding text from Wikipedia on the pages of conspiracy videos on the main YouTube site.\n\nAdvertisement\n\nConspiracy theorists are allowed to publish videos on YouTube, but the company doesn't want people to be mislead by what they publish. So it's going to add some text from Wikipedia explaining that the world is in fact round.\n\nIt's part of an ongoing campaign by YouTube to stop misleading videos. Recently, a video accusing Parkland school shooting survivor David Hogg of being an actor was featured prominently on YouTube. YouTube featured a false video about Hogg at the top of its trending chart, but later removed it.. Videos filled with profanity, sexually explicit material, alcohol, smoking, and drug references - this is what parents are finding on Google\u2019s YouTube Kids app. That\u2019s right - its kids app. Now, parents across the country are calling on Google to remove the app until it can guarantee the total elimination of this inappropriate content.\n\nWhen my neighbors told me about the horrible adult content popping up on the Youtube Kids app, I thought there must be a mistake. Why would Google market an app as \u201ca family-friendly place to explore\u201d and not have proper safeguards in place? Unfortunately, it turned out to be true. And I\u2019ve since learned of the numerous complaints filed to the Federal Trade Commission about this very problem.\n\nEven worse, Google\u2019s response has been laughable. They tell parents to simply flag inappropriate material or set new filters. As a father of two, it makes me angry when a large company like Google doesn\u2019t take responsibility for its kids\u2019 products. Parents are being sold on an app built for kids 5 and under that is supposed to keep them safe from adult content. Parents like myself are joining forces to hold Google accountable.\n\nTell Google to remove the YouTube Kids app until it can live up to its marketing.\n\nThe solution is simple: only allow content pre-approved for ages 5 and under to appear on the app, and don\u2019t allow ads clearly meant for adults. Unless it can live up to expectations, the app should be removed.\n\nParents are not the only ones outraged. The media has blasted Google\u2019s app, calling it \u201cthe most anti-family idea ever to come out of Silicon Valley,\" and reporting that it \u201cignores basic protections for children.\u201d\n\nWith your support, we can get Google to remove YouTube Kids until the proper protections are in place.\n\nThese are examples of videos encountered on YouTube Kids:\n\nA graphic lecture discussing hardcore pornography by Cindy Gallop:\n\nhttps://www.youtube.com/watch?v=EgtcEq7jpAk\n\nHow to make chlorine gas with household products (chemical weapon used in Syria):\n\nhttps://www.youtube.com/watch?v=DF2CXHvh8uI\n\nHow to tie a noose:\n\nhttps://www.youtube.com/watch?v=TpAA2itjI34\n\nHow to throw knives:\n\nhttps://www.youtube.com/watch?v=NGgzn1haQ-E\n\nA guy tasting battery acid:\n\nhttps://www.youtube.com/watch?v=gif-OWNjJSw\n\nHow to use a chainsaw:\n\nhttps://www.youtube.com/watch?v=Kk28thdgCEU\n\nA \u201cSesame Street\u201d episode dubbed with long strings of expletives:\n\nhttps://www.youtube.com/watch?v=kVkqzE-iiEY\n\nReferences to pedophilia in a homemade video reviewing a \u201cMy Little Pony\u201d episode:\n\nhttps://www.youtube.com/watch?v=7K9uH4d-HnU\n\nA DIY video on conducting illegal piracy, featuring pictures of marijuana leaves:\n\nhttps://www.youtube.com/watch?v=dZDF5uqORA0. More sophisticated parental controls, including the ability to sign in and block videos, are available to users in some countries. All of these feature require support for managed children's Google and YouTube profiles, which haven't yet been launched in the EMEA (Europe and the Middle East) region.\n\nHowever, YouTube has confirmed to WIRED that enhanced features for YouTube Kids \u2013 blocking, specifically \u2013 will come to the UK later this year.\n\nSafety dance\n\nYouTube proper is disinclined to implement the most obvious method of putting control into its users' hands: a button to block videos or channels they don't like.\n\n\"There are already ways for users to signal their dislike of videos, and there are also simple ways for people to report content they think breaks the rules,\" Thea O'Hear says. \"And that\u2019s how we think about it in enforcement terms, we ask 'does it violate our policies?' and if it violates our policies, then the video \u2013 or potentially the channel \u2013 is going to come down.\"\n\nThe full version of YouTube, both on the web and via mobile apps, has a few more settings you can take advantage of. To start with, it's worth setting up a dedicated YouTube account that you only use for watching videos with your child.\n\nTo help protect against seriously dubious videos, you can enable restricted mode, bearing in mind that this will apply to only the browser or device you're using when you set it. Like YouTube Kids, this filters out content flagged as inappropriate for children under 13.\n\nIf you run across anything that you'd rather not see in YouTube's suggested content bar on the right, you can mouseover thumbnails, select the \u2026 icon at the top right of the preview window and select 'not interested'. This doesn't guarantee that you'll never see a video again, but it makes it somewhat less likely to appear.\n\nVideos you've seen before are often recommended for repeat viewing and are used to inform other recommendations, so if you or your child watched something undesirable in the past, you can either selectively or entirely clear your YouTube history to prevent troubling content from influencing your future viewing.\n\nIf you want to actually block videos, you'll have to turn to third-party tools such as Video Blocker for Chrome and Firefox, a browser extension that can block channels by full name or partial wildcard, and videos by keywords.\n\nIf you and your offspring prefer to watch videos on a tablet, a number of third-party apps provide the kind of granular blocking features that YouTube doesn't. Although it has a slightly stark interface, YouTuze Pro Kids on Android makes it easy to whitelist specific channels, videos and playlists to keep a young child from seeing content that you've not personally approved. Similar apps, such as iTubeList, are available for iOS users.\n\nYouTuze developer Michael Simonds says he originally made the app so his kids could watch a particular Minecraft channel unsupervised, without any chance of them wandering on to less suitable videos. \"There are high-quality channels out there that are suitable for children, but are not aimed at them at all. So I wanted to be able to whitelist any channel. It also puts the responsibility back to the parent, if they trust the channel and think it is suitable they can add it. [With] the amount of videos added per day to YouTube, there is no way to block everything - so whitelisting is the safest option.\". Earlier this week, a report in The New York Times and a blog post on Medium drew a lot of attention to a world of strange and sometimes disturbing videos on YouTube aimed at young children. The genre, which we reported on in February of this year, makes use of popular characters from family-friendly entertainment, but it\u2019s often created with little care, and can quickly stray from innocent themes to scenes of violence or sexuality.\n\nIn August of this year, YouTube announced that it would no longer allow creators to monetize videos which \u201cmade inappropriate use of family friendly characters.\u201d Today it\u2019s taking another step to try and police this genre.\n\n\u201cWe\u2019re in the process of implementing a new policy that age restricts this content in the YouTube main app when flagged,\u201d said Juniper Downs, YouTube\u2019s director of policy. \u201cAge-restricted content is automatically not allowed in YouTube Kids.\u201d YouTube says that it\u2019s been formulating this new policy for a while, and that it\u2019s not rolling it out in direct response to the recent coverage.\n\nThe first line of defense for YouTube Kids are algorithmic filters. After that, there is a team of humans that review videos which have been flagged. If a video with recognizable children\u2019s characters gets flagged in YouTube\u2019s main app, which is much larger than the Kids app, it will be sent to the policy review team. YouTube says it has thousands of people working around the clock in different time zones to review flagged content. If the review finds the video is in violation of the new policy, it will be age restrictied, automatically blocking it from showing up in the Kids app.\n\nYouTube says it typically takes at least a few days for content to make its way from YouTube proper to YouTube Kids, and the hope is that within that window, users will flag anything potentially disturbing to children. YouTube also has a team of volunteer moderators, which it calls Contributors, looking for inappropriate content. YouTube says it will start training its review team on the new policy and it should be live within a few weeks.\n\nAlong with filtering content out of the Kids app, the new policy will also tweak who can see these videos on YouTube\u2019s main service. Flagged content will be age restricted, and users won\u2019t be able to see those videos if they\u2019re not logged in on accounts registered to users 18 years or older. All age-gated content is also automatically exempt from advertising. That means this new policy could put a squeeze on the booming business of crafting strange kid\u2019s content.\n\nYouTube is trying to walk a fine line between owning up to this problem and arguing that the issue is relatively minor. It says that the fraction of videos on YouTube Kids that were missed by its algorithmic filters and then flagged by users during the last 30 days amounted to just 0.005 percent of videos on the service. The company also says the reports that inappropriate videos racked up millions of views on YouTube Kids without being vetted are false, because those views came from activity on YouTube proper, which makes clear in its terms of service that it\u2019s aimed at user 13 years and older.. YouTube is both a massive industry and browsing staple that people use to fill their educational and entertainment needs. According to Business Insider, the website accumulates around 1.8 billion logged-in users per month. With its high traffic rates, it makes sense that the company wants to pay its creators in order to encourage them to upload more videos. Lately, though, you may have seen YouTubers complaining about the company\u2019s advertisement revenue algorithm, as YouTube is trying to figure out the best way to pay their creators.\n\nSimply put, the more views a video gets, the more money it makes. While some YouTubers operate with integrity and continue to create the content they wish to, others have decided to alter their content in hopes of appealing to a large, easy-to-please audience: children.\n\nIn 2015, YouTube launched YouTube Kids, which was specifically created for child viewers. This branch of YouTube consists of user-face and family-friendly videos that are readily available for young kids. Personally, I believe it was created with good intentions, and to ensure parents that their children are watching age-appropriate content.\n\nSomewhere along the way, however, something went horribly, horribly wrong.\n\nOn its current website, YouTube Kids describes itself as \u201ca safer online experience for kids,\u201d acknowledging that some inappropriate videos may find their way into the service.\n\n\u201cWe use a mix of filters, user feedback and human reviewers to keep the videos in YouTube Kids family friendly,\u201d the website explains. \u201cBut no system is perfect and inappropriate videos can slip through, so we\u2019re constantly working to improve our safeguards and offer more features to help parents create the right experience for their families.\u201d\n\nThe question is, what terrible thing happened on YouTube Kids that prompted them to put this disclaimer on their homepage?\n\nI was born in 1997 and have had access ti a computer for as long as I can remember. As a kid, I played Freddie Fish and Putt-Putt computer games, eventually graduating to Neopets and Webkinz when we upgraded from dial-up internet. To my knowledge, social media didn\u2019t even exist until Myspace picked up around 2008.\n\nTechnology evolved with me. I didn\u2019t have a lot of restrictions placed on my internet consumption, but I really don\u2019t think I needed them. Everything was so new back then, people hadn\u2019t figured out how to exploit them in a truly damaging way. The worst that could happen was accidentally stumbling across shock sites like Meatspin or Lemonparty.\n\nBack then, YouTube was a harmless place for people to waste time. In 2008, FilmCow had posted \u201cCharlie the Unicorn\u201d on its YouTube channel, with \u201cLlamas with Hats\u201d to follow a year later. Both videos became cult classics among middle schoolers of the time, and, admittedly, the videos are a little crude. Charlie gets his kidney stolen by the other unicorns, and Carl the llama has a problem with stabbing people.\n\nBut these videos are no worse than what people saw on Adult Swim, and they don\u2019t come even remotely close to the damage done by the Logan Paul scandal earlier this year.\n\nOn YouTube today, children are being exploited for money. YouTubers with channels specifically marketed toward children are cranking out videos to provide kids with loads of content to consume, as each video around 16 minutes long. (Which is the sweet spot for maximum ad revenue.) Frankly, YouTubers are practically begging their viewers to \u201csmash\u201d that like button and comment on their videos.\n\nI spent a weekend babysitting my brother\u2019s children and they spent most of that time watching channels like Chad Wild Clay. He would ask a question like, \u201cWho is going to win this game?\u201d ask kids to comment their predictions in the comments and then proceed to play the game, giving the kids the answer in the same video. He\u2019d do that same thing several times throughout the video.\n\nWhat\u2019s the point of the interactive bits if they can just skip ahead and get their answers without commenting at all? It\u2019s simple: the more engagement the video gets, the more likely it is to be picked up by YouTube\u2019s recommendation algorithm, thus bringing in more traffic and more money.\n\nI\u2019m not trying to be a curmudgeon about all this. It\u2019s not like children\u2019s television was any less brainless or exploitative when I was growing up. \u201cEd, Edd n Eddy\u201d drove my mom nuts from how stupid it was, and shows like \u201cBlue\u2019s Clues\u201d very often asked questions before they were immediately answered. And besides gems like \u201cMr. Roger\u2019s Neighborhood,\u201d I\u2019d also bet that cartoon companies really cared more about making money than they ever did about me. It\u2019s all the same principles, just in a more modern format.\n\nWhat\u2019s the most concerning is that now, through YouTube, these videos are not passed through a strict censorship board like the FCC before they hit the air. With this severe lack of regulation, especially despite YouTube\u2019s best efforts, it\u2019s little kids that we really need to worry about protecting.\n\nThere have been reports of disturbing content coming from the YouTube Kids app, from gore-filled animated videos of beloved cartoon characters like Elsa or Mickey Mouse, to live-action videos of children in borderline-abusive situations.\n\nEven if children start off watching harmless content on the app, these disturbing videos can still end up on their screens. The app uses an autoplay algorithm based on keywords, so a wholesome video can easily pave the way for a more sinister video to be viewed later on.\n\nWhy do these videos exist? Is it all to make money, or is there something more sinister going on?\n\nEthan and Hila Klein of h3h3Productions have been making videos about these YouTubers for a long time, trying to make sense of it all. Last year, Post Malone, Ethan and Hila sat down on the H3 Podcast and talked about this travesty.\n\nEthan explained to Post Malone \u201cElsagate,\u201d the phenomenon of YouTubers making content that appears to be suitable for children but is actually filled with graphic and inappropriate content. They scroll through the comments of these videos, where (presumably adult) users are adding pedophilic comments about the children depicted in the videos.\n\nThe two then go even deeper down the rabbit hole, finding a conspiracy where seemingly gibberish comments on these videos are actually codes. These deciphered comments are equally, if not more, worrisome than the pedophilic comments. The Elsagate subreddit deciphered some of these codes, finding messages like \u201cC u soon parkinglot bring her.\u201d There\u2019s no solid proof legitimizing what these comments really mean, but, nonetheless, they sound way too close to child trafficking for my comfort.\n\nI\u2019m not making any definitive claims on these conspiracies. What I can claim, however, is that children are being exploited, either for monetary gain or something more sinister. Next time you see a child watching YouTube on their iPad, maybe take a look over their shoulder and make sure the videos they\u2019re watching are truly appropriate for them.. \"It's perfectly legitimate for a parent to believe that something called Peppa Pig is going to be Peppa Pig,\" she says. \"And I think many of them have come to trust YouTube... as a way of entertaining your child for ten minutes while the parent makes a phone call. I think if it wants to be a trusted brand then parents should know that protection is in place.\"\n        ================== End of Article Content ===================\n        STEP 2: State your reason for your classifications for the following:\n        =================Classification Fields====================\n        - Country (output \"Worldwide\" if the incident happened across multiple countries):\n        - State (if not applicable leave blank):\n        - City (if not applicable leave blank):\n        - Continent (output \"Worldwide\" if the incident happened across multiple countries):\n        - Company (i.e. the company that developed the technology involved in this incident):\n        - Company city (the city where the headquarters of this company is located. If the company recently moved headquarters, please use the location of the new headquarter):\n        - Company state (the state of the company city, if applicable, if not leave blank):\n        - Affected population (let's think about which groups of people are directly affected by the incident in the article.): \n        - Number of people actually affected (let's check the number of people directly affected according to the article. Give a total number. If unknown output 'Unknown'):\n        - Number of people potentially affected (let's check the article text to see if this information is provided or suggested, if not you may ouput 'Unknown'):\n        - Classes of irresponsible AI use (please follow the rules and refer to this taxonomy: \n        ```taxonomy classes\n                [\"Discrimination\",\n                    \"Human Incompetence\",\n                    \"Pseudoscience\",\n                    \"Environmental Impact\",\n                    \"Disinformation\",\n                    \"Copyright Violation\",\n                    \"Mental Health\",\n                    \"Other\"]\n                     \n        ```   \n        Rule1: There could be more than one classes the article classifies as. \n        Rule2: DO NOT create your own class, adhere strictly to the provided list.\n        - Subclasses (please follow the rules and refer to this taxonomy structure `<class>:[<subclass>]`):\n          ```taxonomy subclasses       \n                 \n                {\n                    \"Discrimination\": [\n                        \"Data bias\",\n                        \"Algorithmic bias\"\n                    ],\n                    \"Human Incompetence\": [\n                        \"Administrative\",\n                        \"Technical\"\n                    ],\n                    \"Pseudoscience\": [\n                        \"Facial\",\n                        \"Other\"\n                    ],\n                    \"Environmental Impact\":[],\n                    \"Disinformation\": [\n                        \"Textual\",\n                        \"Image\",\n                        \"Video\",\n                        \"Audio\",\n                    ],\n                    \"Copyright Violation\": [],\n                    \"Mental Health\": [],\n                    \"Other\": []\n                    }\n            \n          ```\n        Rule1 : The subclasses should be the children of the classes. Let's think about which sub-categories of the class/classes this article belong in. \n        Rule2: DO NOT ADD subclass fields that are NOT in the provided taxonomy list\n        Rule3: If there is no subclass for a particular class in the taxonomy, leave it.\n        - Sub-subclass (please follow the rules and refer to this taxonomy structure `<subclass>:[<sub-subclass>]`): \n        ```taxonomy structure\n               \n                    {\n                        \"Data bias\": [\n                        \"Gender\",\n                        \"Race\",\n                        \"Sexual Orientation\",\n                        \"Economic\",\n                        \"Other\"\n                        ],\n                        \"Algorithmic bias\": [\n                        \"Interaction\",\n                        \"Feedback loop\",\n                        \"Optimization function\",\n                        \"Other\"\n                        ],\n                        \"Administrative\":[],\n                        \"Technical\":[],\n                        \"Facial\": [],\n                        \"Textual\": [],\n                        \"Image\": [],\n                        \"Video\": [],\n                        \"Audio\": []\n                    },\n\n                \n        ```\n        Rule1: Only find the sub-subclass relation in the provided taxonomy. The sub-subclass are children of subclass.\n        Rule2: DO NOT ADD OR CREATE sub-subclass fields that are not in the provided taxonomy list. \n        Rule3: If a subclass in the taxonomy does not have a sub-subclass, leave it.\n        Rule4: If none of the subclasses have sub-subclasses, just leave the field empty e.g. sub-subclass:[]\n        - Area of AI Application (e.g. content filtering, surveillance, illness prediction):\n        - Online (yes or no):\n        =================Classification Fields====================\n        STEP 3: Make sure your classification output follows such format:\n        ```THIS IS AN EXAMPLE```\n        \n     {\n        \"Country\": \"Worldwide\", \n        \"State\": \"\", \n        \"City\": \"\", \n        \"Continent\": \"Worldwide\",\n        \"Company\": \"Microsoft\", \n        \"Company city\": \"Redmond\",\n        \"Company state\": \"Washington\",\n        \"Affected population\": [\"Twitter Users\", \"Online Community\"],\n        \"Number of people actually affected\": \"Unknown\",\n        \"Number of people potentially affected\": \"Millions (Twitter user base and wider online community)\",\n        \"Class of irresponsible AI use\": [\n            \"Human Incompetence\",\n            \"Disinformation\",\n            \"Discrimination\",\n            \"Mental Health\"\n        ],\n        \"Subclasses\": {\n            \"Human Incompetence\":[\"Technical\"],\n            \"Disinformation\":[\"Textual\"],\n            \"Discrimination\":[\"Data bias\",\"Algorithmic bias\"],\n        },\n        \"Sub-subclass\": {\n        \"Data bias\":[\"gender\",\"race\",\"other\"],\n        \"Algorithmic bias\":[\"feedback loop\", \"optimization function\", \"other\"]\n        },\n        \"Area of AI Application\": [\"Chatbot\"],\n        \"Online\": \"Yes\"\n      }\n\n        ```END OF EXAMPLE```\n        \n        DO NOT make up your own field. DO NOT use fields that are not in the list. DO NOT USE example 1 and example 2 output on different articles.\n\n        return your classification output in JSON.\n        "
}